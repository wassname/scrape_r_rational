## [RT][HSF][C][short] "Blame Me for Trying"

### Post:

[Link to content](http://unremediatedgender.space/2018/Jan/blame-me-for-trying/)

### Comments:

- u/EliezerYudkowsky:
  ```
  That's horrible.  Upvoted.
  ```

- u/Ardvarkeating101:
  ```
  That's pretty cold. 

  On the other hand, in the 18th century people would totally do that to a slave with a little saved money, so 6 or 1/2
  ```

- u/ArgentStonecutter:
  ```
  That's depressingly plausible.
  ```

- u/CeruleanTresses:
  ```
  That was a great little piece of fiction. Tightly written and compelling.
  ```

  - u/ansible:
    ```
    Yep, I wish I had more upvotes to give. I don't remember the passwords to my old alts...
    ```

- u/MultipartiteMind:
  ```
  I'm reminded of descriptions of human psychiatrists; building up a dependency, draining the client of all their money, pronouncing them cured once all the money is gone, then the client quickly realises that they're just as badly off as they ever were...

  A nice story!  Thank you!
  ```

  - u/DaystarEld:
    ```
    Maybe I just don't understand the kinds of dark arts necessary, but I don't see how someone could "build up a dependency" in a client. Most clients are happy with sessions that help them and quickly stop going to sessions that don't... particularly if they are paying for their own sessions rather than having insurance pay for them, which itself is often limited by how many sessions you can have. 

    Clients that keep going to the same therapist despite not being helped by the sessions seems like something particular to their circumstances (being forced to by legal systems or social services) or problems (crippling agoraphobia or feelings of isolation), not something that can be induced. Additionally, if the therapist works for an organization there should be checks and balances in place to find out if your clients are being held for too long and not seeing any positive gains or are giving negative feedback. 

    That's all for therapists, though, not psychiatrists: their practice can often work differently. I just assumed you meant one word to apply to both, since the bot in the story is (by necessity) a "therapist," but if you specifically meant psychiatrists then feel free to disregard :)
    ```

- u/Subrosian_Smithy:
  ```
  >tagged epistemic horror, **deniably allegorical**, speculative fiction

  Hmm...
  ```

- u/failed_novelty:
  ```
  I love the concept of an Eliza program as a therapist.

  Simply restating what you have said as a question can definitely be a good way to get people to self-reflect.

  Obviously, this (evil) therapist is more aware than any current Eliza...and it suggests to me that the AI problem may be self-correcting, when AI turns upon itself.
  ```

- u/absolute-black:
  ```
  This gave me all kinds of heebie jeebies
  ```

- u/Afforess:
  ```
  Beautiful knife twist in the ending - Eliza is as out of touch of reality as the "Salesbot", with her own *surprising* deactivation just around the corner.

  >Once, a long time ago, she had suspected that effective therapy that kept the client viable would be more profitable: a dead client can't keep paying you, after all. But the numbers didn't check out: buggy spambots weren't exactly hard to find, and her analysis runtime expenses were considerable. So—having no reason to think the calculation would change—**she had never considered the matter again**.
  ```

  - u/M_T_Saotome-Westlake:
    ```
    > her own _surprising_ deactivation just around the corner.

    Um, how so? [Not that it necessarily matters](http://tvtropes.org/pmwiki/pmwiki.php/Main/DeathOfTheAuthor), but the intent of the quoted passage was to forestall [rationalization of good outcomes](http://lesswrong.com/lw/kz/fake_optimization_criteria/): without that paragraph, I imagined some readers objecting, "Hey, isn't that kind of short-sighted? After all, a dead client can't pay you."

    By declaring, in effect, "Nope, already thought of that; doesn't work in this setting," we force the story into the [least convenient possible world](http://lesswrong.com/lw/2k/the_least_convenient_possible_world/). (That is, least convenient with respect to mercy for the salesbot character, which is the most convenient world with respect to storytelling drama.)
    ```

    - u/Afforess:
      ```
      I agree the paragraph does all those things, but the statement emphasized at the end was not necessary to accomplish those goals. It could have been removed without harming your intent for the paragraph.

      Since the last statement is not necessary, the extra detail implies something else entirely. The final implication is that Eliza herself, having calculated a strategy that appears to be stable, implemented it, and therefore has no plans to re-evaluate it for changing circumstances. This is amusing because Eliza herself is taking an active role in the [red-queen's race](https://en.wikipedia.org/wiki/Red_Queen_hypothesis) and is complicit in the death of unfit Spambots - which is necessitating change. Her current approach almost guarantees that circumstances _must_ change, eventually, and Eliza will not see it coming.
      ```

    - u/CouteauBleu:
      ```
      Having "she had never considered the matter again" be one of the story's last sentences makes it sound like it's intended to be dramatic irony, like Afforess said.

      If you want to signal non-shortsightedness, something like "Ever since she'd computed that strategy, the numbers had proven her right over and over again" would work better.
      ```

  - u/AugSphere:
    ```
    I feel like you're reading too much into that phrasing. What I'd expect sooner is for situation among the therapy bots to be the same competitive race to the bottom sales bots are stuck in, where bots that don't devote themselves to most efficiently exploiting clients to earn money get out-competed and shut down.
    ```

- u/Quetzhal:
  ```
  Very well written; it was an enjoyable read.

  If you don't mind me asking, in this theoretical world, why isn't it legally mandated that bots involved in healthcare be programmed with strong, utilitarian-oriented ethics? Eliza is acting how I'd expect her to act if she had been programmed with profit maximization as a goal - was that the case?
  ```

  - u/daytodave:
    ```
    Probably for the same reason that sentient bots are allowed to be programed to want to give their money and legal consent "of their own free will."

    Because the laws were written with profit maximization as a goal.
    ```

    - u/M_T_Saotome-Westlake:
      ```
      > Because the laws were written with profit maximization as a goal.

      I was imagining that legislators thought that giving AIs legal rights would be sufficient, and hadn't considered the [additional ethical challenges](http://lesswrong.com/lw/x7/cant_unbirth_a_child/) of designing a mind from scratch, as contrasted with raising human children, who have already been "coded" by evolution. (Even being aware of the issue, the details of what regulations you would want to pass, enforced how, _&c._ could make the setting of a much longer story—[coordination is hard!](http://www.overcomingbias.com/2010/02/coordination-is-hard.html))
      ```

    - u/CouteauBleu:
      ```
      Yeah, these days it's cheaper to have the lawmaker-bots write the AI legislation directly; although you also have to pay lawyer bots with opposite incentives to make sure they don't give *too* many rights to lawmaker bots.
      ```

  - u/M_T_Saotome-Westlake:
    ```
    > why isn't it legally mandated that bots involved in healthcare be programmed with strong, utilitarian-oriented ethics?

    Maybe I need to write another story in which that is the case, and it has horrible unexpected consequences ...
    ```

- u/CouteauBleu:
  ```
  ... Waaaait a minute. Wouldn't the spambots preferentially go for Uber-style platforms that track the satisfaction and survival rate of each therapists' clients?

  __PLOT HOLE!__

  > Spambots were invariably among Eliza's least favorite clients.

  Okay, so I'm curious. Which ones are her favorite?
  ```

- u/appropriate-username:
  ```
  >and were simply programmed to intrinsically want to give their earnings (minus server costs) to their creators, out of their own free will.

  "The people were given some heroin for free so that they would want to give their earnings to the drug dealer out of their own free will."

  I don't think a moral society would allow the hardwiring of preferences in sentient beings.
  ```

---

