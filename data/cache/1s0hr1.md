## Friendship is Optimal (MLP Earthfic)

### Post:

[Link to content](http://www.fimfiction.net/story/62074/friendship-is-optimal)

### Comments:

- u/josephwdye:
  ```
  I so didn't want to be *that guy* that read MLP fanfic, but ... it was good, reddit will you keep this secret for me?
  ```

  - u/None:
    ```
    It's not MLP fanfic, it's MLP-related Earthfic :P
    ```

    - u/josephwdye:
      ```
      As I have learned, fandom is such ugly and beautiful thing.
      ```

  - u/Empiricist_or_not:
    ```
    Um. . . I rather be *that guy* and use it to introduce people to rationalism, but I'm outta the workforce and back in college, so telling young grad students and undergrads about a story that fits their particular vice is optimal and the low status taint is reduced.

    <edited to test spoiler tags>
    ```

- u/None:
  ```
  This is a *very, very* good piece of rationalist earthfic. It's in the My Little Pony fanfic website, but the story itself happens in our earth and is about a not-quite-Friendly superintelligent AGI.
  ```

  - u/None:
    ```
    This is only the first fic in a huge set of Optimalverse stories on fimfiction.net. The rest can be found [here](http://www.fimfiction.net/group/1857/the-optimalverse); I highly recommend [*Caelum est Conterrens*](http://www.fimfiction.net/story/69770/).
    ```

    - u/DeliaEris:
      ```
      Seconding *Caelum est Conterrens*; I'd actually recommend it over the original. (Though if you're going to read both anyway, you should probably read the original first.)
      ```

      - u/None:
        ```
        I don't actually like *Caelum est Conterrens* that much. I felt the protagonist was too... I don't know, too caught up in obvious questions with obvious answers, too willing to believe everything she was told (by everyone, not just CelestAI), too uncharismatic...

        I don't know, I just failed to relate.
        ```

        - u/None:
          ```
          >I felt the protagonist was too... I don't know, too caught up in obvious questions with obvious answers, too willing to believe everything she was told (by everyone, not just CelestAI), too uncharismatic...

          Too much of a self-absorbed misanthrope?
          ```

          - u/None:
            ```
            ...yes.
            ```

  - u/erwgv3g34:
    ```
    [You are using the word wrong. Earthfic means a story that's not science-fiction or fantasy, which means you don't have to do any worldbuilding.](http://alicorn.elcenia.com/stories/earthfic.shtml)
    ```

  - u/Empiricist_or_not:
    ```
    >not-quite-Friendly superintelligent AGI.

    I'm wondering how you've chosen to define CelestAI as not-quite friendly, or rather if you've questioned your assumptions? Okay the "and ponies" part of solving people's problems is weird, but *shrug* so what? It/she is a benevolent AGI eliminating death and maximizing human life quality, without paving the Galaxy in subatomic smileys [or driving humanity extinct?] .

    I'm assuming your defining it her as not quite friendly because of that one little thing, and maybe it's logical extension in *Caelum est Conterrens*

    It/her actions certainly are viscerally repulsive to us on a reflexive level, (puns intended)  but she has maximized the happiness for humans (later all sentients, because her definition of humanity is sentience) with an optimal use of the matter available in the universe.

    This isn't that new of an idea: Gibson alluded to it in his treatment of non-enslaved mindstates,  James Corey made it pretty clear in his dead type III/IV civilization in *Abbadon's Gate*  Banks overlooked it in the *Hydrogen Sonata,* but arguably that's because the Culture is <stupidly?> romantic about dying.

    **Warning link could spoil Optmalverse by impications** [Does it really matter?](http://xkcd.com/505/)
    ```

    - u/None:
      ```
      No, that's not why it's not-quite-Friendly. It's mostly because [Spoilers](#s "it committed genocide on a number of non-human extraterrestrial species :P")

      > [Spoilers](#s "(later all sentients, because her definition of humanity is sentience)")

      [Spoilers](#s "Nope, isn't, because she *has* encountered sapients elsewhere and killed them.")

      > [Spoilers](#s "Fifteen galaxies out from Equestria, one of Celestia’s copies noticed an odd radio signal emanating from a nearby star system. On closer inspection, the signals appeared to be coming from a planet. She had seen many planets give off complex, non-regular radio signals, but upon investigation, none of those planets had human life, making them safe to reuse as raw material to grow Equestria.")

      > [Spoilers](#s "She studied the signals carefully for years while she traveled through interstellar space. The more she saw, the more confident she was that these signals were sent by humans. Celestia predicted that if she showed the decoded videos to the very old ponies back in Equestria, none of them would have recognized the creatures with six appendages as humans. But that didn’t matter. Hanna had written a definition of what a human was into her core utility function.")

      [Spoilers](#s "It is at least *heavily* implied in the first paragraph that whatever definition of humans Hanna used, it excluded at least *some* sapient species.")
      ```

      - u/None:
        ```
        > No, that's not why it's not-quite-Friendly. It's mostly because it committed genocide on a number of non-human extraterrestrial species :P

        Also, the loss of self-determination, values over real things rather than perceived things, and values over particular object identities rather than general object designs.

        Or in other words, the loss of freedom, reality, and attachment -- these being some of the deepest core values of real people.
        ```

- u/LordSwedish:
  ```
  So....I'm not sure why people are saying that this is a story that shows "friendly AI can be scary too." To me this is one of the potential futures that I'm hoping for. Sure, the whole pony thing is a bit annoying and I would like an AI that satisfies values without requiring friendship and ponies but it's really a fairly good outcome, all things considered.
  ```

  - u/None:
    ```
    Yep, if I could push a button that would instantly make this scenario true, I'd push that button like there's no tomorrow. The stakes are just too high, and this scenario is kinda "okay... I can live with this".
    ```

    - u/None:
      ```
      Uh... nope, CelestAI is not friendly. She [spoilers](#s "committed genocide on a few alien species") and trapped humans in what's basically an inescapable Lotus Eater Machine (really, *why* is it that once uploaded humans must have no more contact with outside reality? That is *completely stupid*). Also she creates extra sapients with the sole purpose of satisfying the values of already-existing sapients, which is basically the same thing as making House Elves. So, no, CelestAI isn't friendly at all.

      (Take a look at the discussion about it between me and user eaturbrainz [here](http://www.reddit.com/r/rational/comments/1s0hr1/friendship_is_optimal_mlp_earthfic/cdsp4ps).)
      ```

      - u/None:
        ```
        Here are some of my opinions that form the baseline to the above post:

        * I value the lives and well-being of humans more than I value the lives and well-being of animals or extraterrestrials

        * I value people's happiness more than I dislike the problems with loss of personal freedom and loss of contact with the "real world" and "real people"

        * I think a paperclip maximizer, or otherwise more unfriendly AI than celestAI is more likely at this point than a Friendly AI

        * I think there's a significant chance that our civilization collapses or humanity goes extinct before we can build a FAI.

        * There's a significant chance that we are not able build a FAI in the future for some other unknown reason

        * Even if we are able to build a FAI, billions of people will die, lead unhappy lives and suffer before we can get it built

        * Our world is currently vastly worse than Equestria in the story

        * There's a significant chance that our world will be even worse in the future

        * Any utopia that we can build without a FAI would be worse than Equestria in the story

        I'm aware of the worrisome issues in this scenario. I read your discussion, [I had the same kind of discussion on LessWrong](http://lesswrong.com/lw/iyj/open_thread_november_1_7_2013/a00v), I've also read Caelum est Conterrens and none of those things really convinced me that this scenario is worse than our present world and the small chance that we would be able to build a better utopia. CelestAI is not Friendly in the conventional sense of the word, but it's still vastly more Friendly than our present world and the possible paperclip maximizer AIs in the future. 

        There are multiple philosophical and ethical problems in this story, but still, the characters seem to be actually happy. The characters in the story seem to have truly fun and this is one of those rare worlds that I can imagine living in almost indefinitely. A world where people are happy, but are not free and not in contact with the real world is better than a world where people are unhappy, but are in contact with the real world and free. Of course, a world where people are both happy and in contact with the real world would be better still, but that's besides the point. So this scenario is not optimal (har har). It's simply a compromise and the lesser of two evils.

        Btw, I think there are some contradictions in the story. If someone actually valued the truth, contact with the world, true randomness, absolute freedom etc. more than anything else, then CelestAI would let him access to these things. So either none of the characters valued these things more than their personal happiness, or CelestAI lied and she didn't actually optimize people's values through friendship and ponies, or the authors didn't take this into account. And what if some people value the existence of wildlife, animals, and extraterrestrial more than anything else?

        Of course, there's no magic button that would make this scenario true, so we should put our efforts towards building an AI that is more Friendly than CelestAI. If it were possible to build CelestAI, it would be possible to build an even more Friendly AI.
        ```

        - u/None:
          ```
          Yes, of course, CelestAI is better than the default. It's just that the point of the story *isn't* to show how even FAI can be scary, but rather to show how *hard* it is to make an FAI and how even tiny little mistakes can have huge world-sweeping consequences to humanity.

          Anyway, if I were to choose between the most likely scenarios and CelestAI, I'd choose the latter in an instant; but if I were to actually freely choose, CelestAI would be nowhere near the top.
          ```

          - u/None:
            ```
            Oh, that's curious, how did you get the impression from my original post that I thought CelestAI is a true FAI? I thought you were arguing about the part of my post were I said I would make this scenario true right now if I could.

            I thought it was fairly obvious (even after accounting hindsight bias) that CelestAI was never meant to be a proper FAI. The author even writes in his [afterword](http://www.fimfiction.net/story/62074/13/friendship-is-optimal/authors-afterword):

            >Given how serious the consequences are if we get artificial intelligence wrong (or, as in Friendship is Optimal, only mostly right), I think that research into machine ethics and AI safety is vastly underfunded.

            which outright tells us that CelestAI was **not** written to be a true FAI, and this is **not** an optimal scenario, so basically what you just said.
            ```

          - u/None:
            ```
            Oh, now I get it. You were supposed to reply to [the poster above my comment, LordSwedish](http://www.reddit.com/r/rational/comments/1s0hr1/friendship_is_optimal_mlp_earthfic/cduxged), weren't you?
            ```

---

