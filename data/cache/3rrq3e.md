## [D] Friday Off-Topic Thread

### Post:

Welcome to the Friday Off-Topic Thread! Is there something that you want to talk about with /r/rational, but which isn't rational fiction, or doesn't otherwise belong as a top-level post? This is the place to post it. The idea is that while reddit is a large place, with lots of special little niches, sometimes you just want to talk with a certain group of people about certain sorts of things that aren't related to why you're all here. It's totally understandable that you might want to talk about Japanese game shows with /r/rational instead of going over to /r/japanesegameshows, but it's hopefully also understandable that this isn't really the place for that sort of thing.

So do you want to talk about how your life has been going? Non-rational and/or non-fictional stuff you've been reading? The recent album from your favourite German pop singer? The politics of Southern India? The sexual preferences of the chairman of the Ukrainian soccer league? Different ways to plot meteorological data? The cost of living in Portugal? Corner cases for siteswap notation? All these things and more could possibly be found in the comments below!


### Comments:

- u/Kerbal_NASA:
  ```
  I made a cool thing for Reddit this week! Someone in /r/askhistorians asked why there are so many people named Smith in America. A lot of the answers revolved around their being a lot of people adopting Smith, but I thought a bit of modelling might also provide a bit of explanatory power. So I made a little model that created a population of 100k people and created 100 last names. Each person was assigned a role with a 50% chance each of being a last name giver or taker and also given one of the last names at random (each one being equally probable, resulting in an almost even split of all last names). Then when a generation is born the model randomly pairs givers with takers and each pair gives a pair of offspring (a tiny amount get 3 offspring to make up for the different amount of givers/takers). The offspring get the giver's last name. You can play the web version of the model yourself [here](https://lastnamedistribution.5apps.com/), the colour of the bar represents particular last names (so you can track one over time), the height of the bar is how common it is, and its place in x is its rank (in terms of commonness). After running it for awhile, I got [this distribution](https://i.imgur.com/sosgzc2.png?1) which resembles [this histogram](https://espnfivethirtyeight.files.wordpress.com/2014/11/chalabi-flowers-feature-commonname-2.png?w=610&h=610) from [this article] (https://fivethirtyeight.com/features/whats-the-most-common-name-in-america/). Here's the [source/Github page](https://github.com/kerbalNASA/LastNameDistribution) (its written in Haxe).

  This makes me think its a plausible partial explanation of what's going on. Of course, there's a lot of factors it doesn't model. For example, certain last names may be correlated with different amounts of offspring each generation, as well as different chances of pairing with other last names. I suspect these factors would amplify the speed at which that distribution forms as well as why certain regions go to more extreme forms of the distribution faster than others (e.g., in Vietnam 40% of people are named Nguyen, IIRC).
  ```

- u/raymestalez:
  ```
  Hey, everyone! There's a few things I wanted to talk about:

   1. There's a lot of threads about munchkining/brainstorming/exploiting hypothetical situations. I love these threads a lot and I wish there was more of them, participating and reading responces is very enjoyable. But I've read several comments about people disliking these threads, saying that there's too many of them and that they don't really fit on /r/rational. So, do you think maybe we should create a weekly thread, or a subreddit for that kind of stuff?

   2. Several months ago I have [attempted](https://www.reddit.com/r/rational/comments/3foue8/i_have_created_a_chat_room_for_us/) to create a chat for this community. A few people really liked the idea(42 signups in total), but that wasn't enough to sustain conversation, so the experiment failed. The main problem was that the sign up process on slack was difficult, and not everybody liked the platform. I should've discussed that with people here first. So, do you think we should repeat the experiment? What do you think about that idea?

   3. I have recently created myself a [blog on medium](https://medium.com/@rayalez), so I wanted to share it here. As I'm learning to write, I post my scifi stories over there. They are not "rationalist" - I'm not very good at writing rationalist stories yet - but I still think they are pretty fun, people seem to like them, so come check them out =)

   4. What do you, as writers and readers, think about Medium? I have been very fascinated with this platform recently, it seems pretty cool, and I would really love to hear your thoughts about it. Do you like it? What do you think they are doing right(or wrong)? Do you have a blog over there?

   5. I am listening to "Harmontown" - amazing and hilarious podcast made by Dan Harmon, a genius creator of TV shows "Community" and "Rick and Morty". If you are into that stuff - you sould check it out. You can begin by watching "Harmontown" movie. It doesn't have anything in common with rational fiction, except that it's clever, I love it passionately, and I believe that you will enjoy it.
  ```

  - u/alexanderwales:
    ```
    > There's a lot of threads about munchkining, brainstorming, or exploiting hypothetical situations. I love these threads a lot and I wish there was more of them, participating and reading responces is very enjoyable. But I've read several comments about people disliking these threads, saying that there's too many of them and that they don't really fit on /r/rational. So, do you think maybe we should create a weekly thread, or a subreddit for that kind of stuff?

    As one of those people, it's not that I don't think they fit here, it's that a lot of them are low effort. If the prompt is just "you can move backward in time up to five minutes" then you just end up with lots of questions before you can even get to the meat of it. I would like those threads much better if they started with "here's what I came up with in the first five minutes". Worse, someone will just say "rational X?" which is a complete miss for me and usually earns a downvote.

    (I'm generally against talking too much about works before they're complete, but that's because of my belief that it saps a large amount of the incentive to read/write.)
    ```

  - u/traverseda:
    ```
    > I should've discussed that with people here first. So, do you think we should repeat the experiment? What do you think about that idea?

    I'd idle an irc channel.

    >Do you like it? 

    Too blue-tribe-ey for me, personally.
    ```

  - u/None:
    ```
    > threads about munchkining/brainstorming/exploiting hypothetical situations

    Personally I like find these threads hit and miss. Mostly it comes down to how specific they are: I get a better read from "Given setup X, Y, and Z, can anyone think of a way to do W?" than from "Rational X?".
    ```

- u/AmeteurOpinions:
  ```
  [Continuing to ask](https://www.reddit.com/r/rational/comments/3pwxo5/d_friday_offtopic_thread/cwa5fvj) [unanswerable questions](https://www.reddit.com/r/rational/comments/3quhyp/d_friday_offtopic_thread/cwimhrl), here's another one: Why are hugely wealthy entities (be they people or corporations) so stagnant, relative to the power their wealth enables? Outliers like Elon Musk or even the Koch brothers are just that, outliers. There are millions of people with the funds to do "newsworthy" things, but I feel like I only ever hear about the same thirty or so people or conglomerates doing anything.

  This may be a bad example, but consider: In America, Internet Service Providers infamously hate doing anything to improve the lines in comparison to other nations. *Why?* They have nothing to lose and everything to gain by trying to capture bigger future markets than just squatting on the present one. 

  Hm... a better way to put it is: Why do powerful entities find one utility-generating method that works, then run it into the end of time, instead of trying to get ahead of the curve and thus get richer than the curve?
  ```

  - u/None:
    ```
    >Why are hugely wealthy entities (be they people or corporations) so stagnant, relative to the power their wealth enables?

    It really depends how they got the wealth, but let's list out some obvious reasons:

    * There are upkeep costs to huge amounts of wealth, since there's an extreme scarcity of places to invest gajillions of dollars of capital for very high returns with very low risk.  You or I can say, "woot, a year's salary, let me stick that index funds".  A lifetime's salary can't be invested that same way without being murdered by inflation.

    * Loss aversion, straight up.

    * Debt load and other general forms of illiquidity.  Many large-scale fortunes are simply not liquid.  They are often weighed down by repayment obligations with various time-scales.  It's hard to pour everything into fusion-power research when you need to repay bonds next week.

    * The actual costs of running the business that generates the massive wealth can be, well, very large.  Not so large the capital base can't handle them, but enough that the capital base can't keep the company running *and* put everything into fusion-power research.

    Those are generic to practically every large enterprise.

    >This may be a bad example, but consider: In America, Internet Service Providers infamously hate doing anything to improve the lines in comparison to other nations. Why? They have nothing to lose and everything to gain by trying to capture bigger future markets than just squatting on the present one. 

    Now let's address some structural issues, mostly via plausible speculation that might be totally wrong but fuck it:

    * Wealth inequality and knowledge inequality go together.  If your average cable-company CEO wanted to invent fusion power, what the fuck would he know about whose research to subsidize or how it all works?  He mostly knows about cable and finance, *if that*.  Worse, an American cable-TV CEO is far less likely to have had a really good technical education than someone in, say, some other country where the best technical universities are state-supported and cheap-or-free.

    * Short-termism.  Most leaders can't enact policies their job-security can't cover and be confident everything turns out at all, let alone right.  This especially applies in a day and age where job security is very low, even for CEOs and such, so a 20-year program of investment and research is hard to run.

    * Incentives!  Capitalism gives incentive to generate *profits*, not *value*.  This is why the thing with the ISPs and cable companies: if they've won a monopoly or can rent-seek on something else (land in Manhattan, for instance), they've achieved a local maximum, and their incentive function is going to read anything other than exploiting that local maximum as a "decrease" in net utility.  This becomes a stronger effect when short job tenures are normal, because personal ideology and belief can make less impact when the leader knows he'll be replaced in a few years.

    * Ignorance!  Most people don't really believe in science, except as "that thing which happens in laboratories and big white coats".  Their knowledge base is so far from the research frontier that they tend not to have any idea how much remains undiscovered.  This lack of knowledge also makes it harder to know where more R&D effort would be helpful.

    * Disturbing lack of faith!  Again, *most people don't really believe in science*, not the actual scientific method and *certainly* not that *ever-so-outdated* modernist ideology about "better living through science".  In fact, a great many people believe that scientific knowledge *makes things worse*, and that *real* problems are solved by changing people's personal behavior or through "virtue" or through "opening our minds" or through religion or some other nonsense.

    * They're busy buying politicians!  Hard to buy scientists when you're busy with these other costly public servants who don't add to next year's bottom line.

    * Lack of connections.  So you want to invent fusion power!  Again: who do you talk to?  If you can find their office via online listings, how do you make an appointment when you're just *some guy* with a lot of money, and *they're* as busy as you are?

    * Disturbing lack of vision!  [We've raised whole generations of people to whom financial metrics are the only things about the future that can actually vary!](http://thebaffler.com/salvos/of-flying-cars-and-the-declining-rate-of-profit)  Everything else is seen as predetermined and set-in-stone!  Besides, with all of our science fiction being dystopia and disaster, what the fuck would you invest the money in if you even had it?
    ```

    - u/AmeteurOpinions:
      ```
      Wow, that link has so much insight you ought to give it its own post. Seems to capture a bunch of things I've felt but could not phrase.
      ```

      - u/None:
        ```
        The link is economic history done by an anthropologist.  It's interesting, but also gets some important stuff wrong (like forgetting to acknowledge the '70s stagflation as a causative factor in neoliberalism).
        ```

      - u/None:
        ```
        Hold me to this with a reply: I hereby commit to writing an essay mirroring Graeber's, to be entitled "Life Extension vs Neoliberalism".
        ```

    - u/TaoGaming:
      ```
      A quibble on incentives. Capitalism does typically force you to generate value. (Try selling something that everyone agrees is worthless)  But value is tougo to nail down (see all the fan duel commercials)  But the incentives of the stock market can be notoriously short sighted. Then again elected officials often cannot see past the next election.  

      I would place the blame on the agency problem (managers 
      , including CEOS who have goals differing from owners)
      ```

      - u/None:
        ```
        > Capitalism does typically force you to generate value.

        Well, it forces you to *capture* value.
        ```

  - u/Polycephal_Lee:
    ```
    Henry David Thoreau has a good take on it in reference to government:

    >Unjust laws exist; shall we be content to obey them, or shall we endeavor to amend them, and obey them until we have succeeded, or shall we transgress them at once? Men generally, under such a government as this, think that they ought to wait until they have persuaded the majority to alter them. They think that, if they should resist, the remedy would be worse than the evil. But it is the fault of the government itself that the remedy is worse than the evil. It makes it worse. Why is it not more apt to anticipate and provide for reform? Why does it not cherish its wise minority? Why does it cry and resist before it is hurt? Why does it not encourage its citizens to be on the alert to point out its faults, and do better than it would have them?

    Corporations/people/governments get good at doing one thing, and become complacent. They then try to protect the thing they are doing well, and vehemently oppose all efforts to change for the better because it hurts their livelyhood. Blockbuster, Tower Records, etc, all faced the same problem, an outright refusal to accept that their old profit models were no longer tenable in this world due to changing technology.

    To understand why they don't, you have to understand the capitalist and the profit motive. The profit motive incentivizes a capitalist to strictly increase the accumulated wealth he has. Chasing new technology runs the risk of losing wealth, which is antithetical to accumulation. In aggregate capitalists very rarely take risks to their accumulation, even when it would be beneficial for all in the long run. Short sighted quarterly profit doesn't care about structural analysis and sustainability. So you end up with these corporations trying to protect old profit models, and cutting costs, slowly dying, instead of shedding their skin and being reborn with new technologies and methods.

    tl;dr: Corporations preferentially protect the profit they have instead of chasing new, un-tested profit.
    ```

  - u/traverseda:
    ```
    [Moloch](http://slatestarcodex.com/2014/07/30/meditations-on-moloch/).
    ```

- u/None:
  ```
  There's not much discussion of writing technique, even as it pertains specifically to the discussion of writing rational/ist fiction, on this sub.

  Discuss this lack of discussion.
  ```

  - u/alexanderwales:
    ```
    I feel like I talk about writing (mostly plot structure and character arcs) a lot. But if I wanted to talk about broadly applicable writing topics, I would probably take them to other subreddits, where I could get feedback from a wider, more specialized audience: /r/writing, /r/fantasywriters, /r/scifiwriting, etc.

    More than that though, there are a huge amount of resources for writers such that I don't think discussion is all that valuable. If you want to know writing technique, that's a very well established field and you're almost certainly better off reading the ten best books out of the thousands of books that have been written on the subject. Or you can take college courses from accredited professionals.

    Basically, I do talk about writing technique, but the reason that I don't talk about it *more* is that there are already a whole host of resources that cover almost anything I'd want to say and which do that more eloquently and concisely than I would be able to manage. (I also have a degree in English, which further reduces my expected returns from talking about writing technique here.)
    ```

  - u/raymestalez:
    ```
    Yeah, I wish I could get some hints from /u/alexanderwales, /u/qbsmd, and others.

    Though from what I understand, "writing" and "rationalist" are pretty much separate skillsets.

    "rational" part is mostly about knowing science, philosophy, and beig smart enough to exploit things, and all that is discussed on LW.  

    "writing" part is the same as non-rationalist writing, and is discussed elsewhere.

    It seems like writing rtf comes down to knowing and liking "rationalist" topics, *and* being able to write good fiction in general.

    So from what I can see, there is no "writing rational fiction" technique. There's just "general writing technique", and "having rationalist ideas to write about". Maybe that is why people do not have a lot to discuss.

    Although, if there's any discussion on the subject - I'd be happy to participate!
    ```

    - u/alexanderwales:
      ```
      > So from what I can see, there is no "writing rational fiction" technique. There's just "general writing technique", and "having rationalist ideas to write about". Maybe that is why people do not have a lot to discuss.

      That's part of it, for me.

      I actually think that there's a subset of general writing ability which is "writing things that /r/rational would like", which largely consists of things like:

      * how to write compelling exposition
      * how to properly escalate a conflict
      * how to foreshadow the solution to the plot
      * how to show two people as having a legitimate disagreement

      You can write fiction without knowing these things; there's no need for conflict to escalate, it's just something that /r/rational tends to like. So I do think that there's some room for a more /r/rational focused approach to writing technique, it's just that these things are hard to do well and if you *can* do them well it's hard to distill the practice down into practical advice.
      ```

      - u/None:
        ```
        Well for example the whole writing intelligent characters series is basically rationalist fiction writing 101. You can call that general writing skills if you want but clearly there are particular skills that produce fiction valued by this sub and those might as well be called rationalist fiction skills.
        ```

- u/HeirToGallifrey:
  ```
  I recently watched an episode of American Horror Story. It was the second episode of the series I've ever watched, but that's beside the point. Spoilers up until the most recent episode of the new season, I suppose.

  Anyway, vampires exist in this show, and one of the recently turned vampires is a doctor. She infects one of her dying patients (a young boy of about twelve) to save his life. He then immediately kills his parents, infects his best friend/girlfriend, kills his teacher, and infects his entire class, in that order. Then all of the students blame the killings on a random intruder and escape the school.

  **The point is**, this got me thinking about the rationality of vampirism. Assuming you could become a vampire, and thereby be guaranteed immortality (only getting thirsty every now and then), wouldn't it be rational to do so? And isn't the kid acting quite rationally by granting immortality to his girlfriend? Doing the same for his whole class seemed a bit reckless, but all the others seemed to play along and cooperate, so that worked out well too.

  In the end, two (possibly four) people at the school died to guarantee immortality for twenty children. Is that acceptable? Utilitarianism might say so. What do you think?
  ```

  - u/alexanderwales:
    ```
    This is something that I've given some thought. It really, really depends on your vampires.

    * How much blood does a vampire require? In my *Robot, Wizard, Vampire* story it's one pint a day, which means that a vampire requires donations from a rotating pool of approximately sixty people in order to survive. If vampires need one pint a *week* then that pool of volunteer humans is reduced to eight people. If they need to kill humans, then there's pretty much no societal utility unless you can create some ethical scheme of euthanasia (which is what the last vampire book I read ended up doing).
    * What are the effects of vampirism, beyond immortality? Immortality looks a lot less attractive if it comes with severe restrictions, psychological problems, or changes to basic values. If becoming a vampire makes you evil, as it does in some canons, then a good person wouldn't do it even if it granted immortality, because you wouldn't really be yourself anymore.
    * What does the rest of the universe look like? If there's a provable afterlife and vampirism prohibits you from it, as it does in many vampire canons, then it doesn't make sense to become a vampire unless your afterlife prospects are especially bleak.
    ```

  - u/Charlie___:
    ```
    I think you might like Luminosity :)
    ```

- u/ToaKraka:
  ```
  The book that I'm currently (re-)reading, if anyone's interested, is *[The Financier](http://www.gutenberg.org/ebooks/1840)*. I stumbled across it in the RSS feed of books newly-added to [Project Gutenberg](http://www.gutenberg.org)'s collection, some months ago, and was intrigued enough by the title to give it a glance. I find it quite fascinating. Despite being set in a Philadelphia of the late nineteenth century, it includes detailed descriptions of a lot of complicated financial and political machinations (interspersed with some rather-tiresome romance), through which the protagonist [spoiler](#s "at first rises, but eventually falls"). The climax really impresses on the reader the reasons for which [banks are required to keep reserves of ready cash](https://en.wikipedia.org/wiki/Reserve_requirement), rather than loaning out too much of their money--and it's a fun story, too!

  (There's [a sequel](http://www.gutenberg.org/ebooks/3629) that continues in much the same vein--in Chicago rather than in Philadelphia--but the protagonist's constant, boring romantic intrigues pushed me away from it, last time I tried to read it.)
  ```

  - u/ulyssessword:
    ```
    For anyone interested in banking and fiction, I'd recommend *The Dagger and the Coin* series by Daniel Abraham (starting with *The Dragon's Path*).  The main character is a banker, most of the characters are smart, and the ones who aren't smart are at least consistent and very believable.

    Also, on the topic of fiction and finance, I'll plug *[The Cambist and Lord Iron](http://www.lightspeedmagazine.com/fiction/the-cambist-and-lord-iron-a-fairy-tale-of-economics/?xcbv)* by the same author.  It's a short story, and more about trade than about banking.
    ```

- u/xamueljones:
  ```
  For anyone who likes [Order of the Stick](http://www.giantitp.com/), the author's now selling the first two volumes in pdf form. He said that if there's enough money from the sales, then the rest of the comic will be made into pdfs. So go buy some at the above link and spread the word if you ever wanted the series in digital form!
  ```

- u/TaoGaming:
  ```
  So (as you may have guessed by my username), I write primarly about (board) games. 

  Last monday, I played a beautiful cooperative game where the players are merely trying to survive the Horrors of World War I. It's not a classic game (its good, not great). But it was highly evocative, and beautifully drawn. The artist was a professional cartoonist. 

  At Cherlie Hebdo.

  His name was Tignous and Islamic Radicals murdered him during the massacre, a fact I wasn't aware of until I looked him up.

  And I've been thinking about [that over the last week.] (https://taogaming.wordpress.com/2015/11/07/art-theme-coincidence-and-cartharsis/).
  ```

- u/LiteralHeadCannon:
  ```
  Let's talk about quantum immortality (again), dudes and dudettes of /r/rational!  Or, um, maybe I misunderstood quantum immortality and it's actually something else.  In which case, let's talk about my misunderstanding of it!

  First, a disclaimer.  Among this community, I put pretty high odds on the existence of God and an afterlife.  For this post, though, I will be assuming that there is no afterlife, and that minds cease to exist when they cease to exist, as the alternative would really screw up the entire idea here, which depends on the existence of futures where you don't exist.

  Let's say that you're put in a contraption that, as with Schrodinger's box, has a roughly 50% chance of killing you.  When the device is set off, what odds should you expect for your own survival, for Bayesian purposes?  50%?

  No.  You should actually expect to survive 100% of the time.  Your memory will never contain the event of your death.  You will never experience the loss of a bet that was on your own survival.  There is no point in investing in future universes where you don't exist, because you will never exist in a universe where that investment pays off.

  This has serious implications for probability.  Any being's expectations of probability should eliminate all outcomes that result in their death.  If you flip a coin, and heads indicates a 1/3 chance of my death, and tails indicates a 2/3 chance of my death, I should expect heads to come up 2/3s of the time - because 1/3 of the heads futures are eliminated by death, and 2/3s of the tails futures are eliminated by death.

  As I'm sure you all know, 1 and 0 aren't real probabilities.  This is a physical reality.  In physics, anything *may* happen - it's just pretty much always stupendously unlikely that any given Very Unlikely thing will happen, approaching 0.  A concrete block 1.3 meters in each direction could spontaneously generate a mile above Times Square.  The odds are just so close to 0 that they might as well be 0.

  So if you happen to be a sick and disabled peasant in the middle ages, then you should still expect to live forever.  Something very statistically unusual will happen to get you from where you are to immortality.  Perhaps you'll wind up lost and frozen in ice for a few centuries.

  We, however, don't need to deal with the hypothetical peasant's improbabilities.  We are living in an era where life-extending technology is being developed constantly, and a permanent solution is probably not far behind.  Our immortality is many orders of magnitude likelier than that of the hypothetical peasant.  Our future internal experiences are much more externally likely than those of the hypothetical peasant.

  One thing I'm concerned about is survival optimization.  Humans are, for obvious evolutionary reasons, largely survival-optimizing systems.  Does a full understanding of what I've described break that mechanism, somehow, through rationality?  Is it therefore an infohazard?  Obviously I don't think so, or else I wouldn't have posted it.
  ```

  - u/Chronophilia:
    ```
    I'm fairly sure most of this community doesn't think quantum immortality is real - it's an amusing but flawed thought experiment.

    >No. You should actually expect to survive 100% of the time. Your memory will never contain the event of your death. You will never experience the loss of a bet that was on your own survival.

    You're confusing the event of my death with the memory of it. Just because I don't experience or remember a thing, doesn't mean it hasn't happened to me. Tree falling in the woods, and all that.

    By the same token, should it be impossible to forget anything? Since you can't experience forgetting things - you'd have to be consciously thinking about the thing at the very instant you forget it, which is obviously contradictory.

    >There is no point in investing in future universes where you don't exist, because you will never exist in a universe where that investment pays off.

    Other people will exist in those universes. I like some of these people. I want them to be happy.
    ```

    - u/ArgentStonecutter:
      ```
      > Other people will exist in those universes. I like some of these people. I want them to be happy.

      Bingo.
      ```

    - u/LiteralHeadCannon:
      ```
      If you start to feel moral concern for people in universes separate from your own, you have opened yourself up to a wide assortment of problems.
      ```

      - u/ArgentStonecutter:
        ```
        I feel moral concern for people in this universe, and that moral concern extends to their future versions.

        This is not the same as having moral concern for people who are current versions of people I know but are no longer causally connected to me, which is I think what you're worried about.
        ```

        - u/LiteralHeadCannon:
          ```
          People in future branched universes where you've died *are* current versions of people you know but are no longer causally connected to you.  Just, in the future.
          ```

          - u/ArgentStonecutter:
            ```
            If I can take an action, now, that changes their circumstances in the future... even if I no longer exist in that future... then I am causally connected to them. That's what causality _means_.

            They are not causally connected to that future me, but I'm not that future me yet.
            ```

  - u/raymestalez:
    ```
    I don't know a lot about the topic, but I have questions:

    - What if the substance in Schrodinger's box doesn't kill me immediately, but let's say after a week? Wouldn't a consciousness of a guy who lived in the box for a week before dying be different from the consciousness of a guy who survived? So that they wouldn't be identical, and the guy who lived for a while in a "doomed" timeline dies? 

    - Just because there's no point in investing in the future universes where you die, doesn't mean that there's no such universes. You discard all the outcomes that lead to your death because they are useless for planning, for practical purposes, for guiding your actuons as a rational afent, but not because they don't exist. If you are "doomed", if you are being eaten by a shark in the middle of the ocean it may be *useless* to bet on outcomes where you stop existing, but you still can expect that you will stop existing.

    Or am I horribly misinterpreting something?
    ```

  - u/Transfuturist:
    ```
    You would agree that changes to a person's mind don't constitute a death of experience, correct? Bashing your head on a wall and losing a few brain cells maintains your experience?

    I'll assume this to be true, because it's somewhat idiotic to say otherwise. So, imagine that you have a monotonically decreasing mind. Your brain is not growing, and it is losing a random brain cell every thousandth of a second. We have around 86 billion neurons, so this process takes about 2.7 years. Your mind grows increasingly simpler; imagine Charlie from Flowers for Algernon, and your experience is maintained throughout the entire process. But it doesn't stop at the level of mentally challenged adult. You gradually lose all higher functions, becoming akin to a paralyzed animal or infant, simplifying further and further, until you're no more than an ape, a monkey, a lemur, a vole, a worm. A bundle of nerves. A single neuron. Then nothing.

    Your experiential complexity has decreased smoothly. Even if some sort of quantum immortality or 'fungibility' (from Balthasar999) held at some point in the process, you would not continue experiencing things from the level of your original self, or even from the level of a particularly challenged human. Your experience is not especially differentiated from that of a monkey's, or a vole's. You're made of the same mental construction.

    Your experiential observation is not Platonic, it is the effect of there being a complex substrate that calculates it. When the complex substrate transforms, the experiential observation transforms with it. In the cosmically slow process of, say, a car crash, your brain is shocked and battered, even breached by foreign material, and the computation of experience continues, even as brain cells fail and connections are disrupted. Your *mind* is irrevocably damaged. The causal processes that used to *have the effect* of analyzing information and directing action in a manner identified with intelligence are instead made to have entirely different and not nearly so intelligently-identifiable effects.

    This question comes down to the metaphysical. I am theoretically an adherent of mathematical Platonism, but the unyielding consistency that I have observed in reality makes me question that. If there are Tegmark worlds that contain the exact same mind as myself, then why have I not observed myself being entirely wrong about reality and history? Is it really that much more likely that I am a mentally healthy individual in the world that I expect to be real rather than a delusional sot in any of an infinite plethora of alternative worlds? If fungibility is real, then why is the only fundamentally incorrect experience I have been that of the Berenstain Bears being spelled Berenstein?

    I suppose I could hypothesize that the memories themselves change with the alternate histories. But if my memories themselves change, then how can I even say that this 'fungible self,' this immortal kernel, can even be identified with me? If fungibility is real, then is it not more likely that I am (at any particular instant) actually a historical simulation of more advanced descendants of this universe? Or astronomically more likely that I am in any number of possible simulations of alternate physics in any number of possible real physics?

    I'm going to read the Finale of the Ultimate Meta Mega Crossover again. But ultimately, the answer to this question comes down to whether you consider your experience to be a cause of the material effects you observe, or to be an effect of the material causes.
    ```

  - u/MugaSofer:
    ```
    Firstly, for practical reasons, I think I recall reading that sufficiently "improbable" quantum events run out of quantum stuff to represent them and are probably destroyed. You're not literally lining in a world where everything happens with some small-but-nonzero probability; you're living in a world where certain events "split" events in half, or split off two-thirds, or whatever.

    "Zero and one are not probabilities" is a fact about epistemology, not physics.


    With that said, I don't *think* I buy that this is how anthropic probability works. I don't think I even understand anthropic probability looking *backwards*, let alone *forwards*, but ...

    I think if you say "X would have killed me with 99% probability, and Y would have killed me with 1% probability, but we've no other evidence so *who knows* which one happened?" then you'd be wrong like 99% of the time.

    So that's probably *not* how it works - you can probably treat "I'm not dead" as evidence, which in turn means "I'll die" must have some specific probability, unless we're throwing Bayes out the window here.
    ```

    - u/Transfuturist:
      ```
      The problem with quantum immortality is not in the anthropics; in fact, anthropics is the counter to quantum immortality. Post facto, any observer must realize that they survived, and that survival was absolutely necessary to be an observer. 'In 100% of the worlds that I could possibly observe, I survived.' The problem is that quantum immortality is conditioning on this and treating the conditional probability as the total. Say you have a 50% chance of surviving some event. The point of anthropics is that actually there are 50% of worlds where you did not survive, even though you observe survival as 100% likely. Now then, what happens when all possible worlds contain your death? You *die.* Conditioning on your survival, you see the same 100% survival rate, but the conditioning probability is 0.

      I believe I understand anthropics. Is there anything you know you're particularly confused about?
      ```

      - u/MugaSofer:
        ```
        OK:

        * If I repeatedly survive a potentially-lethal event by "pure chance" over and over, does that strongly imply that the events *would* have killed me, or strongly imply that I was mistaken about how likely they were? Or is it not strong evidence either way?

        * If *Earth* avoids - say, an asteroid impact - thanks to a hilarious string of coincidences; does that suggest it would probably have killed us, or just that it would have massively reduced the population?

        * Does any of this impact your attitude to the Doomsday Argument at all, or vice versa? That definitely confuses me, and I kind of mentally label it "anthropics", but I'm not sure it's the same thing.

        * If I create a copy of myself and then one of me is instantly killed, do I have a 100% chance of ending up as the copy - as if I just teleported a foot to the left? Or is that just survivorship bias, and I had a 50-50 chance of dying?
        * If I create two copies of me, give one a red ball and one a blue, and split the "blue ball" copy into two ... do I have a 2/3rds chance of receiving the blue ball, subjectively, or 1/2 chance? (Modified Sleeping Beauty problem.) Or do I have some kind of 50%-now-but-66%-later probability that varies over time?
        ```

  - u/AugSphere:
    ```
    There is actually a pretty nice interpretation of QM that doesn't lead to many worlds and the associated shenanigans. It's called quantum bayesianism. The basic idea is that QM is about observations and our bets (predictions) on them and our state of knowledge is encoded within state vectors. Within that framework the whole quantum collapse issue seems rather silly, since the "collapse" is actually a bayesian update of probability distributions associated with a quantum state and the discontinuity makes perfect sense.

    It's a beautiful interpretation and provides an interesting counterpoint to many-worlds as far as thought experiments like this go.
    ```

- u/None:
  ```
  GUYS, GUYS!  MARGINALIZATION IS THE INNER PRODUCT OF A CONDITIONAL DISTRIBUTION WITH THE DISTRIBUTION OF THE NUISANCE VARIABLE!

  Fuck, why don't people just *tell* you these things instead of writing out all those damned sigmas!?
  ```

  - u/xamueljones:
    ```
    What topic is this from so I know in which one of my future CS classes to look back at my notes on this post? Thanks!
    ```

    - u/None:
      ```
      Probability theory, particularly in its application to Bayesian statistics where we get the predictive distribution by marginalizing out all the "nuisance" variables we're not trying to predict.
      ```

      - u/xamueljones:
        ```
        *blink, blink*

        I actually understood that and now I understand your previous post.

        All of that studying Bayesian stats and modeling computational networks is working!

        Thanks for the clarification!
        ```

        - u/None:
          ```
          I've been working my way through introduction to computational Bayes methods and it got to the bit about Markov chains and started expanding all the damn terms instead of just saying that:

          * For a finite-state, discrete-transition Markov chain, the state distribution is a vector with an l1-norm of 1.

          * Likewise, the transition matrix of conditional state-to-state transition probabilities is just an assignment of conditional distributions to each source and destination state.

          * Therefore, we can use inner-products to multiply these vectors and matrices just like any *other* vectors and matrices.  It's all *just another fucking Hilbert space*.

          * Therefore, when we generalize to infinite states or continuous transitions, everything *continues* to obey the generalized Hilbert-space laws.

          * Therefore, probability distributions can be treated as vectors in a Hilbert-space *in general*, with the caveat that we have to keep them l1-normed to 1, so we need to modify the normal vector-addition operation to accommodate the actual Sum Law of probability -- but the generalized addition laws should still hold, as should category-theoretic treatments of products and coproducts!

          My undergrad probability and CS theory prof did teach Markov chains in full, but he never did zoom out and generalize to the full Hilbert-space or categorical perspectives.
          ```

- u/xamueljones:
  ```
  What do people here think about how active /r/hpmor still is even after the story finished back in March? I'm very surprised and I'm wondering what might explain this phenomenon.

  They still have 8,383 people in the subreddit for anyone who's curious.
  ```

  - u/brandalizing:
    ```
    For the most part, I think, it's the handful of continuation fics that have been updating pretty consistently. Once those finish (I think one just did) activity will slow down. We'll see where it goes from there.
    ```

    - u/AmeteurOpinions:
      ```
      It'll have a final burst for the last Omake and The Epilogue, but yeah, I don't see it going much further than that.
      ```

---

