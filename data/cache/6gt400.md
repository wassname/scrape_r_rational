## [RT][HSF][TH] Utopia, LOL?

### Post:

[Link to content](http://strangehorizons.com/fiction/utopia-lol/)

### Comments:

- u/Roxolan:
  ```
  Kind of sad that the exponential growth element hasn't been curtailed somehow. There's only so many stars to give.

  "Jamie Wahls [author] works at the Machine Intelligence Research Institute" oh.
  ```

  - u/eroticas:
    ```
    *That's* the sad part? That we go beyond Sol? Not the "humanity lost control of the steering wheel" part? I thought that was why Charles was wistful. 

    The stars will burn out on their own soon enough anyway. And the forever-winter is coming, so every joule translates to a little more human experience before it all ends. 

    This *isn't earth*. The rest of the universe *isn't* a delicately balanced ecosystem which represents our ancestral environment such that altering it generally leads to negative consequences from a human perspective.
    ```

    - u/None:
      ```
      > Not the "humanity lost control of the steering wheel" part?

      I feel like we should spend some time unpacking what it means to have humans or humanity *holding* the steering wheel in the first place.  I mean, do we have the steering wheel right now?

      I kinda feel like *I* definitely *don't*, and neither does anyone else I know or know of.  Even the people who are officially denoted as leaders, whose notional job is to hold the steering wheel, don't seem to have it under control.  Hell, a *certain* bunch of them seem to be almost *deliberately* letting go of the damn wheel.

      So who's holding the wheel?  Nobody, it seems.  Is that better than having a nonhuman holding the wheel?  Is there a way for the immense number of actually-existing humans to share the wheel?  I mean, I hold to the ideal of democracy as much as anyone, but it still seems like if you spread control democratically through *seven billion people*, each person will still have almost no control over humanity's fate as a whole.

      This is kinda making me think that utopia ought to be Bookchinist :-p.
      ```

      - u/eroticas:
        ```
        Think of it in a relative sense. We're not in full control but we're holding the steering wheel much more now than we were when we were just another animal in the ecology, in the sense that the environment is more shaped by our intentional (right or wrong) decisions.
        ```

        - u/None:
          ```
          Ok, and what's the tradeoff?  That is, you know, right now we appear to be *fucking it up*.  How much control do we trade off to get stuff not fucked up, and vice versa?
          ```

          - u/eroticas:
            ```
            *shrug* fucking up is also relative. Staying hunter gatherers until the next asteroid, famine, plague, whatever wiped us (and, eventually, all life) out would be a worse sort of fucking up than what is currently happening. But whatever the optimum worlds are, there *are* optimums, and the AI in that story feels like being on the "too little control" end of one. (Granted, we don't know the world, just this micro interaction. But these individual humans clearly have very little agency.)
            ```

            - u/None:
              ```
              > fucking up is also relative.

              Mass extinction is not relative.  We are causing one.  This is fucking up.

              >the AI in that story feels like being on the "too little control" end of one. (Granted, we don't know the world, just this micro interaction. But these individual humans clearly have very little agency.)

              It does seem to me like they have little agency, but I'm not sure if that's because of the AI.  It could also be because they reached an [End of History](https://en.wikipedia.org/wiki/The_End_of_History_and_the_Last_Man).  Just because *we* haven't reached one, and won't any time soon, doesn't mean that people many hundreds or thousands of years into the future *won't*.

              That is, there may come a time when high-level agency just isn't useful for much.  I kinda hope not, but I also can't help but do the thought experiment and think: what happens when we *do the right thing*?  Is our agency useful after we've already used it well, so to speak?

              Sure, blah blah journey not destination, but there are physical limits we run into as well.

              So I'm kinda unsure about whether an End of History can really happen.  I'm also *worried* that should even an approximate End of History happen, people will start trying to destroy its arrangements out of sheer nihilism for no longer having Grand Causes to which to devote themselves.  I think I detect a little of that today: we kinda know how to run a workable society, but people have been steadily pushing the notion that it's meaningless to do so.
              ```

              - u/eroticas:
                ```
                >Mass extinction is not relative. We are causing one. This is fucking up.

                Yes it is relative! There are extinctions and then there is EXTINCTION. Without humanity, life itself is unlikely to outlive our star. It could be better. It could be *way* worse.

                And, I mean, at least when it comes to mega-fauna the holocene extinction predates modern tech, I'm not entirely sure not having the technological revolution would've fixed the situation...hunter gatherers and subsistence agriculturalists are pretty capable of fucking up the ecology even without...all this other stuff we have now. Really, I think increasing human agency is the only way to go at least for now / given that we don't have capable non-human agencies yet.

                >End of history

                The end of ideoogical conflict is not the end of *everything requiring human agency*, it's only the end of ideological conflict. "Doing the right thing" *means* preserving human agency in my opinion, unless we're just throwing our hands up and saying it's impossible to preserve that value without sacrificing too many others.

                >there may come a time when high-level agency just isn't useful for much. **I kinda hope not**

                That means you've judged, and we agree, right? (Except that I don't see agency in terms of usefulness or being instrumental to another value, but as an end in itself.)
                ```

                - u/None:
                  ```
                  > There are extinctions and then there is EXTINCTION.

                  And AFAIK, we are heading for EXTINCTION, with a significantly high chance.  Complex animal life goes bye-bye, including us.

                  >Really, I think increasing human agency is the only way to go at least for now / given that we don't have capable non-human agencies yet.

                  Definitely.

                  >unless we're just throwing our hands up and saying it's impossible to preserve that value without sacrificing too many others.

                  Not to jump straight to the offensive part, but three words: President Donald Trump.  I really have very little faith that human agency doesn't mean *fucking up* when it's *actually important* and we *really need* a specific outcome.  No, not fucking up.  People deciding to *shit all over* the important outcomes, because they just want to spite other people.

                  >That means you've judged, and we agree, right?

                  *Mostly.*

                  >(Except that I don't see agency in terms of usefulness or being instrumental to another value, but as an end in itself.)

                  What's agency without being agency *over* something, towards some end?  What does it mean to hold the steering wheel if you're not steering between meaningfully different outcomes?  Agency over chocolate versus vanilla isn't agency.

                  And again, agency *over what*?  I definitely feel like "humanity" is in the driver's seat right now, but that's an abstraction.  Individual humans make decisions, those decisions combine into results.  Nobody is really *planning* anything; shit just *happens*.  What can it *mean* for *everyone* to have full agency (causal influence) at the same time?

                  I think there can certainly be tolerances within which completely fucking up is an acceptable proposition if it means we've made our own choices.  I'm just no longer sure that leaving the complete extinction of life as we know it *an open possibility* in the name of agency is acceptable.  That feels too over-compensated in the other direction.

                  Sometimes the choice is more important; sometimes the outcome is more important.  I don't always know which one, especially these days.
                  ```

    - u/Roxolan:
      ```
      >  The rest of the universe isn't a delicately balanced ecosystem which represents our ancestral environment such that altering it generally leads to negative consequences from a human perspective.

      To be clear, I didn't mean what /u/eaturbrainz is saying re: overharvesting / environmental degradation. 

      By all means, go beyond Sol, tear apart the very stars, and process them all into utiliton. 

      My issue is that I intuitively prefer a relatively small number of people having experiences for eons, to a humongous number of people having experiences for a few million years. "There's only so many stars to give" i.e. eventually the exponential growth means each new human only gets a tiny share of the available starpower.

      > Not the "humanity lost control of the steering wheel" part?

      Eh. I'm with Scott Alexander here; to defeat Moloch we'll eventually have to put something non-human (or so seriously post-human it makes no difference) in charge. 

      (Charles might disagree, unclear.)
      ```

      - u/eroticas:
        ```
        In charge, yes. Preventing us from hurting each other, yes. But leaving *us* intact in our self determination. Essentially, like a good parent that encourages growth, rather than a bad one which stifles the kid.
        ```

- u/None:
  ```
  Warning: actual critique.

  I guess what I find disappointing about this story is that a long time into the future, people are sort of stuck playing around with today's internet memes.  That's their lives.  Lulz and deconstructions and elf-sex.

  It's the future.  *What have you learned?*  How are you now wiser and more noble than our present day?  In what ways have you become stronger?  Where's the progress?  Where's the utopia in your utopia?

  Or I dunno, [spoiler for the end](#s "maybe Kit has a personality and a lifestyle carefully configured to eventually propel them towards sending people off to explore other solar systems").
  ```

  - u/narfanator:
    ```
    Nah, you missed it. Kit is bizarrely accessible to people from our era; it's explicitly not even close to the norm in that future.

    It's not that universe doesn't have those people, it's that the story doesn't feature them.

    Also, although I'd definitely say we've *more* wiser and nobler people today (per capita) than a couple thousand years ago, it's not like everyone's gotten that way. Why would you expect it to be any different at an arbitrary point in the future? Or is there some singularity of nobility?
    ```

  - u/Alphanos:
    ```
    [Spoilers for the end](#s "The story describes a sort of choose-your-own-utopia, where each person may go in very different directions.  However we only saw a very infinitesimal portion of that.  From the start, the Allocator was aiming to convince Charles that the future was vapid and boring.  We have little reason to assume that Kit's views and personality are universal, or even necessarily commonplace.")

    I agree with you that from what was described, there are strong elements of dystopia rather than utopia.  But our viewing window was very narrowly aimed and focused.

    Edit: Perhaps the term you were searching for is [Eudaimonia](https://en.wikipedia.org/wiki/Eudaimonia)?  One of the ideas being that in a true utopia, people should grow wiser and nobler.  But, perhaps, did that happen at the end of the story anyway?
    ```

    - u/None:
      ```
      Derpity derp, ok, so our view was *supposed* to look trivial.
      ```

  - u/Ilverin:
    ```
    The utopia is the happiness we feel inside. (for the record I didn't downvote you).

    Charles feels happy being productive and having elf-sex. Kit feels happy making others happy and also having lulz. 

    Personally, I like this utopia because happiness. In addition, for more serious concerns it seems like existential risks are reduced. It's likely any problems that come up that the AI can't handle can be worked on by people like Charles who want to be productive.
    ```

    - u/None:
      ```
      I like happiness too.  I guess I just figured there would be some... deeper sort of happiness.  In the same way that there's a deeper sort of physics.  You might not be interested in quantum mechanics, but it's *there*, holding up the everyday world that includes you and your own intuitive physics.
      ```

  - u/eroticas:
    ```
    I think in this case, the Allocator presents the vapid and boring version of what he could be doing (living in a boring simulation) as a *warning* to Charles, so that he doesn't choose it, and goes off into the stars.

    Had Charles ended up in a more compelling simulation he might have never figured it out and wireheaded. 

    >"Right??" And my blackrom hatecrush was totally justified. "I hate those worlds where everyone talks about how perfect they are and everything is also perfect and nothing ever happens. It's like, you have ultimate access to the fundament of your reality and you've decided the best use of your eternal time is to be smug." 

    Kit gets it, too.  and presumably some version of Kit is off doing Real Things too. The reason she's shallow is because the AI keeps *re-setting* her to an initial state, which she allows because she's the one who is best suited to play this role (give them the stars)
    ```

---

