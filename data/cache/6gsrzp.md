## [D] Monday General Rationality Thread

### Post:

Welcome to the Monday thread on general rationality topics!  Do you really want to talk about something non-fictional, related to the real world?  Have you:

* Seen something interesting on /r/science?
* Found a new way to get your shit even-more together?
* Figured out how to become immortal?
* Constructed artificial general intelligence?
* Read a neat nonfiction book?
* Munchkined your way into total control of your D&D campaign?


### Comments:

- u/alexanderwales:
  ```
  I've been thinking about marriage in the context of game theory and pre-commitment. Since it's relatively hard/painful to get unmarried, marriage is not just a commitment, it's a pre-commitment, since you're limiting your options going forward (in addition to your public declaration, which is itself a pre-commitment in the form of reputational loss etc.).

  The strongest argument that I've seen against marriage is that it's a legal/societal construct created for reasons that *probably* don't match up with what any individual specifically wants from that partnership, and the convenience/social/legal aspects of marriage don't make up for the benefits of being able to roll your own partnership contract.

  Prenuptials interest me from a game theory standpoint. If there's income/wealth disparity, then they act as a defection incentive equalizer, but either way they also decrease the disincentive to defect, since it's easier to get out of the marriage and break commitment. However, defection *within* the marriage is also a thing; if you know that someone has made a substantial pre-commitment, you can use that against them by e.g. being a shitty husband with the knowledge that divorce is very unlikely.

  (This obviously has some parallels to world politics.)
  ```

  - u/robobreasts:
    ```
    > if you know that someone has made a substantial pre-commitment, you can use that against them by e.g. being a shitty husband with the knowledge that divorce is very unlikely.

    I see you've met my wife.
    ```

  - u/TheStevenZubinator:
    ```
    You didn't happen to read this recent post on Death Is Bad, did you? It put many of the considerations you posed in my mind. 

    http://www.deathisbadblog.com/marriage-is-a-hostile-act/
    ```

    - u/alexanderwales:
      ```
      I saw the title linked somewhere and elected not to read it on the basis of that title. The general arguments are nothing new; I've had them on this subreddit a few times already, just usually not coached in game theory terms.
      ```

  - u/ben_oni:
    ```
    I've been thinking about this on and off for the past day, and it disturbs me. I've seen the term "pre-commitment" tossed about in this sub-reddit with some regularity, but now I'm wondering what it means -- or rather, what the people here think it means. What distinguishes a pre-commitment from a regular old commitment?

    For instance, I think of the "point of no return". A common scenario: "Once we've crossed this line, we will no longer be able to turn back. We will be committed to this course of action." This is the normal language used. Is your "pre-commitment" somehow different from that?
    ```

    - u/alexanderwales:
      ```
      Commitment is playing chicken and saying "I will not turn the steering wheel". Pre-commitment is throwing the steering wheel out the window so that turning is impossible.

      However, commitment devices differ in severity, with that being a more extreme case; a lesser example might be the difference between saying "I will lose 20 pounds" as a commitment, versus giving a friend $50 and telling them not to give it back to you unless they verify that you have lost 20 pounds. It's not absolute, since you can still fail, but the principle is the same.

      (I do see a lot of people get this wrong, or talk about pre-commitment without discussing any commitment device, sometimes in situations where there's no conceivable commitment device.)

      In the case of marriage, you're usually saying "I will be with this person forever" and then the commitment devices that cut off (or weigh down) future options vary on the basis of whether or not you sign a prenuptial, the terms of that prenuptial, the laws in your country/state, etc.
      ```

      - u/GaBeRockKing:
        ```
        > or talk about pre-commitment without discussing any commitment device, sometimes in situations where there's no conceivable commitment device.

        In this case, the commitment device is the very fact that you claimed it was a precommitment-- if you break it, you're breaking a promise. If you become known for breaking promises, you're no longer capable of making promises without having more stringent commitment enforcers than the threat of people no longer respecting your commitments.
        ```

- u/None:
  ```
  [deleted]
  ```

  - u/KilotonDefenestrator:
    ```
    I am firmly in the "would not teleport" camp. 

    I agree that a perfect copy would be an instance of me. But I see no point in terminating *this* instance, for any reason. 

    Like you say, this instance quite enjoys its existence. What is the value in ending it? While the net result would be just one me, the same that we started with, it also has added one death, which has a rather huge negative value. And if asked if they want to die, both instances would answer a resounding "no". Making it not just a death, but a *murder*.
    ```

  - u/eternal-potato:
    ```
    If you don't let instances diverge (halt simulation of the original before making a copy and restarting it at another place) there is simply never "another you" that can die.

    It's like, imagine you're​ on your computer and copied you favorite photo out of your photos directory into another one. And then, _without modifying either_, you somehow believe that by deleting either one of them you'll lose something important.
    ```

    - u/Sarkavonsy:
      ```
      And, as I said in my comment but will summarize here because your comment is currently higher rated than mine, if your instances DO diverge then they stop being the same person and it stops being okay to kill one.
      ```

      - u/OutOfNiceUsernames:
        ```
        IMO, it’s a “sliding scale of the surveyees’ emphasizing” problem. That is, how much, in the person’s opinion, should minds A and B differ from each other for the person to consider them two separate entities. 

        There was a nice demo on this in Doctor Who’s [*Christmas Carol* rendition.](https://en.wikipedia.org/wiki/A_Christmas_Carol_\(Doctor_Who\)) In this story, the antagonist was the only person whose commands were accepted by a certain mind-reading machine, so the Doctor uses his Therapy no Jutsu and time-travel shenanigans to convince him to help them out in solving the story’s crisis. Only, by the time the antagonist becomes convinced enough the machine judges him to be too divergent and doesn’t recognize him as the person entitled to issue the commands any more.

        So in terms of this DW episode, different people would have different criteria for their “mind-diff subroutine”. Some would consider it a murder even all the difference between two instances of the “same” mind were that one has been shown a card with a square on it while the other a card with a circle. And some would tie the necessary amount of changes to things like key values, principles, etc.

        TL;DR: Uniqueness of a personality is in the eye of the beholder and all that.
        ```

  - u/Loiathal:
    ```
    I'm totally with you.

    It's fine for OTHER people to want to quantum teleport-- I'll never know the difference between the versions of themselves who get destroyed/created. But I rather like experiencing things, and I see no reasons why THIS INSTANCE of me would continue experiencing things after a teleport.
    ```

  - u/None:
    ```
    I think a big problem is that people can rationalize the moral implications of teleportation on an emotional level but as soon as you change a minor aspect it recontextualizes the problem and suddenly the "gut reaction" is completely different.

    Suppose that a teleporter transmitter consisted of a scanner, a destroyer and a data transmitter and a teleporter receiver consisted of a constructor and a data receiver.

    * If you do the classical startrek teleportation "thing" and scan->destroy->send->recieve->construct then people feel like consciousness is neither created nor destroyed and whatever gut-level "law of conservation" exists isn't violated.

    * If you however upload the data to a handheld data drive and "revive" the person months later then people think that's wrong.


    * If you have two teleporters next to each other and the destroyer glitches suddenly you have two people and it would be unethical to kill the earlier one.

    * If you wait until you know that the data packets have been received and the person successfully reconstructed before you engage the destroyer the same problem applies
    ```

  - u/Noumero:
    ```
    > I still stop experiencing everything if the brain I'm using gets destroyed 

    Why would that be so? Consider a copy of you uploaded in a computer. Suppose that copy would be able to transfer between computers at will. Destuction of the computer on which the copy was initially uploaded wouldn't kill him/her, if the copy already transfered from it at the moment of destruction. Thus, the continuity of consciousness would be preserved, even though the only thing that would survive is data.

    Or do you believe that the upload would experience death in the process of *transfer* between computers, in this case? What if that process is gradual? Imagine computers standing nearby, connected by a physical cable.
    ```

    - u/Loiathal:
      ```
      > Destuction of the computer on which the copy was initially uploaded wouldn't kill him/her, if the copy already transfered from it at the moment of destruction. 

      I think this depends on how the "consciousness" of the AI worked.
      ```

    - u/KilotonDefenestrator:
      ```
      > Suppose that copy would be able to transfer between computers at will.

      How would my consciousness *transfer* to my copy on my death? That seems very close to talking about souls. Identical information being somewhere else does not equal transferal of a live process.
      ```

    - u/DeterminedThrowaway:
      ```
      Thanks for your thought experiment, but now I think I'm even _more_ confused for the moment. Previously I could imagine replacing each neuron in my brain with another substrate, and as long as the process was gradual and each neuron functioned identically to the one it was replacing, there would be no way for me to really tell. I could be the same mind running on a different physical brain, no problem. But then your thought experiment made me recall the ship of Theseus for some reason, and it occurred to me that if the neurons that were taken out were assembled back into a brain again... well, I'm not even sure of all the implications just yet. It makes me feel incredibly weird, and I need to go think for a while now.
      ```

  - u/Polycephal_Lee:
    ```
    You've got to think about the nature of a self. It's like a song, it doesn't matter what speaker it's playing on, it's still the same song. Likewise, it doesn't matter which atoms make me up, or where in space I'm located.

    That being said, I would not get into a machine that promises to disassemble me. By all means, create the copy, but don't destroy any copies of the pattern.
    ```

  - u/ben_oni:
    ```
    This sounds like an issue that crops up in programming with some frequency. Equality.

    We start with some object, call it X. As long as we pass references of X around from place to place, all references refer to the same X, and are equal. We can even call X by the name Y if we wanted, and X = Y would still hold. You would continue to be you.

    But sometimes references are not sufficient. Sometimes we need to make a deep-copy of X. Now, Y, which is a deep copy of X, is equal to X in a structural sense, but not in a referential (shallow) sense. That is, references to X are not equal to references to Y, even though the data is identical.

    If X and Y are deep copies, and allowed to evolve, that is, the structure or data of one or both changes, then X and Y are no longer equal in any sense (so long as they aren't changing synchronously).

    But what if after creating Y as a deep copy of X, we immediately remove all references to X and zero out the memory location of X. X is gone, destroyed. Does the expression X = Y mean anything anymore? Since all references to X are gone, you can't even pose the question. For a programmer, it doesn't matter: we can rename Y to X and continue on as though nothing had happened. If you want X back, you can just copy Y, after all. Since they never exist simultaneously, and no information is lost, it doesn't matter whether Y is a deep or shallow copy of X.

    This happens behind the scenes all the time inside computers; The system needs to run a garbage collection pass, moves some objects from one section of memory to another (for defragmentation purposes), and updates the object handles. The executing program never even knows anything happened.

    Teleportation of this sort is nothing more than moving data in exactly this sort of way. I know you "think" you are unique, but computation is also data. So go ahead, get in the teleporter, you'll be fine. Unless you think you have a soul that is intrinsically linked to your particular collection of atoms? Or do you think the universe would be better off with two of you?
    ```

- u/Noumero:
  ```
  Is it possible to resurrect someone who suffered an information-theoretic death (had the brain destroyed)?

  The knee-jerk answer is no: the information constitutes the mind; the information is lost, the mind is lost. There's no *process* that could pull back together a brain that got splattered across the floor, as far as we know.

  It's possible to work *around* that by pulling information from other sources: basics of human psychology, memories of other people, camera feeds, Internet activity, etc., building a model of the person. The result, though, would probably only narrow it to *several* possible minds, different from each other in important ways. And even if someone who died yesterday could be reconstructed nearly-perfectly, what to do about random peasants of XVIII century that nobody bothered to write about?

  If we could resurrect nearly-perfectly every person who died in modern ages, we could use *their* simulated memories to guess at what people they met during their lives, cross-check memories of *all* first-level resurrectees, then reconstruct second-level resurrectees based on *that*. Do the same with third-level, fourth-level, and so on ad infinitum.

  But errors would multiply. Even if it's possible to reconstruct an n-level resurrectee with 80% accuracy based on (n-1)-level's information, third-level resurrectees would already be 49% inaccurate, and I suspect that the actual numbers would be even lower. That idea is impractical.

  ___

  But. The set of all possible human minds is not infinite. We have a finite amount of neurons, finite amount of connections between them, which means that there could be only a finite number of possible distinct human minds, even if it's a combinatorially large number.

  So, why not resurrect *everyone*? As in, generate every possible sufficiently-unique brain that could correspond to a functional human, then give them bodies? Or put them in simulations to lower space and matter expenditure.

  It would require a large amount of resources, granted, but a galaxy's worth of Matrioshka Brains is ought to be enough.

  This method seems blatantly obvious to me, yet people very rarely talk about it, and even the most longterm-thinking and ambitious transhumanists seem to sadly accept permanence of the infodeath.

  Why? Am I missing something? And no, I am pretty sure that continuity of consciousness would be preserved here, as much as it would be with a normal upload.
  ```

  - u/electrace:
    ```
    This is highly related to [Answer to Job](http://slatestarcodex.com/2015/03/15/answer-to-job/).

    Besides that, it's important to realize that every time you simulate someone, you're necessarily taking away simulated time from everyone else. And also, I'm not very convinced by "the area is technically finite, so a galaxy worth of Matrioshka Brains out to be enough" line of argument.
    ```

  - u/None:
    ```
    > So, why not resurrect everyone? As in, generate every possible sufficiently-unique brain that could correspond to a functional human, then give them bodies? Or put them in simulations to lower space and matter expenditure.

    Because most of those brain-states correspond to being randomly pulled out of your own place and time and shoved into this weird new one you never asked for.

    Also, "combinatorially large" quickly reaches "larger than the observable universe can handle".  Remember, it already does so for chess positions and Go positions.  "Possible human consciousnesses", even constrained by a very good structural model, is *waaaaaaay* beyond what the universe can handle.
    ```

    - u/vash3r:
      ```
      > Because most of those brain-states correspond to being randomly pulled out of your own place and time and shoved into this weird new one you never asked for.

      If I recall, this happens in one of the later parts of Accelerando.
      ```

      - u/SvalbardCaretaker:
        ```
        The matrioshika brain spawn also apparently have a project where they try to simulate the entire human experience phase room... Which seems far beyond computability.
        ```

  - u/Norseman2:
    ```
    >But. The set of all possible human minds is not infinite. We have a finite amount of neurons, finite amount of connections between them, which means that there could be only a finite number of possible distinct human minds, even if it's a combinatorially large number.

    The adult human brain has around 86±8 billion neurons. On average, each neuron in an adult human brain has 7,000 synaptic connections to other neurons. Adults retain about 1/2 to 1/10th of their synaptic connections from childhood.

    Even if you were cloning people and growing them under identical conditions so that every child starts off with identical neuron and synapse configurations, this would mean that by adulthood each neuron would be in one of at least 2^14000 possible states of synapse connections. As a result, your final set of minimal possible brain configurations is going to be at least 2^14000 × 8.6 × 10^10 × 0.5. You end up with 1.1 × 10^4225 possible combinations. There's only about 10^80 atoms in the observable universe. That's the best case scenario even assuming you're only working with brains that all started off exactly the same.
    ```

  - u/ben_oni:
    ```
    This, sir, is absurd.

    This is not resurrection of any sort. What you are proposing is to create intelligent entities at random. This is not resurrection. You would, create every permutation of everyone who has ever lived, and also everyone who never existed. And no way to tell the difference.

    A note to anyone proposing the resurrection of the deceased, "information-theoretic" or not: please consider the morality of resurrection before proposing it. It is not an objective good. The state of being dead is morally neutral, almost by definition. Think carefully before disturbing that equilibrium.
    ```

  - u/Cruithne:
    ```
    Someone wrote a story about it on one of the story threads here. I can't remember what it was called but one character claimed to be able to simulate all possible neuron combinations, 'reducing immortality to a search problem.'
    ```

    - u/Noumero:
      ```
      Yes. u/eniteris' [*The Immortality of Anthony Weever*](https://www.reddit.com/r/rational/comments/40uw4i/biweekly_challenge_immortality/cyxv2zp/). This is literally the *only* time I saw this idea mentioned anywhere that wasn't my mind.
      ```

      - u/eniteris:
        ```
        It's brute-force, and probably too resource intensive.

        Brute force storage of 1 bit per graph results in 10^400 bytes, whereas the number of atoms in the universe are ~10^80. You can probably reduce it, but that's just to store all the combinations. Running each one would take a lot more resources.

        Also, that's only limited to _unmodified_ human minds. When we start getting into transhumanism, we're going to have many more minds that won't fit into that mindspace.
        ```

        - u/Noumero:
          ```
          Sure, but how many of these combinations would correspond to a functional human mind? And to minds that were *distinct*, whose difference from some others wouldn't be just one bit or one unimportant memory? The number of human personalities should be significantly lower.

          > Also, that's only limited to *unmodified* human minds

          Irrelevant. We're talking about resurrection of people who died in ages past. If transhumans would have unrecoverable deaths in the future, we've already failed.
          ```

  - u/artifex0:
    ```
    >...generate every possible sufficiently-unique brain that could correspond to a functional human...

    I feel like the math may not work out for that.

    Imagine simulating every possible combination of a deck of cards- that's 52!, or about 8x10^67 possible states.  However, there are only 10^50 atoms in the Earth.  If it's possible to simulate every deck of cards with the material of our solar system, it would be pretty difficult.

    Of course, when it comes to minds, you could simplify the problem by only simulating some relatively infinitesimal, but important or representative subset of possible minds- after all, a person might think of two technically different but extremely similar minds as the same person.

    You could also get into some tough questions about where the line is between understanding a consciousness and simulating it actually is.  If an AI has a perfect conceptual model of a mind, to what level of detail does it have to imagine that mind before it can be called individually conscious?  What if an AI has a perfect abstract understanding of the sorts of minds that can arise?  How abstract does something have to be before can no longer be called a consciousness?  Depending on what consciousness actually is, you might be able to get away with simulating some abstract concepts instead of a lot of individual mental states.

    Even so, I think it's easy to get over-awed by the vastness of the universe and our relative insignificance, and mis-judge how simple it would be to do something like simulating every possible mind.
    ```

  - u/ShiranaiWakaranai:
    ```
    Hold up, you're assuming humans are just their number of neurons and their connection patterns. That doesn't seem like a valid assumption to me. For one thing, we already know about DNA molecules, so two people with the exact same configuration of neurons can still be very distinct humans if their DNA molecules are different. 

    I also suspect that positioning is going to be extremely important here. The slightest shift in the position of an atom could manifest in large behavioral changes. We already know this because of things like prion diseases and chemical imbalances and various enzymes. Therefore, the set of all possible human minds could actually be infinite, since you can keep moving things around in infinitesimally small units.
    ```

    - u/scruiser:
      ```
      >  The slightest shift in the position of an atom could manifest in large behavioral changes. 

      If that's true, then just thermal noise and slight differences in stimuli could also make large behavioral changes... which I suppose I don't have empirical evidence against this, but it seems to violate my intuitions about human behavior.
      ```

  - u/CCC_037:
    ```
    There are more optimisations possible. First of all, you only need to simulate any individual brain for a single clock cycle. (Why? Well, after that clock cycle, it's still a viable mind - which will turn up somewhere else in your simulation). You *could* run an algorithm that will eventually run all possible brains with all possible inputs - and thus, over the millenia, simulate every possible human life (exception: you'd have some maximal brain complexity for the simulation). However, this has two problems: first of all, you are also simulating every possible form of torture (an ethical problem) and secondly, you are simulating an unreasonably large amount of data (a computing problem). Fortunately, these two problems can be solved; if you're a superintelligent AI, you can presumably calculate in advance how 'good' a given mindstate will be (for some metric of 'good' which rewards happiness and prevents torture), and then simulate mindstates from the most 'good' on down, perhaps to some arbitrary limit.

    As far as the simulated mindstates go, they will simply live - from an external viewpoint, in a staggeringly nonlinear temporal fashion, this mind existing for one instant *now* and another instant ten years in the future followed by an instant that had been simulated twenty centuries in the past, but they won't notice that - they will simply live, believing themselves to be, well, wherever their simulated senses say they will be. In times of torture, pain, or other things decided to be 'Bad' by the simulation, they will simply... not exist, coming smoothly back into existence once the simulation again declares them sufficiently 'good'.
    ```

  - u/lsparrish:
    ```
    One possible reason not to do it is if there is disutility associated with someone having a fake past. The number of people whose past is genuine generated in such a system would be a lot lower than those whose memories are fake.

    Also, assuming they are all placed in cohesive worlds, each person, even if assuming their own past is accurate, could still be virtually certain that the people they are interacting with in particular (despite being indistinguishable) all have false pasts to some extent. This would be true even in the subset of worlds where everyone's past is in fact accurate, i.e. they would (falsely, as a special case) have every reason to suspect their reality to be fabricated.

    Another nontrivial issue would be that you'd be instantiating a bunch of memories of suffering that never happened historically. Fake memories of suffering might carry a huge amount of disutility relative to only historical suffering.

    Still, if the alternative is everyone just randomly awakening for brief instants as Boltzmann Brains, it might be better. You could at least limit the memories to suffering that is actually possible in realistic historically consistent physical universes, which would be a tiny subset of total possible hells.
    ```

  - u/dirk_bruere:
    ```
    https://ieet.org/index.php/IEET2/more/bruere20121015
    ```

- u/General_Urist:
  ```
  I found on /r/askreddit a [surprisingly thought-out plan for how a single person might uplift a society, given said person is immortal.](https://www.reddit.com/r/AskReddit/comments/6hl62g/youre_given_immortality_but_as_a_cost_you_are/dizmx83/)

  What do you guys think? Workable, or critically flawed?
  ```

---

