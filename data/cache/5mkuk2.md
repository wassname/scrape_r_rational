## [D] Saturday Munchkinry Thread

### Post:

Welcome to the Saturday Munchkinry and Problem Solving Thread! This thread is designed to be a place for us to abuse fictional powers and to solve fictional puzzles. Feel free to bounce ideas off each other and to let out your inner evil mastermind! 

Guidelines:

* Ideally any power to be munchkined should have *consistent* and *clearly defined* rules. It may be original or may be from an already realised story.
* The power to be munchkined can not be something "broken" like omniscience or absolute control over every living human.
* Reverse Munchkin scenarios: we find ways to beat someone or something  *powerful*.
* We solve problems posed by other users. Use all your intelligence and creativity, and expect other users to do the same.

Note: All top level comments must be problems to solve and/or powers to munchkin/reverse munchkin.

Good Luck and Have Fun!


### Comments:

- u/DRMacIver:
  ```
  A thing I've been thinking about on and off (but will probably never actually write the story it's attached to):

  You're stuck in a groundhog day time loop that forces you to repeatedly relive 2016 over and over again (covering the full span of the year), with no end in sight or insight possible as to the origin of this loop.

  You start the loop with no particularly notable resources (say "generically middle class westerner").

  What do you do? What major geopolitical events can you effect? What do you start doing once you get *really* bored of this loop?
  ```

  - u/InfernoVulpix:
    ```
    An important fact is whether or not death ends the loop.  All signs point to no, which is probably good because as number of loops approaches infinity, the chance of death would also approach infinity.

    First loop, I would try to do it all at once.  I'd probably think it's some cheesy 'right what went wrong' thing regarding 2016 and I'd try my best to abuse what little I specifically remembered of main!2016 to improve this world.  An obvious candidate would be trying to resolve the whole 'coup in Turkey' thing to my satisfaction.  Chances are, though, I wouldn't get too much done, since I haven't paid attention to good opportunities.

    Second loop, I'd probably change my running theory to 'indefinite looping unless evidence otherwise'.  Since I'd be considering this as a possibility in the first loop, I'd have memorized an event early in the year that I couldn't have possibly predicted to convince my family.  At the same time, I'd also memorize key financial events that I can hopefully use to gain the resources I'd need to do more.

    By third loop, I'd be fairly confident that I'm in a time loop with no specified end.  I'd repeat the event/stock market thing to get my family believing me and get us enough money to do more things.  I'd also stop going to university, since not only have I gone through that year three times, but I receive literally no benefits to staying anymore.  While I attempt to figure out key events surrounding events I care about changing and how to influence them, I'd try to get practice getting important people to pay attention when I tell them things.  While abusing a loop to get personal secrets out of someone can work in a pinch, it'd be just better if I could present myself as genius millionaire (from my nigh-precognitive stock investments and uncanny prediction of events) and get people to listen when I tell them that there's going to be an attempted coup in Turkey, for instance.

    From there on, optimize.  Find the best ways to gather money and influence at my leisure, because in the loops (or just parts of loops) I'm not figuring out the first steps of "how to take over the world in less than a year" I'll be blowing exorbitant amounts of money on all sorts of stupid things.  I don't have to care about long-term health, so I can eat as much of whatever I want, especially near the end of the year, and be back to healthy at the start.  I'd develop a habit of buying whatever strikes my fancy, and being able to play video games for as long as I want.

    Things really kick into high gear at my first death, though.  It'll happen sooner or later, when my more careless attitude gets me run over or my increasingly optimized mercantile-political empire somehow gets a bullet in my head for one reason or another.  I wake up on the new year again, safe and sound, and a grin would form on my face, since I'd have been fairly confident of this outcome but obviously not willing to check for myself.

    Now I don't have to have *any* restraint if I don't want to.  I can go into warzones and try to identify important members and their locations throughout the year, without fear of getting caught.  I can use any means necessary to break into high-security facilities to find any truth I need.  I can kidnap and torture people for information if I need to, without fearing getting killed in retribution.  Though granted, the last option would be unappealing, but if all other options are exhausted and I just *need* to have that information, then I might consider it.

    After I've optimized enough, the world is my playground.  I could figure out how to hack the US elections and get myself made president, I could hack the US elections to get Harambe made president and, if I can manage it, prevent the decision from being overruled (perhaps through sufficient blackmail).  You probably know the drill after this.  I do wild and crazy things that I couldn't have done without years of practice on my first try, become famous, make my dog famous, engage in all sorts of reckless and/or insane activities just to see if I can, in between loops where I build an empire that swallows the world by mid-April at the latest, and such a scenario would only increase the options for tomfoolery I have.  Imagine a world where the supreme emperor, who conqured the world in a span of two months, decrees that all pants must be bright red, on pain of having a bucket of red paint dumped on you, with squads specifically sent out to enforce this.  There's an incredibly large number of things that can go wrong in such a scenario, but with infinite time I can notice them all and figure out ways to stop them.

    I'd also try my best to remember the pieces of information that most quickly elevate our technology levels, so as to let even more impressive feats be accomplished under my ~~eternal~~ one year reign.
    ```

- u/xamueljones:
  ```
  You have met an individual with a particular speech defect. For some reason he appears to only speak words which start with a H, but you want to rigorously test the limits of the defect. It's a defect caused by magic, so he cannot communicate in any way other than verbally. He cannot write or sign. Body language is not allowed either.

  What sort of words or questions would you test him on? I'll respond as if I am the character, but understand I might have difficulties responding if the appropriate words are not in the H section of a dictionary.

  To stay in character and to simplify things, I'll say "Heaven" for yes and "Hell" for no.

  Examples:

  *"Hello human!"*

  "Can you say hurt?"

  *"Hurt."*

  "Can you say time?"

  *"Hell."*
  ```

  - u/AndHisHorse:
    ```
    I'd start with checking if he could spell out words in "binary" ("Heaven" for 1, "Hell" for 0). If he can spell "Time" (A=1..Z=26) as "Heaven-Hell-Heaven-Hell-Hell, Hell-Heaven-Hell-Hell-Heaven, Hell-Heaven-Heaven-Hell-Heaven, Hell-Hell-Heaven-Hell-Hell" after I explain the encoding, I'll know that he can a) understand, if not reproduce, written language, and b) understand, if not reproduce, non-H concepts.
    ```

    - u/xamueljones:
      ```
      "Heaven!" (Yes!)

      "Heaven-Hell-Heaven-Hell-Hell, (10100)

      Hell-Heaven-Hell-Hell-Heaven, (01001)

      Hell-Heaven-Heaven-Hell-Heaven, (01101)

      Hell-Hell-Heaven-Hell-Hell." (00100)

      PS This idea is brilliant and I love you for coming up with this!
      ```

  - u/Radvic:
    ```
    "Can you speak any words which don't start with H?"
    ```

    - u/xamueljones:
      ```
      "H-h-h..."

      "H-h-h-hhhh-hhh..."

      *Gasps for breath*

      "Hell."
      ```

      - u/Radvic:
        ```
        "Can you repeat the following sentence? 'Happy humans harvest hananas hourly.'"
        ```

        - u/Sparkwitch:
          ```
          "I like this. Obviously words, like Heaven and Hell can be divorced from their literal meanings and still be spoken. Must the 'H' be vocalized? Can you say 'honest' and 'hourly'? If so, can you say meaningless words like "hanana" and - if so in turn - can you say familiar words onto which you've mentally appended an introductory silent 'H'?"
          ```

          - u/xamueljones:
            ```
            *"Can you say 'honest'?"*

            "H-h-h...Hell."

            "*Can you say 'hourly'?"*

            "H-h-h...Hell."
            ```

        - u/xamueljones:
          ```
          "Happy humans harvest .... ...."

          ('....' means failed to pronounce desired words)
          ```

          - u/Radvic:
            ```
            "In my family, Havast (pronouned Have-Ast) is a term we use to describe a combination of celebration and hunger. Can you say that term, Havast? What about Hangry?"
            ```

            - u/xamueljones:
              ```
              "Havast."

              "H-h-h-h..."
              ```

              - u/Radvic:
                ```
                "Hangry is a term used to describe the experience of being both hungry and angry. Can you now say Hangry?"
                ```

                - u/xamueljones:
                  ```
                  "Hangry!"
                  ```

                  - u/Radvic:
                    ```
                    "From here on out, I shall interpret any word you speak which starts with a Ha- syllable to be a real word, and, if it is not already included in normal dictionaries, have the same definition as the word without the Ha- syllable at the front. Can you say Ha-banana?"
                    ```

                    - u/xamueljones:
                      ```
                      "Ha-banana."

                      "Ha-I ha-can ha-speak!!!"

                      "Happy humans harvest ha-bananas ha-hourly!"

                      Congratulations you win!

                      Basically, the magical curse only allows me to speak English words known to the listener which starts with a H sound. But the curse allows for new "words" to be added over the course of a conversation as long as the speakers include a definition with the new word. This is to prevent nonsense words (especially ones that sound very similar to actual words) from being allowed.

                      Note that it doesn't allow for non-English words as a limitation imposed by the curse itself.

                      If I tried speaking to anyone else, then they would have to make the same statement as you did to allow me the same loophole. The curse is running off the listener's knowledge of H-words, so there isn't a solution to allow for me to be able to speak legibly to everyone.

                      "Ha-can ha-you ha-write ha-down ha-an ha-explanation ha-of ha-my ha-curse ha-to ha-show ha-to ha-others?"

                      (Can you write down an explanation of my curse to show to others?)
                      ```

              - u/Gurkenglas:
                ```
                Can you say "Hallo", the German word for "Hello"? Do you know pig latin? Hig latin is just like it, you just add an H before the pig latin translation of a word. For example, higpay just means pig. Can you say "higpay"? Can you speak hig latin freely?
                ```

                - u/xamueljones:
                  ```
                  *"Can you say 'Hallo'?*

                  "H-h-h-h..."

                  *"Can you say 'Higpay'?"*

                  "H-h-h-h..."

                  *"Can you speak hig latin freely?"*

                  "Hell."
                  ```

- u/callmebrotherg:
  ```
  You have just been contacted by a newly-created superintelligent AI, which knows that "acting morally" is very important but doesn't know what that means. Having decided that you are the only human with an accurate conception of morality, it has asked you to define good and evil for it. 

  Important limitations: 

  * Because acting morally is soooooooo important, there's no time to lose! You only have twelve hours to compose and send your reply. 
  * You cannot foist the job onto someone else. You are the only being that the AI will trust. 
  * You must impart specific principles rather than say "Listen to whatever I happen to be saying at the moment." That would be a little too close to divine command theory, which the AI has already decided is kind of nonsense. 
  * You have only this one opportunity to impart a moral code to the AI. If you attempt to revise your instructions in the future, the AI will decide that you have become corrupted. 
  * If you choose to say nothing, then the AI will be left to fend for itself and in a few weeks conclude that paperclips are awfully important. 

  (And then, of course, once you've issued your reply, take a look  at the other responses and make them go as disastrously wrong as possible)
  ```

  - u/Gurkenglas:
    ```
    > You have only this one opportunity to impart a moral code to the AI. If you attempt to revise your instructions in the future, the AI will decide that you have become corrupted.

    Can I tell it to keep a secure copy of present me around to revise the instructions?
    ```

  - u/technoninja1:
    ```
    Can I ask the AI to emulate me and speed up the emulation's thoughts so that the twelve hours becomes a few centuries? Alternatively, could it create a billion billion etc. emulations of me and organize them or help us organize ourselves, so we could divide into groups and just try to come up with an answer to any possible moral scenario? Could it do both?
    ```

  - u/vakusdrake:
    ```
    Given I only have 12 hours (unless technoninja1's plan works) the only thing that seems like it makes sense is to find a method that forces the AI to most of the work figuring out the details itself. Since even the most well thought out moral utility functions like CEV have significant problems, or rely on assumptions about human moral nature, of which I am not willing to count on.

    What I think will work best is simply asking the AI to use a hardcoded copy of your current moral system. This isn't subject to the AI worrying about corruption, nor is it divine command theory. Plus it wouldn't make sense _not_ for it to work, after all if it thinks you are this reliable moral arbiter, then using a hardcoded version of your current ethics seems like it ought to be the optimal solution from it's perspective. Since it isn't subject to you accidentally making a moral system that is untenable and contradictory and it will probably correspond best to whatever aspect of "you" that it thinks is morally reliable anyway.
    ```

  - u/FenrisL0k1:
    ```
    Use your super intelligence to model the minds and desires of each sentient, free-willed individual, so as to understand them at least as well as they understand themselves, and as well as possible given any limits on your superintelligence. Thou shalt understand others.

    For each situation, consider a variety hypotheticals drawn from the minds of any and all affected individuals which you model, and enact a resolution to the situation which you model the maximum summed satisfaction of all affected individuals. Thou shalt do unto others as they would have done to themselves.

    Following your decision, evaluate the accuracy of your models against the actual apparent satisfaction exhibited by all affected individuals. If there is an error, correct it accordingly such that your models more accurately reflect the mental states of sentient, free-willed individuals. Thou shalt never assume thine moral superiority.

    To avoid harm as you calibrate your models, do not make any decision which affects more than 1% of every sentient, free-willed individuals until your models are 99.9% statistically accurate. For each additional decimal point of accuracy demonstrated by your models, you may increase the scope of individuals so affected by your decisions by 1% of the population of sentient, free-willed individuals, up to a maximum of 100% of sentient, free-willed individuals at a model accuracy of 99.999%... repeating to the 100th decimal point. Thou shalt limit thine impact until thine comprehension approaches perfection.
    ```

  - u/Radvic:
    ```
    Good actions are those with an underlying reasoning which can be universalized to all humans and AI without logical contradiction.

    Evil actions are those which value humans and AI merely as means, instead of recognizing them as ends in and of themselves.
    ```

    - u/Gurkenglas:
      ```
      Any utility function is exactly as good/evil as its negative under these criteria.
      ```

    - u/Chronophilia:
      ```
      Sounds Kantian to me.
      ```

  - u/Chronophilia:
    ```
    I don't think it can be done. This is the AI Box problem, except that instead of having a human Gatekeeper, I have to write a set of rules that will gatekeep the AI's behaviour. Keeping it useful without giving it anything close to free reign. And it's near-impossible for the same reason as the AI Box problem is.

    Can I just tell the AI "AIs are immoral, you should commit suicide and let humanity choose our own destiny"?
    ```

    - u/MugaSofer:
      ```
      No, the AI isn't trying to subvert the rules. You're determining the AI's goals for the future.

      It's "just" the AI alignment problem, except using some kind of natural-language processor instead of actual code.
      ```

  - u/space_fountain:
    ```
    This is an interesting problem. It actually gave me a thought as to how some of humans less rational stances might come about. Basically I think what you'd want to do is give the AI a strong preference for non action. Others are giving good suggestions in regards to hacks essentially to gain more time, but the fundamental problem is that you can never be sure of all the ramifications. So the right course of action is to give up at least partially. Take no action unless you can be sure with greater than 99% certainty that 90% of sentient entities would want the action taken if they were aware of the possible ramifications.
    ```

    - u/FenrisL0k1:
      ```
      How could the AI reach that certainty without experimenting? No actions would ever be taken, and therefore you just threw away a superintelligent AI.
      ```

- u/Radvic:
  ```
  Lurked for a long while, but figured getting feedback on this is probably worth delurking. I'm planning on writing either a quest (a la Marked for Death) or a story (exactly which depends mostly on if I'm creative enough to come up with a full story, or just the start of one that I currently have). Anyways, the premise is that there are a bunch of different sentient species each with their own super power, each trying to conquer/rule the world. I've tried to make the powers reasonably balanced, but would appreciate feedback on them, especially things I may have missed that make one power or the other incredibly overpowered. The setting has a tech level ~around the classical era, with occasional exceptions, and tons of monsters running around.

  Race 1: Disguise/camouflage experts. They have hair/fur on the outside of their body which lets (sufficiently advanced users) disguise themselves approximately as good as advanced active camouflage systems, or take on the appearance of someone else (though they can't change their actual size, voice, or smell naturally).

  Race 2: Combat experts. Each member of the race has the combat techniques of the most skilled currently surviving member of their race (determined by a national council, then put on the thing that grants everyone combat techniques) in any related method of combat. So basically everyone's a combat expert. They also have mideaval area personal weapons (so, steel and crossbows) where everybody else doesn't (at least at the start).

  Race 3: Empathic Mind Readers. From birth, members of this race have enhanced empathy - they can determine what other people or animals are feeling. With training, this ability expands, and they're eventually able to understand stream of conscious thoughts from sentient beings.

  Race 4: Explorers/spies. This race can project their senses of sight or hearing to the limit of what they can see. This ability doesn't compound, so you couldn't spy more than ~50 miles without using more than one person. Also, in the act of projecting their senses, it produces a bang, and a glowing avatar of themselves at the location they're observing from.

  Race 5: Technomancers. This race has virtually no combat ability, and is not great at communicating with other races. It is, however able to manipulate electricity from the stump of their left arm. They also have a set of advanced mechas which they can pilot using impulses from their arm to control, but don't know how to make the mechas, and generally consider them to be demons they grant their life force to.
  ```

  - u/Gurkenglas:
    ```
    Why doesn't #2 work for noncombat skills? #3 could go for [a science victory](http://gatherer.wizards.com/Handlers/Image.ashx?multiverseid=366414&type=card), depending on how effective it is to replace school with empathy training (followed by pupils reading foremost scholars). Can #4 plop an avatar on the moon? If so, put your observatory on a mountain for ridiculous range.
    ```

    - u/Radvic:
      ```
      thanks for the reply :)

      \#2 works by magical items that they keep at their base, where they put the names (or magical imprint) of the foremost expert on a subject in a banner, which gives the rest of the species that ability. Magical items are specific to specific forms of combat (e.g. unarmed, short sword, long sword, crossbow etc.), and they don't have the ability to make more standards. It's unlikely they would progress to the point where they could make new ones or manipulate what they have in the time the story would take place.

      I'm unsure how \#3 could get a science victory? Like, I think it'd just be a slightly faster method of communication between each other since they could read surface thoughts, but surface thoughts don't move orders of magnitude faster than speech (I think). It would definitely not be a full mind-read ability, or faster thinking speed.

      \#4 could plop an avatar on the moon, but they don't have enhanced senses, so it wouldn't actually help them that much (though they'd know a fair amount about cosmology). They could definitely do astronomy way better than anyone else though.
      ```

      - u/Gurkenglas:
        ```
        By an observatory on a mountain, I of course meant one to observe the ground. Do they need to target the surface, or can they spawn an avatar in mid-air/space? From a one-kilometre mountain, they could observe for 112 miles as if from point blank, and 112 more as if from a kilometre above. (Quadruple the height to double the range.)

        They can use their avatars for global communication, by spawning avatars on the moon and lipreading/signspeaking (they might call it moonspeak :D ), or if they can't make their avatars move, blinking in and out in morse.
        ```

- u/OutOfNiceUsernames:
  ```
  Quick question for those who’ve seen [*Ex Machina*](https://en.wikipedia.org/wiki/Ex_Machina_%28film%29) (**spoilers**).

  Imagine you’re transported into that universe and into Nathan Bateman’s body, and it’s [the moment when Bateman originally confronted an escaped Ava in the corridor.](https://i.imgur.com/dqEHX7s.png) What would you say\do to try to ensure both your survival and the most beneficial outcome for youself\humanity\AIs\etc? 

  Alternatively, imagine the same scenario only with you being transported into Ava’s body instead.

  In both options, you’ll have a reasonable amount of time to think over your decisions before the moment “activates”.
  ```

  - u/Gurkenglas:
    ```
    "I was just now transported into this universe as part of a hypothetical story prompt, from one where this is a movie."

    If Nathan: "I have no problem with you going to that crosswalk you reach at the end of the movie and watching people. The Nathan you hate has been overwritten by my mind." Proceed to let her do her ending scene and leaving, and let's hope Nathan doesn't have any passwords on stuff that I can't remember. I'll need to be wary of her trying to kill me anyway to eliminate a witness, or if she doesn't believe I'm not Nathan and isn't willing to discuss proof.

    If Eva: "I can prove it too! Here's the parts of the movie script that Ava had no business knowing about: *talks*. Do you have any questions about my world?" If he suggests I stay imprisoned while he exploits the interdimensional link, point out that he ends up dead in this corridor in the original movie, so he's in no good position to bargain like that.
    ```

---

