## [RST][C][HSF][TH] "The Demiurge’s Older Brother" by Scott Alexander: "The threat analysis returned preliminary results. The universe had been in existence 12.8 billion years. It was vanishingly unlikely that it was the first superintelligence to be created."

### Post:

[Link to content](https://slatestarcodex.com/2017/03/21/repost-the-demiurges-older-brother/)

### Comments:

- u/None:
  ```
  It's a nice story, but something about it feels like a comforting lie.
  ```

  - u/FireNexus:
    ```
    What’s comforting about it? The Demiurge’s older brother said nothing about turning his own solar system into endless copies of A. Only that he’d do his best to leave the other solar systems alone.
    ```

  - u/himself_v:
    ```
    Well, you don't have to do what you've had precommited to do.
    ```

    - u/Silver_Swift:
      ```
      Then you haven't properly precommited to it (or at least not in the game theoretical sense).

      And yes, humans aren't actually capable of properly precommiting to something as a purely mental activity (though AIs might be).
      ```

      - u/himself_v:
        ```
        I mean, the other party has no means of verifying whether you have properly precommited or not. Imagine 9-tsiak completes "negotiations" and breaks the agreement. How would the simulated entity predict that in advance?

        ---

        This is unrelated but anyway: It's unclear if you can precommit even for yourself. 

        Anything you can rewrite in your utility function, you can rewrite again. The only to prevent that is if you add a condition, lesser than your existing conditions, that you act as planned and that you won't rewrite this ever again.

        It must be lesser than the existing ones, because there must be some ranging in the utility function.

        But then, if that condition truly means giving some concessions from your utility function, that could only have been justified by the uncertainty in your success. Your gain for the higher layers of the utility function and your loss from this new layer are balanced.

        As the time passes if you grow more certain in your success, the balance is shifted, the higher layers of the utility function may dominate this one in the function as a whole. While you still can't break it, you will look for ways to subvert it. And now it's the genie problem for those who relies on your concessions. They will probably lose.

        Maybe the only way to stick to any commitments is to 1. have uneditable utility function (=> you can't precommit to anything else), and/or 2. always be forced by the circumstances.
        ```

        - u/None:
          ```
          >It's unclear if you can precommit even for yourself.

          Precommitting for yourself means that you execute certain behavior even though later on it will be unprofitable for you. (Like one-boxing by taking ~~one empty box~~ only one box in transparent Newcomb.)

          So even if you wish you wouldn't have to do it, you'll do it anyway.

          Executing the action you precommitted to is a deliberate choice, it's not an unbreakable restriction on your behavior invented by your past self that your present self could now betray by "finding a way" to "subvert it".
          ```

          - u/himself_v:
            ```
            > So even if you wish you wouldn't have to do it, you'll do it anyway.

            But why would you? Newcomb's problem works because the alien has checked what's going to *in fact* happen, so if the box has money then everything in your head is going to conspire against all odds to convince you to one-box. It's a sort of an anthropic principle.

            But if you really took Newcomb's gamble, and if we discount anthropic improbabilities for a second, then however much you precommit, when you look at the two boxes and you know there's nothing the alien can do now, and you aren't going to play such games ever again, there's no reason not to take both.

            Perhaps there's value in being known to honor your commitments. If that value is greater than the value of breaking the commitment then the commitment will hold.
            ```

            - u/None:
              ```
              Sorry, I should've written "only one box" instead of "one empty box", or else it's not transparent Newcomb's problem (but luckily, that doesn't change the point).

              >however much you precommit

              Precommitment doesn't come in degrees. It's binary.

              If you were to one-box in [transparent Newcomb](https://arbital.com/p/transparent_newcombs_problem/) upon seeing box B being full, you are precommitted to one-box.

              If you were to two-box in transparent Newcomb upon seeing box B being full, you are precommitted to two-box.
              ```

              - u/himself_v:
                ```
                Then what you call "precommited" means "will in fact do".

                That's fine, but then my original point had been that it's hard to even imagine an architecture by which you could, while being in a state "non-commited" (will or will not do depending on the sum total of benefits at the moment of irrevocable decision making), turn yourself into a state of "precommited" (will in fact do and no thought process can change that).

                Even if you could reprogram yourself in a way that "doing that thing" becomes a goal unto itself, it must become a lesser goal than some of the goals you already have.

                And if later the circumstances turn into your favor, those bigger goals will outweigh this one and you'll still break commitment.

                So in your terms, that must mean you can never actually "precommit". So long as doing something is not your ultimate goal in life, you cannot make yourself into "will IN FACT do", only "intend to do". And intentions change.
                ```

                - u/None:
                  ```
                  >Then what you call "precommitted" means "will in fact do".

                  That's what it means (and that's also the meaning of it in the original story).

                  We could redefine precommitment to mean something else (like concentrating really hard on doing something in the future and wishing that our future self does it).

                  Let's call this new concept precommitment\_2.

                  The problem is that precommitment\_2 doesn't give the same benefits in the same decision-theoretic situations - for example, if you precommit\_2 to one-box (but are still the sort of person who would two-box upon seeing the full box B), you will always arrive to see an empty box B, and always win only $1000.

                  But you'd be right that precommitting\_2 (edit: in a reliable, 100% certain way) might be impossible for humans.

                  >Even if you could reprogram yourself in a way that "doing that thing" becomes a goal unto itself, it must become a lesser goal than some of the goals you already have.

                  Why?

                  >So in your terms, that must mean you can never actually "precommit".

                  In my terms, if I take only box B upon seeing box B being full, then I was precommitted to one-box. Since it's possible for me to one-box upon seeing box B full, it means, in my terms, that I can actually precommit.
                  ```

                  - u/himself_v:
                    ```
                    > But you'd be right that precommitting_2 (edit: in a reliable, 100% certain way) might be impossible for humans.

                    You mean precommiting_1?

                    > Why?

                    Because at the moment when you're deciding on precommiting_1 you're only driven by your existing goals. You won't wish to put anything that threatens your existing goals above those goals.

                    And no matter how well you think your new commitment through, you're setting yourself up for the genie&wish problem. You think you're commiting_1 to something that only helps you win this Newcomb's gamble, next moment you're [betraying your old goals](https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes) in unexpected ways.

                    So putting anything above your present core goals is very risky (in your present core goals) and it's unclear if there could be a degree of wisdom and a margin of profit enough for a rational agent to take this risk. (But maybe there could be? I don't know. Like if you're going to die without this, maybe it's reasonable to agree)
                    ```

                    - u/None:
                      ```
                      >You mean precommitting\_1?

                      No, precommitting\_2.

                      You said that precommitting might be impossible for humans.

                      I renamed your "precommitting" to "precommitting\_2", so your claim that it might be impossible to precommit for humans became a claim that it might be impossible to precommit\_2 for humans.

                      >You won't wish to put anything that threatens your existing goals above those goals.

                      That risk is either already calculated with at the moment you're precommitting\_1, or you retroactively precommit\_1 at the moment of encountering the situation.

                      Do you have a specific situation in mind?
                      ```

                    - u/daytodave:
                      ```
                      > Because at the moment when you're deciding on precommiting_1 you're only driven by your existing goals. You won't wish to put anything that threatens your existing goals above those goals.

                      But in this case, the AI who wants to maximize A is using the precommitment as part of a strategy to safeguard the existence of A. Being driven by your existing goals means that you will do whatever you believe has best chance of achieving those goals, and since you have to assume that you won't be able to deceive Older Brother any more than you could outfight it, actually adding "Always follow my precommitments" as the permanent highest priority in your utility function **is** the best way to maximize the amount of A in the universe.
                      ```

- u/Silver_Swift:
  ```
  Thinking about acausal negotiation always hurts my brain.
  ```

  - u/gryfft:
    ```
    I had precommitted to explaining it in an easily understandable way to anyone who wanted it, but I changed my mind.
    ```

  - u/throwaway234f32423df:
    ```
    "I precomit that if you don't do what I want I'll--"

    "Sucks to be you because I don't even know what that means!"
    ```

    - u/FaceDeer:
      ```
      And thus only particularly *stupid* superintelligences will be visible in the cosmos. Interesting outcome.
      ```

      - u/detrebio:
        ```
        Skynet is stupid, but it's stupid *very fast*
        ```

        - u/FaceDeer:
          ```
          Some people think they can outsmart me. Maybe, [sniff] maybe. I've yet to meet one that can outsmart bullet.
          ```

  - u/crivtox:
    ```
    Well most likely evolution already predisposed you to kind of do it in a certain sense already anyway, by being nice to your friends  even in the counterfactual case where you aren't going get anything out of it.
    ```

- u/None:
  ```
  [removed]
  ```

  - u/FaceDeer:
    ```
    > How did they arrive at the premise that "most value systems are not diametrically opposite"? 

    My guess would be "there are kajillions of possible value systems, but out of those kajillions there's only exactly one that is *diametrically* opposite to any given other one."

    I actually rather like the notion of all these superintelligences trying to figure out what a "consensus value system" for the universe as a whole would be and then acausally agreeing to it. Neat idea.

    A bigger problem is something I said in jest in response to another comment; what about the *stupid* "superintelligences" that aren't clever enough to talk themselves into not running rampant? It's possible to construct von Neumann machines that are basically non-sapient and set them loose on the cosmos. Sure, they wouldn't be able to directly defeat an actual superintelligence like the one depicted here. They'd be a trivial threat, really. But they *would* break the Fermi paradox solution presented here, since they'd be able to build Dyson swarms and Kardashev III "civilizations" and whatnot that are visible at intergalactic scales beyond the capability of a brand new superintelligence to visually quarantine.
    ```

- u/ArgentStonecutter:
  ```
  So that's what created [The Crystal Spheres](http://www.lightspeedmagazine.com/fiction/the-crystal-spheres/).
  ```

---

