## Why can't AI safety be solved by giving AI limited, satisfiable goals?

### Post:

Hi! Maybe this is the wrong sub, but I feel like people here will probably be able to answer my question.

As far as I understand it, the problem with AI safety is that AI needs to have values/goals to do things, and if those aren't aligned with our values, it'll just keep pursuing them indefinitely sacrificing everything in the process(converting all the matter into infinite amount of paperclips).

But why can't we just replace the values it would pursue indefinitely with the goals that can be easily satisfied? Instead of "make more paperclips" value, can it just have "make 10 paperclips and then stop" value?

Real life intelligences don't always pursue their values indefinitely. People want infinite amount of money and sex for example, but can also have a goal like "climb everest" or "eat a sandwich", and then feel content/satisfied once they're done.

Can it's utility function just be "think for an hour, tell me something useful about how to cure cancer, and then shut down, whether you succeed or not"? If it's goal is to just "cure cancer", things could get out of hand if the problem is difficult, it might need to take over the world to gather resources it needs, or it might want to create more cancers in the world just so it could cure them. 

But if the utility function is limited to something specific, limited to resources AI has, and then it is satisfied and doesn't drive AI to do more of that thing, wouldn't that solve this problem?

Sure, it could go about getting rid of cancer in some unpredictable and horrific way(like by killing all humans), but then we just tell it "figure out the best solution you can given the limited amount of resources and tell us what to do". It wouldn't be incentivized to trick us into letting it out of the box because making the best use of the resources it has and then shutting down would be the thing it ultimately wants. And then human scientist uses his own judgement to make sure there aren't any other issues with the proposed solution.

Am I missing something here?

### Comments:

- u/None:
  ```
  Specifying a limited, satisfiable goal about something in the real world is in fact an extremely difficult problem. Figuring it out would take us a large fraction of the way towards creating an AGI that's safe even with goals that aren't limited, but we aren't close to figuring it out.

  How would you go about specifying (in mathematically precise language) what a paperclip is? What "making something" is? Or even what "stopping" is?
  ```

  - u/Veedrac:
    ```
    More than this, *not* specifying the full range of things we care about means the AI is indifferent to human wellbeing. It means if the AI did think that the best way to achieve its goal involved quickly wiping out the human race, or misleading its inventors and going dark, or duplicating itself onto every accessible computer system, or making a quintillion paperclip variants for redundancy, it would have no motive not to do so. When we're talking about trivial constrained goals like ‘make me 10 paperclips’ we *might* be able to reason over the complete space of reasonable possibilities (though ‘respect the unknown unknowns’ is always good advice), but as soon as we ask it to do anything intellectually sophisticated our ability to model it like that flies out the window. Generally, if your safety relies on knowing exactly what the AI is going to decide on doing before it decides it, you're adding no power but much needless risk by letting the AI do that deciding.

    Further, the difference between having a goal to solve a task (what most AGI is assumed to be doing) and incidentally being optimized to solve a problem (like current ML models) is very large, since the former comes with instrumental goals like ‘become more certain’ and ‘become smarter’ and ‘prevent the humans from changing their mind and asking me to do something else’. Those are dangerous.
    ```

  - u/Krabbyos:
    ```
    i'll add to this with some links (note: the arbital links don't seem to load for me at all with adblocker on, and sometimes fail to load at all even with it off)

    https://arbital.com/p/task_goal/

    https://arbital.com/p/task_agi/

    https://arbital.com/p/taskagi_open_problems/

    The type of ai op described would be termed "Genie" in the typology of Nick Bostrom's Superintelligence. Seems to be called "task-directed AGI" too.

    (edit: found a site that scraped the webpages of arbital and actually loads)

    https://www.obormot.net/arbital/page/task_goal.html

    https://www.obormot.net/arbital/page/AGI_typology.html

    https://www.obormot.net/arbital/page/task_agi.html

    https://www.obormot.net/arbital/page/taskagi_open_problems.html
    ```

  - u/SimoneNonvelodico:
    ```
    > Or even what "stopping" is?

    That seems quite straightforward to me. Stopping is not prosecuting any goal any more. I mean, I get that you need to make it a technical definition of some kind, but if the AI doesn't know what "stopping" means it's either dumb or a straight up malicious intelligence that's trying to lawyer its way out of constraints.

    In principle, if an AI is gated into a box, "stopping" is simply "turn this switch off". There aren't many ways around it.
    ```

- u/xachariah:
  ```
  You only need to fail once.  

  Your company has their AI set to make 10 paperclips.  Medical Company has theirs set to make 10 vaccines.  Shoe company has it set to make 10 shoes.  A gaming miniature company has it set to unlimited miniatures.  Whoops, looks like humanity gets replaced by little plastic army men.

  Additionally, the way business works means it'll always walk towards less restricted AI.  *Your company* has their AI to always produce 10 paperclips.  Cool, but your competitor <A> has it produce 20.  Hey looks like it's more efficient to make even more at a time because it allows the AI to perform better by searching for a better global maximum within the expanded scope; you go out of business.  New competitor <B> makes 40 at a time, so <A> goes out of business.  Fast forward 10 years, and Competitor <Z> is at 1,000,000,000 paperclips which is big enough to be achievable by modifying humanity's evolution towards a paperclip-philic path.  And this is all assuming that company <C> didn't see they'd lose out like their predecessors, and just remove the limits on their AI.
  ```

  - u/BlackKnightG93M:
    ```
    This very example is what terrifies me on a fundamental level with regard to AGI development. Rushing the process of building a fricking God to stay profitable and competitive in the market.
    ```

- u/multi-core:
  ```
  https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start is Eliezer Yudkowsky discussing the problems with trying to get an AI that just wants to do one bounded task. IIRC he thinks task AGI still *is* the way to go in the short term but not many of the problems have been solved yet.
  ```

  - u/Veedrac:
    ```
    I can vouch for this talk being good.
    ```

- u/None:
  ```
  Sorta. You're solving the problem by avoiding the problem. What you're talking about is basically a script, not an intelligence. General AI is viewed as scary for a few reasons which don't apply to the limited intelligences you describe. It only takes one person failing to be safe to accidentally unleash a maximizing intelligence on the universe. Solving *hard* problems requires a maximizer, in the sense that you want your AI to be able to self-improve so it can smart enough to solve the problem. Even if you give your general AI a "safe" goal, the same genie logic applies, where if you don't describe it well enough, it will maximize past the goal you thought you gave it. Et cetera.
  ```

  - u/lumenwrites:
    ```
    But we don't really want an "intelligence" as in a conscious thing with it's own goals and values. We just want a program that is smarter than us and can solve difficult problems for us.

    It can still self improve, just tell it to self improve using this specified limited amount of resources, instead of doing whatever it takes.

    Which only leaves the problem of evil/dumm humans recklessly wielding a powerful tool, but that's not really an AI safety issue, it's powerful technology safety issue. Sure someone could ask it to help them do 9/11 or feed them infinite pancakes, but here it's just a matter of making sure it doesn't get into the wrong hands, like we do with nuclear weapons, viruses, stuff like that.
    ```

    - u/None:
      ```
      If you want a thing that is legitimately smarter than you, you may as well call that thing an intelligence. That's what "smart" means. Doesn't matter if it's running on silicon or flesh.

      All AI safety issues are technology safety issues. The only difference is that with AI safety, if one mistake is made, it has a very real chance of ballooning to the point where it destroys the world. Similar to how people were once worried that a nuclear bomb being detonated would ignite the atmosphere and destroy all life on the planet.

      The limits you describe, even assuming they work when used, only need to not be used once. One mistake, and the GAI eats the planet. And once self-improving AIs can be made, it's only a matter of time until that one mistake happens. This is why the race to make a "good" GAI is portrayed as a race. The only genuine, permanent solution to the risk of GAI is to make the first one right, in such a way that it has an insurmountable advantage over all subsequent GAIs, and can therefore prevent them from eating us.
      ```

      - u/lumenwrites:
        ```
        If we have have 2 options:

        1. Try to create the aligned AGI that has good values, pursues them indefinitely, takes over the world, and steers it in the right direction making sure no evil AGIs are created in the future.

        2. Try to create specific AGIs with narrow goals that solve the problems we want AGI for, but are less likely to want to want take over the world and control everything, and then try to control how these AGIs are used.

        Is it safer to try to create an aligned AGI that indefinitely pursues some set of "correct" values, than to stick to creating AGIs with narrow specific goals that solve specific problems?

        Both seem dangerous, but I think the 2nd one seems safer, because in the 2nd case humans remain in charge.

        Although if AGI with runaway values is inevitable - then yeah, we have to figure out how to build the right one eventually anyway. But isn't it easier to first try to create a specific-goal-driven AGI instead of the indefinitely-pursuing-values AGI?
        ```

        - u/None:
          ```
          You fail to understand my point. The former is a solution, the latter is a holding action. Pursuing number 2 presents an illusion of safety and control, but all it really does is (at best) delay the inevitable. It may actually hasten the inevitable unleashing of a maximizer, because it advances technology and spreads knowledge about AI without sufficient safety measures in place to prevent that knowledge from being misused. (Although, in fairness, there are no sufficient safety measures when talking about world-ending threats.) Again, it only takes one misstep when using AI to destroy the planet.

          Lemme try a metaphor. Hypothetically, what if you could choose to do one of these things:

          1. Give every human being on Earth a small amount of magic. This magic could make their lives easier, make them healthier and stronger, etc. It's powerful and versatile, and it even has the benefit of being stronger on defense than offense, meaning it's not very useful as a tool to harm others. However, each time a person uses their magic, there is a very small, one in a trillion chance that they'll spontaneously explode and open a portal to a hell dimension inhabited by immensely powerful, sadistic demons, each of which can open more portals.
          2. Create a god. This god would be vastly powerful, easily capable of creating and destroying entire planets, with incredibly precision control. You can create this god with any properties you wish, but once it's made, it's irreversible. If you create it wrong, that will go very poorly. If you create it right, you've basically created a force of infinite good.

          I know, not a very subtle metaphor. The first choice offers an illusion of safety, because you're not upsetting the status quo too much, and you're giving power to humans. But in the end, it really represents a ticking time bomb, because the power in question can't be safely used by humans, by its very nature. The second choice is scary, because it has the potential to go much more horribly, permanently wrong. But, unlike the first choice, it has a very real chance of *not* going wrong, and being a genuine benefit. Neither choice is ideal, but only one has even a chance of not killing us all, in the long term.
          ```

          - u/lumenwrites:
            ```
            I think I do understand your argument, it does make a lot of sense. Thanks for thoughtful replies by the way, I'll need to think about this more.

            But one counterargument does come to mind immediately. What if creating any kind of AGI is difficult and will remain difficult? What if it'll take google-sized or government-sized amount of computing resources and competent people to make an AGI that works, instead of it being available to any average joe.

            In that case, wouldn't it be safer to rely on google engineers getting narrow-goal subservient AGI right, rather than a god-like AGI that takes over and does the right thing?

            Right now we can control nuclear weapons and dangerous viruses because they're very difficult to create. If anyone could make a nuke in their basement, the world would be destroyed already.

            **If** AI could only be created with a lot of effort from a lot of smart and competent people, and **if** it is much more difficult to create value-AGI than goal-AGI, wouldn't it be safer for people to stay in charge and try to do the right thing, and manage the risk the way we manage risks of all potentially world-destroying technologies, rather than trying to create a god that is aligned and controls all these things for us?

            I guess it comes down to this - what is more difficult, creating and controlling non-evil AGI, or controlling people and making sure they don't do stupid and crazy things. If there's only 1-3 corporations in the world that are capable of creating AGI, I'd probably bet on google engineers, if anyone can make AGI in their basement, we would need a value-AGI to prevent that.
            ```

            - u/None:
              ```
              That's a valid argument. It's something worth considering. If, hypothetically, you could demonstrably prove that it's incredibly hard to make an AGI, and it will *always* be hard, no matter how much technology progresses, that would be evidence in favor of your idea of limited goal-AGI as a serious long term strategy.

              But, since at present that doesn't seem to be the case (At the very least, there is no direct evidence to support it) we basically need to take the possibility of an AGI with a bad utility function destroying the world seriously, which means trying to figure out how to make a "good" AGI, despite the risks.
              ```

              - u/lumenwrites:
                ```
                Yeah, that makes sense. Thanks again for the awesome discussion! =)
                ```

                - u/None:
                  ```
                  Likewise. I had fun.
                  ```

- u/jtolmar:
  ```
  This is a useful safety mechanism but it's nowhere near sufficient.

  A task-oriented AI will pursue the most effective / fastest / easiest method towards reaching its goal. What method follows those criteria is unknown - when we're talking about AI safety we're generally talking about a hypothetical AI that is smarter than you, so its methods may be something you'd never have thought of.

  This leads to several confounding problems:

  1. The options on the table will surprise you. We always have hidden assumptions about acceptable goals, which are fine among humans who generally agree on these things, but need to be manually programmed into a goal. A tetris-playing AI learned to pause the game when it's about to lose so it won't lose. An AI tasked with piloting a robot to fetch a ball might decide to sit still until a researcher decides its buggy and brings the ball closer to debug sensors, then go for it. You can win at American Football by hospitalizing everyone in the other team as long as one of your players doesn't participate. You can kill any virus (or cancer) by killing the patient.

  2. What's easy may be surprising. Clever solutions change what's easy or even possible. If something like self-replicating nanomachines turns out to be unexpectedly easy, that opens up a lot more solutions with large amounts of collateral damage. Self-replicating computer viruses *are* easy. Building a smarter AI may be on the table.

  3. The goal itself may be surprising. Transcription errors happen. You may specify to make ten paperclips, specify the volume and mass of a paperclip in the process, and make an error that requires the "paperclip" to be made of denser, radioactive isotopes. The more complex the goal, the more numerous and subtle errors are available.

  Finally, task-oriented AI does not prevent excessive followthrough. Tell an AI to pilot a robot to a location as fast as possible, and the robot will accelerate all the way to that location, then go careening off in whatever direction because the task is over. In this very simple situation you can update the goal to be "stop at this point," but with more complex and useful goals the stopping criteria becomes arbitrarily complex. You can't just add "without side-effects" because that's no longer a task-oriented AI, it has a permanent goal.

  Excessive followthrough is particularly bad for any task where the easiest solution is to build a smarter AI to figure it out. If the goal is to make one million paperclips, and the AI decides to build a smarter AI to figure it out, the first AI can skip some unnecessary coding work (making its job easier) by skipping the "one million" limit and just building a paperclip maximizer. The maximizer will make one million paperclips on its way to filling the universe with them, so the original task-oriented AI has solved its goal.
  ```

- u/sdmat:
  ```
  Working in machine learning, I can assure you that your plan is perfect except for the part where you have to implement "limited", "satisfiable", and "goals".

  High level concepts don't exist in the world, we can't use them to control the behavior of AI. You have to build sufficiently precise correlates to the concepts from scratch, and this turns out to be much, much, much harder than it sounds.

  Leaving aside the overall utility function, let's talk paperclips. You'll need to build a representation of "paperclip" that approximates the concept well enough for manufacturing purposes. And that's assuming you actually understand what a paperclip is well enough - do you? (how thick is the wire? which alloy is it made of and why? how is the metal treated? what are the manufacturing tolerances?). If you feel that the AI should figure all this out by itself so that a few words or a picture of a paperclip is an adequate specification, then we're back to loosely constrained AGI.

  And if that's the situation for "paperclip", good luck with getting the AI to do what you mean by "make 10 and then stop".
  ```

- u/Nepene:
  ```
  1. Goal oriented AIs are economically effective, and are very good at solving particular economic problems if built well. Companies want to solve economic goals. You can certainly built limited goals, but because of economic pressure in the end people are likely to stretch those limited goals so that they approximate more open ended ones. 

  2. Goal oriented AIs are less vulnerable to human stupidity and corruption. If you build an AI to cure cancer, they will try to cure cancer. If you set regular limited goals there's more likelihood that heavy human intervention will cause problems- a drug company might tweak them to produce a more expensive cure, for example, or there might be poor coordination in their goals between people setting it up. 

  3. We have a lot of theorywork on how to build good utility functions, so there isn't necessarily a need to make very short ones.

  4. AIs which can self modify might wirehead. If you set up their goal to be happy when they answer you, they might wirehead to make themselves happy whenever they answer any question, and so sneak out onto the web. 

  5. They might act in annoying ways. Suppose you tell them to make ten paperclips. They might devote all of the world's resources to protecting those ten paperclips and verifying they exist.
  ```

- u/Roxolan:
  ```
  You got plenty of answers, so effectively this was the right sub, but the *appropriate* sub for this question is /r/controlproblem.
  ```

- u/Frommerman:
  ```
  You've told the AI that paperclips are important, and that making exactly 10 of them is important. Given sufficient creativity, it might conclude that being extra, super sure that it makes exactly 10 paperclips, no more, no less, is a task which deserves an amount of processing power producible only by transforming all matter and energy in the observable universe into computronium. That's the way to be the most sure it did the task correctly after all, right?
  ```

- u/MisterCommonMarket:
  ```
  Nick Bostroms book, Superintelligence, anwsers all of your questions. There are many problems with giving AI a goal of creating 10 paperclips and then stopping their creation. It would take pages to go through them all with nuance (read the book, it is amazing) so i will focus on two. 

  Lets say the AI creates the 10 paperclips. If one of them was destroyed, it would not have 10 anymore. This is not acceptable because its task is to make and be in possession of 10 paperclips so the AI gets rid of all threats that could destroy a paperclip and to make sure it always has 10 paperclips, it converts all matter that is not used to protect the paperclips, in to more paperclips. If some are lost or destroyed, it still has the required number.

  The second problem is a matter of probability and confidence (maybe not the right word for this?). The AI makes 10 paperclips. How sure is the AI, that it has made 10 paperclips and that it has 10 paperclips? Not 100% sure. It could have made a mistake. It's programming could be faulty. There might be a glitch. It's sensors might be mistaken. How sure are you, that the sun will rise tomorrow. Pretty sure right. But you would not give this event a 100% probability. Lets say the AI is 99.99999% sure that it made 10 paperclips. That means that it will convert all matter to more paperclips, to make sure the probability of it having 10 paperclips is as high as it could be. Even when it has converted the entire universe into paperclips it cannot be 100% certain that it has 10 paperclips. It could be hallucinating the entire experience.
  ```

- u/None:
  ```
  [deleted]
  ```

  - u/varno2:
    ```
    don't you mean

    DO [ RPMs++; ] WHILE [RPMs != X] //relevant difference is <

    as the wrong statement. the code written would not exhibit the bug expected.
    ```

  - u/Veedrac:
    ```
    Predominantly AI risk is about AI being too smart, not about it being too dumb. Until we know how to correctly make an aligned AI, semibroken AI will be less dangerous, not more, simply because they're less likely to be superintelligent.
    ```

---

