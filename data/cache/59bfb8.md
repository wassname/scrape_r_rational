## Who listens to Rationally Speaking podcast? Also thoughts on raising the sanity waterline

### Post:

I just started listening to it yesterday and it's really good. Also, episode 156 might give you some good ideas for raising the sanity waterline.

http://nycskeptics.org/storage/feeds/rs.xml

On another related note, something I've been wondering is how much should we should be raising the sanity waterline? On the one hand, a lot of people aren't very smart or don't have much time or energy for deep analytical thinking, and that could put a large constraint on their ability to learn and practice rationality skills. This could potentially cause serious externalities if we try to raise the sanity waterline for some people with insufficient mental health or ability, since it's dangerous to be half a rationalist, and telling people about biases can hurt them.

On the other hand, not raising the sanity waterline enough could also be very problematic because people not making sensible decisions in high stakes situations that affect lots of people.

So how much should we be raising the sanity waterline, or where should we be focusing our efforts to raise the waterline? Thoughts?

### Comments:

- u/TheStevenZubinator:
  ```
  I've been listening to this show for years and I always enjoy it. As far as best efforts to raise the sanity water line, that's a tough question. Julia Galef's non-profit, the Center for Applied Rationality, is working on investigating that question (as well as several others).
  ```

  - u/drewfer:
    ```
    I'd heard of CFAR through the podcast.  What are some of the other groups working on the question?
    ```

    - u/EricHerboso:
      ```
      You talked in the original post about raising the sanity waterline for the general population. Lots of groups do this, usually by working in a specific area, like general math literacy, or informing the public on better driving skills. Some also do general stuff, like the lifehacking community.

      But CFAR is really focused on increasing rationality at the high level. In this respect, they're fairly unique as an organization. Some of the skills they teach are complex enough that you wouldn't ordinarily expect them to be taught underneath the undergraduate level. Because of this, I wouldn't say they're increasing the sanity waterline so much as to focus on making the a smaller subset of the population significantly more rational.

      Of course, I say this only as an outsider looking at what CFAR does. For all I know, they may have a desire inside the org to reach out more to the general public.
      ```

      - u/TK17Studios:
        ```
        I'm the curriculum director at CFAR and would be happy to answer (most) questions.
        ```

        - u/iSuggestViolence:
          ```
          uh oh bud, you just outed yourself as a FF writer. I won't tell.  
          On a more serious note, does CFAR have plans to open more centers/have workshops in the CA central coast area? I have always wanted to attend, but never had the opportunity because of work obligations. Also, thanks for contributing to rationality education :)
          ```

          - u/TK17Studios:
            ```
            Yeah, but r!Animorphs is *snooty* fanfiction.  =P

            We don't have any plans to expand to additional centers in the foreseeable future, but a major goal of the next four months is to basically double our instructor staff, allowing us to run two or three times as many workshops in 2017.  Most of those will be within an hour or two of Berkeley, CA (and the time commitment is usually Wednesday evening through Monday morning, with a recommended day off Tuesday, so four days off from work).

            Thanks for the thanks!  It's pretty sweet to spend my time thinking about thinking and then finding ways to make it better.
            ```

        - u/nick5a1:
          ```
          Why doesn't CFAR offer any remotely taught classes? This could be as simple as recordings of one of the workshops.
          ```

          - u/TK17Studios:
            ```
            Two separate answers, seemingly contradictory and both true:

            1) It's a thing we're working on, via improving our write-ups (both in content and in ease-of-comprehension) and experimenting with other things like [videos](https://www.youtube.com/watch?v=YKpH-7fs9zE) and one-topic seminars.  Ideally, we'll have at least 3-4 pieces of broadly-disseminated and useful content out there in the next year.

            2) We've tried it, and it doesn't work.  Actual experiments, actual data.  I know where your "80% of the results" thing comes from, because I had the same belief going into my workshop, and it's only after seeing a variety of experiments first-hand that that belief has been shaken.

            There are several things at play, here (longer, in-depth discussion [here](http://lesswrong.com/lw/n5h/unofficial_canon_on_applied_rationality/ddco?context=3)).  First, the concepts interact with everyone's personal beliefs, preconceptions, problems, and goals differently, and you'd need something like fifty different versions to actually get the thing to land (and the difference between "mostly got it" and "really got it" is *in fact* enormous—you get something like *20%* of the results from an 80% version of the concepts).

            Second, there are issues with idea inoculation, where people will *think* that they get it, but in fact they don't, or they get something nearby, and this causes them to stop seeking and become complacent (and become resistant to further attempts to clarify or re-teach).  In-person transfer prevents this at a much higher rate than simply dumping the information online.

            Third, people just don't actually do the work—believing that Technique X is worth using with your System 2 (analytic, verbal, reasoning brain) doesn't lead to people remembering or putting forth actual effort, and instead leads to "Huh.  Neat." followed by *no change in behavior.*  The workshop environment, on the other hand, forces people to try things repeatedly until their System 1 (intuitive, nonverbal, emotional brain) learns "Hey, this *actually* makes a difference," which results in *some change in behavior for some people.*

            (Note that getting people to update in the direction of rationality is *actually hard,* and that we're in fact trying many things to see if we can improve outcomes in this direction.)

            There are other concerns in the mix, like intellectual property and continuing to sustain our research through workshop revenue, but those are honestly waaaay down the list, and make up something like 5-10% of our reluctance to publish.  Overall, the reason we haven't done a blitz of online education or published a ton of content is because we've tried it in the past in small ways, and it's actually worse for the world.  It makes some people get the wrong ideas, makes others "full" and "satisfied" and stops them from seeking further growth, and simply doesn't work at all for the rest.

            That being said, we have indeed occasionally provided our content (i.e. our ~200pg handbook) to those who've asked.  There are people who are deeply suspicious of "you just have to come and try it," and I can't really fault their suspicion, as it pattern-matches to a lot of things that ARE fake or bad.  But I'll note that NONE of the people who've just received our handbook and read it have reported leveling up, switching to better careers, doubling their output, etc. etc., which are things that happen fairly frequently after workshops.
            ```

          - u/None:
            ```
            Disclaimer: Not the guy you asked.

            I remember reading that the CFAR classes tend to be more group focused - there's a reason that the workshops are 5 nights long. It's a bit like math camp. If you just wanted to remotely teach somebody about biases and rationality, you would make the *Sequences*. (admittedly, this isn't in video form. I've been thinking about making a video series on the sequences for a little while, and it seems like the logical step for people to take)
            ```

    - u/TheStevenZubinator:
      ```
      CFAR is the only organization I know of that specifically focuses on the question of how to make people more rational.
      ```

- u/FishNetwork:
  ```
  > So how much should we be raising the sanity waterline, or where should we be focusing our efforts to raise the waterline? Thoughts?

  We'll have our best shot at convincing people to do things that are fun or useful.

  "Fun" seems kind of dangerous.  If I'm being honest, the most fun I have with rationality is attacking beliefs that I'm already predisposed to dislike.  Mostly, that turns out to be anti-bigfoot-style silliness.  Or articles making some tribal point.  

  I'm not sure that "fun" is the best tactic because it's not going to accomplish all that much that's useful.  We don't help people by giving them new ways to confirm their existing beliefs.

  So, if we want to do it, I'd focus on "useful."  And this would come down to measurement.

  Do our approaches measurably improve our lives?  Are we, as rationalists, healthier than other people in our demographics?  Are we more likely to exercise?  Happier with our jobs?

  If so, I'd focus on the places where rationalism matters and get people in that way.
  ```

  - u/trekie140:
    ```
    I think another important question is why people are or aren't drawn to rationality. I have found our community to be overwhelmingly composed of members of the WEIRD demographic, which is a limited pool to draw from considering we want to appeal to the whole human race, and I think also biases our community towards views held by that demographic that aren't necessarily rational. My concerns may be overblown, but I still think it's important to understand who are becoming rationalists and why.
    ```

    - u/Dwood15:
      ```
      I've noticed that we do have a high number of people in that category, and I think that has to do with the fact that most 'rational' people were led here from places such as fanfiction.net, SV, tumblr, etc.

      Very few people actually read fanfiction at all except for the most dedicated of fans like you'd find in our demographic. I'm sure it will get better as rational writers start getting bigger and branching out into OC.
      ```

    - u/DaystarEld:
      ```
      Rationalists *are* weird. By definition of being in the minority, we're not the usual. But as I said in the other comment, the idea impicit in the concecpt of "raising the waterline" is that this is something that can change, and should change. That it is better to be (by and large) weird in the ways that we are weird, and that that weirdness should not be weird eventually, but the norm.
      ```

      - u/catno:
        ```
        I'm pretty sure the parent comment didn't mean "weird" as a word, but rather an acronym for "Western, Educated, Industrialized, Rich, and Democratic".
        ```

        - u/DaystarEld:
          ```
          Huh. That is legitimately the first time I've ever heard that acronym. Interesting.

          I'd be surprised if most people here are rich though, unless it means "relative to the average poverty level in the world." I also see quite a lot of libertarians in rational circles, certainly more than the average sample of Western countries in general.
          ```

          - u/Wiron:
            ```
            Yes, it's Rich in global sense. Also, Democratic refers to living in democratic country, not supporting U.S. Democratic Party. WEIRD acronym was created to describe how many subjects of psychological studies comes from narrow demographic, and how that may skew results.
            ```

    - u/None:
      ```
      It also seems to me like the community is dominated by men. Of all the ff authors who post/get posted here, Alicorn is, as far as I know, the only woman.
      ```

- u/None:
  ```
  Teaching rationality as a single block seems doomed to failure. Should probably be asking what specific technique significantly increases the odds of seeking out rationality.
  ```

  - u/TK17Studios:
    ```
    Doesn't seem to be failing so far, though it also doesn't seem to be growing exponentially or anything.
    ```

    - u/None:
      ```
      You require exponential growth if your stated goal is to 'raise the sanity waterline'.
      ```

- u/trekie140:
  ```
  It's never been clear to me what "raising the sanity waterline" actually means, and the phrasing has always made me shirk. I instinctively assume that it means that they're extending the definition of insanity to cover holding irrational beliefs, which leaves me worried that I would fall under that umbrella as a theist. I know my spiritual beliefs aren't epistemically supported, but that hasn't made me stop believing in them.

  Sometimes I worry that many rationalists see themselves as an island of sanity in a world of maddness, which has some distressing implications no matter which side of the line you're on. I am very likely to be overreacting to something that I don't understand very well, but it might still indicate a problem with marketing this idea. People like me need our fears dispelled before we buy in.
  ```

  - u/DaystarEld:
    ```
    I'll try to put this very gently, and I'm sorry in advance if what I say hurts you: I don't mean to make you feel pain for the sake of pain, but for the sake of truth and growth. We've talked before about your feeling of not "belonging" here, and I don't want you to feel further estranged: ultimately this subreddit is about rational stories, not rationality. But you expressed confusion, so I feel compelled to answer:

    Your fears will not be dispelled before you "buy in."  Your fears are totally justified.

    To admit that you know your theistic beliefs aren't epistemically supported, but that you continue to hold them anyway, puts you under the "sanity waterline." That is partially what the point of it is: to measure the clear difference between people who can recognize and care about how rational their beliefs are, and those that don't know, or don't care.

    The word "sanity" was originally used tongue-in-cheek, if I remember correctly. It's not that people who aren't explicitly pro-rationality and empricism are clinically, pathologically insane, just that they do seem at times "crazy" in particular areas of their thoughts/actions, and *do* make rationalists at times feel like an island of sanity in a world of madness.

    Not perfect beacons of sanity. Not flawless machines of sterling logic. Not incarnations and avatars of Truth.

    But *rational enough* to want to further understand our own brains and avoid what we know are its weaknesses. To resist the tide of biases and cultural baggage that crashes on the shores of our minds. And willing to say yes that matters and yes it's objectively better than the alternative.

    That's going to be offputting to some people no matter what. Maybe it's going to be offputting to MOST people in most current cultures. But implicit in the idea of raising the sanity waterline is the recognition that this isn't set in stone: that people can learn, improve, get better at thinking more logically.

    Superstitions, religiosity, and prejudice are all shrinking in many western countries as education becomes more widespread and information more widely available. That is not an accident, to me: that's evidence that while people are born with some irrational biases, others are culturally imprinted, and the attitude of not caring about those irrational biases is itself something that can change.
    ```

    - u/TK17Studios:
      ```
      This is perhaps my favorite expression of this point, and I want to add (for trekie140 and other people with similar perspective) that the first-order goal is less about "release all irrational beliefs" and more about "*recognize* your irrational beliefs for what they are."

      I catch myself having irrational beliefs *all the time,* in part because I work for a rationality education organization, and we incentivize *noticing.*  Literally everyone has hundreds of irrational beliefs, and it's not something to feel shame or guilt or weakness about, but rather something to include in your awareness, and build into your understanding of the broader universe.

      Both "rationalists" and thoughtful, perceptive, introspective "non-rationalists" can and often do that.  The difference is what happens *next,* which is that, upon noticing an unfounded or unjustified belief, a rationalist then takes explicit steps to "update their map" and replace that belief with a correct, justified, or at least *less wrong* new belief.  The central assumption is that there isn't really any such thing as a "harmless" false belief—many, many, many of the weird things our brains do are *useful,* but the usefulness of a false belief comes at the high cost of distancing us from a deep understanding of how things *really* work.

      Rationalists are those who, for inductive reasons, believe that the correct move is to sacrifice some degree of comfort, blissful ignorance, social normativity, and general chillness about what goes on behind the curtain in favor of moving toward truth, accuracy, openness to objective reality, and (thereby) greater ability to successfully cause good things to happen in the world.
      ```

      - u/DaystarEld:
        ```
        > Literally everyone has hundreds of irrational beliefs, and it's not something to feel shame or guilt or weakness about, but rather something to include in your awareness, and build into your understanding of the broader universe.

        Well put. I don't want /u/trekie140 to feel like people are looking down on him for his irrational beliefs, because as you say, we all have those. I really like how you define the distinguishing factor of what makes a rationalist *what happens next:*

        >...sacrifice some degree of comfort, blissful ignorance, social normativity, and general chillness... in favor of moving toward truth, accuracy, openness to objective reality, and (thereby) greater ability to successfully cause good things to happen in the world.

        Doesn't mean people who disagree can't still enjoy rational, consistent, high quality, plot-hole-free fiction, but if they feel any kind of friction while doing so (if it's more of a rationalist story) or from the community, it's likely going to come from that difference in outlook.
        ```

        - u/trekie140:
          ```
          Thank you for answering my question so well and dissuading me from yet another existential crisis. I do really like rationality, HPMOR and LessWrong have taught me things that have made my life objectively better. Rationality is something I cherish, but it's things like this make me doubt myself, or at least give my anxiety and depression an excuse to latch onto that I can't easily throw off.

          Ever since I first started reading Yudkowsky's LessWrong posts I found myself agreeing with his reasoning, but disagreeing with many of his beliefs. At the time, I decided that was okay since I was still learning useful heuristics that were improving my life. Over time, though, I got deeper into rationality and have become more judgmental of myself out of a implicit loyalty to the group's moral paradigm.

          If if is rational to be an atheist, how could I claim to be rational if I am a spiritualist? If my values prevented me from giving up a belief that lacked objective evidence, why did I even want to be a rationalist? If I believe differently from them and I think I'm right even after they criticized my beliefs, shouldn't I think they're wrong and leave their community? 

          In the end, I decided those questions were irrelevant and I was just looking for excuses to beat myself up, but I've never managed to resolve the paradox that is me being here if I don't fit in. Religion is just one example. I'm the odd man out on a lot of the popular fiction posted here, either because I have different tastes or I'm outside the fandom they cater to, and I not only sided with Robin Hanson in the AI Foom Debate but consider the goal of building an AI God to be immoral.
          ```

          - u/DaystarEld:
            ```
            > Ever since I first started reading Yudkowsky's LessWrong posts I found myself agreeing with his reasoning, but disagreeing with many of his beliefs. At the time, I decided that was okay since I was still learning useful heuristics that were improving my life. Over time, though, I got deeper into rationality and have become more judgmental of myself out of a implicit loyalty to the group's moral paradigm.

            I respect EY immensely for many different reasons, but still disagree with him on a number of perspectives. Many of them make me think that I just don't know enough to understand the issue the way he does, but others I believe he would change his mind on if he had more info.

            There's nothing wrong with disagreeing with someone you respect, as long as you do your best to be honest to yourself about why. If you can articulate your reasons for believing what you do, and defend them logically or empirically, that's great, even if you don't at the end of the day end up changing someone's mind.

            >If is rational to be an atheist, how could I claim to be rational if I am a spiritualist?

            Are you a spiritualist, or a theist? You said theist earlier on, but there's a marked difference in irrationality between them. So maybe you can't be fully rational if you're a spiritualist, but you're still avoiding the blatant irrationality and hypocrisy rife throughout specific theologies.

            >If my values prevented me from giving up a belief that lacked objective evidence, why did I even want to be a rationalist? If I believe differently from them and I think I'm right even after they criticized my beliefs, shouldn't I think they're wrong and leave their community?

            Ultimately, it's better to be a rationalist to some degree than not at all. If your life is improving from rationality, then that's the most important factor. Don't worry too much about how much you "fit in." Maybe that'll come in time, when you're older and wiser and learn more, maybe it won't and you'll find another community that you feel more part of.

            >I not only sided with Robin Hanson in the AI Foom Debate but consider the goal of building an AI God to be immoral.

            Personally, I think building an "AI God" isn't about morality but necessity. It's going to happen eventually. Someone will figure it out. I want the people who do to have enough virtue and intelligence to do it in a way that benefits mankind rather than ends up causing suffering or calamity.
            ```

            - u/trekie140:
              ```
              I was under the impression that spiritualism was considered a form of theism, though it makes sense that it wouldn't be now that I think about it. It's a religion I've made for myself out of personal experiences and philosophical ideas I liked rather than an ideology I was raised in. I haven't had loyalty to a particular religion since I began to take issue with the Catholicism I was raised in, and even moved away from the trappings of New Age when I discovered I was embracing pseudoscience.

              I said AI God because the way I've often seen a singleton presented is similar to Friendship is Optimal, even if the AI's values more acceptable. I don't think it's possible to build an optimizer AI, don't understand why anyone would ever want to build it, and find the idea of it ruling over humanity to be horrifying even if it is benevolent. If it possessed the same legal rights and responsibilities as a human then I would accept it as a politician, but it often sounds like people intend to create something specifically to take over the world, which I disapprove of no matter how benevolent it is.
              ```

              - u/DaystarEld:
                ```
                As long as you stay away from the same failure modes inherent to traditional religions and New Age pseudoscience philosophies, you're probably on the more-rational side of the spectrum.  It may be that all you need are some courses on probability and epistemic philosophy to let go of the last vestiges. Just recognize that any friction you feel between them and the promotion of objective truths is going to occur for justified reasons, and don't worry about proving to others whether you're right or they're wrong, if you do at all. If your beliefs are based on subjective things, you know you can't do that anyway, so why beat yourself up over it?

                If your personal beliefs make you happy and don't harm others, then it's really not that big a deal. Many people are smart and competent and still have irrational beliefs.  That does make them "less rational", and if that bothers you, ultimately that friction is up to you to resolve at some point. I know plenty of people here agree with me in saying we'd be happy to help, but it probably won't happen as long as you *want* to believe in spiritual things, or they provide you some irreplaceable value for now. Which again, is fine. No rush.

                >but it often sounds like people intend to create something specifically to take over the world, which I disapprove of no matter how benevolent it is.

                Super AI will take over or destroy the world when it comes into existence, or at the very least change it completely in unforeseeable ways. Specifically wanting to build a "good" one doesn't have to conflict with the belief that one shouldn't be built, but *recognizing that it will* regardless of whether we want it to allows us to aim our beliefs and preferences toward "build a good one" rather than stand on "not wanting one to be built" as if that matters.

                It's like talking about nuclear bombs. It was going to happen, eventually. Someone would discover how to utilize them. The choice was to either decide they shouldn't be built, and be at the mercy of others who do make them, or  build your own so you could have some say in how events unfolded.
                ```

                - u/trekie140:
                  ```
                  Well...thanks. I can tell you're just tolerating my beliefs rather than approving of them, but I still appreciate you working to bridge the gap for me. Seriously, reading that response did make me feel a little better about this when I would normally be looking for a reason to be depressed, so I'm calling this a victory for my self-actualization.
                  ```

---

