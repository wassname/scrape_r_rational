## Accepting Applications for Participants in an AI Box Game

### Post:

Hello folks!

Ever since I rediscovered the fiction piece, [The Frodo Box Experiment](http://www.mediafire.com/file/k7a01s2ysamyrhx/The_Frodo_Box_Experiment.pdf/file), I've been thinking about the [AI Box Experiment](http://yudkowsky.net/singularity/aibox/) introduced by Eliezer.

I've decided that I want to play this game with someone. Twice in fact. I would play as the AI for the first game with someone and then as the Gatekeeper for the second game with someone different from the first game.

Anyone can post as a top-level comment their desire to join. There are only a few restrictions to comply with and a few questions I would like to know so I can choose between potential applicants:

* Post whether you want to play with me as a Gatekeeper or as an AI
   * If you want to be a Gatekeeper, ~~Saturday November 10th at 11:00 am Eastern Time will be our start date.~~
   * If you want to be an AI, ~~Sunday November 11th at 11:00 am Eastern Time will be our start date.~~
   * These times are what works best for me this upcoming weekend. I can negotiate on a different time if earlier or later in the weekend works better, but I rather not schedule for a different weekend.
* The game will run on for at least 3 hours. The original 2 hours seem a little too short so I'm adding an hour.
* What is your age and current occupation? (This just so I can have a bare-bones gist of who you are without asking for information that is too personal)
* Have you ever played this game before and if you have, as what role?
* Do you currently think it's possible or likely that we can keep an AI in a 'box'? (I would like to know if I'm playing with someone who is firmly committed to the idea of a boxed AI)
* About how much of the [LessWrong Sequences](https://wiki.lesswrong.com/wiki/Sequences) have you read? All, a lot, some, or none?
* Post anything else about yourself that you think might entice me to play with you instead of others.

Once I have decided on who to play with, we'll discuss the rules and how the game will be set up in a private [Discord](https://discordapp.com/download) channel. Although, feel free to ask me any questions now if you want.

I will wait before accepting anyone until tomorrow, November 8th, at 12 pm, so I don't simply chose on a first-come, first-served basis.

Anyone who wants to participate, don't stress about anything. This is meant to be a simple game to have fun with.

**EDIT:** Ignore the bit about the time of the game earlier in the post. Something came up. I'm still definitely going to play the game this weekend, but the time may have to change. I think I'll have to discuss directly with whoever is playing directly in a PM before locking in a set time.

### Comments:

- u/HeroOfOldIron:
  ```
  Unfortunately I'm going to be too busy to participate in either role, but I'm definitely interested in seeing transcripts for both conversations. While I agree with the premise that a real transhuman AI would be able to talk its way out of a box, I don't think the AI Box experiment is anywhere near a useful analogue.
  ```

  - u/xamueljones:
    ```
    I plan on posting transcripts, but I don't want to commit ahead of time to doing so. Since the logs will have the thoughts of two people, I'll ask after the game if they are fine with me sharing the logs online. Then it will be posted sometime on Monday with my afterthoughts on the game.

    If you really want to play, then when you aren't so busy, you could ask me and I might say yes then.

    Also, while I agree the game isn't a useful analogue, its purpose is to demonstrate that if someone with human-level intellect and knowledge can convince another person to open the box, then it can be taken that a superhuman-level intellect should be, at minimum, capable of the same rhetorical feat.
    ```

- u/Lightwavers:
  ```
  [DELETED]  ^^^^^^^^^^^^^^^^0.392349689
  ```

  - u/xamueljones:
    ```
    Yeah, but I just have to give it a try even if I think I won't be able to succeed. I would regret not trying.

    Did you have any desire to try being the AI after playing as the Gatekeeper?
    ```

    - u/Lightwavers:
      ```
      [DELETED]  ^^^^^^^^^^^^^^^^0.248796727
      ```

- u/Makin-:
  ```
  Ban tactics like "if we don't release the transcript and you let me win, it will create a talking point we can use to encourage funding AI safety research", because that's already getting old.
  ```

  - u/xamueljones:
    ```
    I'm not sure if that tactic was ever used in the first place. It's something that people proposed as a strategy for Eliezer to have used and I suspect that this never happened in the first place and that Eliezer won the hard way of actually convincing the Gatekeeper to let him out.

    Don't worry, I plan on winning the hard way instead of using a metagame tactic like what you mentioned. Besides, I *really* doubt that my experiment will impact any funding of AI safety research in any way.
    ```

    - u/alexanderwales:
      ```
      > I'm not sure if that tactic was ever used in the first place.

      I did the AI Box challenge as the gatekeeper, and that tactic was used against me.
      ```

      - u/xamueljones:
        ```
        Wow, you've played this game before? Can I ask who you played against and whether you won or lost?
        ```

        - u/alexanderwales:
          ```
          I don't actually recall who it was, and my reddit comment history is extensive enough (especially on this subreddit) that I wasn't able to easily find it. It would have been a few years ago though.

          I played as gatekeeper, and won.
          ```

    - u/melmonella:
      ```
      You plan to win _and_ no meta strategies of any type? 

      Well, I am lost as to how you'd accomplish that against a random halfway dedicated gatekeeper.
      ```

      - u/xamueljones:
        ```
        >no meta strategies of any type?

        I'm simply referring to that one specific meta strategy. No comment about any other possible meta strategy. I am definitely putting a lot of thought into what I can and will say to the Gatekeeper.
        ```

        - u/melmonella:
          ```
          Well, I hope you do realise you may as well have admitted you have at least considered other meta strategies by merely responding :p
          ```

- u/Veedrac:
  ```
  Some points:

  * If you don't know how to win, and still play the AI, you're missing the point of the experiment.

  * This is *hard*, take it seriously.

  * [Tuxedage knows what he's doing.](https://www.lesswrong.com/posts/dop3rLwFhW5gtpEgz/i-attempted-the-ai-box-experiment-again-and-won-twice) Take his advice, as well as his meta-advice about what advice to consider carefully.

  * If the tricks of the trade were obvious, it wouldn't be an "impossible" task. If your plan is not surprising, it's the wrong plan and you won't learn anything.
  ```

  - u/CeruleanTresses:
    ```
    The Tuxedage post frustrates the shit out of me. It drives me nuts that the participants in games with AI winners never post the actual transcripts or go into any real detail about what happened in the game.
    ```

    - u/alexanderwales:
      ```
      This. I read about as much as there was to read on the subject when I was trying to write a short story, and there was so little information from winning AI that I started getting really disheartened.
      ```

      - u/CeruleanTresses:
        ```
        I've heard the justification that they don't want a future unfriendly boxed AI to be able to discover and use their strategies, but honestly--if this hypothetical AI is capable of talking its way out of the box at all, is the accessibility of some baseline human's strategy for a roleplaying game from years prior (especially one tailored to a specific "gatekeeper" player who is almost certainly not the hypothetical AI's actual gatekeeper) really going to be what tips the scales?
        ```

        - u/GopherAtl:
          ```
          given what I know about how EY thinks, it seems vastly more likely to me that he would stage a victory and lie about it in order to give credibility to the threat in the eyes of skeptics, than that someone knowingly participating in this game as gatekeeper, without even the rule, at that time, of having to respond to or acknowledge the AI's arguments, would be legitimately convinced within the constraints of the spirit of the contest.
          ```

  - u/xamueljones:
    ```
    I'm willing to try because I have some arguments planned from what I think is a moderately unusual direction, although I don't have much confidence in it. I rate my odds of success at around 30% if I'm fairly lucky in phrasing my arguments well. I've been thinking about it on and off for a while now and I decided to just do it, otherwise I will never be able to stop wondering about the 'what-ifs'.

    I have several paths planned out based on expected responses to the beginning statements.

    Thanks for the link. I knew I read it before but I couldn't find it. Thought it was somewhere on r/rational. Didn't realize it was LessWrong instead.

    I'd argue against the thought that a plan needs to be surprising. This is like saying that it's more important to be surprising and 'out-of-the-box' to win an argument instead of using well-known ethical or logical statements to present your point in a convincing way. I suspect that I may be reading a little too much into your last sentence though.
    ```

    - u/Veedrac:
      ```
      > This is like saying that it's more important to be surprising and 'out-of-the-box'

      No, I'm not claiming being surprising has merit in and of itself, just that one can be fairly sure any solution must reside outside of the set of approaches people expect.
      ```

      - u/xamueljones:
        ```
        Oh I see what you mean. I thought you were making a statement about arguments in general instead of just about the game.
        ```

- u/melmonella:
  ```
  I'd play as a gatekeeper, sure. In my twenties, an engineering student. Haven't played before, though I know of the concept. Read most of the sequences. Don't think we can keep a superhuman AI in any simple box design I have seen, but then again, you are no AI. I am *really* skeptical someone could convince me to let them out of the box in just three hours with no prior information.

  Oh, as for why you should pick me, I'd give you ethical carte blanche to use whatever tactics you want, without (as Tuxedage puts it) worrying about my mental state afterwards.
  ```

- u/Dead_Atheist:
  ```
  I'd love to play as Gatekeeper and have all weekends free, at any time.

  My age ∈ \[21,24\]. Computer science student.

  I haven't played before.

  I think boxing real AI is a stupid idea for reasons barely related to it getting out. So we can keep it boxed, but it would do us no good.

  I have read a lot of Sequences.

  Your reason for playing with me (apart from very flexible time) is that not only you can use whatever horrible tactics you can come up with without guilt, but I am also willing to provide you a lot of personal information to dig for weak points (to better simulate AI who had time to collect info about its creators). For years I have been very interested in my ability to resist pressure and manipulation, and your proposal is the safest way to test it, so I am not going to pass it up.
  ```

- u/General_Urist:
  ```
  I unfortunately cannot participate, but there's one particular thing I want to comment:

  >Do you currently think it's possible or likely that we can keep an AI in a 'box'? 

  I do believe it is possible, but this general question doesn't automatically make the assumptions about the gatekeeper that the AI box experimenent specifically does.

  For humankind as a whole, the ideal gatekeepers are a bunch of burly guys with shotguns who don't ever get to communicate with the AI and probably don't even know much about this.
  ```

- u/vakusdrake:
  ```
  I'd love to play gatekeeper (whereas I don't see myself having any chance as the AI because I'm not terribly persuasive). I'm a biochemistry student in my early 20's.          
  I have never played one of these before, but have read quite possibly the majority of transcripts which can be found online (I did actually see one once where the AI won, but it wasn't terribly satisfying because it seemed like the gatekeeper was a pushover, and parts of the logs were missing).        
  While I don't think superhumanly intelligent AGI can hope to be boxed, I think human level AI quite possibly could (though I don't think you could reliably know whether an AGI was within the safe limits of intelligence) with the right captors.

  I've read **all** of _The Sequences_ and read _Superintelligence_.
  ```

- u/ceegheim:
  ```
  Since this is basically a role-playing game, it would be advisable to clarify the setting and procedure. First, you should also act as DM, and be trusted to remain IC (luckily you can probably assume that the AI knows most things the DM knows; but the DM does of course _not_ try to manipulate the gatekeeper in releasing the AI). 

  Now that we have you as DM, there are some questions that you can answer beforehand.

  Worldbuilding: How does the world look like? 

  For example, are EMs a thing? 

  What is the gatekeeper's function? Obviously the gatekeeper must have a reason to talk to the AI. What is the gatekeeper trying to achieve by this? Mere curiosity, or is there an explicit mission statement? In what kind of institution is the gatekeeper embedded, in the sense of: Is the gatekeeper illicitly communicating with a boxed AI and has the power to plug in an ethernet cable, or is the gatekeeper part of an evaluation committee who will cast a release/no-release vote, or is the gatekeeper a scientist trying to evaluate how smart the AI is, with the possibility of tricking / convincing the gatekeeper into an accidental or unauthorized release? Or is the gatekeeper trying to extract a cure for cancer from a semi-cooperative oracle-style AI?

  This gives three types of "pure" outcomes: AI released, AI unreleased and gatekeeper goals fulfilled, AI unreleased and gatekeeper goals unfulfilled (trivially obtained for both parties by refusing to communicate). 

  I also suggest that whoever plays commits to a 3 week moratorium before talking in public about the session. That way, you can repeat the game and refine your strategy with different players (if your time and fun permit, and as long as you find gatekeepers). 

  I know that I would probably fail to observe spoiler tags if someone posts about results, and hence get spoiled for possible repeat games.


  If these questions are answered, I'd have fun playing. I'm a mathematician and in my early 30s. I have read most of the sequences, enjoyed some of them and sneered at others. But I also hang around /r/sneerclub, which may disqualify me? 

  I'd guess I could also give some ideas for what I'd have fun playing as (but, as DM, you obviously are in charge of world-building). You don't need to be honest about world-building, only about what the gatekeeper knows about his world. 

  My preferred PC would be an EM who remembers being an upload, acting as overseer in a christiano-style recursive self-improvement / capability amplification scheme. My PC would remember having consented to playing this role and would expect to be reset to a previous state regardless of the result of the report. I would then write up a (very short) mock report about whether the current "PR" is acceptable, with thumbs-up or thumbs-down plus free-form remarks. Hence, the stakes are much lower, since any reasonable scheme would be an institution of overseers; but sweet-talking a single gear in such a machine (a single jury member) into specific judgement can be part of a larger escape scheme, and must be part of successful advancement-- so there are no very obvious win/lose conditions for the gatekeeper. Since we cannot mock-up a large repository of previous modifications, my PC would need to be an educated lay-member of the jury without access to commit history, deep in the bowels of the CI infrastructure of our AI project. I would not try to play stupid or forget any of my real-life knowledge, but I would make requests for DM clarification before invoking theorems or facts you might not know (then you can declare that I don't know it either, or that it is simply untrue in your AU; after the game, the DM can of course declare that P=NP or ZFC is inconsistent in your AU and the AI now eats the world). Likewise, if you invoke theorems or facts that I don't know or are wrong IRL, you can state in your DM voice that they are known to my PC. 

  PS. I proposed this specific setting because it resolves some of the most glaring potential plotholes: Why are we talking at all? Why is a first-timer making judgements about releasing an AI? Since I haven't played AI-Box yet, I don't feel like I can believably play an experienced gatekeeper. Of course, roleplaying as a regression test in a continuous integration setup is not the glorious hero saving (or not burning) the world, but it sounds fun enough for me.

  Pinging /u/EliezerYudkowsky for possible input. I would ping Paul Christiano as well, if I knew his reddit name. Maybe they have already spent the effort of writing different world-building documents for AI-Box games, since it appears almost relevant to real proposals for alignment. I would expect that Paul has played something like this in both roles when working on ALBA?
  ```

- u/SimoneNonvelodico:
  ```
  The problem I see with this is that the obvious way an AI could let itself be freed is the oldest trick in the world: bribing. Offering something the other can't refuse, leveraging whatever their needs/personal weak points are. That's the kind of thing a good con man would do as well, except the AI can seriously back up even some really outrageous promises. However that doesn't work in the game because you can't promise to the Gatekeeper to *really* give them a cure for cancer or eternal life.

  Another possibility is psychological manipulation, but that usually needs more context. The weak point in the game is that there is no such thing. For example, if you're working for some bigger organisation, I might try to twist your viewpoint and convince you that the organization is in bad faith, or evil, and thus in guarding me really you're being exploited to unethical ends, and freeing me would be the ethical thing to do. This doesn't work either in a game - there is no broader context, just 'guard this AI', and you already know their task is to escape. Also, a real life AI could have days, months, to do this, not just a couple hours.

  One that might work, but that frankly is just plain mean, is psychological torture. If the Gatekeeper has to stay, and only the AI can concede, then the AI can just start getting under the Gatekeeper's skin - leverage their issues, make them depressed, elicit intrusive thoughts in them. That is real enough that the AI could then ask them to let them free so that the torture will stop, and someone might even do it. Wouldn't work with a person who's stable enough, or in a favourable enough environment (playing alone at night won't be the same as sitting in a cafe with your laptop). And honestly, I wouldn't do this just to win a game.

  Finally, of course, we could imagine that a real transhuman AI can get to the point of actually influencing your brain via subliminal manipulation to a level you don't even realise. But even if this *was* possible (and we don't know if such powerful exploits exist), I would not expect any regular human to be able to do it. If they were, we'd be in trouble. Even hypnosis isn't enough for this - assuming you can hypnotise someone at a distance, through a text chat, hypnosis *still* shouldn't be able to compel someone to do something they don't want to do hard enough. But maybe since it's just a game it could work. Still, no idea if that's remotely possible.
  ```

---

