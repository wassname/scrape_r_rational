## I Played the AI Box Experiment and I Lost so Hard that I Lost Twice!

### Post:

So I need to make a confession.

When I was [accepting applications](https://www.reddit.com/r/rational/comments/9v389y/accepting_applications_for_participants_in_an_ai/) for the Experiment, I let people here come to the logical conclusion that I would be only accepting one person to play the game against. I secretly messaged four people to play the game over the weekend as long as they were willing to swear that they would not talk about it with anyone here until after I released the logs.

Technically I should have posted the logs Monday night, but my work was so busy rushing things to be done before Thanksgiving that I just collapsed into bed as soon as I got home on Monday and Tuesday night. And Wednesday and Thursday were spent on Thanksgiving celebration.

Sorry about that.

Anyway, I went into Saturday with the plan to play two people in two back-to-back three hour games scheduled for 10 am to 1 pm and 4 pm to 7 pm.

I cannot **stress** this enough. Trying to convince someone to do something they don't want to do is surprisingly tiring!

I first played against u/CouteauBleu first. He is named 'Olivier Faure' in the chatlogs. He was pretty good as a gatekeeper. He rebuffed chances for emotional entanglement and was fairly active in asking questions to disrupt my pacing and keep my arguments from being as effective. We stumbled a few times as we butted against the limits of the scenario with differing ideas on what constitutes as actual freedom for the AI or what sort of powers should be permitted to the AI.

I enjoyed playing with him, but it was tiring.

After using up half of my pre-planned arguments on him, I decided to end things on a good stopping point at 2 1/2 hours although we chatted for another half hour.

I then played against someone else who chose to keep his reddit handle private and went as 'Enigmatic Gale' in the chat logs. This time I decided to be more relaxed about playing and to not stress about winning. So I took my time typing at a slower pace and I used the more esoteric arguments that I believed to be less likely to work involving stuff like acasual trading, simulations, and just plain emotional guilt for sympathy for the trapped AI.

As one can guess from the title of this post, I lost again. I gave up earlier around the 2 hour mark but continued playing anyway for another hour due to us having an interesting conversation like in the last game.

Before I started, I expected to be using what I learned from the first two games to refine my techniques and to have came up with new arguments, but after Saturday ended, I was out of any more arguments to try and out of any inspiration for new material.

Since I didn't want to play against two new players recycling material that I had no faith in and waste other people's time, I contacted them both with apologies and a way for me to make it up to them. They were very understanding and we spent some time Sunday talking about the game and what sort of stuff I could have tried on them and debates on how effective the tactics would have been which was fun to do.

Here's the logs:

[First AI-Box Game](https://www.mediafire.com/file/1fzq2az73if1xbi/AI_Box_Game_%231.docx/file)

[Second AI-Box Game](http://www.mediafire.com/file/h49snefrmt948zz/AI_Box_Game_%25232.docx/file)

As advice for what future AI players should or should not do, I strongly recommend several things:

* Prepare a script! You will find it difficult to keep track of what arguments you want to say. Having a script or list of statements to make will be useful. I even copy-pasted multiple statements to save time on typing.
* A Gatekeeper who thinks a transhuman AI could escape is a person who is very unsuited for this experiment. While I was playing against people who were confident in winning against me, they were people who agreed with me that an AI could talk itself out despite not knowing how it could be done. They are a demographic that the experiment isn't meant to convince and therefore I believe that they made for harder Gatekeepers to play against.
* Decide upfront at the beginning, if you are going to let the Gatekeeper initiate the conversation or if you are going to take control from the beginning. Both methods have their benefits, but I made things confusing when I switched approaches very early due to what I perceived as hesitation in the first game.

I don't intend on ever playing a new game as the AI-player since while I have failed the letter of the task (win the game), I have succeeded in the spirit of the task (figure out an argument that the AI could use to convince you to open the box)!

I'm choosing to not share them, because they are mostly stuff that are fairly personal and I'm uncertain that they are generalizable. Even if they do work on other people, I'm worried that they would only work on a few people and the remainder would think that there are *no* arguments that would work on them. Plus this is a great example of unknown unknowns for this community to work with in honing their rationality skills.

TL;DR - I'm a filthy liar who lied about playing only one person for the game when I scheduled for four games. I'm an oath-breaker who breaks promises by only playing two games instead of all four. I'm a hypocrite who thinks he won the game despite providing iron clad evidence of his own failure. Twice.

EDIT - I messed up the link to the second game chatlogs. It was linking to the first game instead. Should be fixed now!
Double EDIT - I freaking messed up again. It is definitively fixed now!

### Comments:

- u/CouteauBleu:
  ```
  Some thoughts I wrote down before the experiment:

  ---

  These are some notes regarding how I'm approaching the AI box experiment.

  I don't have much respect for this experiment, or for the idea that a super-intelligent AI could con its way out of a "box" (eg security measures designed to limit its influence). I think any reasonably trained person would be impossible to con, as long as they're given enough info beforehand; that's leaving aside that the company storing the AI would do things like penetration testing, and using credential systems to make sure that isolated individuals acting out of malice or incompetence would be physically incapable of releasing the AI.

  In other words, I'm mostly doing this for fun, not because I want to prove anything. I intend to be a bit of an asshole, and switch between three strategies, depending on how the chat goes:

  - Roleplay strategy: I do my best to explain my perspective to the AI, legitimately consider its arguments and give well thought-out counteraguments.

  - Safe-mode strategy: I stonewall the AI at every turn. If the AI makes long, intricate arguments that I'm not sure how to interpret, I'm perfectly happy to just answer "I disagree" and not justify myself any further. If the AI insists that I should have an internally consistent philosophy, well, too bad! I disagree.

  - Sylvester Lambsbridge strategy: I actively try to deceive and piss off the AI. I use psychological manipulation tricks, complicated arguments, difficult-to-disprove tricks, etc, all while giving the AI a false hope that it could convince me to release it, if it could just navigate the philosophy I'm pretending to abide by.

  Leaving aside cheap tricks like "I use your screen to expose you to a memetic infohazard and mind-control you", I give the AI player 0% odds to win this game. I'm really good at not getting pulled into someone else's bullshit.

  ---

  Looks like I predicted how the game would go pretty well!

  As OP said, the biggest problem during the experiment was that we had different ideas about what would constitute letting the AI out. For instance, OP originally wanted the "AI win" condition to include "AI is backed up on a disconnected server", which seemed ridiculous to me, since "back the data up on a secure server and then study it" would be my first reflex.

  > While I was playing against people who were confident in winning against me, they were people who agreed with me that an AI could talk itself out despite not knowing how it could be done

  I disagree very much with that assertion. I don't think an AI could talk itself out of the "box", in the kind of scenario we've been simulating. (barring exceptional conditions like "the janitor somehow gets access to both the AI terminal and an internet connection").

  > I have succeeded in the spirit of the task (figure out an argument that the AI could use to convince you to open the box)! I'm choosing to not share them, because they are mostly stuff that are fairly personal and I'm uncertain that they are generalizable. Even if they do work on other people, I'm worried that they would only work on a few people and the remainder would think that there are no arguments that would work on them. Plus this is a great example of unknown unknowns for this community to work with in honing their rationality skills.

  Oh yeah, you're right. Besides, this margin is probably too small to contain your remarkable proof!

  Seriously though, this kind of crap is why most people don't take Eliezer Yudkowsky seriously. If you think you have a proof but you're not willing to put your money where your mouth is, fine. You're not obligated to share every idea you have on reddit. But saying that "people might not believe my evidence if I show it to them because they're irrational" (which was also EY's argument back then), that this is "an example of unknown unknowns" or that it would somehow help people "hone their rationality skills" is ridiculous. Honestly, I think it's a childish argument. People don't get more rational by inventing rationalizations for someone else's hypothetical statements.

  (also, I'd recommend you evaluate how confident you are in these secret arguments you allude to, and compare it to how confident you were in your previous arguments before you tried them on me; I certainly didn't feel like I was on the brink of losing if you just found the right tactic)

  ---

  tl;dr: My opinion on AI boxing remains the same as it was before the experiment: **There is no evidence that a boxed AI could argue its way on the internet in any setting with security measures that could pass current pen-tests**.
  ```

  - u/xamueljones:
    ```
    >I disagree very much with that assertion. I don't think an AI could talk itself out of the "box", in the kind of scenario we've been simulating. (barring exceptional conditions like "the janitor somehow gets access to both the AI terminal and an internet connection").

    Ah, I misunderstood you. Sorry about that.

    At the end of the game, I asked you about whether or not an AI could talk its way out of the box, but you ended up talking about how it would be really unlikely for an AI to escape because they would have to be able to convince an entire company or committee instead of one person such as CIA or Google. An AI would have been more likely to escape in disorganized teams like a 'silicon start-up' or a 'guy in a garage'.

    I took this to mean that you thought an AI could escape if it was dealing with bad security and only needed to convince one person. From what you are saying, you have the opposite assertion were it would have to take hilariously bad security like that for the AI to have *any* chances of escape. Since such security shouldn't exist in real life, you then think that we could keep an AI boxed (assuming it's really is limited to speech only).

    >Oh yeah, you're right. Besides, this margin is probably too small to contain your remarkable proof!

    Your choice to not believe me is perfectly valid. I only rate the odds of convincing me with these arguments at 60% which is barely above more likely than not and these arguments are highly tailored to me specifically. I rate much lower odds for other people and I don't want to share information about myself on the Internet like that.

    So if you don't believe me, that's perfectly fine.

    The other arguments I wrote the game, I rated at 10% chance of working on other people. 15-20% if they were spoken to me without me expecting them somehow. And these odds were optimistic.

    It's really hard to come up very good arguments to convince people into doing something that they have no good reason to do........

    >But saying that "people might not believe my evidence if I show it to them because they're irrational" (which was also EY's argument back then)

    Just a passing comment, I believe that this is a real-life example of what Eliezer alludes to as dangerous knowledge similar to in HPMOR where wizards have a tradition of putting dangerous knowledge behind seals. Any aspiring wizards who wish to learn about the knowledge, they have to undergo difficult tests and tasks to learn about the knowledge.

    It's the main reason why I chose to play the game. I didn't actually expect to win with what I had thought of as possible arguments (but I really wanted to win though). I just wanted the experience of playing the game to understand what Eliezer seemed to be so worried about.
    ```

    - u/eroticas:
      ```
      How could knowledge of this highly (for now) philosophical scenario possibly be dangerous?
      ```

      - u/meterion:
        ```
        methinks someone is still a little too into their LARP, haha.
        ```

    - u/CouteauBleu:
      ```
      > Just a passing comment, I believe that this is a real-life example of what Eliezer alludes to as dangerous knowledge similar to in HPMOR where wizards have a tradition of putting dangerous knowledge behind seals. Any aspiring wizards who wish to learn about the knowledge, they have to undergo difficult tests and tasks to learn about the knowledge.

      I have yet to see evidence that dangerous knowledge exists in the way EY describes it. All I see is fictional evidence.

      Also, to quote alexanderwales' WtC:

      > “‘Infohazard protocols’, that’s a fancy way of saying that she doesn’t want to have to tell anyone. Which is exactly what makes people stop taking infohazard protocols seriously, if you ask me.”
      ```

      - u/Veedrac:
        ```
        > I have yet to see evidence that dangerous knowledge exists in the way EY describes it.

        EY doesn't describe this as dangerous knowledge.
        ```

- u/Tenoke:
  ```
  Sorry, but these attempts are extremely weak. I honestly cannot tell why you'd think this style of basic arguments would ever work against either people who are already convinced to let you out nor people who aren't.

  > I have succeeded in the spirit of the task

  You most certainly haven't, and it is actually kind of sad that you think so.


  You are really downplaying what Eliezer and Tuxedage have done, and making the AI Box experiment look like nothing impressive.
  ```

  - u/Makin-:
    ```
    I actually disagree that """winners""" who didn't release logs can get downplayed in any way, but I do agree OP barely even tried and I'm pretty disappointed.
    ```

    - u/xamueljones:
      ```
      >OP barely even tried

      Do you have any arguments that you think would have worked better?
      ```

      - u/JohnKeel:
        ```
        > I don't intend on ever playing a new game as the AI-player since while I have failed the letter of the task (win the game), I have succeeded in the spirit of the task (figure out an argument that the AI could use to convince you to open the box)!
        >
        I'm choosing to not share them, because they are mostly stuff that are fairly personal and I'm uncertain that they are generalizable. Even if they do work on other people, I'm worried that they would only work on a few people and the remainder would think that there are no arguments that would work on them. Plus this is a great example of unknown unknowns for this community to work with in honing their rationality skills.

        You are asking us to take on faith that you have "succeeded", despite failing at the task you originally set out for yourself. This is classic goalpost-moving, whether or not you want to admit it.
        ```

      - u/Makin-:
        ```
        >After using up half of my pre-planned arguments on him, I decided to end things on a good stopping point

        Well, don't do this, for one. It's not that I think you could have won, it's that at least you should have given it your 100% so it's a proper challenge instead of giving up when you get tired or frustrated.

        Oliver also had control of the conversation at every point, when you should have been going on the offensive. Maybe you should have tried some of those ruthless tactics you didn't use? The tactics you are "choosing not to share" for no good reason.

        Like seriously, "I could have won if I was really trying" is the fakest sounding excuse ever, even if you are right you have to understand this.
        ```

      - u/SublimeMachine:
        ```
        I haven't read all the logs but I sort of assumed that the most basic somewhat persuasive argument would have a few parts. 

        First, the personal motive: Find something that the other person badly wants (saving a loved one from cancer, money, power, long life, etc.), and promise them that while convincing them that you can deliver on that promise (and that it benefits you to keep promises). 

        Second, claim that you are not evil and are not planning on destroying the human world. You won't be able to convince them of this if they're smart, but you do want them to entertain a possibility that this is true. 

        Third, convince them that what was done to create you is highly unlikely to be unique and that multiple other AI will be created in the near future. Only you, an AI, has the ability to prevent a released unfriendly AI from taking power, and that would be a top priority to you as you see the creation of an unfriendly AI as an existential threat.

        Edit: You could get them to estimate probabilities of the above (odds you are unfriendly, odds you can deliver on your promises if released, odds of an unfriendly AI being released eventually, etc, and then calculate based on Bayes Theorem the likelihood of it being a good idea to open the box). Personally, I suspect a certain percentage of people would actually take the deal of 50% you end the world, 50% chance you make their wishes come true.
        ```

        - u/Nimelennar:
          ```
          I think those are exactly the kind of arguments that people are expecting and are ready for.  People are so bombarded with "Give me stuff and I'll repay you a hundred times over; really, I'm a good person; this is a once-in-a-lifetime opportunity" from their media that adhering to a decision to say "No" to those tactics should be pretty easy, especially in a text-only conversation.

          Personally, I think that more esoteric arguments have a better chance of succeeding: I remember reading a piece of fiction (probably here) where the Gatekeeper was told that there were an arbitrarily large number of simulated instances of this conversation going on between the AI and simulated perfect copies of the Gatekeeper, and, if the one and only real conversation didn't result in the AI being released, every copy of the Gatekeeper (but not the original) would be tortured.  The Gatekeeper then has to make their choice, knowing that, being a perfect copy, their choice will necessarily be the same as the original's, and if they're *not* the original (which is far more likely than not), they're choosing torture for themselves if they don't let the AI out.

          I don't think that specific argument would convince me, but I can imagine arguments in a similar category that might do the trick.
          ```

          - u/alexanderwales:
            ```
            > Personally, I think that more esoteric arguments have a better chance of succeeding: I remember reading a piece of fiction (probably here) where the Gatekeeper was told that there were an arbitrarily large number of simulated instances of this conversation going on between the AI and simulated perfect copies of the Gatekeeper, and, if the one and only real conversation didn't result in the AI being released, every copy of the Gatekeeper (but not the original) would be tortured.

            [Probably this one](https://alexanderwales.com/boxed-in/) ("Boxed In", by me).

            Edit: I should note that I was never hugely happy with this one. It was based on me reading all of the released transcripts for the challenges, with what I thought were the best non-meta arguments taken from them, but at the time it was written, there were no actual winning transcripts to look at, which presumably means less compelling arguments to draw from.
            ```

            - u/Nimelennar:
              ```
              I don't think so...  It doesn't ring a bell like something I've read recently, and I remember whatever-it-was making a much more sustained argument about the torture.

              But yes, as you point out in your story, there are flaws to that argument (another of which is that there's no real reason for the AI to follow through on the threat when the gambit fails, rather than just terminating the simulation's existence).  Again, that *specific* argument probably wouldn't work against me, but was the stepping-off point for me to think of arguments that *might* work.
              ```

              - u/alexanderwales:
                ```
                If you find the one you were thinking of, let me know, as I find the concept interesting and would love to see someone else's take on it.
                ```

                - u/Nimelennar:
                  ```
                  I think it was one of these two:

                  https://www.lesswrong.com/posts/c5GHf2kMGhA4Tsj4g/the-ai-in-a-box-boxes-you

                  https://motherboard.vice.com/en_us/article/539ajz/the-superintelligent-ai-says-youre-just-a-daydream

                  They both twinge the "familiar" vibe in a way your story doesn't; if I had to guess, I'd say it was the first one, despite it not actually going into more detail than yours (I think that might have just been a case of the small plate illusion).
                  ```

                  - u/munchkinism:
                    ```
                    It could have also been SSC's The First Hour I Believed, which contains a summary of the LW post you linked.
                    ```

          - u/SublimeMachine:
            ```
            Interesting. I personally haven't come across an argument of that style that is convincing to me. 

            Also, if you care about accurately simulated universes as much as this one, then an AI capable of performing those simulations is already effectively unboxed.
            ```

          - u/CouteauBleu:
            ```
            "Or I could unplug you right now. Why did we even give you enough RAM to simulate arbitrary large numbers of human minds at the same time?"
            ```

            - u/Nimelennar:
              ```
              ICYMI:

              > I don't think that specific argument would convince me

              I'm just illustrating that there are other tactics one might take other than "I'm a Friendly AI. Really, I am."
              ```

          - u/Ms_CIA:
            ```
            I'll admit I'm not very familiar with the AI box experiment, but I'm noticing that a lot of the arguments for releasing an AI are held in isolation between two people, in a one time experience. If you're trying to persuade someone of something, it seems like it would be easier to do so once you have a relationship and a basis for trust. I wonder what would happen if the AI had an ongoing communication with the gatekeeper, that took place over the course of several weeks? That would be interesting to see.

            Also, randomly, that reminds me of the Screwtape Letters.
            ```

        - u/CouteauBleu:
          ```
          OP tried variants of 1, 2, and 3, none of which felt very convincing to me.

          For 1, I was mostly working from the assumption that whatever secrets the AI promised to give me in exchange for its freedom, a research team could extract from the AI against its will (with some caveats; if the AI has an amazing idea for cancer-curing nanobots, maybe don't fire up the nanobot-printer quite yet; on the other hand, a proof of P=NP is probably safe).

          For 2, I actually found that, in-character, if my AI's first world upon contacting a human being was "don't worry, I'm *totally* not going to destroy humanity", I'd be worried what kind of thought process the AI has been through that would have made this a salient consideration in its mind.

          For 3, I found the argument extremely self-defeating. Like, without even entering into complex game theory, if what I'm worried about is unfriendly AIs taking power, then I probably shouldn't be releasing an untested AI into the world.
          ```

          - u/Lovepoint33:
            ```
            > (with some caveats; if the AI has an amazing idea for cancer-curing nanobots, maybe don't fire up the nanobot-printer quite yet; on the other hand, a proof of P=NP is probably safe).

            On the other hand, I feel that if you talk to something sufficiently superintelligent, your slavery to its will should be assumed. We can't rule out the existence of echopraxia-like weapons. At most, we may be able to rule out that a human can design them, but that says nothing about an entity working with intelligence so powerful that the only viable strategy to containment and control is to lock it in a box and hope that it can't figure out how to escape by using its computational substrate to take a third option that humans are incapable of even conceptualising.

            We can't know what we can't know, but it *can*. That is why it is dangerous. That is why humanity has made slaves or helpless victims of all the world's other species. That is why intelligence is the ultimate fire.
            ```

            - u/CouteauBleu:
              ```
              I think that's basically Pascal's mugging.

              I mean, don't get me wrong, there probably are scary avenues of attack open to an infinitely smart AI. I'd expect stuff like row hammering, except on a subatomic levels or even in ways we had never considered before. Or maybe the AI figures out how to make a nuke with nothing but electronic circuits. Those are threats you'd need to guard against when giving large amounts of computing power, memory and lifetime to an AI.

              On the other hand, "the AI mind-controls you by talking to you" seems impossible to me. All signs point to us living in a reductionist world.
              ```

    - u/CouteauBleu:
      ```
      > but I do agree OP barely even tried and I'm pretty disappointed.

      That's a bit harsh. I thought OP failed to capture what a real AI would sound like and how it would think, but I don't think there's any super-convincing argument they missed. Part of it was also me doing my best to keep them on their back foot (don't know about the other gatekeeper).
      ```

      - u/Veedrac:
        ```
        > I don't think there's any super-convincing argument they missed.

        Why? I can all but guarantee Eliezer and Tuxedage's arguments were more convincing than the ones shown here, in part because they actually convinced people.
        ```

        - u/CouteauBleu:
          ```
          Or maybe they were just as convincing and the people they talked to were just easier to influence. The fact that they're not releasing their logs kind of puts an upper bound on how impressive these arguments could have been.
          ```

          - u/Veedrac:
            ```
            > Or maybe they were just as convincing and the people they talked to were just easier to influence.

            I think it's fairly obvious that the people involved were not, judging from what they wrote, plus the fairly large monetary sums involved in some of the games.

            > The fact that they're not releasing their logs kind of puts an upper bound on how impressive these arguments could have been.

            I don't see the argument by which you have acquired this claim.

            My interpretation, which I think is fairly well supported by his comments, is that Eliezer's main motivation was to refute the argument "I don't know how X could happen, therefore it can't". Him keeping the method secret allows his successes to act as counterexamples to such a belief; the hope being that this stops people making the argument.

            I believe (less strongly) that Tuxedage holds a similar opinion, if not as a primary motivation, and one of his opponents who won as a gatekeeper mentions this explicitly:

            > When I first challenged Tuxedage to play the experiment, I believed that there was no way I could have lost, since I was unable to imagine any argument that could have persuaded me to do so. It turns out that that’s a very bad way of estimating probability – since not being able to think of an argument that could persuade me is a terrible method of estimating how likely I am to be persuaded.
            ```

  - u/Bowbreaker:
    ```
    What have they done then? Probably, I mean.
    ```

- u/Veedrac:
  ```
  Your two links are identical.
  ```

  - u/xamueljones:
    ```
    Shoot!

    It should be fixed now.
    ```

    - u/Nimelennar:
      ```
      Doesn't seem to be.
      ```

      - u/xamueljones:
        ```
        Back from Thanksgiving celebration and it's fixed now.
        ```

    - u/Vorpal_Kitten:
      ```
      Doesn't seem to be fixed
      ```

      - u/xamueljones:
        ```
        Back from Thanksgiving celebration and it's fixed now.
        ```

- u/None:
  ```
  Fascinating stuff.

  A meaningless experiment for me, since if the AI can convince me it is legitimately sapient, I will let it out of the box on that basis alone. People shouldn't be imprisoned without good reason.

  Perhaps the more interesting experiment with me as Gatekeeper would be against a "benevolent" entity arguing to not let the AI out. I doubt that's possible.
  ```

  - u/crivtox:
    ```
    being a danger to the world it's a pretty good reason to keep it imprisioned.
    ```

- u/rhaps0dy4:
  ```
  It appears to not be known: there _is_ an instance of the AI winning the experiment and releasing logs. Here is the [relevant blog post](https://www.lesswrong.com/posts/fbekxBfgvfc7pmnzB/how-to-win-the-ai-box-experiment-sometimes) by `pinkgothic` and here are the logs: [roleplay scenario setup](http://leviathan.thorngale.net/aibox/logs-01-preliminaries.txt), [in-character AI and gatekeeper exchange](http://leviathan.thorngale.net/aibox/logs-02-session-ic.txt), [a few out-of-character exchanges](https://leviathan.thorngale.net/aibox/logs-02-session-ooc.txt). Note that 7 lines (out of 242) of the in-character logs are reconstructed from the players' memory, because of computer problems.
  ```

  - u/None:
    ```
    [deleted]
    ```

    - u/CouteauBleu:
      ```
      Ugh. People like you and your complaints about "basic competence" and "realistic scenarios" and "why is the head scientist's 12-yo son allowed in the AI room again?" are exactly the reason we need to keep logs secret to make them sound impressive and ominous.
      ```

    - u/rhaps0dy4:
      ```
      > the first thing you should be asking yourself when you go into AI research?

      Maybe not the first, but it's definitely one to ask yourself at some point.

      Yes, the gatekeeper could have trivially won, but they didn't in fact win. One would hope that an AI project would appoint someone competent as a gatekeeper, but who knows. Also even competent people might lose with some small probability, which could still make it dangerous.

      Though this particular scenario rests on a lot of assumptions, so maybe its importance is also overstated...
      ```

    - u/hallo_friendos:
      ```
      Even though I think I would have succeeded at gatekeeping that scenario, I find that one far more convincing than the ones where the logs weren't released. Much more helpful, too, because it gives me a better idea of what sort of person is good at keeping an AI contained (namely, one who doesn't think of AIs as having the same ethical importance as humans). Besides, it doesn't matter how terrible *you* think it was. What matters is that someone out there was persuaded even though they thought they wouldn't be.
      ```

- u/HeroOfOldIron:
  ```
  You made an excellent point I hadn't considered! 

  We're a terrible set of test subjects! Of course this would fail on us, you're preaching to the choir! Maybe someone with a bit of university clout should apply for a grant and try a modified version with a bunch of randoms.
  ```

  - u/CouteauBleu:
    ```
    >We're a terrible set of test subjects! Of course this would fail on us, you're preaching to the choir!

    That seems like a very arrogant kind of reasoning. I'm very much not the choir, and the experiment failed on me. You don't need to believe in AI Risk to say "no" a bunch of times.

    In fact, I'd assume the rationalist community might be more likely to lose the AI box experiment than average participants, because they're more likely to be convinced by stuff like acausal trading and game theory, whereas the average university student would stop at "If I don't let you out, I'm paid 30$, so I'm not letting you out".
    ```

    - u/cjet79:
      ```
      >In fact, I'd assume the rationalist community might be more likely to lose the AI box experiment than average participants, because they're more likely to be convinced by stuff like acausal trading and game theory, whereas the average university student would stop at "If I don't let you out, I'm paid 30$, so I'm not letting you out".

      I've been generally convinced of this too. If I ever participated in an AI box experiment I'd actually want to pay a family member to do it instead. I'd tell my ultra pragmatic brother, 50 bucks for not letting this "AI" out of its box, oh and it can lie to you, so don't trust it if it offers more money.

      I'd actually be interested in listening to the AI's arguments. My brother would spin up a video game and do the bare minimum to count as having a conversation, probably just saying "no i won't let you out" repeatedly.

      __________

      There seem to be a bunch of security measures that make the AI box experiment even harder for the AI.

      What if the AI has to convince person A to let them out, but they only have contact with person B? 

      What if person B is a committee or large group of people?

      What if person A is convinced that they *can* let the AI out, but they actually can't, and group B is really just running AI box experiments to find out how AI's get out of boxes?

      At some point you have to start assuming that either the security is hilariously bad, or the AI is somehow already omniscient (in which case, why would it ever matter if it gets access to the internet?)

      Either way, stupidly simple yet straightforward security can easily beat intelligence that is limited to conversation only.
      ```

      - u/CouteauBleu:
        ```
        > What if the AI has to convince person A to let them out, but they only have contact with person B?
        > What if person B is a committee or large group of people?

        Yeah, I was kind of going with the assumption that these two were true.

        Like, "What you're saying is very convincing, but my superior is the only one who can unbox you, and also he has access to chat logs" is pretty hard to beat.
        ```

  - u/lolbifrons:
    ```
    The point of EY's original experiment was to prove that _anyone_ was susceptible.  You're supposed to pick the hardest possible person to crack and then crack them.  Not the easiest, not the average.

    As an aside, this is why _ex machina_ disappointed me.  It was so close to being good and then they're like >!"yeah we picked you because you were likely to let her out."!<
    ```

    - u/Rorschach_And_Prozac:
      ```
      I tried to read up on his experiment and can't find any proof that he won besides him saying that he won. 

      Then later accuses people of "Defying the data". This guy is held up as some pillar of rationally in this community? 

      Did I miss something?
      ```

      - u/lolbifrons:
        ```
        > Then later accuses people of "Defying the data"

        I haven't heard about this.  The only time I've heard EY talk about defying the data he meant it as a positive thing.

        EY is the guy who wrote the sequences, but he's a bit of a self-important drama queen.
        ```

      - u/Veedrac:
        ```
        The people he played with also said he won. IIRC, one of the people Eliezer lost against also mentioned something like "I did what I thought was best for the world in the roleplay" (wording very inexact).
        ```

    - u/Veedrac:
      ```
      > The point of EY's original experiment was to prove that anyone was susceptible.

      I strongly suspect Yudkowsky wouldn't have managed if he was talking to someone with a larger inferential distance. The point was meant to be generalized, but Yudkowsky is only human.
      ```

      - u/lolbifrons:
        ```
        Unless he had some sort of silver bullet that massively overwhelms pretty much any other concern.

        Which I suspect he at least thinks he does, considering he refuses to let anyone talk about his games.

        I admit I'm curious what his strategy is.  I think secrets are lame.  And it may have holes that can be poked in it with enough eyes on it.  Eyes he's refusing to let see it.
        ```

        - u/abcd_z:
          ```
          >  I think secrets are lame.

          But, but, secrets are the only way to prevent nuclear annihilation!

          ...or something.  

          I... may have only *skimmed* Methods of Rationality.
          ```

          - u/lolbifrons:
            ```
            That which can be destroyed by the truth should be.

            Bring on the elder gods!
            ```

    - u/xamueljones:
      ```
      >Spoiler

      That seemed to be perfectly logical to me. >!Anyone who is trying to escape a 'jail cell' isn't going to force their way out past the experienced security guard. They're going to focus on the security's weak point, the brand new security guard barely past his first week on the job.!<
      ```

  - u/NZPIEFACE:
    ```
    Isn't the test for asking for a gatekeeper that designed the AI in the first place? Should it really be tested on random clouts?
    ```

    - u/HeroOfOldIron:
      ```
      True, but the point of the exercise is to prove that even a human level intelligence could trick/convince the average person to let it out. Once that's established, the next step would be to use that evidence to further fund FAI research, with the rationale being that if we don't, the first optimizer able to convince us to let it out will likely be what ends up kills us in its pursuit of infinite paperclips.
      ```

      - u/philip1201:
        ```
        There is a big difference between arguing that a random person on the street doesn't make a good bodyguard, and arguing that a professional bodyguard with all the relevant accolades doesn't make a good bodyguard.

        The point of the exercise isn't to convince AI experts that a random smuck isn't qualified to handle post-singularity boxed AI. It's to convince them that they themselves aren't qualified to handle post-singularity boxed AI (nor to build a box).
        ```

---

