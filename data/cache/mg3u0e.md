## [Superman FF][Incomplete] A Common Sense Guide to Doing the Most Good

### Post:

[Link to content](https://archiveofourown.org/works/30351690)

### Comments:

- u/alexanderwales:
  ```
  I was doing a usual overview of stuff that's in my drafts folder, and saw this sitting there for the umpteenth time, still without a proper ending, or really even a plot to speak of. I think when I wrote the first bit of this, maybe five years ago, I was intending it to be a companion piece to [*The Metropolitan Man*](https://archiveofourown.org/works/17356310/chapters/40838042), dealing with epistemic humility and dealing with experts when you, yourself, are not an expert, and the toll that always pushing yourself to do the most good can have, and ... it's kind of not there, and nothing that I want to try to salvage for spare parts in any future project. Because there's no proper plot, I'm fine with putting it up, especially because it has a particular kind of density that I like. However, you should be duly warned that there's not a satisfying wrap up to this.
  ```

  - u/WantToVent:
    ```
    I know it is not related to the current story, but in Archive of Our own when you try to see the entire "Worth the Candle" only shows up to chapter 221.
    ```

    - u/alexanderwales:
      ```
      Have you tried clearing your cache? I think it's related to that, as it loads for me. Other people have had the same problem and that's fixed it.

      (At this point, the full work of WtC is quite large, and I know AO3 has some issues with size (the story used to, and might still, break some of their download links). The fact that so many people are getting an old version might be related to some arcana of webdev.)
      ```

- u/IamJackFox:
  ```
  You say that this doesn't have an ending. I think it might imply an ending after all, and a rather tragic one at that.

  We know—from a meta perspective—that Luthor is evil, although Superman in-text only knows that he dislikes him. Luthor admits that:

  1)	He would lie, kill, and destroy if he had Superman’s powers, supposedly to create a greater good.

  2)	From his point of view, there should be essentially no difference between him having Superman’s powers and Clark having Superman’s powers. They should both be doing the same thing, which is what Luthor wants (ostensibly improving quality-adjusted-life-years as a metric)

  We know that Luthor is smarter than Clark. We know, based on Clark’s observations, that Luthor’s charitable impulses prior to joining MoSI were a sham, used primarily for leverage and virtue signaling.

  The very obvious conclusion is that Luthor is going to manipulate Clark into following a moral framework that suits Luthor’s ends. And, if we draw an unfortunate parallel to the AI-In-A-Box experiment, we have to assume he will succeed. Luthor is Clark’s psychologist and has executive powers to decide when he talks to Clark, and for how long. 

  In that alone, he has far more success than canon Luther ever did, and far greater destructive potential. The opportunity to befriend and manipulate Superman should have been the first thing a power-hungry Luthor thought of, not something reserved for fanfiction.

  Admittedly, the difference in intelligence is not as large it is in the AI-Box experiment—Luthor is only mortal, after all.

  …but it just might be enough. And I think the implication, as the story closes, is that it *will* be enough.
  ```

  - u/None:
    ```
    I think it's worth noting that there are limits on what outcome Luthor can manipulate him to. Not *big* limits, but unlike normal patsies to installing a power hungry dictator if Clark stops wanting to support Luthor and instead tries to topple him, well, that's it, good game Lex. He *can* put the ai back in the box, albeit not costlessly. And there's a lot you can do with the sunk cost fallacy and rationalization but that only goes so far, especially since Lex can't stop Superman from speaking to others unless superman agrees to it.
    ```

    - u/None:
      ```
      [deleted]
      ```

      - u/CCC_037:
        ```
        > The entire asteroid thing is potentially a situation contrived by Luther to make Clark vulnerable.

        Hmmm. It's an asteroid, a thing in space.

        What do you think the odds are that it's glowing green?
        ```

        - u/pochinha:
          ```
          I immediately assumed it was and that's where the story was going. The conversation about the ethics of being a dictator caught me completely off guard.
          ```

      - u/NestorDempster:
        ```
        I think you guys might not be giving enough sympathy to Luthor. Per cannon, Luthor really is quite good at being a benevolent dictator, and in this scenario it doesn't seem like he's accumulating power and influence just for their sake, or for the sake of great material comfort, but rather to change the world for the better.
        ```

- u/Krossfireo:
  ```
  I actually really like where this ended. It explored the problems, stated a philosophical question, and allows the reader to answer it on their own
  ```

- u/Roneitis:
  ```
  It's good. It lacks an ending, because an ending would need an answer, and we, at this time, can't give an answer.
  ```

- u/The_Wadapan:
  ```
  feels like tagging this as incomplete or playing up that side of it is a little silly, because if not for that you really wouldn't know, and it seems a shame that people might be put off and miss out. It sounds like the story turned out differently to how you intended, but I don't think "there isn't a satisfying answer" is the same as "there isn't a satisfying ending"; I found the ending satisfying simply in resolving to sit down and try to solve a problem that seems unsolvable. Any presentation of an actual considered solution would undercut that theme imo!
  ```

  - u/alexanderwales:
    ```
    My experience with posting shorter, more ambiguous stuff that doesn't follow a conventional plot structure is that I'll always get comments wondering where the rest of the story is, or people feeling that there should have been another 80K words to round out the story. In this case, I think there's quite a bit of merit to it, but I did post it because I thought it *kind of* worked on its own, or at least had some interesting explorations of ideas. I don't know, I would rather that people don't get their hopes up and be pleasantly surprised.
    ```

- u/Gigglen0t:
  ```
  This was amazing! I get so frustrated when I read a super hero fic where they just go out and hurr durr I punch lung real good. 

  What about roads, buildings in danger of collapse, construction, demolition, emergency services? 

  So thank you!
  ```

- u/daniels220:
  ```
  So first of /u/Roneitis is entirely right—writing an ending for this would mean coming up with a conclusive answer, and that would completely change the nature of the piece.

  I do have...uh, it turned into a cluster of related additional arguments...to contribute, here. I'm pretty sure I've heard forms of this in the rationalist community before—I'm afraid I can't say where, I'm less well-read and less involved in the community than I might be, and I don't habitually keep citation notes of things I pick up by osmosis. That said, lemme try to attack this from a few different angles—and these may not all be aimed at precisely the same central point, though I hope they're at least circling the same region of concepts:

  I basically don't buy that if the quants actually did their job right, the choice would be as clear-cut as Lex thinks and Superman fears. Trust is difficult to quantify, and critically _being observed quantifying trust breeds mistrust_. I suspect that if the quants did, on running all their analyses, come down on Lex's side and MoSI went ahead, they'd look back 5-10 years later from either a pseudo-benevolent 1984 scenario or a starkly divided world of MoSI and a sort of "give me liberty or give me death" opposition coalition and wonder where it all went wrong...

  _Perhaps_ —perhaps! This is not certain!—MoSI's scientists and ethicists and quants can do a better job than the rest of humanity at deciding how the world should be run. Certainly they can do a better job than the worst of the worst, the dictatorships and such. But they cannot act on that without also, in a certain sense, legitimizing _everyone else_ acting like _they_ know best how the world should be run. The amount of backlash that kind of intervention is likely to provoke from well-meaning moderates who don't actually know what they're doing seems to me like it might _far_ outweigh the benefits of wiping out the obvious evils.

  Re: both of the above, of course they would do their best to do as much as they can subtly. The thing about that is that _secrets always come out eventually_, and the more of them you have the angrier people get when they find out. (Or, well, maybe I have a little too much fiction on the brain saying "always", but, "often", and the backlash is potentially enormous.)

  Short and pithy but, I think, not worthless: Don't be Cauldron. What good is saving humanity if you give up your humanity to do it? "It's for your own good", 'nuff said. Etc. etc. etc. (Yeah, the Cauldron example specifically is more extreme than I think even Lex wants MoSI to be, but I think some of the same lessons, about how "making hard choices" can so easily turn into "being a colossal power-tripping asshole—_or being perceived that way and reacted to that way even if it's not true_" still apply...)

  Note that I don't think the implied path Superman is on right now is correct either. For instance given the example with scouting for Tomahawk missiles, a proposal: Assign whatever the actuarial value of a life is (_not_ the incremental cost-to-save-a-life, which is much lower) to each person killed by a military operation, and have MoSI "fine" that military accordingly (not to take the fine money by force, but to refuse to work for anyone associated with them until it is paid). Alternatively, just declare the killing people is _not fucking okay_ and if you fire missiles Superman will shoot them down. Then offer the service of "enforced negotiation"—honestly it would probably take Superman less time to just go _capture_ the people who would have been killed by the missile strikes than the cost of the missiles (15min of Supes' time is a _lot_). Then bring them to a neutral location under the guarantee that both sides get to go home unharmed if they can't come to some agreement.

  Mind you this is only the barest beginnings of a start and would fail horribly as written—I leave it to MoSI's crowd of ethicists, political scientists, diplomats, etc. to hammer it into working order :). But the basic idea is, simply put: _act like heroes._ Be _smart_ about it, do all the EA stuff too, but when intervening in politics, don't do so as politicians. Be transparent, even if it allows your "enemies" a clearer picture of your actions—but be quick to demand transparency in return and hammer the point home if people refuse. Draw clear bright lines in the sand, even if _in this case_ you might get a better result by bending them. Set an example, expect others to follow, and protect them from those who would try to hurt them for doing so.

  ----

  Oh, I also think this piece downplays the impact of market disruption, both on MoSI and on everyone else, specifically when discussing the asteroid thing. When operating on that kind of scale, money is barely even relevant. The question is, what can be _done_ with those resources, and what benefits will those actions provide? Who will need to be involved in them and what else could they (_would_ they) be doing instead? Will those changes end up actually _hurting_ the people you're trying to raise money for charity to help? Because I feel like they really might... Blech, I'm grasping at something here that I don't fully understand—maybe someone with more economics background can do better?
  ```

  - u/CouteauBleu:
    ```
    > I basically don't buy that if the quants actually did their job right, the choice would be as clear-cut as Lex thinks and Superman fears. Trust is difficult to quantify, and critically 
    > being observed quantifying trust breeds mistrust
    > . I suspect that if the quants did, on running all their analyses, come down on Lex's side and MoSI went ahead, they'd look back 5-10 years later from either a pseudo-benevolent 1984 scenario or a starkly divided world of MoSI and a sort of "give me liberty or give me death" opposition coalition and wonder where it all went wrong...

    Yeah, my reaction when reading this was that it MoSI sounds like it has some institutional problems; I would not take the word of the quants as gospell.

    For instance, the quants seem to be pushing for direct landing of the asteroid instead of space-mining because it has lower capital costs, but... come on. They're talking about an asteroid that can give them more value than the entire US GDP; that will give them so much money that it's hard to predict what the economy will look like once they start introducing that much money in it, and they're starting to think about what to do once they hit diminishing returns on the very idea of "spending money".

    With all that in mind, they want to pick the option with increased X-risks because it's cheaper? I'm extremely skeptical of the thought process there.
    ```

- u/aeschenkarnos:
  ```
  I really enjoyed this, though the big missing part of the effective altruism argument is, obviously, personal satisfaction: Jon Kent gained great personal satisfaction from selling popcorn. Of course you know this, but it’s not quite loud enough in the story. This version of Superman is completely denying himself personal satisfaction.

  I think you can progress it further by introducing at least one, probably two, other perspectives: Bruce Wayne and Diana Prince. Barry Allen (cf The Ballad) and J’onn Jonzz (what use is personal INT when you can leverage *everyone’s* INT; imagine a Martian Manhunter as smart as the sum of everyone in a 50m radius) might be interesting to add in too, especially if that “asteroid” turns out to be a similarly INT-boosted Brainiac. 

  Which just seems to be the obvious-as-hell foreshadowing: Brainiac is the perfect adversary for INT-boosted Superman, because Brainiac canonically is ten times smarter than Superman, Luthor, or Batman, and yet *still* has reasoned that the best way for it to act, is  to be a dick to everybody. (And perhaps it is correct; iron sharpens iron, and He Who Comes Next ... DARKSEID ... the most effective possible anti-altruist ... needs to be fought by experienced heroes, not noobs.)

  Have you read Ted Chiang’s *Understand*? That, for me, is the gold standard of writing super intelligent characters.

  Also (re)read the classic Astro City story with Samaritan and Winged Victory having a conversation on a date.
  ```

  - u/DuplexFields:
    ```
    Bringing in some of the larger Superman-related DCCU, BrainiAC makes the logical end-villain, Supergirl and Kandor would change MoSI’s outlook dramatically, and the asteroid being Kryptonite would add a more immediate drama than the existential issues Luthor raised in this chapter.

    One thing this Superman has avoided is being seen as the wrestler to beat to be the big dog. Anyone who tries to take him out is obviously a total jerk wad and bad guy.

    What it has cost him is being an inspiration, as opposed to being merely a good man. And just as it looks like he might be heading back toward becoming the symbol, Luthor seems to be ready to convince him to be the full utilitarian.

    Surely this Clark has read books of wisdom and morality, and has come across the theory that traditional moralities exist with some ridiculousnesses because they solved rare side-effects in opaque ways. If so, he has grounds to refuse Luthor’s path heading into less moral utilitarianism, even if he can’t prove it the best idea.
    ```

  - u/VanPeer:
    ```
    Thanks for the recs!
    I am checking out the Ted Chiang anthology that contains *Understand* from the library. On the Astro City story, do you recall which volume that is ? I might go to a comic store and browse for that. I haven't heard about the Astro City series till now.
    ```

- u/sparr:
  ```
  > because he was running a charity.

  This line is weird. It's an explanation of why he's charging people, so the line should be "because he was not running a charity", except he IS running a charity.
  ```

  - u/Olivedoggy:
    ```
    I loved that line.
    ```

  - u/LeifCarrotson:
    ```
    I found it a nice chunk of micro-humor. It was slightly discolored with my experience with some friends in nonprofits who are under-paid and under-respected, but I suppose that's exactly why it hits so well.
    ```

  - u/ketura:
    ```
    That was my favorite line. Such a great reversal.
    ```

- u/PresN:
  ```
  One thing I found interesting about the piece was the way, intentionally or not, that it ignored second-order effects. Superman has destroyed the spaceflight industry (or rather, has replaced it with himself)- presumably there is no quantification of the benefit to humanity to develop new spaceflight technology, either in terms of "Superman will do it forever" or "there's no issue with having to restart spaceflight decades later with a bunch of satellites you can't reach or replace". The asteroid, as well- it may make MOSI a hojillion dollars to spend charitably, but it will also essentially eradicate the mining industry in the rest of the world. Possibly that would shift MOSI's spending to help the communities that now abruptly have no industry, though there's no acknowledgement of that.

  More nebulously, MOSI is essentially replacing the "welfare" role of governments (at least governments where the problems are "low-hanging fruit") due to their larger resources and efficacy- what does it do to have governments that don't have a need to take care of their constituent people even if they could because MOSI will do it and do it better? Can that be quantified?

  This is all, of course, getting far afield from what the piece was trying to do, but I can't but draw the parallel between the likely effects of MOSI's dominance in the areas they focus in and comic book-Luthor's cry against Superman- that by solving problems for us, he makes humanity reliant on him instead of humanity improving. MOSI has a plan for keeping Superman running- do they have a plan for what to do if he dies?
  ```

  - u/--MCMC--:
    ```
    amusingly, I recall discussing [the rocketry industry thing](https://www.reddit.com/r/rational/comments/4uy79d/challenge_companion_superheroes/d5vbh74/) with OP some number of years back (& a few accounts ago)
    ```

    - u/PresN:
      ```
      Wow, that conversation either directly inspired the fic or they were thinking about the fic when they responded to you, thanks for linking it!
      ```

  - u/aeschenkarnos:
    ```
    > what to do if he dies

    This line of thought gives a *lot* of options for introducing further characters.

    “We opened a portal to another dimension, this girl came through, says she’s your cousin?” (“You want me to do *what*, Kal? You’ve been doing it for *how long*? How is that not slavery with extra steps?”)

    “STAR Labs/Cadmus needs some more of your semen, you need to go spend five minutes jerking off. If you want, you can think about me, I’ve texted you a pic.” ... “Uh, gee, thanks Lois.”

    “The Mars colony say they have found a sealed vault in the ruins, they want you to fly out there and check it out.”
    ```

- u/Gedusa:
  ```
  Great fic, I'm a sucker for "somewhat implausibly good, motivated and powerful person changes the world" stories.

  Deciding what the heck to **do** with MoSI's resources kinda reminds me of issues EAs are having right now with how to spend the [ludicrous](https://www.vox.com/recode/2021/3/20/22335209/sam-bankman-fried-joe-biden-ftx-cryptocurrency-effective-altruism) [amounts](https://en.wikipedia.org/wiki/Good_Ventures) [of money](https://en.wikipedia.org/wiki/Bill_%26_Melinda_Gates_Foundation) now circulating in the space of EAish charities. Fun to think about what you would do with a trillion dollars or more of resources and an extremely unique set of skills.

  I liked the theme of how much you try to work out yourself, versus deferring to experts whose analysis you can barely understand - and who may have their own agenda. Again it shadows what goes on in EA spaces.
  ```

  - u/--MCMC--:
    ```
    Ya, the removal of a lot of funding constraint in recent years is why mainstream EA's pivoted so hard towards 'finding and fostering talent' over earning-to-give, at least at the level of supergenius computer whizzes thinking about joining hedgefunds or whatever. Still lots of encouragement to donate to effective charities at the level of everyday folk tho!
    ```

- u/None:
  ```
  I have to ask, did you deliberately go with "common sense" as opposed to "practical"?
  ```

- u/ArmokGoB:
  ```
  Crap, I am definitely 100% Lex Luthor.
  ```

  - u/aeschenkarnos:
    ```
    Possibly a good motive for Luthor in this story is to find a way to take or copy Superman's powers for himself, rather than simply killing Superman.

    Killing Superman because he *might* become an existential threat (cf AW's *The Metropolitan Man*) becomes an increasingly stupid motivation, as the number of Superman-level entities proliferate. I mean, why the fuck not kill Green Lantern too? Same reasoning applies. Once there's a dozen or so of them, killing Superman would achieve *nothing* to make the world safer, and would obviously make the world *less* safe since many of those S-class entities are obviously far less moral (and from Evil's point of view, manipulable) than he is.
    ```

---

