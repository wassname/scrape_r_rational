## Worth the Candle - Soul self-modification ethics

### Post:

[removed]

### Comments:

- u/Audere_of_the_Grey:
  ```
  It's not about morality. It's about a slippery slope which is in fact kinda slippery.

  Say you have some ordered list of values S. Now consider the set of ordered lists of values which would be acceptable to you (as a replacement for your current values.) Let's say that this set can be derived from your current set of values S and call it S'.

  So for example, an ordered list of values which is identical to S except that the value of vanilla ice cream has been swapped with the value of strawberry ice cream is probably also acceptable to you and so would be a member of S', but an ordered list of values which is identical to S except that the value of your own life has been swapped with the value of strawberry ice cream is probably not acceptable to you, and is therefore not a member of S'. Notably, S' contains S and also very likely some ordered lists of values which are not S.

  Now consider the set of ordered lists of values which are acceptable to some version of you that has altered your values to be some member of S'. We can call this S''. Once again, S'' includes all members of S', and also likely some ordered lists of values which are not in S'. You can think of S'' as the set of possible souls which are "reachable" from your original soul in two steps of alteration.

  As you may have noticed, the set of reachable souls increases with each allowed step. Also, the set of souls reachable after n+1 steps likely includes some souls not reachable after n steps.

  This is quite concerning, because if a soul is not reachable after n steps, that means that it's not acceptable to a version of you which has modified its soul n-1 times.

  So if you modify your soul a bunch of times, even if the modifications are acceptable to you at each step, your soul might reach a state which would be unacceptable to the version of you with your initial soul.

  Now, notice that it is not *necessarily the case* that after modifying your soul a bunch of times you reach a state that would be unacceptable to the original you. It could be, for example, that you swap the values of strawberry ice cream and vanilla ice cream back and forth 20 times.

  However, if you model each successive decision as having some element of randomness, such that at each step each acceptable soul state has a non-zero chance of being selected, then as you modify your soul more and more times, the probability of you reaching a state which would be unacceptable to your original self approaches 1.

  Thus it's *very risky* to not set some kind of Schelling fence on modification to one's soul, and more conservative fences are stronger. If you just modify your soul whenever doing so seems convenient, you're very likely to end up someone that the original you would not have wanted to become. If you modify your soul only when it seems absolutely necessary, you're less likely to end up as an unacceptable person. Whenever you modify your soul for some reason which is less significant than your reasons for modifying your soul thus far, you effectively lower your threshold for soul-modification, and increase your chances of becoming an unacceptable person.

  Now, how is this different from modifying yourself in other ways, like therapy, or growing as a person, or making friends?

  The difference is that soul modification has no cap on how fast or drastic it can be. Changing as a person tends to be a gradual process, and so there's some element of slipperiness (a risk of becoming a person that you-from-the-past would not have wanted to become) but not nearly as much. You do actually want to be kinda careful about drastically changing yourself through means other than soul magic. If my friend suddenly went from being a total introvert to an extreme extrovert or vice versa, I'd be concerned. If that change was due to self-modification of their soul, I would simply be *much more* concerned.

  TL;DR: If you modify your soul willy nilly you might end up as Murder Ghandi, which would be bad, and modifying your soul for at all (particularly in ways that aren't *absolutely necessary*) can make you more likely to modify your soul in the future and end up as Murder Ghandi.
  ```

  - u/LunarTulip:
    ```
    You don't need to trust that every change you make will be an improvement, though, just that the net effect of *all* the changes you make will remain in S'.

    Personally, I self-modify *a lot*, and sometimes very quickly. (It took me maybe a minute or two, once I figured out self-hypnosis, to remove longstanding aversions to the smell of vinegar and the taste of ginger which had up to that point been plaguing me since childhood.) And it seems very plausible that some of the changes I make go against the values of the earliest me who decided that self-modifying a lot was a good thing to do. But nonetheless *my life, overall*, is far more in accord with her values than a more restrained approach to self-modification would have left me.

    (And this holds even given a shift in *terminal values* at one point. The me who got into self-modification was something vaguely resembling an egoist. After some consideration, a later me decided that the optimally-selfish thing to do would be to turn into a utilitarian. And she was right; the change made me notably happier.)

    In short: you don't just need to account for the risk of ending up in not-S', you also need to account for the *reward* of ending up in a *better* part of S'. If a given change has an 80% chance of increasing utility by 2, and a 20% chance of enabling undesired value drift in such a way as to decrease utility by 4, that's still a change worth making. And risk of undesired value drift *is* predictable; I'm a lot more cautious when I go anywhere near editing my terminal values than I am when I edit how much I like foods, for example, because a clumsy perturbation to the former will potentially pretty thoroughly derail my future actions, whereas a clumsy perturbation to the latter will most likely just change my liking of the relevant foods in the wrong direction, generally fixably.
    ```

    - u/WildFowl82:
      ```
      I once researched self-hypnosis for precisely this reason, but all I found were shady, unscientific resources that strongly set off my inner skeptic. It just.. sounds too good to be true for this to be a thing. Do you have any resources or an approach to self-hypnosis you'd like to share?
      ```

      - u/LunarTulip:
        ```
        Nothing much in the way of resources. As far as approach... I'm not sure the way I approached it will work for everyone? It was pretty thoroughly improvised, not dependent on any external advice-sources, and it was very plausibly dependent on quirks of how my mind is structured. But I can try to summarize it anyway, just in case it *does* work for other people.

        Step 1: get hypnotized by not-yourself, ideally several times, and get a sense of what the experience of it is like. (I personally did this by way of the various not-too-hard-to-find hypnosis audio files one can dig up online; alternatives exist, if you're not inclined in that direction or if you find it ineffective. Being hypnotized is very much a skill one picks up with practice, so the first time is likely to be the hardest.)

        Step 2: while *not* being hypnotized by someone else, run through the same sorts of mental actions you run through during the induction when being hypnotized by someone else, and try to enter the same sort of mental state. The trance will probably not be as deep; but hopefully you'll be able to enter it at all.

        Step 3: use whatever corners of your mind *aren't* tranced (which, due to the aforementioned not-as-deep thing, will probably be present) to generate suggestions for the hopefully-larger parts which are. Suggest whatever seems both useful and easy-to-do-with-hypnosis.

        (The first time I did self-hypnosis, I was testing it out as an idle distraction while waiting for a friend to show up for a Nerf axe fight; I used it to help prevent myself from forgetting my planned tactics mid-fight (which had happened to me during several previous such fights). That sort of attention-redirection is, at least for me, among the easiest effects to produce. Other things I did, over the next couple weeks, included ignoring discomfort from a too-cold floor, and shifting myself from adrenaline mode back to sleep mode after a false fire alarm woke me up several hours early.)

        Step 4: practice and experiment. Practice getting the trance deeper; try out different ways of generating suggestions for yourself to see what's most effective; try new sorts of mental actions, and see which ones do or don't work; try setting some triggers for yourself to access useful effects more quickly when not mid-trance, and then try to learn the mental actions which result from those triggers so well that you no longer *need* the triggers, you can just do the actions directly; try other things I'm failing to think of while writing this; in general, see what you can or can't do, and try to stretch the limits of that as far as you can.

        ...and that's it. When I followed this trajectory, in the first couple weeks, the effects I could get mostly were interpretable as placebo effects, albeit ones that it was very convenient to be able to generate on demand; after a couple weeks of practice, I became able to do much more unambiguously not-placebo actually-hypnosis effects. The first couple months were filled with huge numbers of low-hanging fruit of suddenly-easy self-modification that was previously inaccessible, as well as some scary moments of realizing that things I'd rather *not* do self-modification-wise were *also* suddenly easily accessible if I were to get clumsy. Eventually I cleared most of the low-hanging fruit, and from there self-hypnosis became a much less central part of my self-modificatory arsenal, moving from being The New Big Dramatically Overpowered Thing to being just one more useful tool among many.

        Your experiences may or may not match mine, in terms of all of this; but hopefully this summary is useful regardless.
        ```

        - u/WildFowl82:
          ```
          Fascinating, thanks for taking the time to type that out! I'd love to hear more about the "low-hanging fruit of suddenly-easy self-modification that *was previously inaccessible*" part. But I understand if you don't want to over-share.

          Does this mean you're able to act according to your values near-100% of the time now? Do you prefer veggies over sweets? Do you enjoy exercising? Are you able to get to bed at a set time every day and fall asleep rapidly? (Assuming you'd want those things.) Put differently, are you able to always act rationally (meaning congruently with your values)?

          That does seriously sound overpowered. I could do anything with powers like that. Why aren't people like you taking over the world?
          ```

          - u/LunarTulip:
            ```
            I *tried* the act-according-to-my-values-basically-all-the-time thing, but it didn't really work. It *is* possible for me to use hypnosis to hack willpower, but the hacking looks like *increasing the amount of willpower I can throw at a problem*, not like *reducing willpower costs*. Still potentially useful in the short-term, but not long-term sustainable. When I try to do self-hypnosis to get myself to do a thing I lack the willpower to keep myself doing normally, I can potentially get it to work for a while, but if I *keep* pushing it I end up developing an aversion to *hypnotizing myself with the expectation that I'll make myself do the thing some more* in the same way that I'm averse to the thing itself.

            (Which doesn't mean that I *can't* use it to contribute useful willpower-mustering effects when needed—during those first couple months, two examples I noted down were using it in order to help myself focus on writing papers for my finals in school (notably, that was the most intensely stressful paper-writing week of my entire time at school), and then later to get myself to start the occasional conversation which I had a strong social-anxiety-induced aversions to starting—but it means that I need to be relatively restrained with it, rather than having anything close to the superpower that you're envisioning.)

            In terms of low-hanging fruit... I don't have direct memories of everything, but I did some digging through my diary from the time, and I can at least give a broad sampling.

            So, first, there were various short-term maneuvers, like the previously-mentioned "focus less on the unpleasantness of the cold floor" thing and "go back to sleep after the fire alarm" thing, fuzzing my perceptions of text I was reading in order to avoid absorbing any of the spoilers I knew were littered around, calming myself during a particularly-stressful afternoon, getting up sooner after waking up in mornings, et cetera. Nice to be able to do where needed, even in the absence of direct long-term effects.

            In terms of long-term effects, there was the previously-mentioned thing with removing my aversions to the smell of vinegar and the taste of ginger. The central idea there was to generate a short-term hypnotic effect (a long-term one wouldn't have stuck in place, and would have been too high-effort to refresh constantly) to render them non-unpleasant, then use that short-term non-unpleasantness to reset my priors on how unpleasant they'd be next time, thereby bootstrapping the short-term effect into a new default.

            (How easy that maneuver is for me to pull off, I should note, is dependent on exactly where the unpleasantness of a given thing comes from; in this case, it was by longstanding negative associations dating back to my childhood, rather than by any more concrete in-the-moment effect, so it was relatively easy. But trying to do the same thing with apples, which cause an unpleasant physical sensation when I bite into their skin, was harder, because I'd need to remove the unpleasantness of *that entire class of sensations* rather than just of the narrow experience of biting-into-an-apple; I got partial results from that one, but it wasn't nearly as unambiguously effective.)

            By a similar token, I got rid of my tendency towards fainting when medical people stuck needles into me, which was previously somewhat inconvenient. I did that one by giving myself a trigger to shift my brain in a non-fainting-shaped direction and then activating it while being poked. It took a couple applications before I was really confident in that one and able to activate it by habit without needing to think hard about it, but ultimately it worked.

            Also, I managed to somewhat reduce my body-related dysphoria by giving myself a habit of avoiding paying attention to the most unpleasant-to-pay-attention-to bits of my body. I didn't come anywhere close to actually *getting rid* of it, but the reduction was still nice.

            Overall... self-hypnosis works as a willpower substitute for me, but only limitedly, and using it that way is much less sustainably effective than using it in certain other ways. Said other ways include redirecting my attention in non-willpower-demanding ways (as with the dysphoria and the spoiler-dodging), and putting myself into short-term altered mental states which *do* cost willpower but which don't need to be held for long enough or repeated often enough for that cost to be a problem (either because they serve their purpose quickly (as with putting myself to sleep) or because they can be bootstrapped into habits which don't require further maintenance (as with the smell of vinegar)).
            ```

            - u/Amonwilde:
              ```
              Are there any hypnosis recordings you recommend? Seems like one of those areas where there would be a lot of junk.
              ```

- u/GeeJo:
  ```
  I think at least a portion of it is Joon's ego and that the *specific* change Amaryllis was making was, effectively, to like him less. It's not noble, but knowing that someone—someone you yourself find incredibly attractive—pines for you is gratifying in a very base way.

  If the change Amaryllis was making was instead to fall out of love with Grak, my model of Joon as a person is that while he would still have reservations, he would not have put up quite the same level of resistance. It would be an "Amaryllis and Grak" problem rather than an "Amaryllis and me" problem. That she modifies herself out of love with him feels on some level like a rejection.
  ```

  - u/wren42:
    ```
    I think that's true for that one change. however *everyone* seems super freaked out by soul modification, even self soul modification. 

    There is a cultural/historical source for this, but it needs to have some real basis as well that made me he culture fear it in the first place.

    I think there is fear of addiction and snowball effects. Yes, people change naturally all the time, but the ability to make root level permanent changes to every aspect of personality and values with the flick of a switch is dangerous.  It is the same thing that makes self-mosifying supe-AI scary. 

    When you make one change, you can't be certain that the new you won't want further changes that the current self doesn't. The runaway effect could essentially (heh) destroy you as a person, and create a monster that only cares about "improving" itself...and others. 

    This is apparently what happened with many soul mages, and why it became illegal.  The tempation of continued changes snowballs into destroying the self and becoming an optimization machine.
    ```

    - u/xachariah:
      ```
      The Ghandi Murder pill isn't something anyone seriously worries about in real life because it doesn't exist.  

      I imagine it is well worried about in Aerb, since they've had to deal with it probably hundreds of independent times.
      ```

      - u/wren42:
        ```
        right, the nature of the magic available makes this a very real possibility.
        ```

- u/Fredlage:
  ```
  I’m not sure I’m the best person to explain it but I think the basic idea is: once you make that sort of thing you’re not you anymore, you’re a new person. And who’s to say that new person won’t decide there’s some other modification they’d like to do (maybe one the original you wouldn’t even really object to) and now they’re a third person. Said person decides on another modification and so on and on... it’s a slippery slope thing... at some point the person deciding on the next modification is so far removed from the original that they’re likely to make changes the original wouldn’t want, maybe even that they’d find unacceptable. 
  I think that’s the basic reasoning for Juniper’s rejection of the idea. Which isn’t to say his feelings and his ego don’t play any part in it, but I think it’s uncharitable to say that’s all there is.
  ```

  - u/SimoneNonvelodico:
    ```
    > I’m not sure I’m the best person to explain it but I think the basic idea is: once you make that sort of thing you’re not you anymore, you’re a new person.

    If you think changing your ideas, thoughts, and knowledge equates to killing yourself and creating a new person, I've got bad news to relate to you about what happened to ten-years-ago-you.
    ```

    - u/wren42:
      ```
      The problem is the speed and mechanism of the change, and that soul mages can do this to *other people.*. You start "optimizing" yourself, and soon become a person that only cares about optimization, and then start secretly optimizing others to fix the world and align it to your values. It's an addictive mental virus, in effect.  There's evidence this happened with other souls mages which is why it was made illegal.
      ```

      - u/SimoneNonvelodico:
        ```
        Well, but OP says that obviously doing it to other people is bad. I agreed myself above that it's super dangerous, but dangerous is not the same as evil. It all depends on what safeguards you put in place, but not the act of self-change itself.

        I guess as a comparison I could use this: is drinking alcohol evil? No. Is drinking alcohol, despite knowing specifically that you lose all control of your actions when drunk, while you're working at the control station of a nuclear power plant, evil? Well...
        ```

        - u/wren42:
          ```
          that's exactly the problem though.  a future version of you may cease to believe that doing it to other people is bad.  what safeguard can you put in place, other than what they are doing? (no changes without approval from the council, regular check-ups from other soul mages to ensure no drift or changes occurred) 

          the metaphor fails because drinking the alcohol in this case changes your mental state *permanently*, and you could potentially keep drinking more to change it further, indefinitely.
          ```

    - u/derefr:
      ```
      Your soul in WtC isn't your "ideas, thoughts, and knowledge." It's your preferences. Your preferences are all *you* are.

      Think about it this way: if a clone of you, who has all your *ideas, thoughts, and knowledge*, but different *preferences* — your evil twin! — kills you, that's a tragedy for you. You don't want that, and will resist it. They're "replacing" you, but it's not *just* that — they're going to go off and shape the world in ways you disagree with.

      Meanwhile, if you die, but in your place lives someone who shares *none* of your ideas+thoughts+knowledge but *all* of your preferences... well, that's just amnesia. People are usually pretty comfortable with that.

      It's also the reason people can be okay with teleportation, or with being temporarily simulated, etc. — if *their* conscious experience ends, but the conscious experience of someone *with their same preferences* goes on, then they can "rest assured" that the world will continue to be optimized in the way they wanted to optimize it. (Or, to go deeper on this, continuous conscious experience is really just a kind of animation/slideshow of a succession of different "yous" who happen to share preferences. You're *always* being replaced by the next "you" a moment later, but that's okay, because *they're* going to want what *you* wanted.)

      This is also why people feel that having children obviates mortality, somewhat — to the degree that you can make a child "carry on your legacy" (i.e. share your preferences, and carry them out in the world as an optimizer in your place), "you" aren't gone.

      When you dig into these philosophical edge-cases, most people find that what "you" are, all along, is just the set of things you do and don't want the world to become. The set of possible worlds that drive you to make them real; and the set of possible worlds that drive you to prevent them.

      And, in either case, changing the boundaries of those sets, makes for a person that the current you would probably fight to the death to stop.

      (If you can get behind the above, then here's a philosophical rabbit-hole of a statement to evaluate: "puberty is a violation of your human rights, since you're being replaced by someone with different preferences. Nobody should have to go through that without the consent of the prior person to be so replaced — where that consent would imply that they already had preferences that were compatible with the ones they'll get hardwired into them by the change.")
      ```

      - u/SimoneNonvelodico:
        ```
        > People are usually pretty comfortable with that.

        I wouldn't be. And this all is born out of the (fictional) conceit that you somehow can draw a sharp line between ideas and acquired knowledge and preferences. As if there wasn't a perpetual feedback loop between the two. My preferences are shaped by my experience. Had I grown into a war-torn country, or as a peasant in 1300, or as a king in Ancient Greece, I would be a different person. 

        > It's also the reason people can be okay with teleportation, or with being temporarily simulated, etc. — if their conscious experience ends, but the conscious experience of someone with their same preferences goes on, then they can "rest assured" that the world will continue to be optimized in the way they wanted to optimize it.

        I think this is a pretty personal viewpoint. The issue of the continuity of the self isn't this clear cut and in fact the paradox of teleportation lies in that - it's not clear that teleportation does not REALLY kill you and replace you with a copy. Mostly because teleportation doesn't exist and for all we know it might be impossible (and perhaps this impossibility makes the question itself nonsense).

        Again, consider this: someone is depressed. They are suicidal. They have a preference for death over life. Then they take antidepressants. Now they are not suicidal any more. Should they not do that because it's erasing who they are as a person? I can be okay with erasing 1% if it allows the remaining 99% to achieve its goals better, no?
        ```

        - u/derefr:
          ```
          > I think this is a pretty personal viewpoint.

          It's a viewpoint *some* people have. But, note my original phrasing — these are reasons that people *can* be okay with these things. As in, *for the people who are* okay with these things, these are the most likely justifications that such people would give (I mean, if you give them time to rationalize what are probably emotional visceral reations.)

          > Should they not do that because it's erasing who they are as a person?

          Well, yes, but actually no.

          If you're suicidal, you have *a preference for self-modifying to no longer have preferences* (or, equivalently, a preference for no longer being able to personally act to optimize for your other preferences.) In other words, you want there to not be a "you" in the world, in the sense of there being someone with exactly your preferences who has to put in the work to make the world match those preferences. A suicidal person who has shadow clones would *presumably* want to murder their shadow clones "for their own good" before ultimately killing their original body. (And indeed, in reality, sometimes you'll hear of depressed parents killing their own children "for their own good" before killing themselves. See above about children carrying on parents' preferences.)

          If you have a preference for "you" not going on, then that usually translates to no longer having any "selfish" preferences, e.g. preferences for the dispensation of your estate in ways that align with your interests. If you cared about these things, you'd want to stay alive in order to optimize the world toward them!

          But being suicidal *doesn't* usually translate to not having any *selfless* preferences, i.e. preferences about things happening that are "good" in some global-human-utility sense.

          And *usually* these remaining preferences, despite being nominally "selfless", still hew closer to the side of "selfish" than "selfless." Even if you don't want "you" to exist in the world, you'll still tend to care about/worry about *people like you* having good things happen to them in this world-without-"you", more than *people not like you* having good things happen to them in said world. E.g. you tend to worry about how your family/friends will go on without you, rather than worrying about what difference you could have made through efficient altruism.

          And when you apply that logic to the dispensation of *your own body* after your [ego-]death — well, what could be a better possible use for it than for someone *almost, but not quite, exactly like you* to possess it and make use of it?

          And that's basically what you're doing by taking the anti-depressants (or performing any other kind of discontinuous ego self-modification): you're "dying" and donating your body for the incarnation of a person who is almost, but not exactly, "you", to use as they please.

          The same instinct that drives people to donate a kidney to save a sibling, means that they'll tend to be okay with passing their body over to the control of a "mental branch-offshoot" of their own mind.

          (This presumably means that exactly the sort of sociopathic people who *wouldn't* donate a kidney to save a sibling, also will tend to be *much less okay* with the idea of self-modification even as a solution to suicidality. This is a testable hypothesis!)
          ```

    - u/ZorbaTHut:
      ```
      Legally, we attach different weights to "murder" and "natural death". It makes sense that we'd do the same with self-murder and self-natural-death.
      ```

      - u/LunarTulip:
        ```
        By that standard, self-murder happens all the time too, though. Character development isn't something that just passively comes to people, it's something people actively seek out. Why should soul-magic-based character development be treated as any more murder-y than mundane introspection-and-modification of the sort that I, and I suspect many others on this subreddit, do all the time, and which many people deliberately seek out help with in the form of therapy?
        ```

        - u/ZorbaTHut:
          ```
          I think people consider there to be a big difference between "working to change yourself over time" and "explicitly replacing yourself with a different self with a sharp discontinuity".

          Like, if you have a girlfriend, and you want her to be better at chess, you could teach your girlfriend chess, or you could break up with your girlfriend and go out with someone who's better at chess. But in one of those cases there's a discontinuity and replacement, while in the other case there isn't.

          I'm not sure this *matters*, note. I'm not taking the position that self-modification is inherently wrong. But I am taking the position that it's recognizably different, and that I can understand if people want to then treat it differently.
          ```

          - u/SimoneNonvelodico:
            ```
            Ok, but consider something like the Unbreakable Vow at the end of HPMOR. Would placing similar compulsions and restrictions *on yourself* voluntarily, for the purpose of fostering what you consider positive habits, be something you'd consider evil?

            Suppose I was an addict and suffered from it, if I could erase the tendency to addiction from my mind/soul in a snap, why would it be evil for me to do so?
            ```

        - u/Paxona:
          ```
          His point is idiotic in other ways too. We prescribe personality alteration drugs all the time, like hormones, anti depressants, stimulants, drugs for schizofrenia.

          It's nonsense.
          ```

          - u/SimoneNonvelodico:
            ```
            *sips coffee*

            "WHY ARE YOU SHARPLY AND SUDDENLY ALTERING YOUR OWN STATE OF MIND?"

            *goes to sleep*

            "ARE YOU TRYING TO MURDER YOUR OWN CONSCIOUSNESS?!?"
            ```

      - u/Paxona:
        ```
        'A man was depressed and sought a therapist. Said therapist prescribed him drugs.'

        Is this situation murder?
        ```

        - u/ZorbaTHut:
          ```
          I think there's an argument that it's kind of an assisted combination murder/suicide/birth, yes. I wouldn't insist it's morally wrong, but "well, it's arguably suicide but maybe not morally wrong" applies to other forms of more-universally-agreed-upon suicide as well.

          If someone gets kidnapped and force-fed permanently addictive drugs that change their entire personality and behavior, then set free, what is this? I think there's an argument that this could be considered murder, in a world where the self is considered more important than the vessel.

          (Which, largely, isn't the world we reside in, but is I think defensible.)
          ```

          - u/SimoneNonvelodico:
            ```
            But by that metric the self dies and is reborn literally every instant. Can you even murder something that has a lifespan of an infinitesimal?
            ```

      - u/SimoneNonvelodico:
        ```
        I mean, self-murder is just suicide and while we can argue to Hell and back whether it makes sense or if anyone can even be considered to be of sound mind while wishing for their own death, I would never call suicide "immoral".
        ```

- u/WalterTFD:
  ```
  You are killing me!  Making a post in /rational about Worth The Candle when we haven't had an updoot in forever?  Got my hopes all up!
  ```

- u/None:
  ```
  I didn't read the story, but the qualitative difference is this:

  Going to therapy changes us by changing the inputs of our mind, and those different inputs will (might) change our personality/behavior.

  With magic, the character changes their mind-software *directly*, without going through the input/output interface of their mind, and it therefore introduces a sort of discontinuity/rule-breaking on the meta level, which is not present when you go to therapy.
  ```

- u/SimoneNonvelodico:
  ```
  Premise: haven't read WTC, just exploring the concept.

  I couldn't see a serious argument for why doing something like this would be in itself intrinsically *evil* - as you said, "I'm going to see a therapist" or "I'm going to learn to play the piano" is effectively self-modification too, and no one sees an issue with it.

  I can however see a lot of arguments for why it would be *dangerous as fuck*, if we're talking modifications that are sudden and that affect your judgment - potentially on their very own success. It could also be considered ethically problematic depending on the circumstances: if you undertake a modification that could make you evil, and have no backups, you will not be able to judge yourself as being evil and thus going back afterwards. So you just created an evil being and added it to the world. Good job!
  ```

- u/GreenSatyr:
  ```
  Because of his bad experience with getting wiped out by Level Up, because of bad associations with Fhallatere or however you spell it doing it to his friends, and because pressing levers which do things that you don't understand is dangerous (although you'd think he'd worry more about int and soc). 

  Consider that the person in charge of implementing modifications is the DM, who is something of an adversary.
  ```

- u/Mr-Mister:
  ```
  It’s not that the action itself is antiethical, but that it can lead to antiethical actions: The whole group consensus thing is to minimize the risk of self-altering to a state where you genuinely think that certain actions (including further soul modifications) are positive and proceed to do them, when in reality they are contrary to the group’s ethics (and your previous self’s).
  ```

- u/None:
  ```
  I don't know if Joon would think this way, but it's similar to the argument against suicide---it's a decision you could change your mind on in the future, but making the decision stops you from being able to do so. With suicide obviously the thing stopping you from changing the decision is death, but with soul modification it's easy enough to continue modifying your soul to prevent it changing back, and changing your values on something will significantly influence your decision to continue doing so. Additionally, it's a drastic change. Obviously there's other forms of self-modification he would be okay with, but they're either not as drastic, not as permanent, or not as self-reinforcing.
  ```

- u/sicutumbo:
  ```
  I think there's an element of being unsure of how safe the procedures are. Juniper's skill in soul magic is piped into his brain by a not terribly benevolent deity, and at various points it's obvious that he sometimes lacks some crucial knowledge/skill in a magic, magic that he has significant skill in, that regular mages know really well. He would be using this magic to make fundamental changes to who he is as a person, with no good method for undoing those changes should he make a mistake. Soul magic has a relatively convenient interface for making those changes, it seems like it should be a bit harder to make significant mistakes, but it's really not something to approach lightly.  And these fears are definitely not unfounded. Raven and presumably other people can give histories of soul mages making exactly these kinds of mistakes.

  He could make bodily changes with relatively less risk of a slippery slope, but he doesn't seem to have any special knowledge of biology. Given how fine grained the bodily modifications seem to be, I don't see any reason he couldn't accidentally make changes that don't biologically function.

  All of the above goes doubly for spirit. He's the *only* practitioner for spirit, so no one else would even know what to do if he made a mistake, the interface is much worse, and spirit is a *lot* better at changing a person than soul magic is. It's a lot more equivalent to doing surgery on your own brain.
  ```

- u/RidesThe7:
  ```
  Perhaps there are concerns akin to those at play in the Legend of Murder Gandhi.  As best I recall it: Gandhi, offered a million dollars to take a pill that would make him a vicious murderer, declined.  But he was willing to accept the money to become 1 percent more murderous, this not being particularly murderous.  But a 1 percent more murderous Gandhi, being slightly more inclined towards murder, was now willing to enter into a deal where he took another pill, becoming 1 percent more murderous---and so it goes, until Gandhi had become very murderous indeed, far more than original Gandhi would have agreed to for any amount of money.
  ```

- u/None:
  ```
  [deleted]
  ```

  - u/GANDHI-BOT:
    ```
    Go stand in the corner & think about what you have done. Just so you know, the correct spelling is [Gandhi](https://en.wikipedia.org/wiki/Mahatma_Gandhi).
    ```

- u/WildFowl82:
  ```
  I never really got this part either. It just seems like an efficient way to improve yourself.

  Are you not yourself anymore after changing your priorities? It seems absurd.
  ```

  - u/Putnam3145:
    ```
    > Are you not yourself anymore after changing your priorities

    Take your most precious belief and imagine yourself with your most precious belief being the opposite of what it is. Is this person you?
    ```

    - u/WildFowl82:
      ```
      To answer your question: no, that person isn't 100% me, but 95% perhaps? Not actually that far off in my optics. 

      But it's a bit of a weird thought experiment because different values aren't independent. It doesn't make sense to change a single value, it'd revert because of other values supporting it, just as happens in story.

      Furthermore, in-story you get to choose which values to change. You'd only change values that you actually prefer to be different.

      So take your most precious belief, and then point to a secondary belief that limits you from pursuing the first. Remove the secondary belief. Is this person still you? I think so. All you've done is make your values less contradictory.
      ```

---

