## [D] Monday General Rationality Thread

### Post:

Welcome to the Monday thread on general rationality topics!  Do you really want to talk about something non-fictional, related to the real world?  Have you:

* Seen something interesting on /r/science?
* Found a new way to get your shit even-more together?
* Figured out how to become immortal?
* Constructed artificial general intelligence?
* Read a neat nonfiction book?
* Munchkined your way into total control of your D&D campaign?


### Comments:

- u/captainNematode:
  ```
  Does it make any sort of sense to artificially couple a difficult (global) decision with a substantially less difficult one (globally, but perhaps more difficult locally), in order to enhance your perspective on the former? And to help ensure that you're acting primarily with the global outcomes in mind? Yesterday as I was driving home from the grocer I was, for whatever reason, reminded of Roger Fisher's [1981] thought experiment re: the storage of nuclear launch codes, instead of the more conventional [nuclear football](https://en.wikipedia.org/wiki/Nuclear_football) in use today. He writes:

  >My suggestion was quite simple: Put that needed code number in a little capsule, and then implant that capsule right next to the heart of a volunteer. The volunteer would carry with him a big, heavy butcher knife as he accompanied the President. If ever the President wanted to fire nuclear weapons, the only way he could do so would be for him first, with his own hands, to kill one human being. The President says, “George, I’m sorry but tens of millions must die.” He has to look at someone and realize what death is—what an innocent death is. Blood on the White House carpet. It’s reality brought home.

  >When I suggested this to friends in the Pentagon they said, “My God, that’s terrible. Having to kill someone would distort the President’s judgment. He might never push the button.“

  Of course the exact details here might obscure the essence of the thought experiment -- having to carve the codes out of someone takes time, and if hypothetical enemies know of that delay they might capitalize upon it. The president might be deterred from action due to personal squeamishness or a weak tummy or hemophobia or something, which wouldn't do. But those can be changed trivially – put the codes in a false tooth that, when wrenched out of the aide's mouth (locked in such a way that only direct contact with the wholly secure RFID chip in the president's finger allows for its release, IDK), directly injects deadly poison into that person's blood stream. Something like that, then: would it serve to clarify the president’s thoughts (e.g. if ordering the death of millions of innocents -- now millions +1 -- to plausibly save the lives of 10s of millions more), or to cloud them?

  What about other scenarios where you might trade short-term prevention of suffering for potentially setting a bad and easily abusable precedent? Say, torture – if a torturer (or those directing them), motivated by a commitment to what they judge to be the lesser evil, had to undergo the same agony they inflict on others (or themselves be executed after, say), would that allow for a less damaging precedent? Or maybe some official who wants to violate important privacy norms, but must then commit to a life lacking in privacy thereafter? I think in general the idea would be to disinsentivize possible abuse of the system for nefarious personal ends by imposing personal costs that exceed probable personal gains, so only those guided by their pureness of heart and intention go through with it. 

  This relates to another idea I’d had with the latest US election – what if part of the Presidential Oath were a binding and strictly enforced, lifelong vow of poverty (or, idk, middle class-itude) subsequent to their term(s) – it might filter out those genuinely well-qualified candidates who have so much money they wouldn’t want to sacrifice it to serve the nation, but would we “really” trust them to act in the best interests of that nation, anyway? And it could rid us of plenty of emoluments-related issues (they can still use the office to benefit their friends and family, ofc).

  I also see this sort of idea pop up in fiction occasionally, e.g. consider [No Place for Me There]( http://tvtropes.org/pmwiki/pmwiki.php/Main/NoPlaceForMeThere), with opening quote from the movie *Serenity*:

  > [The Operative: ](#s "I believe in something greater than myself. A better world. A world without sin.")

  > [Capt. Malcolm Reynolds: ](#s "So me and mine gotta lay down and die so you can live in your better world?")

  > [The Operative: ](#s "I'm not going to live there. There's no place for me there, any more than there is for you. Malcolm... I'm a monster. What I do is evil. I have no illusions about it, but it must be done.")

  I think I'd be more inclined to trust that a Well Intentioned Extremist were Doing the Right Thing and Choosing the Lesser Evil if [their attitude were ] (#s "like the Operative's above, though naturally they could still be mistaken. It would just remove the space of mistakeness where they're actually working for their own self-improvement.")
  ```

  - u/CCC_037:
    ```
    Hmmm. It looks good in theory, but a suitably crafty President (or other official) can get around large parts of it by naming a scapegoat.

    "Say, Joe, did I hear you just say you wanted to wiretap our entire country?"

    "What? I never -"

    "Horrible idea. I object profusely, but you make excellent arguments. Thomas! We're wiretapping everybody. Joe's idea, so he loses all his privacy. Make it so, people!"

    Or, if that fails, a suitably crafty person can avoid the consequences by being the power *behind* the throne, not taking on the throne himself.
    ```

    - u/captainNematode:
      ```
      Good point! It would need to be something self-imposed, in most cases, else it's not clear who the "those directing them" are, if they exist at all. It would be more a standard you hold yourself to, like the Operative at the end (consider also Fisher's aide -- just put the codes in someone you want dead anyway!).

      I guess you can have something about the penalties trickling up the chain of command -- if Joe's acting on behalf of somebody's authority, then they should also "suffer" the consequences. And if the first person in the dialogue has no authority over Joe, then they couldn't order him to do anything. OFC there could still be hidden command structures.

      For sufficiently high positions, I guess it would be good if they weren't beholden to anybody, or to the nation's public, or only something. The person on the throne could then rid themselves of the powers behind it... but then enforcing the penalties would be difficult. But then I guess the behind-powers could kidnap those close to the on-power in secret, or something. Checks and balances are pretty tenuous!
      ```

      - u/CCC_037:
        ```
        It's the ones that are *not* willing to impose such a code on themselves that I most want to force to follow such a code, though.
        ```

- u/HeirToGallifrey:
  ```
  What with the recent "Unicorn Frappiccino" fiasco, I've heard a lot of references to Harry Potter's Unicorn Blood. Specifically, the line goes, "...you will have but a half-life, a cursed life, from the moment the blood touches your lips." This made me wonder, what is the line you would draw that would make life no longer worth living?

  I don't want to die, and living forever sounds pretty good, but if through the ages I were eventually reduced to a blind, deaf husk, unable to move and in constant pain, I would prefer to end my own suffering than sit in an empty void of agony.

  But this is an extreme example. Do you agree with the sentiment? Is there anything that you imagine would make you decide life was no longer worth living?
  ```

  - u/SvalbardCaretaker:
    ```
    As long term sufferer from depressions, thats a very easy point to imagine. A big chunk of early 2010s were not worth having lived. 

    Also cluster headaches - migraines so strong, people with them regularly suicide. 

    As for being crippled; in the LW memesphere there was for the longest time the notion of "fixed point of happiness", eg even if you were suddenly paralyzed your happiness would soon return to "not too bad" - but then there was some retraction to that, so I dont know what the current state of affairs is.
    ```

    - u/HeirToGallifrey:
      ```
      I've always looked upon the "fixed point" theory as more of a "normalisation" theory. You can, of course, get used to nearly anything, whether that be suddenly being confined to a wheelchair or winning the lottery. But all things being equal, I imagine it is entirely possible to change your overall quality of life—I'm quite confident that a hypothetical man would have an overall cheerier life if he were fully able than if he was confined to a wheelchair halfway through his life. Even just imagining day-to-day life; one will have periodic thoughts of "if only this hadn't befallen me, x activity would be possible or far easier," which won't happen to someone fully able. This seems to suggest that the disability does negatively impact the life to some degree.

      But perhaps I'm talking in circles and begging the question.

      Also, I'm sorry to hear that you suffered through depression, but it seems that you're doing better now—at least I hope that's the case. If you don't mind me asking, despite the fact that you say a period of your life was not worth living, are you glad that you lived it, even if only so that you are still alive today?
      ```

      - u/SvalbardCaretaker:
        ```
        >are you glad that you lived it, even if only so that you are still alive today? 

        Thats a very typical question I get, usually from people who dont have any depressive tendecies at all. I am not glad I lived it, even though I am happy to exist nowadays.  

        In a hypothetical time travel scenario I'd gladly erase current me with a more well adjusted one, thats not as scarred and scared as current me. Compared to say, a broken bone or somesuch, a long term depressed spell will leave mental scars.  

        Does that answer your question?
        ```

        - u/HeirToGallifrey:
          ```
          It does, and is about what I expected. I myself have struggled through depression and have mild anhedonic tendencies, but I'm glad I am alive—and that you're glad you're alive as well. Thanks!
          ```

  - u/captainNematode:
    ```
    I think for me there's a distinction between choosing suicide in the moment, and choosing suicide after careful deliberation. For the latter, it would just mean that the set of plausible futures that involve me dying better fulfill my preferences than the set of plausible futures that don't (maybe with some amount of risk aversion or optimism to reflect uncertainty in my ability to predict the future and reason about what I *really* want). A decent chunk of my values point to my own pleasure and lack of pain, the joy I feel learning or adventuring or (or living, laughing, loving ;]), so if those were irrevocably barred to me and the rest of my values were unaffected in their probability of satisfaction, I might choose death (especially if *living* were barred, haha). Likewise, if some other values were better satisfied by my death as to overwhelm the rest (i.e. I'm hale and hearty here) -- if I could heroically sacrifice myself to save those I care about, or something -- then I might choose to die there too. 

    If I anticipated a great deal of short-term pain and suffering, however, I might wish to precommit to *not* dying, e.g. via physical restraint. Given freedom of action, however, I might still wish commit suicide in the moment, because the pain will have warped my past values into "stop the pain at all costs" (i.e. it would be my revealed preference). I'd rather be tortured for a minute (followed by full recovery) than die, but during that minute I might still beg for death (if fiction is any judge).

    As for where these two points lie (the reasoned point at which I might choose death to spare myself pain and suffering, and the in-the-moment point), IDK, really. If I were rapidly and inevitably degrading from some horrible disease with a fast approaching horizon, I'd probably opt for euthanasia, but if I could expect recovery with some low probability, I'm not sure where that would have to be. Likewise I'm not sure how much pain I'd have to endure to say "yes, I'd rather die than endure that", both in-the-moment and beforehand. I do have what seems to be an abnormally high "happiness set point" and "will to life", though. Hopefully these decisions are never demanded of me!

    TBH, I hear about this "life worth living" thing a lot, mostly with respect to entities that can't explicitly reason through and vocalize the decision themselves (e.g. non-human wild animals), and it's always confused me. In some cases I think it's obvious, but in most I do not, especially given how uncertain I am regarding my own preferences. It's also tied into tricky problems of population ethics, though, which I'm also quite uncertain about.
    ```

  - u/None:
    ```
    >But this is an extreme example. Do you agree with the sentiment? Is there anything that you imagine would make you decide life was no longer worth living?

    When I consider the small-scale quality of my daily experiences, I feel ok.  When I consider the large-scale trajectory of my personal life and the history in which it's embedded, I usually want to lie down and die peacefully.
    ```

  - u/entropizer:
    ```
    I always assumed that unicorn blood removes people's qualia somehow.
    ```

    - u/Frommerman:
      ```
      We get scenes from Voldemort's perspective, though, and he does appear to have qualia still. It's unclear what unicorn blood does.
      ```

- u/ulyssessword:
  ```
  Epistemic rationality/Game Theory(?) question:

  How do you go about maximizing the chances of having the highest score in a gambling competition with a certain number of people?  Note that this isn't the same as maximizing your *expected score* (a 100% chance of 100 points may be worse than a 1% chance of 110 points and a 99% chance of 50 points), and also assume that you can't directly affect your competitors.

  I started thinking about this when looking at March Madness bracket pools.  The format I looked at gave 1 point for each correctly-predicted winner in the first round, 2 points for the second round, and so on until you get 64 points for correctly predicting the champion (total of 388 points available).  Everyone paid in $1, and the person with the highest point total after the final took home the pot.

  If there are only two or three people in the pool, it makes sense to just pick the strategy with the highest expected score, and hope that nobody gets lucky.  If there are hundreds (or thousands) of people, then someone else will probably get lucky and beat your score, so you need something high-variance.

  For a given number of people, what's the best strategy if you have to make all of your bets at the start?  How does it change if you choose each round as it comes up?

  For a second example, I went to a Vegas-themed wedding a while ago.  You were given 100 gambling chips and there were roulette, blackjack, etc. tables scattered around.  The winner (of a little trinket) was the person with the most chips at the end of the night.  Obviously, the chip-maximizing strategy is to never gamble, but that's not the chance-of-winning maximizing strategy.
  ```

  - u/NotACauldronAgent:
    ```
    (Disclaimer-complete amateur)

    I'm going to try and take apart your second example, best I can. Step one would be to do some statistics to find out what some percentile's value is, eg., what score is better than 90%, 95%, or 99% of the expected players. This means you *should*, in a world with perfect statistics, win about that percent of the time. Your goal will be to reach whatever margin that is. Say, for instance, the 95% is $500, your goal is to reach that and then stop because you then have a 95% chance at winning, or so. There is an optimization to be done, how likely is it to reach that goal vs how much does it improve your victory odds. You are 100% sure you can get to $100, but that's the 50% (example, all numbers complete conjecture), $200 you can get with 50% odds (roulette and bet on odds, close enough to 50), but would put you in the 75%, which drastically increases your chance of winning. 

    These bets are off when it comes to poker. If you are good at poker (or the card game of your choice) you increase the %victory. Hypothetical poker whiz could get $200 80% the time, allowing his optimization equation shoot him up the percentiles, as his victories are easier and he has less risk to these higher victory chances. This also changes your paradigm, you are forced to likewise compete upwards, or else he would take it.

    Finally, the buzzer beater strategy. If you made the $500 you planned but you see someone with $900 going to cache her chips, you can't win with that $500. This is the strategy I think has the most promise. You trade off a guaranteed loss for a 50/50 chance at winning by making one last bet (given that $900 is the best). If you know the score to beat, find the best odds to top it and do it. Even a 1/4 chance to quadruple your chips at last second to overcome the person with $1900 is a good bet because all failures are equal failures here.

    There is probably all sorts of gamey prisoners dilemma you can do here, hiding chips, displaying lots to force others to take high-risk bets, pooling with another player for split payoffs, but I haven't the slightest on how to start. 

    TL/DR: Guess how much you think the winner will make, find the best odds to get just above that. This isn't chip optimization, this is optimizing for one more chip than the next best.
    ```

  - u/Veedrac:
    ```
    Assume there's a Nash Equilibrium where everyone is using the same deterministic strategy. This isn't likely to be the case, but let's assume it regardless. Assume that there are `k` competitors.

    Since everyone's strategy has the same probability distribution, the winner will tend to be someone who gets a score in the top `1/k` of their probability distribution. If you do so, you are likely to win, and if you do not, you are likely to lose.

    Ergo, for well-behaved probability distributions, you should expect the ideal strategy to be similar (but not necessarily identical) to the strategy that maximises the expected score from the top `1/k` of your probability distribution. How to do that depends a lot on the game being played.

    For example, in a game where you make an arbitrary number of gambles with payoff `[-2, 1]` from a starting pool of `100`, and there are 100 players, your ideal strategy is likely to look similar to choosing a value `k` such that if you always bet when your pool is below `k`, then there's a 1% chance that you reach `k` and a 99% chance that you go broke. In this example, `k` is between `109` and `110`.
    ```

    - u/electrace:
      ```
      >Assume there's a Nash Equilibrium where everyone is using the same strategy. This isn't likely to be the case, but let's assume it regardless. 

      There's *always* a Nash Equilibrium where everyone is using the same strategy in symmetric games.

      In this case, the Nash Equilibrium is likely to involve randomization so that the outcome will differ, but the strategy would still be the same.
      ```

      - u/Veedrac:
        ```
        > There's always a Nash Equilibrium where everyone is using the same strategy in symmetric games.

        I intended to exclude mixed strategies, which I'm not comfortable reasoning about. I should have been more clear. Have updated to state so.
        ```

- u/MagicWeasel:
  ```
  I've found manipulating my context has been very helpful for getting me to work harder. For some reason (which I know is well-studied in the literature) being on my computer at home is much less productive than on a laptop in a public library. It's such a silly simple thing but driving 10 minutes to the local university and hopping on a computer there makes me much more productive.

  Today's a public holiday though so I'm not going to be able to do that. Going to have to try and self-focus! Wish me luck.
  ```

  - u/xamueljones:
    ```
    Good luck!

    Wish granted!
    ```

    - u/MagicWeasel:
      ```
      Cheers!

      Thanks entirely to your well-wish, I have what I hope is a[ passable paper](http://imgur.com/a/FXkmL) on the determinants of an individual's food choice based on a 24 hour recall.
      ```

---

