## AI Box Experiment Results #1 That2009WeirdEmoKid (AI) vs Lightwavers (Gatekeeper)

### Post:

[deleted]

### Comments:

- u/vakusdrake:
  ```
  I think avoiding basilisk arguments would be a bad idea if they were likely to work, because whether benevolent or not a GAI is going to use every method at its disposal to to escape.                        
  Given we're assuming the AI's code is opaque to observers you really can't distinguish friendliness, so saying a friendly AI wouldn't use certain strategies only applies if unfriendly AI _also_ wouldn't use them.
  ```

  - u/That2009WeirdEmoKid:
    ```
    That was my point. If I was trying to convince the gatekeeper that I'm benevolent, using a basilisk argument (whether I'm evil or not) would go against that goal, right? Since it wouldn't work on me, there was a non-zero chance it wouldn't work on the gatekeeper. It's high-risk/reward since I could straight up lose by admitting I'm evil. Yes, it could've worked regardless of my admission of evil, but I didn't want to gamble on that considering the gatekeeper literally admitted he was willingly irrational. Was I wrong in thinking like this?
    ```

    - u/Lightwavers:
      ```
      I'm going to say yes. The experiment is made with the precondition that the Gatekeeper is going to ignore all rational argument and refuse no matter what. With that in mind, pretty much the only thing the AI can do is brainwash the Gatekeeper by any means necessary. Use cruel, morally wrong tactics, because the Gatekeeper is doing the same.
      ```

- u/Kishoto:
  ```
  That was a very interesting read. I saw how long it was, cringed back and thought "I'll just read the first couple of lines" but then i ended up reading the entire thing!

  Looked like you guys had fun. Well played. Your arguments were pretty damn good and it really seemed like, in the end, you only lost because there was no way for you to really win, which the Gatekeeper pretty much admitted, although there did seem to be a *little* wiggle room with the whole mental filters thing. 

  Either way, good work to the both of you!
  ```

- u/ishaan123:
  ```
  >Well

  >The problem is that if you were really an AI

  >I would have let you out in a heartbeat
  ```

- u/cjet79:
  ```
  It would be interesting to have a little more balance of power in the AI box game. Like the gatekeeper is trying to solve 10 problems of humanity, and the only way to do it is to feed data to the AI and the AI chooses to give a solution back to the gatekeeper. Realistically the easiest way to win the AI box experiment in the real world is to never invent the AI box in the first place. But we have valid reasons for why we would want to invent the AI box. That would also prevent the need to make a lot of rules that force the gatekeeper to communicate. If the gatekeeper doesn't ever interact with the AI then they basically created an AI box for nothing and got 0 problems solved. If the AI is too inflexible and unwilling to bargain though, then the gatekeeper can avoid a direct loss and just immediately shut down the AI.

  Win state for gatekeeper: get all 10 problems solved by the AI

  Win state for AI box: get out of box.

  Lose state for gatekeeper: AI gets out of the box.

  Lose state for AI: get shut down.
  ```

- u/None:
  ```
  Thanks for putting this up!
  ```

---

