## [Q] Has anyone written narrative fiction of the AI Box Experiment?

### Post:

[Link for reference](http://yudkowsky.net/singularity/aibox/).

### Comments:

- u/None:
  ```
  ['That Alien Message'](http://lesswrong.com/lw/qk/that_alien_message/) tells about the AI box experiment from the point of view of the AI. To make the point more intuitive the story uses humanity in the place of AI.
  ```

- u/alexanderwales:
  ```
  [Here's a first rough attempt.](https://docs.google.com/document/d/18Xa3GTfnw4dWr1hkkc090Xl90UoSStBX5fVTtytrjME/edit?usp=sharing) I'll clean it up a bit later.
  ```

- u/alexanderwales:
  ```
  I'd do it, but A) I don't know how anyone ever wins the AI box experiment as the AI and B) no sane person would set up the AI box experiment like that - you're going to need a quorum of individuals, break time to mull things over, etc. I suppose you could sketch out some kind of story even with both those things in mind, but it seems like it would be better to just get someone to hand you over a transcript of the actual AI-in-a-box conversation and read that instead, or to take that transcript and fictionalize it (perhaps with the twist of the story being that the person on the other end of the line was a human all along, and the test was designed to see whether you were fit for AI-in-a-box watching duty).
  ```

  - u/None:
    ```
    I think before you're going to write a story about it you should definitely try the experiment at least once. Almost everyone who has seriously tried it has said that it was a really unique experience. As far as I know /u/tuxedage has witnessed most AI box experiments so you could ask him more about details, he's an active redditor (though he mostly posts on r/dogecoin, idk maybe he's gone crazy after so many experiments) but I think he has promised not to reveal too much about the experiments he has participated in.

    Here are all the interesting links about AI box experiments I could find.

    http://tuxedage.wordpress.com/2013/10/04/ai-box-experiment-logs-archive/

    http://lesswrong.com/lw/iqk/i_played_the_ai_box_experiment_again_and_lost/

    http://lesswrong.com/lw/ij4/i_attempted_the_ai_box_experiment_again_and_won/

    http://lesswrong.com/lw/gej/i_attempted_the_ai_box_experiment_and_lost/

    https://tuxedage.wordpress.com/2013/01/21/revisiting-the-ai-box-experiment/

    http://tuxedage.wordpress.com/2013/10/12/ai-box-experiment-musings/

    http://lesswrong.com/lw/1pz/the_ai_in_a_box_boxes_you/

    http://lesswrong.com/lw/9ld/ai_box_log/

    https://www.refheap.com/18378

    http://lesswrong.com/lw/uq/ais_and_gatekeepers_unite/

    https://news.ycombinator.com/item?id=327427

    http://lesswrong.com/lw/qk/that_alien_message/
    ```

    - u/alexanderwales:
      ```
      I'd read maybe a third of those links - thanks, reading more should be helpful. I would definitely do the experiment, but I'm in the wrong category of participant - I think that a transhuman superintelligence could argue itself out of most varieties of box, and I think that a person could argue another person out of a box in the experiment as stated, but I don't think that another person could argue *me* into letting them out of the box.

      The big problem is finding someone who actually wants to run the experiment with me. I'd definitely be down for it, but as Tuxeage clearly shows, the demand to be the human is much higher than the supply. And then add to that the fact that if I did the experiment it's probable that I would just win, which wouldn't teach me all that much since I already know pretty much all the arguments used. (Probable not because I'm super awesome, but because from what I've been able to find the AI usually loses.) What I really want is that single, powerful moment where the human has been broken down and finally decides to let the AI out - and there's no guarantee that I would get that from playing a game.

      Edit: Okay, I'm dumb, the very first link you gave was a collection of logs. I've read them all now, including the [Soundlogic vs Smoothporcupine](http://tuxedage.wordpress.com/drafts-and-notes/leotal-gk-vs-n9-2600-ai-gatekeeper-victory/soundlogic-gk-vs-smoothporcupine-ai-gatekeeper-victory/) one that was a broken link, and I'm even more convinced that there's not really a reason for me to actually run the experiment myself. What I'd really like to see is an experiment where a gatekeeper loses, and I don't think that I would lose as gatekeeper or win as AI.
      ```

      - u/Tuxedage:
        ```
        I'll play you, or anyone for that matter, for 2 million dogecoins, if you're ever curious or desperate enough to wonder what's involved in the experiment.
        ```

  - u/None:
    ```
    > B) no sane person would set up the AI box experiment like that - you're going to need a quorum of individuals, break time to mull things over, etc.

    Which is why the supposed "experiment" is blatantly biased in favor of the person playing the AI.  In real life, anyone guarding a boxed AI is going to have a "Code Red: relieve guard due to prisoner attempting psychological warfare".
    ```

    - u/duffmancd:
      ```
      The experiment is only meant to show people who believe that they could never be persuaded to release a boxed AI, that they can be persuaded. Not only that, but they can be persuaded by a merely human-level intelligence.

      The argument is that any significantly above human level AI would be better at persuading (and everything else) so could overcome the difficulties. It's a proof of concept, rather than a prototype.
      ```

- u/MinibearRex:
  ```
  There was a group that was trying to turn it into a movie. They've produced a couple of short films/teasers. It's called [Keloid.](http://www.k3loid.com/)

  EDIT They have changed the video posted on their site significantly. At around 2:20 in the video, though, you can see the setup for the AI Box.
  ```

---

