## The Asteroid Strike: Unconceivable Threats in Waves Arisen and HPMOR

### Post:

Imagine being a dinosaur. You’re chilling, eating grass, laying eggs, standing upright, whatever. Suddenly, an asteroid strikes Mexico, and you and everyone you know are dead.

This event, of extreme importance, had *nothing* to do with *anything* else that happened in your life. Not your choices, nor the choices of others so far as you could see, nor anything in the cosmic harmony of the universe that you were aware of called for such an event. It was a black swan, so to speak, or even more so: There was no sense in which a triceratops could have placed even an extremely low probability on an asteroid strike; “asteroid strike” was *not part of the hypothesis space*.

Imagine reading prophetic dinosaur literature, perhaps some fanfiction jotted out by the local equivalent of a Cassandra. It’s all well and good, standard fantasy stuff of a Chosen One completing their Destiny and facing off against the Bad Guys. Until chapter seven, when the Chosen One’s training in the ancient Dino Ruins to master the Super Duper Sword is interrupted by an asteroid strike that kills everyone.

It would be kind of shit. It obviates everything that came before it,  anything already written in chapters 1-6 and, just as importantly, the *promise* of chapters 7-12 (plus the two sequels, and the spinoff prequels, and the movies, and the toy line and the surprisingly decent video game), are wiped of all significance. *You’re not supposed to do that when you write.*

But in light of how dinosaurs *actually died*, there’s clearly an important message contained in this type of story. The challenge is in expressing it in a way that doesn’t make the reader hurl their copy into the bin.

Which brings me to two of my favorite fanfictions, *Harry Potter and the Methods of Rationality* and *Waves Arisen*. The former was written by “Lesswrong,” aka Eliezer Yudkowsky, and the latter was written by SOME GUY who *might* be Eliezer Yudkowsky or might just be someone exactly *like* Eliezer Yudkowsky, WE CAN’T POSSIBLY KNOW. Despite the potentially differing authors (snigger), these stories share common themes. In particular, they both fictionalize the Asteroid Strike.

(This essay contains complete spoilers for both of the above stories. In particular, if you haven’t read [*Waves Arisen*](https://wertifloke.wordpress.com/2015/01/25/chapter-1/), I highly recommend you do so before reading this essay.)

*Harry Potter and the Methods of Rationality*, or HPMOR, tells the story of an idiot who becomes ontologically incapable of being an idiot thanks to the magical intervention of a sociopathic, snakeghostmonster version of himself. HPMOR has an unusual structure, and never is it more unusual than in its last arc. Over the course of a lengthy and arduous journey to get the Philosopher’s Stone and resurrect Hermione, it is revealed that the entire life(s) of Tom Riddle has been one giant Asteroid Strike. Of relatively minor significance, we find out that Voldemort has been at Hogwarts the entire time, manipulating the students and teachers around him to engineer a situation to acquire immortality and, presumably, power enough to conquer the world, which Harry did nothing whatsoever to resist or even *notice*. But this isn’t really an Asteroid, because it was *conceivable*, even likely, that the Defense Professor was evil, as Hermione often pointed out. Moody even suspected him of being an outright Dark Wizard that was only pretending to be the Defense Professor. (When in fact he was a Dark Wizard pretending to be a Light Wizard pretending to be a Defense Professor—and even pretended to one Auror that he was a Light Wizard pretending to be a Dark Wizard pretending to be a Defense Professor, making him a Dark Wizard pretending to be a Light Wizard pretending to be a Dark Wizard pretending to be a Defense Professor. And since he honestly *was* trying to teach Battle Magic to raise up valuable wizards under him, he was a Dark Wizard pretending to be a Light Wizard pretending to be a Dark Wizard pretending to be a Defense Professor, *while actually being a Defense Professor*.) 

("Does the Dark Lord really use this many levels of meta in his plans?"

"Son, do my balls itch when it rains?" Moody asked. *Ah, so you have the Itchy Balls of Time*, thought Harry, while he put on the face of a naive and slightly perturbed eleven-year-old.)

("The word 'pretending' has stopped meaning anything to me," said Albus Dumbledore, averting nineteen ways in which the world could be destroyed. Then he ate a shoe.)

(Severus Snape quietly looked up all academic articles that mentioned "justified true belief" and had them destroyed.)

(Minerva took some more of the Muggle pills Harry had recommended.)

(Hermione was doing her homework and not reading any silly essays on the Internet until she was done.)

No, a teacher being Voldemort was the normal, literary event. (It literally happened in normal, albeit excellent, literature.)  The real asteroid strike comes in chapter 88 when Voldemort finds out that Harry is going to destroy the universe.

Voldemort says that the degree to which Harry can shatter the world and thus threaten his immortality was something he thought impossible. The Pioneer Horcrux was meant to be his failsafe against any possible disaster still achievable by wizardkind in the aftermath of Merlin’s Great Copyright. 

Harry thinks a lot in literary terms, and while some of that comes from his obsession with books, Voldemort (in the guise of Quirrell) also often speaks in literary terms—he is very much aware of the tropes and how others reason as if they live in a book. We know Voldemort is a collector of old texts and legends in the quest for power, and one can surmise that some of Harry’s appreciation for literature comes from Voldemort’s influence—I think it would be *hard* for someone who didn’t like books to acquire power in the HPMOR world. Voldemort, no doubt, has some view of the world in literary terms—he certainly found it entertaining, for example, to play the role of the Dark Lord, and apparently expected to find it entertaining to play the role of the Hero. 

Voldemort’s self-story of the Hero crusading against (his own) death and (everyone else’s) idiocy are shattered by a prophecy that makes his entire life’s quest seem utterly inconsequential.

And we realize something that would have made both Tom Riddles squirm:

*Time never cared about Tom Riddle.*

*It cares a lot about the sixteen year old Muggleborn with a wand and a physics textbook, idly Transfiguring something on a whim—*

*—who lives in New Zealand, has mediocre grades, has a personality that, if it were a food, would be best compared to oatmeal but with less flavor—*

*—has a crush on Sally Goatfucker or whatever people are named in New Zealand—*

*—who is totally, completely uninteresting, who can’t even be killed with the Killing Curse 2.0 because being indifferent to his existence is like dividing by zero—*

*—who has no place in stories, no place in prophecies, no place in the same literary universe with someone as interesting, as ambitious, as dynamic, cool, and awesome as either Tom Riddle—*

*—who is more important than both of them, and Merlin, and the Founders of Hogwarts, and literally every life-form that has ever existed in this universe and any others he may inadvertently destroy—*

*—because he is an asteroid, and the universe does not care about what black swans your hypothesis space can conceive of, it will blow you up because the Rules have you scheduled to be blown up, and you have failed the Pachinko Game of Life—*

Voldemort was just another tyrant, another dictator who would have killed and made miserable a lot of people until being overthrown. (Though, he might have found it amusing to dramatically improve Britain and the world, just to prove a point.) Harry even thinks about this explicitly when contemplating what to do with the unconscious Voldemort: in the grand scheme of things, Voldemort did not stand out in history. Just another murderer, another broken life—until his final act in binding Harry’s volition, Voldemort certainly had less impact on the world than Hitler, or Genghis Khan, or even someone like William the Conqueror, or any of the other historical unworthies of Time’s attention.

(Though most stories have the Evil King being the focus of Time. Because to even think of the Asteroid Strike story, you have to conceive of an asteroid, and that is hard when you are a dinosaur. And so what could be the most significant negative outcome but for the Tyrannosaurus to rule everything in his tiny, hilarious fist?)

Voldemort was mostly normal, for all his evil, for all his power, and if his intelligence was exceptional, well, so is the way my feet smell if I don’t put a cream on them. But this is not a thing the prophets speak of except to say, "Please put some socks on."

And so even Voldemort came very close to earning a grade of "Meets Expectations" in only Battle Magic class that ever mattered.

("I, I have to do this," the Headmaster explained gravely to Minerva, as he balanced on one leg while pouring tea into his hat, "you know not what lies in the balance," and Minerva turned to the door, lips trembling slightly, and deciding that she needed a break from Hogwarts, perhaps she would take an excursion to greet and help the next Muggleborn on their 11th birthday instead of having Hagrid do it as usual—)

Voldemort gets hit by an asteroid, the out-of-nowhere event that, without warning, without sign, is suddenly HERE, presenting an existential threat. And the rest of the wizarding world gets hit by the fallout only a short while later. In the aftermath of how Voldemort prevents the end of the world, Harry has FOOMed, going from a mere first-year student to a wielder of several powerful magical artifacts and having as Chief Morality Advisor a heroine with skill ranks in Being Immortal and the Bazooka Mastery feat. This happens over the course of…an hour? To the rest of the world, this occurrence is just, like, absurd. Like, life is just normal, and then BAM! HARRY POTTER! AZKABAN’S GONE! EVERYONE LIVES FOREVER NOW! 

And yet at no point does anything weird happen. Voldemort and Harry fight, Harry wins, takes Voldemort’s loot, and puts Hermione in the active party. Thus, he is now A Really Big Deal. *There was no break from the natural course of events that Voldemort led them on, which did not seem to be leading to FOOM*—and yet if you were a person in the world of HPMOR, it would feel like the world had just flipped upside-down in a completely incomprehensible way. 

The "FOOM" scenario sounds mysterious, and maybe even stupid—until you walk through it from the perspective of the one who is FOOMing, in which case it feels totally normal and not even particularly rapid or jarring, until you reflect back and realize you accidentally conquered the world when you were eleven. (By virtue of being an idiot, too—Voldemort basically ends up handing Harry immense power in the form of the Stone and a Trollmionicorn, because of the way in which Harry maneuvers himself into Voldemort’s power and moreover, would have otherwise destroyed the world in his carelessness and rationalizing. Thus also showing the idea that, mostly, FOOM destroys the world, and only through great and particular efforts can that be averted. This isn’t a *real-world argument*, it’s a method of *sharing the internal experience of a particular belief or set of beliefs,* letting you feel the algorithm from the inside; the piggies from Speaker for the Dead do not exist, but I know what it is like to be one more than I know what it is like to be, say, a Ukrainian person.)

Waves Arisen takes the Asteroid Strike much further than HPMOR.

If the Asteroid Strike is literary unfairness in-universe (*diabolus ex machina*, as Harry calls it), then Waves Arisen is brutally unfair to its main character—and to any readers expecting the story to grant Naruto certain privileges that are standard to protagonists. These privileges are things like extraordinary luck, unnatural wit, and a tendency for mysteries to be resolved, probability be damned. Waves Arisen does grant Naruto the anthropic fortune that nearly any story requires, and allows most of its scenes to have better comedic and dramatic timing than would ever likely occur in reality, but it does *not* answer many of the mysteries in Naruto’s life. Who killed the Hokage? Who killed Kakashi and Guy, and why? What was Sai up to? What was Kabuto thinking? Why do I have to know topology to read a fucking Naruto fanfiction? 

(The story gives plausible answers to all these questions but doesn't tell you outright—thus showing the best way to write a mystery is to just write the plain reality as the viewpoint character observes it and then *not spell out the answer*.)

Harry experiences what it’s like to face a foe not bound by narrative constraints when his efforts to protect Hermione are invalidated by a smarter, stronger foe who doesn’t acknowledge the camera and therefore has no qualms about rendering her defenseless off-screen. Naruto is struck by a number of tragedies he can do nothing to prevent. He has no warning, and the consequences are already permanent by the time he has any information of the event—most noticeably, Kakashi’s death. 

If Harry has to deal with an Asteroid Strike, then Naruto faces the Asteroid Field—and without the supernaturally lucky Han Solo to navigate it, he’s struck a number of times. 

The Asteroid Field works beautifully in a ninja story. A ninja world is inherently one of uncertainty and imperfect information, with plots within plots concealed behind masks that are smiling faces. Amid such a storm of varying ignorance and conflicting intentions, a ninja has to observe, evaluate, and act *without sufficient prior knowledge or ex post confirmation or denial*. This is most clear in what is probably the most ninja-ish scene I have ever read, in which Naruto speculates that Sai might be trying to kill them, finds a secret way to communicate this to Sasuke, and Sasuke then decides to kill Sai without even informing Naruto of his intention. Neither Naruto nor Sasuke had sufficient information to justify an execution, nor could either be certain of a successful execution, and they never find out if their actions were in fact correct. Nevertheless, while they did not have sufficient information to justify killing someone, within a ninja context one could also say that they did not have sufficient information to justify *not* killing someone. And it is in that space of uncertainty of goals, abilities, and outcomes that a ninja tale is at its most exciting. ("We might be sent to our deaths with a mission as the pretense," is not something Naruto considers prior to finding himself in the middle of exactly that scenario.)

And of course, if Naruto gets struck by a number of asteroids, then at the end of the story the ninja world basically wakes up one day to see a mega-asteroid hurtling toward them with “I DID THE DINOSAURS AND NOW I’M BACK FOR THE REST OF YOU FUCKERS” written on its Earth-facing surface. Naruto has FOOMed, and has come to reform the ninja world with…a deluge of cheap Japanese electronics, thus drawing an analogy between globalization of the world order and the resulting “superstar” economy that rewards intense specialization and talent at the expense of the average laborer, which is seen in how Naruto alone is more productive than ten thousand regular ninjas. Hinata is Hillary Clinton, unable to express herself honestly, which leaves her future uncertain, and Sasuke is George Bush, an idiot on a crusade who also happens to be hysterically funny to watch.

(I got an A in literary analysis in school. Well, I got an F, but I interpreted it as an A.)

(I also got an F in topology and tried interpreting that as an A. It didn’t work.)

(And that, ladies and gentlemen, is the difference between the humanities and the sciences. ...I also got an F in philosophy, but that’s just because I never went to class.)

We walk with Naruto through his FOOMing just as we do with Harry’s. Naruto is actually in control of his, but it still happens by accident. Through sheer coincidence, a couple of techniques he learns plus an inborn demon advantage gives him access to infinite chakra, near-immortality, super-fast learning techniques, near omnipresence, and lots of cheap, cheap manual labor. Never do the normal rules of the ninja world break apart, nor is there any great external shock or conceptual leap forward. Instead, just putting a few already-known parts together results in pseudo-omnipotence, just because they happen to do that when put together that way. It’s plausible why this has never happened before—the requirements are demon fox plus the protected shadow clone technique plus water element affinity plus, I think, sage mode—but all of these are known. In principle, anyone could have theorycrafted FOOM in this regard—and were Naruto a tabletop game, it would have taken the players about a week. Yet it’s clear why no one has even theorycrafted this. For one, no one has bothered to do rudimentary scientific activity with respect to chakra and ninjutsu, which is very plausible looking at human history. Moreover, no one is thinking about this stuff; everyone is focused on survival and politics and immediate, relatively small-potatoes struggles for power; no one has the ninja equivalent of the Sequences to broaden their horizons and expand their mind.

So it’s clear why this has never happened before or been thought about. Yet it’s also clear from the natural and plausible road that Naruto walks on the path to becoming God that the ninja world has absolutely no protection against this happening other than its prior unlikeliness. The ninja world has no laser defense system to protect against asteroids; they are still in the primitive mode of mostly never thinking about the problem while counting on pure luck to see them through. *Unlike us.*

If Naruto were evil instead of good, and there is nothing about the process of taking measurements on chakra and learning water clone and sage techniques that requires or creates goodness, then MegaSatanHitler would have conquered the world because people were too busy rooting for Team Leaf or Team Stone to notice just how exposed and fragile their weak and ignorant world really was.

*Unlike us.*

The ninjas worship television. *Unlike us.*

You shouldn’t believe anything because a fictional story made it sound plausible. In fact, there’s an [essay](https://www.lesswrong.com/posts/rHBdcHGLJ7KvLJQPk/the-logical-fallacy-of-generalization-from-fictional) about it, which wertifloke can neither confirm nor deny he authored. But you should *believe in the possibility of believing in it*. You should be able, if the story was successful, to "get it." After reading HPMOR, I can or think I can put myself in Yudkowsky’s head to some degree when he thinks of FOOM, the mundane processes he pictures when he imagines it happening, and ~~OH FUCK ROBIN HANSON IS VOLDEMORT~~

At the risk of delving into an interminable and pointless but pleasantly distracting discussion on What Rational Fiction Really Is, the Asteroid Strike stands out to me as the clearest way in which HPMOR’s structure diverges from standard literature in a manner that is particularly conducive to rational fiction. I’m not saying that you should all put Asteroid Strikes in your stories. But you definitely should.

But I’m not telling you to but do it.

(And don’t go too far with it—the rationalfic *inversion* of the Asteroid Strike happens in the forest when Naruto and his team wait in ambush for another ninja team. The ambush goes off without a hitch, they get the scroll, and head on their way. Naruto even reflects on how this isn’t surprising: a well-laid plain ought to work, that’s the whole of planning. Just as the villain will try to Asteroid Strike the heroes, the heroes will also try to win with Asteroid Strikes too, which is just a metaphor for a strategy that the opponent can’t interact with in any way before it defeats them. Since everyone wants to win non-interactively, the challenge is in either constructing a scenario in which interaction nevertheless happens or in writing through the non-interactivity, as both HPMOR and Waves Arisen do, albeit in different ways.)

In conclu*boom*

### Comments:

- u/EliezerYudkowsky:
  ```
  Now and then I'm accused of having written something that doesn't appear under my name, and sometimes in one of those cases somebody talks about "Eliezer's style".  There is much in this world that I cannot confirm or deny, but I'll at least say this much:  I did at least once go and write a piece of online fiction to test my writing skills, not under my own name.  This work isn't what you could call prominent, but it is sometimes casually mentioned in somebody's "Best Of" list.  To the best of my knowledge nobody has ever accused Eliezer Yudkowsky of having written it.  Because I tried to write in a different style, and so far as I know, it totally worked.

  Nobody responding to this message will mention the work in question either, unless somebody throws out an enormous shotgun list - say, a list so long as to contain at least 9 other works.  The writing didn't especially sound like Eliezer, or rather didn't sound like you imagine Eliezer to be; so you didn't notice.

  (Of course I could be wrong about that, in which case I will privately be very impressed that you called it; but needless to say I shall not confirm nor deny.)

  It is also the case that I've at least once been falsely accused by more than one accuser of having written an online fiction piece to which I in fact made no contribution at all, not even beta reading or advice about plot or dialogue adjustments.  Authors sometimes try to do the same things I tried to do in HPMOR, and then somebody thinks that's \*Eliezer's style\* instead of a particular set of writing goals... so it goes.
  ```

  - u/xamueljones:
    ```
    I know that you will not want to share what was the unknown work was, but I have a fear of missing out on an interesting story to read. Can you at least tell me if you have anonymously posted it to r/rational before? I would feel better because if it was a story shared there, then the odds are very good that I have read it.

    Thanks.
    ```

  - u/wren42:
    ```
    \> so it goes. 

    &#x200B;

    oh my god he wrote Slaughterhouse Five and then ironically time traveled to plant it in the past!
    ```

  - u/Makin-:
    ```
    Wait, didn't you also originally release HPMOR pseudoanonymously? If I remember correctly people did guess that you wrote it before you came out and admitted it. 

    Your reply doesn't really seem like it adds a lot to the discussion if that's the case.
    ```

    - u/gbear605:
      ```
      IIRC, it was released under the username LessWrong, and he was the biggest poster on LessWrong, so it wasn't much of a stretch.
      ```

- u/CCC_037:
  ```
  > (I also got an F in topology and tried interpreting that as an A. It didn’t work.)

  Of course not. F and A have different numbers of loops (zero and one, respectively). You'd have better luck reinterpreting your F as a C.
  ```

  - u/JARSInc:
    ```
    I thought an F could only be re-interpreted as E, T, Y, and some types of G and J. Aren't we losing a node in a C interpretation?
    ```

    - u/CCC_037:
      ```
      No, we're just reducing the length of that leg to zero.
      ```

- u/DaystarEld:
  ```
  This was fantastic. Great analysis and made me laugh multiple times :) Thanks for writing it!
  ```

- u/SimoneNonvelodico:
  ```
  I frankly disagree with anything in HPMOR being an "asteroid strike". HPMOR followed very much narrative convention in its core structure: it had a hero, a villain, a challenge, foreshadowing galore, a final reckoning. While the way we get there is very different, the fundamental structure isn't all that different from the original HP books: Harry and Voldemort are fated enemies, linked one to each other, and in the end one *has* to defeat the other - and he does. That's not an asteroid. Neither are the concerns about Harry destroying the universe, which are mostly a plot device, and potentially sequel fuel.

  It would have been an asteroid strike if Harry had *actually* inadvertently brought forth the collapse of a metastable void state with his partial transmutation experiments, thus obliterating the Earth and producing a lightspeed expanding bubble of annihilation, and leaving all plot threads midway through.

  I agree that this would be an interesting way to end a story. Hard to make it not feel a blatant troll, and perhaps it would be more fit for a short story than a novel-sized one, where the investment of the reader would grow a lot before being frustrated so blatantly, which would make the whole experience just really unpleasant.

  BTW, it's not exactly rational, but this reminds me of *Inuyashiki*. A superhero manga where [Inuyashiki ending spoilers](#s " in the end, the long anticipated final fight between the good guy and the villain who we've been following since chapter 1 never happens because an asteroid is about to hit the Earth and they both sacrifice themselves to deflect it and protect their respective loved ones").
  ```

  - u/himself_v:
    ```
    They meant this was an asteroid to Voldemort.
    ```

    - u/SimoneNonvelodico:
      ```
      It still wasn't. An asteroid to Voldemort would have been *a literal asteroid hitting Voldemort*. This was Voldemort pointing his telescope at the sky and spotting an asteroid hurling towards his position, giving him time enough to plan for its arrival (in a way that conveniently set up his own eventual defeat). It wasn't sudden, it wasn't disruptive of the plot, it was a perfectly normal occurrence, a plot twist, but not one that killed the complexity of the story or completely upended its narrative structure.
      ```

      - u/himself_v:
        ```
        For him that was a story about him fighting the prophecy of "each will kill the other", with the plot twists and near deaths related to that. Then on page 240 he's informed that he, his prophecy and his storyline are irrelevant because the real trouble is the kid ending the world.

        It might work as a "quest upgrade" if you frame it right, but coupled with that Voldemort lost... Well, I guess you can frame it as a "Won once, got quest-upgraded, sacrificed himself but saved the world" kind of story. But for Voldemort it would have strong undertones of "where the hell *that* came from?!"
        ```

        - u/SimoneNonvelodico:
          ```
          It certainly was an event that changed the direction of the story, but those aren't unheard of in stories. No one died and no previous buildup had to be radically discarded; they only had to revise their objectives. By comparison, even the [GoT spoilers](#s "Red Wedding") is a much bigger upset.

          The "asteroid-like" event would be something that completely runs counter any narrative wisdom. Something where for example a long time spent to build up a character and the reader's affection towards them is willingly thrown out of the window because of a completely random occurrence unrelated to anything else happening in the plot. Like, dunno, the Hogwarts Express crashes from one of those very tall bridges due to a structural failure. Everyone dies at the beginning of Year 2. The rest of the story is the professors dealing with guilt and grief. Also, Voldemort is  now unopposed and triumphs. End of HP. 

          Stuff like this isn't usually *done* in stories because it would just sort of ruin their point. It would make them empty, purposeless, meaningless. Like, y'know, real life. But exactly for that reason it would also be interesting to see it happen, sometimes. It's definitely nothing like the Voldie thing though. First and foremost, a true "asteroid-like" event would be *enraging as fuck*. Like, you could see the cleverness afterwards, but the first reaction of any invested reader should probably be to just slam the book against the wall.
          ```

          - u/FeepingCreature:
            ```
            I mean, the asteroid running counter to the narrative wisdom is still privileging the meta-narrative. You _are_ physically allowed to point your telescope at the sky and spot the thing.
            ```

      - u/None:
        ```
        I thought it was about how the prophecy was an Asteroid to Voldemort. Not a perfect one, mind you, because he would have known prophecies existed, but still an Asteroid, since they still seem really rare in-universe.

        Voldemort was going about his life, on his own personal mission, just kind of mucking about here and there, killing a few idiots, maybe planning to take over a government or two. Nothing special. Nothing that hadn't happened a thousand times before him, and would continue to happen long after he had died. He was like a T-Rex, hunting and killing and, while limited, still essentially the master of their own destiny. 

        And then the Prophecy happened, and nothing else mattered. Nothing else was important, because this was Happening now. There was nothing he could have done to avoid it. There was nothing he could have done to predict it. And once it happened, there was nothing he could do to escape it. From Voldemort's point of view, the Prophecy being made was like an Asteroid; a sudden, incredibly impactful event that cannot be combated, avoided, or even predicted.

        All the events of HPMOR are just Voldemort trying to fight the Asteroid, and failing; proving once and for all just how futile it all was. He was like a T-Rex, roaring at the Big Rock. Powerful and terrible in his own way, but ultimately irrelevant in the face of such overwhelming force.
        ```

        - u/SimoneNonvelodico:
          ```
          I just don't think it's especially interesting though because what would make the Asteroid unusual is it happening to the protagonists, in a way that upsets the story. Asteroids happening off screen aren't that rare. Every time you have a story where "X suddenly dies" and the protagonist has to deal with it, that's an Asteroid for X - but it doesn't break the main story.
          ```

          - u/None:
            ```
            Depends on the kind of story to be honest, and how important the character is. The asteroid in HPMOR is interesting because of how important Voldemort is to the plot. Theorising on what the plot was like from his perspective is interesting because he was an interesting character. 
            Otherwise I agree with you.
            ```

  - u/wren42:
    ```
    the asteroid strike to me is the prophecy of world destruction.  everyone is carrying on in literary tropes learning magic or being evil or whatever, and behind the scenes a massive unavoidable disaster is hurtling toward them all.  Harry's life and ultimate victory are more driven by the unseen actions of dumbledore and fate that got him to that point than his own competence.
    ```

- u/Flashbunny:
  ```
  I apologise for being off-topic on the first comment, but I must know - have you ever actually come actoss Time Cube fanfiction? What was it like?
  ```

  - u/HeroOfOldIron:
    ```
    I honestly didn't read any of this essay, I'm just here for this.
    ```

    - u/Escapement:
      ```
      The only Time Cube fanfic I've ever seen: [The Wisest Steel Man by Scott Alexander](https://archive.fo/uwJUc)
      ```

- u/Sparkwitch:
  ```
  Whatever camp you're in, I'm in the other cabin. We've got the same counselors, we obey the same rules, we've got more or less the same schedule of events... but we're on *different teams*.

  By the time a protagonist breaks the world, I have already lost interest. Sometimes the story is good enough on its own merits that I'll keep reading anyway, but sometimes it isn't. HPMOR was at its best, for me, when it pointed out aspects of the world of Harry Potter that weren't particularly well thought out. The last arc you loved so much was the one that well and truly lost me. I'd been wanting to see how the story ended for a long time by that point, and I was *profoundly* disappointed.

  I can't even remember the point at which I got tired of The Waves Arisen. I had wiped whatever I had read from memory so thoroughly that I went to check up on it and recognized the map. Reading the first chapter brought back enough that I don't have to read any more.

  To explore my frustration without resorting to fiction: [Final Exams](https://www.reddit.com/r/FinalExams/)

  A particular selection of HPMOR fans were so proud of their ability to save Harry at the end that they thought they were so *unlike* the ninjas that they would apply the same brainstorming process to such oncoming asteroids as whether P=NP and the outcome of Israeli elections. It didn't last long.

  One can know all the rules of The Wizarding World easier than one can know all the rules of emergency room efficiency. Because the rules of the former were made by one person - in the case of fanfiction, two - and the rules of the latter are deliberately confounded by a bazillion of them, hungrily exploiting loopholes to their advantages and then closing those loopholes and opening others.

  The world we live in is the result of thousands of years of such scheming. People more clever and better informed than Big Yud on any and every particular subject have been beating their collective heads against the Greatest Problems since before anybody knew what time it was. They continue to.

  They are inspired to "shut up and do the impossible" and, as you say, the fact that they believe they *might* do the impossible is what keeps the world turning. That and the conservation of angular momentum.

  I want more deconstruction rational fiction, where it turns out that everybody else already did the thinking that our conventional protagonist might and witness the way that world looks *afterwards*... or, if authors want a real challenge, a deep reconstruction where we explore the hitherto unseen scaffolding that supports the way things are. The protagonist's successes are smaller, more focused, and less likely to destroy the potential for sequels that match what we loved about the original.

  Imagining that by strict application of first principles you might implement the FOOM isn't *rational*. Inspiring, sure, and more likely to bring about positive results than my cynicism, but it wouldn't be a smart bet. An asteroid strike isn't a black swan if you can see it coming.
  ```

  - u/DaystarEld:
    ```
    This is also really well said. Probably the only part of your post I have serious disagreement with (on a "claim about reality" level, I actually totally get your taste preference too, a *large* part of me also would have much preferred an HPMOR where The Prophecy never happened) is this one:

    > People more clever and better informed than Big Yud on any and every particular subject have been beating their collective heads against the Greatest Problems since before anybody knew what time it was. They continue to.

    I have very low expectations that EY is not actually smarter than the vast majority of people, even those trying to solve the Greatest Problems. This is not a statement about the limits or lack thereof of EY's current domain expertise, or current capabilities or skills or knowledge about any particular fields. I don't agree with everything EY says, and obviously think he would be even smarter and more clever if he could clone himself and learn all the things he doesn't know he doesn't know and then merge to integrate it all, same as anyone. 

    But if you've met many of the Serious People of the world and think they're remotely as actually-clever-and-sane-and-intelligent as EY, I'd love to meet the ones you're thinking of, because the ones I've met are marginally-smarter-than-average at best, and most have either lucked into wherever they're at or leveraged a particular set of expertise at a particularly opportune moment.

    Like, there are a lot of super clever and intelligent people throughout history and the current world, and some are on EY's level and some are even higher, but this line just rubbed me the wrong way. It assumes a level of intelligence and cleverness that I have failed to see in most people who are in charge of institutions. I don't mean to imply there aren't *good reasons* for that, or that many of the world's hard problems *really aren't as simple as depositing one EY into the equation would help solve,* but I wouldn't rule that hypothesis out until it actually had a chance to be tested.

    (This isn't as specific to EY as it sounds, btw, there are others in the rationality community and outside of it that I think are far cleverer than the majority of people in positions of power and prestige, and that's an important failure mode to pay attention to in considering why the world is in the state that it's in)
    ```

    - u/SimoneNonvelodico:
      ```
      I think he means more scientists and philosophers than, say, politicians. The latter too have a form of intelligence, often very advanced, but it's usually more geared towards manipulating social/emotional variables than towards manipulating symbolic logic (which is the scientist's usual mindset), because that's what usually all possible roads to political power - from plotting at the court of the King to democratic elections - select for.
      ```

      - u/DaystarEld:
        ```
        *shrugs* Personally I've found that even accomplished and celebrated scientists and philosophers are *incredibly limited* in their rationality.  This does not by any means make them stupid or incompetent, only limited in things like their ability to apply epistemological rigor to their beliefs and carefully assess complex systems and social dynamics with minimal prejudice or bias.
        ```

        - u/SimoneNonvelodico:
          ```
          And EY isn't?

          We've all got flaws and blind spots. Sure, not everyone in science is a novel Einstein or even necessarily able to think out of the box, but that's also a function of how *big* science is these days - it's an industry in itself. And even people with blind spots can be very good at cracking *some* topics and problems. For example, a genius solution to global warming would be worthless without the political ability to get it accepted and espoused by the general populace (in fact, I would say this is a big problem science has had with GW and other topics: relying too much on scientists who want to do everything and are unwilling to engage fully with politics has led to piss-poor communication choices that have chipped away at its credibility, making anti-science instead a viable political position to simply harvest certain forms of discontent).

          Ultimately, whenever the solution to a problem is a sort of "spherical chicken" approach, that's not a solution at all. As EY himself aptly put it, if the people who wrote stories about conquering the world with smarts were really *that* smart... they'd be conquering the world. Which isn't possible with sheer rationality and careful planning exactly because the world doesn't work like a story and throws, not necessarily asteroids, but at least micrometeorites at you *all the time*.
          ```

          - u/DaystarEld:
            ```
            Isn't what? Incredibly limited in his rationality? Quite frankly, if he is, I'd love to see your example of someone who is less so.

            I feel like you're throwing the goalposts across the field. We've gone from 

            >people more clever and better informed than EY are beating their heads against every Big Problem

            which is the claim I objected to, to 

            >even people with blind spots can be good at cracking *some* topics and problems

            which I agree with, but doesn't address the original point, and

            >As EY himself aptly put it, if the people who wrote stories about conquering the world with smarts were really that smart... they'd be conquering the world.

            Which is rather a massive step to leap to, cleanly dodging the point that *no one* is currently conquering the world on their own thinking, which means penalizing EY for not doing so (or whatever the metaphor "conquering the world" is acting as substitute for) does not actually demonstrate that "people more clever and better informed than EY" are working on all the major problems, which was the original argument.

            You have to actually demonstrate that EY is stuck with spherical chicken approaches before you assert things like "EY isn't fixing X therefor EY can't fix X," and even then you would not have proven that people more clever and better informed are working on fixing X.

            Also, kind of off topic but I'm not sure this:

            >relying too much on scientists who want to do everything and are unwilling to engage fully with politics

            Is actually why anti-science political views exist. I'm pretty sure it's because when there are vested and powerful interests that are inconvenienced by truth, those interests are incentivized to spend massive amounts of resources to trick or mislead people, which includes politicians and their constituents. 

            Like, yeah, politics is super important and scientists are not always great communicators, but I have no idea why you'd think that the reason GW is not commonly accepted as true is that there just aren't enough people willing to clearly articulate the problem, and feel like you need to understand Conflict Theory better.
            ```

            - u/SimoneNonvelodico:
              ```
              > does not actually demonstrate that "people more clever and better informed than EY" are working on all the major problems, which was the original argument.

              Ok, agreed. So let's put it this way - of course I can't *demonstrate* something this vague (after all, it's not like we can even measure something like cleverness in an objective way). My general idea is, for any given field, there exists a compendium of accumulated knowledge through centuries or millennia of studies by reasonably brilliant people. It seems reasonable to assume such a compendium will be in general superior to the output of a single human, however brilliant they can be, because it constitutes the synthesis (through filtering and selection of ideas, in an evolution-like process) of the work of dozens and dozens of people probably similarly brilliant. Any specialist in a field spends a significant part of their life studying such compendia - significant enough that you simply wouldn't have enough time in your life to get the same level of expertise in *two* separate fields. So basically, any specialist will probably be only really good at *one* thing, no matter how clever they are. For any other things, they still can contribute valuable opinions and viewpoints - even occasionally with the benefit of outsider perspective - but they'll also probably have a lots of blind spots, unknown unknowns, and suffer a bit from Dunning-Kruger syndrome. Applying one's own expertise to tackle a completely different field is [very tempting](https://imgs.xkcd.com/comics/here_to_help.png), but more often than not only seems like a good idea because you're just *that* ignorant.

              > Is actually why anti-science political views exist. I'm pretty sure it's because when there are vested and powerful interests that are inconvenienced by truth, those interests are incentivized to spend massive amounts of resources to trick or mislead people, which includes politicians and their constituents.

              > Like, yeah, politics is super important and scientists are not always great communicators, but I have no idea why you'd think that the reason GW is not commonly accepted as true is that there just aren't enough people willing to clearly articulate the problem, and feel like you need to understand Conflict Theory better.

              Of course there's actual powerful interests. That's the point, that's why you need to be a good politician to push this stuff. There's *always* powerful interests. Expecting that all you need to do is spell out the truth and it'll magically stick is naive. To anyone listening who doesn't have the first hand evidence that you do, it's no more truth than what someone else says - it's just a matter of *who do they trust more*. Of course GW is already a difficult narrative to sell to begin with (time scales beyond human scope, requires to go against social inertia, etc.) so it wasn't an easy task either way, but more political savviness could have probably helped. How exactly? I don't know, I'm a scientist myself so probably would have screwed it up as badly.
              ```

              - u/AugSphere:
                ```
                > My general idea is, for any given field, there exists a compendium of accumulated knowledge through centuries or millennia of studies by reasonably brilliant people. It seems reasonable to assume such a compendium will be in general superior to the output of a single human, however brilliant they can be, because it constitutes the synthesis (through filtering and selection of ideas, in an evolution-like process) of the work of dozens and dozens of people probably similarly brilliant.  

                > Applying one's own expertise to tackle a completely different field is very tempting, but more often than not only seems like a good idea because you're just that ignorant.

                Didn't EY just recently write a whole book on why this argument is wrong? With an example how he single-handedly outperformed the sum of medical knowledge when treating seasonal affective disorder, no less. That's not to say any random shmuck can expect to pull this off reliably, but we *are* talking about EY in particular.
                ```

                - u/I_am_your_BRAIN:
                  ```
                  Such a claim, if one he made, just absolutely bleeds Dunning-Kreger by the torrentful.
                  ```

                  - u/Veedrac:
                    ```
                    FWIW Dunning-Kruger's original claim is normally misinterpreted.

                    > The critical point to note is that there’s a clear *positive* correlation between actual performance (gray line) and perceived performance (black line): the people in the top quartile for actual performance think they perform better than the people in the second quartile, who in turn think they perform better than the people in the third quartile, and so on. So the bias is definitively *not* that incompetent people think they’re better than competent people. Rather, it’s that *incompetent people think they’re much better than they actually are*. But they typically still don’t think they’re quite as good as people who, you know, actually *are good*. (It’s important to note that Dunning and Kruger never claimed to show that the unskilled think they’re better than the skilled; that’s just the way the finding is often interpreted by others.)

                    https://www.talyarkoni.org/blog/2010/07/07/what-the-dunning-kruger-effect-is-and-isnt/
                    ```

                - u/SimoneNonvelodico:
                  ```
                  > Didn't EY just recently write a whole book on why this argument is wrong? With an example how he single-handedly outperformed the sum of medical knowledge when treating seasonal affective disorder, no less.

                  Link? If true, that'd be really outstanding, I'll concede.
                  ```

                  - u/FeepingCreature:
                    ```
                    [Inadequate Equilibria](https://equilibriabook.com/).

                    Where Eliezer makes the point that you can't expect the accumulation of expertise into a field of knowledge that's smarter than any one participant if there isn't a mechanism that _specifically_ rewards this, and how to spot fields in which this mechanism is absent.

                    Also notable for the rejoinder that everybody misunderstands efficient markets - just because you can't profit doesn't mean that other people can profit; it might just be the case that _nobody_ can profit in which case the market is doomed to mediocrity. Markets hit the ceiling, but the ceiling might be low.

                    (My personal pet peeve: _philosophy._ I have unlearnt how to think of free will in a noncompatibilist fashion, and so the fact that philosophy as an institution has not yet converged on strong compatibilism as the authoritively correct explanation for free will makes me embarassed for the field. It's obviously correct, guys, get a clue!)
                    ```

                    - u/SimoneNonvelodico:
                      ```
                      I agree that you can't just accept the notion that things work as they are, and entire fields can lock themselves into a rut. I actually read recently a book by Sabine Hossenfelder on how this seems to be the case for modern particle physics, [Lost in Math](https://www.amazon.co.uk/Lost-Math-Beauty-Physics-Astray/dp/0465094252/), and I tend to agree with her on that. But I'm also really wary of anything that suggests that I should take at face value the idea that this thing that looks simple to crack *is* actually that simple and everyone else is just too stuck in their own assumptions and can't think outside the box.

                      To make an example, I *do* have a feeling that medicine suffers from some of these problems. Seen from the outside, a lot of the field seems stuffy, locked into practices and habits (especially in terms of how things are taught and learned) that seem more the product of its historical tradition than of sensible didactic practice. The gap between the state-of-the-art researchers and your common GP seems immense. It seems hardly believable that there could be *anything* beyond simple gut-level decisions or optimism-biased assumptions going on when a doctor hears you describing a bunch of symptoms and rules out that it's just some trivial thing in three minutes without even touching you. I don't think there is, in fact. But the problem is also, while I do have these suspicions (and the right to express them, I think), every time I talked with people who actually *are* doctors about them I always got the answer that basically the whole shebang is just such a convoluted fucking mess that these sort of heuristics are still the only effective way we seem to have to navigate them. Of course, one could argue, maybe they just say that *because they're doctors*; they've been trained a certain way and can't see beyond the habits and prejudices that have been drilled into them together with the knowledge. That can be true. But either way, I can't imagine any serious reform coming from anyone *but* a doctor who still understands the knowledge sufficiently to realise what could be done to amend the way it's applied, because mine are just surface level feelings. There's too many things I ignore to actually make any kind of *constructive* proposal.

                      And when EY for example makes the Bank of Japan example in his book (just started reading), he *does* mention the opinion of professional economists. It's not something out of thin air. Rather, you get actual experts who don't have the sort of ties that will coax them into a socially-reinforced or biased position ("I don't want to criticise my senior colleagues, therefore I will not openly question their decisions even if I think they're wrong), and develop a contrarian opinion. It's not literally *just one person*.
                      ```

                      - u/FeepingCreature:
                        ```
                        > It's not _literally just one person._

                        Good comment! Just saying, to be fair, neither is AI safety.
                        ```

                        - u/SimoneNonvelodico:
                          ```
                          True, but at a difference with economy, no one can truly say to be an expert on what a "true" AI would be like - experience with modern ML systems hardly helps much in assessing that. Doesn't mean that there aren't necessarily any risks of course, just that in this case it's all a lot more speculative for all parts involved.

                          And of course there's also a lot of cross-disciplinary stuff involved. For example, as a physicist, regardless of whether an AI can develop malicious intent, or otherwise be willing to overstep its boundaries, I am highly dubious that there is room in the laws of physics to allow some sort of cataclysmic scenarios (such as grey goo). My impression is that there are so many fundamental limits once you try to go beyond a certain level of performance you just stop achieving meaningful results. You can only munchkin Nature so much.
                          ```

                          - u/FeepingCreature:
                            ```
                            > (such as grey goo)

                            Epistemic status: layman theorizing.

                            Note that life exists. How close do you suppose plant cells are in their performance to the limits imposed by the laws of physics?

                            I suspect that "grey goo" invokes the wrong association by its association with the somewhat discredited notion of autonomous mechanical nanobots. It may make more sense to instead think in terms of making a plant or bacterium "grey gooey." Given how fixedly our biosphere's chemistry seems to have been determined by a few initial random decisions, it does not seem safe to assume that there aren't much more efficient and competitive ways to build cells out there.
                            ```

                            - u/crivtox:
                              ```
                              You don't even  have to do much better than life.If you were a magical thought experiment being on earth before life and were worried about "green goo" covering the planet to the point it looks green from the atmosphere and  alters the composition of the atmosphere  you would be pretty right.

                              Anyway its not like grey goo being a thing is necessary for AI being dangerous.
                              ```

                            - u/SimoneNonvelodico:
                              ```
                              > Note that life exists. How close do you suppose plant cells are in their performance to the limits imposed by the laws of physics?

                              For stuff like photosynthesis, for example, probably skirting it, given their other constraints. Even in the lab we can't do much better with photochemical devices than the top performer in the plant kingdom (sugarcane, ~8% conversion).

                              > Given how determined our biosphere's chemistry seems to be by a few initial random decisions, it does not seem safe to assume that there aren't much more efficient and competitive ways to build cells out there.

                              Maybe those attempts *have* existed at the very beginning - and have been completely outperformed by the current paradigm.
                              ```

                              - u/FeepingCreature:
                                ```
                                > Maybe those attempts have existed at the very beginning - and have been completely outperformed by the current paradigm.

                                True, but note that building a cell lets you, in theory, chain together dozens of decisions that only pay off in combination. Evolution can't really do that, it can only retroengineer it by coincidence.

                                IMO for cells as we know it to be optimal, the efficiency ceiling has to be either low or reachable by many different paths, so that almost every beginning state can be moved into a near-optimal endstate by near-monotonous incremental improvement. Evolution never even hit the wheel; it certainly would never have hit the nuclear reactor. Such structural and systemic improvements that are so plentiful in the macro would have to be near-absent in the micro. This just seems intuitively implausible to me, though I acknowledge that's not much of an empirical argument.
                                ```

                                - u/SimoneNonvelodico:
                                  ```
                                  I think it's less that it never hit the wheel, and more that the wheel isn't a practical solution *given the other constraints*: namely, that everything has to be able to assemble itself, be made of cells, etc. In order to "hit the wheel" you need to have something resembling disjointed parts, and that's just hard to do while also working within the restrictions of self-assembling biomechanics.
                                  ```

                                  - u/fassina2:
                                    ```
                                    EY makes a point of showing how bad evolution is as an optimization process in his rationality book. A recurrent example he uses is how the human eye is designed, and how nature developed the wheel only a few times.

                                    He even criticizes the scientific method for not focusing on testing out the most probable theories first, and basically accepting any theory no matter how unlikely to be worth tested before more statistically likely theories, while at the same time seeing this type of failure as good and productive.

                                    EY says this makes it slower than it should be, and wastes resources. He says it's not an optimized system, it's just an inherited crude system we inherited from people from centuries ago, but we are too stuck in our ways to bother taking the time and effort necessary to fix it.

                                    &#x200B;

                                    I believe he's mostly right on both cases. Evolution could, create the most effective methods for something like cell architecture given infinite time, I just find it unlikely to have happened already given it's low ability to go back and optimize designs that already work because this would risk a period of lower performance and to an extent make this sort of thing risky to the individuals and make said trait less common. 

                                    i.e if to optimize your eye evolution has to risk a high rate of blindness for a period, this trait will rarely be passed on, therefore your eye will not be optimized. If to improve or optimize anything in evolution involves any potential limited time downgrade evolution will actually make it more unlikely to happen. 

                                    &#x200B;

                                    Just as an FYI purple is a better color to absorb uv light, in fact most organisms that filled this niche were purple before the green ones showed up. The green ones won because their by product, oxygen, was toxic to it's competitors. In this case, for random reasons the less efficient method won evolutionary wise.

                                    &#x200B;

                                    Here's a video talking about it from PBS Eons: [https://www.youtube.com/watch?v=IIA-k\_bBcL0](https://www.youtube.com/watch?v=IIA-k_bBcL0)
                                    ```

                                    - u/SimoneNonvelodico:
                                      ```
                                      > He even criticizes the scientific method for not focusing on testing out the most probable theories first, and basically accepting any theory no matter how unlikely to be worth tested before more statistically likely theories, while at the same time seeing this type of failure as good and productive.

                                      I think this involves another really deep rabbit hole, which is having a theory of how probable theories are. These sort of heuristics are very tricky. I cited earlier "Lost in Math", that looks exactly at how people in particle physics built up this notion of "naturalness" to discriminate likely from unlikely theories based simply on aesthetic criteria and the vague notion that it's sort of similar to the theories that worked until now. Even though it can be shown that in fact it's not really that way - rather, *our aesthetic sense* about theories is now shaped by existing ones. So basically we're just looking for more stuff resembling what we already know, like people in the late XIX century were looking for the luminiferous aether because mechanical waves through material mediums was all they knew.

                                      So yeah, not disagreeing that we would need to rethink our methods, especially in some fields. And I've never been very enthusiastic on Popper's notion of falsifiability, that while useful, sort of dumps the entire complexity of the induction problem into the "theory proposing" step, which becomes the really critical one for the *speed* of scientific development. I'm just not sure a unique way to determine which theories are *more probable* exists. Of course, in some cases it's obvious. If I meet a new phenomenon, I first try to explain it with existing models than with some novel never-heard-before energy-conservation-violating new quantum theory of matter.
                                      ```

                                      - u/fassina2:
                                        ```
                                        >  I'm just not sure a unique way to determine which theories are more probable exists. Of course, in some cases it's obvious. If I meet a new phenomenon, I first try to explain it with existing models than with some novel never-heard-before energy-conservation-violating new quantum theory of matter.

                                        You are correct. But a lot of scientists get fixated on these more unlikely theories and waste time and resources, and when proven wrong they are told what they did was good and productive. He says he himself could have wasted a decade with this if he hadn't been lucky.

                                        EY argues that you can find the most likely theories to work using Occam's razor / it's updated version Solomonoff induction..

                                        He even has a series of articles criticizing how silly it was for our physicists to accept quantum collapse for as long as they did, while some physicists today still accept it. 
                                        Even though it goes completely against Occam's razor.
                                        ```

                                        - u/SimoneNonvelodico:
                                          ```
                                          Heh, quantum collapse is an old pet peeve of mine... I'm reading right now a long book about the Solvay conference of 1927, which basically was the tipping point that made that idea prevail. An interesting (if technical) read on how much personal politics and circumstances can shift the course of science: [Quantum Theory at the Crossroads](https://arxiv.org/abs/quant-ph/0609184). I'm doing this in preparation for a talk I will give on some modern developments of pilot wave theory, so... you may imagine this is a topic close to my heart.

                                          However, I think some freedom of pursuing even seemingly unlikely ideas is useful. Sometimes the unlikely idea can be a weird road to a great one. If you read the book, you'll find a lot of weird roads that were taken to get to modern QM - it's not like one day Schroedinger woke up and dreamed his equation into existence. The obsession with trying to maximise efficiency along the lines of only supporting the most sane-sounding research which seems to promise the safest returns is *exactly* what has generated the modern publish or perish setup, and for all we know, could be a major contributor to the current dearth of breakthroughs physics is experiencing.
                                          ```

                          - u/crivtox:
                            ```
                            There are things we can know  about what an AGI will  be like  and there is a lot of obvious basic research to do.

                            You are just  making some assumptions about what can and cant be known about what true ai would be like ,but thats  not  something trivial that you can easily  determine without actually trying to do basic research .

                            Maybe it turns out its not posible to do usefull research before we know how to make human level AGI .But we have  reseach first to figure that out  , you cant just decide that it looks like we can't.

                            Have you actually tried to figure out what kind of useful  research could be done now  before concluding that its imposible?

                            All fields have to start at some point.Ai safety its just a new one , that happened to start a bit  more than ten years ago  whith a few people that included Eliezer and the organization he founded.

                            At some point someone has to start doing basic research that no one has ever thought of doing for a field to start.But If what you want its authority and amount of scientists that agree,  the field has been growing for years.

                            It never was literally only Eliezer anyway .But now other groups of people like  Open ai are doing ai safety research .

                            And there is also this paper for example [https://arxiv.org/pdf/1606.06565.pdf](https://arxiv.org/pdf/1606.06565.pdf)
                            ```

                            - u/SimoneNonvelodico:
                              ```
                              I'm not saying that useful research can't be done. I'm more saying that at this point no one's opinion is necessarily *especially* more valuable than others, because there's still no AI safety experts with actual practical experience around. We don't have Dr. Susan Calvin. Everyone is just speculating, thinking by analogy, and drawing their own conclusions. It's a fresher, more open field, one where people are still roaming the virgin plains and colonising them, is what I mean.
                              ```

                              - u/crivtox:
                                ```
                                Well Eliezer did start working on the problem when almost nobody else was doing it .So if it is posible to do basic research , and Eliezer was doing it before most others.Then he was doing research that had to be done before everyone else.

                                Its difficult to judge what is going to be useful in the end.Even if his most important  contribution ends just being getting a lot of people interested on the topic that still would mean that there was low hanging fruit to be had there .There were some actions that nobody was taking ,  no  people more clever and better informed than" Big Yud" on any and every particular subject had been beating their head against this Greatest Problem before he did .And hes currently doing research that nobody else is doing and is not obiously wrong.I wouldn't be surprised if in future ai safety textbooks contain things Eliezer wrote.

                                You can have expertise in  game theory statistics machine learning and all other related fields anyway.And on ehat people are doing in the field , what problems are open and what has been tried etc.

                                &#x200B;

                                And the research is not  mostly resoning by analogy right now .There is reasoning by analogy yes , but like there is concrete  research to be done about formalizing things  and Eliezer seems to be working on that so dunno.  
                                If you mean about the forecasting stuff  and that kind of thing maybe , but that seem to be mostly on the question of if ai safety is important in the first place ,and seeing how deep mind is hiring ai safety researchers i would say the consensus is slowly moving towards that.Of course if you rely on big consensus as an heuristic to believe things or not you are not going to be convinced yet.And its not that its a bad heuristic  , but then you are not going to be able to judge new things whith few researchers on them, only the obviously true stuff, and sometimes yo will still be wrong.If you want we can discuss in more detail what things you think are likely to be true , what things you think we cant know now and how likely it is that  other day(now I'm a bit sleepy and I'm not sure how coherent this will look tomorrow so I'll just turn off my phone and go sleep)
                                ```

                - u/wren42:
                  ```
                  I just read the first chapter of the book.  One of the main points is that you CAN expect civilization to perform well in situations where the incentives are correct, and that relying on experts in those cases is a good idea.  Even in his first example with the Japanese monetary situation, he was just assessing economics bloggers, not coming up with a new idea himself.

                  &#x200B;

                  I think the original statement stands, that there are other competent people that are working on the big problems in any given field, and collectively the top experts in a field are going to be better than him in their area of expertise.  

                  &#x200B;

                  That doesn't mean there aren't opportunities to make improvements all the time!  Obviously the world isn't anywhere near optimal. But thinking EY is somehow unique in being able to come up with novel solutions is hero worship I doubt he would approve of.
                  ```

                - u/xartab:
                  ```
                  Sorry, what? I think I definitely need a reference for this one.
                  ```

                  - u/AugSphere:
                    ```
                    You've probably already found it in other replies, but just in case it's [Inadequate Equilibria](https://equilibriabook.com/).
                    ```

              - u/DaystarEld:
                ```
                I feel like most of what I'd respond to this with has been said by others now, but I do want to repeat from my first post:

                >This is not a statement about the limits or lack thereof of EY's current domain expertise, or current capabilities or skills or knowledge about any particular fields. 

                I feel like you've missed my cruxes, if you think the things you're saying in this post are things I disagree with or are arguing against. The main point I'm making is that assuming that "there are experts in other fields who are more knowledgeable about those fields than EY" is not the same thing as them being more clever, and while ultimately EY will never be able to accumulate as much knowledge in multiple fields as he will on his major focuses, one of them being rationality is actually far more useful to such cross-domain analysis and problem solving than practically any other domain expertise. 

                It doesn't automatically make him *right* or *knowledgeable* or anything like that when talking about other fields. But he still very often more insightful when talking about other fields in a way most other "smart people" don't when stepping outside their domain, and his rationality/cleverness is why.
                ```

                - u/SimoneNonvelodico:
                  ```
                  > "there are experts in other fields who are more knowledgeable about those fields than EY" is not the same thing as them being more clever, and while ultimately EY will never be able to accumulate as much knowledge in multiple fields as he will on his major focuses, one of them being rationality is actually far more useful to such cross-domain analysis and problem solving than practically any other domain expertise.

                  Honestly, I've been reading "Inadequate Equilibria" now (approximately 60% in, thanks for the link!) and I feel like what he's saying comes across a bit better to me because it sounds like a more measured claim. He never puts things in terms of "X is more clever than Y". In fact, in almost all his dialogues, "people are stupid" is Simplicio's argument. He talks about systemic failures that happen *regardless of how clever the people involved are*. In fact, if you have a broken enough system of incentives, the rational people who are however looking out mostly for themselves might just run with it better than the others. A very good rational scientist might be *amazing* at putting out high impact factor publishable papers and still not produce great science - because they'd just have optimised the ability the system they're in is *really* selecting for.

                  I don't have a problem with believing this sort of thing. My mom isn't a rationalist of any sort, and she actually sometimes pulled off better diagnoses than our family doctor for me when I was a child using information available to amateurs, in a time in which we didn't even have the internet.

                  I agree with EY's arguments on "modest epistemology" and how it can fail. It is a topic in fact dear to me and that I see discussed a lot in my bubble in some of its specific realisations (for example, there's a certain doctor in Italy who has built a fame out of heavily criticising the anti-vaxxer movement but has soon transformed from an educator into an arrogant prick who positively touts and promotes a universal Argument from Authority - basically arguing that if you don't have a degree in medicine all you should ever do is shut up and listen to your betters. This obviously is largely seen as a very stupid and harmful approach by sane people, as it effectively harms the overall credibility of medicine even more when it does happen to actually fuck up). I guess what I don't appreciate in your way of framing this is that I think you're giving too much credit to EY's *skill* and less to his *position* as someone who simply had different incentives. For example, the SAD thing didn't require being especially clever. It just required $600 dollars. A child could have come up with the same idea. The reason why medical science did not test for it has nothing to do with the lack of clever people.
                  ```

                  - u/DaystarEld:
                    ```
                    I'm glad we're closer to agreement then we were, but this:

                    >I guess what I don't appreciate in your way of framing this is that I think you're giving too much credit to EY's skill and less to his position as someone who simply had different incentives. For example, the SAD thing didn't require being especially clever. It just required $600 dollars. A child could have come up with the same idea. The reason why medical science did not test for it has nothing to do with the lack of clever people.

                    Still feels like a massive missing of the point. Most people with SAD have $600 dollars or more. Furthermore, "a child could have come up with X" is honestly one of the most insulting ways to dismiss something someone cleverer than you managed to do, just because it seems simple in retrospect. It's not *always* about cleverness, as you point out, position and incentives are important too, but unless you've done something similarly impressive along the same axis, there are a million people sitting in armchairs happy to gab about how *they* could have come up with that or how they had the *same idea* as that writer or director or inventor or entrepreneur or whatever, but somehow it's still just one person who was the first to make the thing and a million of them who were nowhere near actually doing so. 

                    So to me, the reason why medical science didn't test for it has *exactly* to do with a lack of clever people, if "clever" is being used in the same way I use it to describe high level rationalists. I mean I wasn't the first one to use the word "clever" for that, I'd rather just say "rational" when in a community that I hope would know what I mean when I use that word. I guess this may not be as much the case as I thought.
                    ```

                    - u/SimoneNonvelodico:
                      ```
                      > Still feels like a massive missing of the point. Most people with SAD have $600 dollars or more. Furthermore, "a child could have come up with X" is honestly one of the most insulting ways to dismiss something someone cleverer than you managed to do, just because it seems simple in retrospect.

                      I'm just saying "clever" is the wrong axis. It's insulting to suggest that all those people lack the intelligence to deduce that if "some light" is helpful, then "more light" is even more helpful. In fact, a more clever person could know more about non-linear functions and start making up excellent rationalizations for why that might not be the case. I am confident that EY would back me up on this. Rather, it makes far more sense to think that everyone is falling prey to the $20 note fallacy: if it's been on the ground for so long, *it must be fake*. EY wasn't the only one clever enough to try it. He was the only one who actually shrugged and went "why the hell not" (the only one *that we know of*, I might add; maybe other people did similar things and didn't document it. And we'd still need to replicate his findings in a more controlled study to confirm it isn't just placebo or some unrelated effect that did the trick). Which is in itself a quality, but it's not "cleverness". More, like... willingness to break the mold? Unconventional-ness? Resistance to peer pressure? It's a social intelligence thing, not a smarts thing, though it often happens that science-smart people *also* possess this trait. In fact it's a trait that might be *disadvantageous* in some contexts. So EY may be able to use it positively to help his girlfriend with her SAD, but he also might be affected negatively by it when he comes off as too smug or self-assured to others, because going along with the majority is a powerful bonding mechanism important to social trust. It's why utilitarianism isn't quite as popular as virtue ethics.

                      I'm not saying that EY is not clever either, of course. Just that this specifically wasn't one situation where that aspect was the prominent one.

                      The idea of "high level rationalists" in general also just irks me. It feels like simply sliding back in basic primate mode: find a pack leader, someone to respect and admire, and defer to them. That's the surefire way to creating new problems instead of fixing old ones. Everyone has skills that can be useful, but no one is fundamentally smarter than the problems that we are all victims of, nor the problems themselves exist for want of intelligence. Simple random occurrences can push crowds of rational, intelligent actors into toxic equilibria that will then become unbreakable. It's no one's fault and it's got nothing to do with how stupid or smart the people involved in it are. It could be broken thanks to someone incredibly smart or someone incredibly stupid, on purpose or not, as well. It's a natural feature of complex systems. It's a phenomenon we need to study, prepare for and mitigate, collectively, just as we do with earthquakes.
                      ```

                      - u/AugSphere:
                        ```
                        > The idea of "high level rationalists" in general also just irks me. It feels like simply sliding back in basic primate mode: find a pack leader, someone to respect and admire, and defer to them. That's the surefire way to creating new problems instead of fixing old ones. Everyone has skills that can be useful, but no one is fundamentally smarter than the problems that we are all victims of, nor the problems themselves exist for want of intelligence. Simple random occurrences can push crowds of rational, intelligent actors into toxic equilibria that will then become unbreakable. It's no one's fault and it's got nothing to do with how stupid or smart the people involved in it are. It could be broken thanks to someone incredibly smart or someone incredibly stupid, on purpose or not, as well. It's a natural feature of complex systems. It's a phenomenon we need to study, prepare for and mitigate, collectively, just as we do with earthquakes.

                        I can't help but feel that studying and mitigating it would go better if we didn't have to pretend outcomes are completely and utterly independent of intelligence and rationality.
                        ```

                        - u/SimoneNonvelodico:
                          ```
                          Sure, but:

                          1) some of these problems are just too complex to solve from first principles with just human level intelligence anyway, so anyone is almost equally powerless in front of them if alone;

                          2) we're talking about social problems anyway, that need cooperation to fix. So in a very pragmatic sense, it *will* help you do that if you don't do the thing where you consider yourself automatically bestowed with better powers of reasoning (even if you *may* be) because that usually nets you the result that however good your ideas, no one will hear you because they dislike you personally and are suspicious of your motives.

                          In general, I think it's important to stay grounded, and more important, to keep ourselves from being prey of our natural instinct to seek idols and heroes to defer to unconditionally. We do our best if we can both neither overestimate our own ability nor other people's.
                          ```

                          - u/AugSphere:
                            ```
                            > we're talking about social problems anyway, that need cooperation to fix. So in a very pragmatic sense, it will help you do that if you don't do the thing where you consider yourself automatically bestowed with better powers of reasoning (even if you may be) because that usually nets you the result that however good your ideas, no one will hear you because they dislike you personally and are suspicious of your motives.

                            Are you one of those strange folk who are somehow capable of brainwashing themselves into deliberately changing their mind to believe something false? By all means *hide* the fact you think you're smarter than people to better coordinate with them, certainly, but actively trying to unlearn it? I always find this sort of "you should give up on believing true thing X, it's politically inconvenient" argument puzzling both because I have no idea how you'd go about doing it effectively in practice and because it seems like there are much better interventions that solve the problem without mangling your ability to reason.
                            ```

                            - u/SimoneNonvelodico:
                              ```
                              It's not really about convincing yourself, more about keeping a healthy level of doubt and inner dialectic, IMHO. The moment when you're too self-assured about being smarter than everyone in the room is the moment in which you're sure to stop being so. So, you try to falsify the "being smarter" thesis all the time. "But what if I'm wrong?" can't be the dominating thought in your head - that'd be paralysing - but it *has* to be in there somewhere at all times.

                              Also, and this may sound very cynical, but yes, if you *really* tried to deceive people about something, the first thing to do would be convince yourself of that something too. However I don't think I try to do anything like that. I'm neither that cunning nor that modest. All I'm doing is conceding that no matter how smart I may think I am, by definition, wherever I have a blind spot, I won't see it. So anyone might be able to help me fix that blind spot, because their sight there might be better than mine. Intelligence isn't some kind of number where greater beats lower all the time on any battlefield. There's plenty of things I tend to be really good at with a tiny amount of effort, and plenty of things I tend to be absolutely awful at even if I put my heart and soul into them.

                              Plus, as a general rule: facts speak louder than words. If you think you're smarter, and you want people to acknowledge it and follow you on that account, start by doing something that *proves* your skill and earn their respect. After all, if all you have is just a high propensity for abstract thinking without any real skill to apply it to, you don't really have much except for potential. What really moves the world is getting things done.
                              ```

                              - u/AugSphere:
                                ```
                                Feh. If you're Richard Feynman sitting in a strip club, "I'm the smartest man in this room" is very much a safe bet. Granted, not many of us are on that level, but telling those who *are* that they must genuflect and constantly try to find reasons to believe otherwise seems childish. 

                                > Plus, as a general rule: facts speak louder than words. If you think you're smarter, and you want people to acknowledge it and follow you on that account, start by doing something that proves your skill and earn their respect. After all, if all you have is just a high propensity for abstract thinking without any real skill to apply it to, you don't really have much except for potential. What really moves the world is getting things done.

                                Obviously "I'm smarter than you" isn't much of an argument, unless you actually have evidence to back it up. Once you *do* have the evidence that demonstrates that you *are* in fact better than someone at something (where said something can perfectly well be domain-general ability to learn and reason about novel things to achieve useful results), attempts to conjure up false modesty just look like inept politicking.
                                ```

                                - u/SimoneNonvelodico:
                                  ```
                                  Being better than someone at something is a much more specific and verifiable claim though. I guess that's also my problem with talking about smarts or intelligence: it's too vague. It feels like talking about a panacea, a meta-skill that will just apply to pretty much anything. I'm not saying it doesn't exist, but it's not as clearly measurable. If I play chess with you ten times, and beat you 8 out of 10, "I'm better than you at chess" doesn't sound like an outrageous claim or a boast because it's far more rooted in evidence. As long as I don't act like an ass about it, you'll probably easily accept it too without much argument.
                                  ```

                                  - u/AugSphere:
                                    ```
                                    Have you heard of IQ and how well it predicts outcomes?
                                    ```

                                    - u/SimoneNonvelodico:
                                      ```
                                      Sure. Here's actually an interesting summary about it tho':

                                      https://www.forbes.com/sites/quora/2015/09/16/is-iq-a-predictor-of-success/#2286d0063604

                                      Specifically, sure, IQ correlates with certain outcomes. However IQ is a metric that attempts to measure intelligence, but specifically, it gauges *certain skills* (I've never taken one of the professional tests it mentions there but I suppose it's usually spatial, verbal manipulation, etc.). So it's more like a metric of a subset of specific skills that can have real-life applications. In addition, correlation =/= causation, so of course there's a whole other host of possible failure points there. For example you could argue that IQ-related skills are "signallers", things that our specific society takes as representative of good intelligence and therefore will lead you to be hired more, get more promotions etc. regardless of whether those things *actually* contribute to what you're doing.

                                      What I mean is, there's lots of trappings in trying to capture something as general as intelligence. Even if you define intelligence as that one thing that helps you achieve success regardless of the circumstances (and that's already damn vague), then you have the problem of defining success itself. If people with higher IQ get jobs that pay more, for example, does that mean success? Someone could argue "but people with high IQ are also on average more depressed and suffer more from various forms of mental illness, so *surely* that's a sign their lives are actually worse!" and that success is to *not* suffer from depression, something that IQ does not help with. 

                                      In other words, I find philosophically much less troublesome to simply talk about differences in skill on specific field. Sure, there is a quality people possess that makes sense to call "intelligence" and that usually manifests itself as a high level of skill in a number of different but usually contiguous fields (note: not necessarily *the same* fields, since different types of intelligence exist; a very intelligent politician or lawyer couldn't do a scientist's job, nor the scientist could do the politician's). But I still think the best way to think about it is through the ways in which it manifests itself, rather than the unknown quantity itself. And I don't particularly think we'd benefit much in practice from a good way to measure it. Almost all of our problems come less from a lack of understanding of "who's better than who" and more from a lack of agreement on what "better" even *means*.
                                      ```

                                      - u/AugSphere:
                                        ```
                                        If one is really motivated, they'll have no trouble coming up with a plausible-sounding argument against intelligence being relevant to anything. And there's certainly a ready motivator of "it's unfair some people are just innately better able to reason in general and there's no way to change that". However when it comes down to it and you need to pick one of two people to do something that is crucially important gets done well and all you know about the candidates is that one has IQ two standard deviations above mean and the other two deviations below? Vague philosophical arguments don't give much of a way out there, you just pick the person more likely to succeed, i.e. the one with the higher one. 

                                        Certainly if the task is narrow and concrete and you have data about performance of people on that exact task, you pick the one better able to perform it, regardless of what their IQs are. In a lot of situations, however, you need to answer something along the lines of "which one is better able to learn new skills" and ignoring rationality and IQ there would be shooting yourself in the foot. No matter how public-relation-unfriendly the reason of "that person is just better able to reason, I expect they'll have an easier time achieving success in a new field" sounds.
                                        ```

                                        - u/SimoneNonvelodico:
                                          ```
                                          > all you know about the candidates is that one has IQ two standard deviations above mean and the other two deviations below?

                                          Well, that's one *big* difference - big enough that it'd probably be apparent just by chatting with the people in question for five minutes. We all have a fuzzy notion of intelligence. The problem of putting a number to it is that it removes part of the fuzziness. If you told me that an IQ of 120 proves that someone is more intelligent than one with an IQ of 112 I'd say that's probably bullshit.

                                          > you need to answer something along the lines of "which one is better able to learn new skills" and ignoring rationality and IQ there would be shooting yourself in the foot.

                                          Sure, but again, we do that sort of judgement all the time. We don't need IQ for that. Also, here we're talking also about evaluating *oneself*, where the interest isn't in getting the job done as well as possible, but in evaluating one as high as possible (because we just enjoy thinking highly of ourselves). So they're different situations.
                                          ```

                      - u/DaystarEld:
                        ```
                        (edit to make sure I let you know ahead of time that I don't mean this comment nearly as antagonistically as it might come off, but it's late and I'm tired so I hope you believe me if I just say that :P)

                        >It's insulting to suggest that all those people lack the intelligence to deduce that if "some light" is helpful, then "more light" is even more helpful. 

                        If I'm insulting all the people who didn't figure it out and you're insulting the person who did, then I don't really see why I should care about what's "insulting" rather than what's accurate. Again, intelligence is *not the thing I'm pointing at.* Rationality is far more than that.

                        >In fact, a more clever person could know more about non-linear functions and start making up excellent rationalizations for why that might not be the case. I am confident that EY would back me up on this. 

                        I'm happy to have /u/eliezeryudkowsky himself show up and tell me I'm wrong, but I still think you're missing my point. And we definitely need to veto "cleverness." I only kept using that word because I thought it was being used by others in this thread as an offhand substitute for rationality. This whole thing:

                        >EY wasn't the only one clever enough to try it. He was the only one who actually shrugged and went "why the hell not"... Which is in itself a quality, but it's not "cleverness". More, like... willingness to break the mold? Unconventional-ness? Resistance to peer pressure? It's a social intelligence thing, not a smarts thing, though it often happens that science-smart people also possess this trait. In fact it's a trait that might be disadvantageous in some contexts... 

                        Just seems wrong.  "Social intelligence" makes no sense here. What you're actually pointing at seems to fit more with what I'd call "agency." I'm having a hard time picturing what you think a socially-intelligent person looks or acts like, but "let me try this experiment because I think it might work and it's not enough to just trust that smarter people would have published on it if it did or didn't" is not what I would fit under that label, and I think most people wouldn't either.

                        Also, saying this afterward,

                        >but he also might be affected negatively by it when he comes off as too smug or self-assured to others, because going along with the majority is a powerful bonding mechanism important to social trust. It's why utilitarianism isn't quite as popular as virtue ethics.

                        Should make it pretty obvious that you're not actually talking about social intelligence, but something else, because if the attribute you're pointing at is *actually* a premium in social intelligence then him being "negatively affected by it" is pretty nonsensical, right? Like, you wouldn't say "This person is super intelligent at math, but he gets affected negatively by it sometimes by not being able to remember his multiplication tables."

                        I get that the idea you're working with is more nuanced than that, I'm just trying to point out that there are sensible joints in that nuance that make it clearly two different domains. 

                        >The idea of "high level rationalists" in general also just irks me. It feels like simply sliding back in basic primate mode: find a pack leader, someone to respect and admire, and defer to them. 

                        If that's your idea of high level rationalists then I get why this is bothersome to you, but I think you're "doing it wrong," or the people you're thinking of are, or something, because this is not how it works in my head or in the rationalist social circles I've been part of. 

                        When I see someone smarter than me, I want to *feed* off them. I don't want to follow them and dress like them and get their autograph, and I don't want to signal allegiance so I'll survive the tribal purge that will inevitably result from their ascent to power. I'm not trying to share in the spoils of their conquest, or have them tell me what to do so I don't have to think about it. I want to *absorb the patterns their brainwaves form* in whatever medium they demonstrate their intelligence and rationality in, and then I want to merge *my* ideas with theirs and see what other awesome stuff might come out. Because it measurably improves my life and the lives of those I interact with.

                        I don't give EY's facebook or blog posts a high priority spot in my attention list because he's a "pack leader," I do it because he's demonstrated value. Same with others who can teach me new things that make a difference in my life or others'. If you're not willing to accept frames like "high level rationalists" then I don't really know how your internal system of cached attention-direction works. My suspicion is that you have one of some sort, I would be very surprised if you just paid equal attention to everyone because the idea of recognizing that some people actually have more valuable things to say than others is offensive to you somehow. 

                        If what you *actually* meant by this was simply that the phrase irks you for all of what you said after that, specifically because it lulls people into a false sense of security about "okay here's this really smart person so they will know how to solve X super complex problem that everyone's being stuck in," then that's much more sensible, but also seems like less of a concern than the failure mode that's actually occurring in the world.
                        ```

                        - u/SimoneNonvelodico:
                          ```
                          > If I'm insulting all the people who didn't figure it out and you're insulting the person who did, then I don't really see why I should care about what's "insulting" rather than what's accurate. Again, intelligence is not the thing I'm pointing at. Rationality is far more than that.

                          It's not about insulting this or that. I'm just saying, I don't feel like the reason why more people don't try the SAD cure with lamps is because they can't logically come up with it. Some of them might just be unwilling to take that financial risk (even though a lot of people have $600, not a lot of people are in condition to spend and potentially sink them on a hunch), for example. Some might have done it, but not said it on the internet. Some might have done it *and it might not have worked for them*. Or they had other logistic problems, people or pets in the house who couldn't put up with it, couldn't afford the electricity bill, and so on. There's a bunch of reasons why people don't do this sort of thing that aren't as simple as "they're just not clever enough". In this case, EY's application of rationality is to the idea that there are perfectly good reasons why, without considering yourself better at medicine than doctors, you can imagine coming up with a good therapy for this specific thing. It's not in *actually doing the thing*. In fact, his rationality was used to overcome a hurdle *that only moderately rational people to begin with* encounter: the "wait, but wouldn't doctors know better if the solution was really this obvious?" one. A less rational or knowledgeable person might just enter full Dunning-Kruger mode and string their house with lamps without ever wondering about that - and just happen to stumble upon the solution.

                          > Should make it pretty obvious that you're not actually talking about social intelligence, but something else, because if the attribute you're pointing at is actually a premium in social intelligence then him being "negatively affected by it" is pretty nonsensical, right? Like, you wouldn't say "This person is super intelligent at math, but he gets affected negatively by it sometimes by not being able to remember his multiplication tables."

                          I suppose what I meant was more that "it belongs to the social intelligence sphere". It's not like any type of intelligence is a single D&D like stat anyway - there's a complex landscape of possible skills and axes which will give you bonuses in certain situations and maluses in others. That's what I meant, more or less. 

                          > If what you actually meant by this was simply that the phrase irks you for all of what you said after that, specifically because it lulls people into a false sense of security about "okay here's this really smart person so they will know how to solve X super complex problem that everyone's being stuck in," then that's much more sensible, but also seems like less of a concern than the failure mode that's actually occurring in the world.

                          That's pretty much it; also I think the phrase itself carries a lot of that connotation already. We're all nerdy-ish people here I expect; "high level rationalist" to me evokes the image of a level system, so a linear scale of skill of sorts. I think that's the wrong way of thinking about pretty much any real world skill outside of relatively easily gauged and consistent ones like how good you are at chess or go (but those tend to be very specialised). It irks me also because when talking about a "rationalist" there's already an implication we're talking about an expert in *meta*-knowledge, which in itself establishes a sense of primacy.

                          So, if I have to be rational about it: yes, I appreciate EY's writings. I think a lot of them can be insightful, if only in helping me put into words things that I only understand at an intuitive level. I don't think it's even in dispute that I would have a lot to learn from him both in the field of AI and in general in that of statistics and probability (which I know, but probably not quite as well as I should or could). However, I think there's a problem when dealing with someone who's very good at making rational arguments (including myself): they'll also be very good at making rational-*sounding* arguments with subtle, almost undetectable fallacies. I don't mean in bad faith; usually the person we're out to deceive the most is ourselves. So there's a double edge to this weapon. As you improve your defences, you also improve your own ability to attack them and wreck them. I don't know EY as a person. I know him from what he writes. To call him truly a "rationalist" in the sense of one who's able to *put in practice* what he talks about (rather than just, say, an expert in logic, or Bayesian epistemology) I would need to, well, run the test; which would require me knowing him in person and getting a feel for whether he really is better at avoiding fallacies or at thinking outside of his own perspective than me (I don't have any doubt that he's better at these things than the *average person*; but 50% of the people are, so that's not much). 

                          In other words, I think it's important to always take claims, ideas and teachings with a pinch of salt - which is the whole point of rationality, after all. As such I guess you could say I feel like attaching certain special labels to people - and especially a label such as "high level rationalist", with the implied meanings it carries - tends in itself to be a mechanism that favours the arising of certain biases we want to avoid inside our minds. I do have a classification of course, but I think not attaching names to things sometimes is a useful strategy to keep them more fluid and think about them in a more elastic way. You could say I prefer having in mind a gaussian curve of rationality where EY may place somewhere around plus three or four sigmas above the average, maybe, but not reduce the gaussian to a binned histogram and give big bold names to the classes. In *my* mind, I think that helps me keeping a more elastic perspective and putting less resistance to any need to move a point up or down the curve. I assume a similar process may happen in other minds as well, especially as I see a lot of situations in which attaching names and labels seems to cement the status quo in people's mental models of reality and exacerbate divisions. BTW thanks for bringing this up, putting it into words actually helped me clear my feelings on the topic.
                          ```

        - u/None:
          ```
          I think trying to equate intelligence with rationality is a mistake. If rationality is your measuring stick for intelligence, then of course EY comes out on top. The man is so dedicated to rationality that he's written books on the topic; it is one of his areas of expertise.

          This, in my opinion, does not make him instantly smarter than expert scientists. Rationality does not equal intelligence; if it did than EY would have been stupid to write the Sequences, because there would have been absolutely no point. In the same way it's possible to be a fool and still be rational, I think it's perfectly possible to be a genius, and irrational. 

          I don't find it at all implausible that a genius physicist can be smarter than EY, and a better physicist than EY ever could have been, and yet still be less rational, because to me 'smarter' doesn't have much to do with 'more rational'.

          Judging intelligence by rationality is unfairly biased towards EY, because while other smart people have chosen to specialise in biology or chemistry or physics, he choose to specialise in rationality. It would be like judging intelligence based on skill in Primatology, and then claiming Jane Goodall is smarter than Einstein. Technically correct, if that's the scale you're using; but it's the scale that's the problem.
          ```

          - u/DaystarEld:
            ```
            Oh, I'm definitely not equating rationality with intelligence. I value rationality far more.  The ratio of people skilled in rational thought to people who are "just" intelligent is massively lopsided, and it's my belief that this is *why* so many problems in the world remain problems. Because it's *not* enough to just be intelligent. It's super useful and valuable, but for the really hard shit in life, intelligent people struggle almost as much as anyone else, in my experiences.

            So I strongly disagree with the idea that rationality as an "area of expertise" is a poor measuring stick or a domain expertise that's as unrelated to general evaluations of people's value toward solving problems as other domain expertise, like primatology. Frankly I'm not really sure what you even mean when you think of rationality, if that's how you see it. Some list of memorized biases? Some convoluted and esoteric philosophical position?  That's not the rationality that EY has learned and developed and taught, and it's not what I've been trying to teach at ESPR or SPARC or in my own story. It's a set of methodology for *determining what's true* and *figuring out what works*, above all else. It's *hella* more useful than any single other domain expertise, even if those domains are *still obviously necessary* to become educated on to solve problems in their fields.
            ```

            - u/None:
              ```
              I think we're having a misunderstanding. What I was disagreeing with, was the assertion that somehow there weren't that many people smarter than EY, working on important problems. The actual comment I replied to was, if I remember correctly, talking about how even distinguished scientists can be limited in their rationality; and I think that saying that just because a talented scientist seems limited in their rationality means EY would be a better scientist than them, unfair.
              I find that, in my experience, even if scientists don't seem rational in daily life, they are extremely good at applying what we define as rationality to their craft. Science is all about determining what is true and figuring out what works. An award winning scientist definitely used rationality to get that award, even if they aren't explicitly rational, and this is why I find that assertion that we can just assume EY would do a better job so strange. 
              I see rationality as a poor measuring stick, because in many cases people who don't know about it or haven't heard about it aren't trying to be rational all the time. I think it's a poor measuring stick for someone's ability to solve problems, because if we think that award scientists don't have it in spades, then we are clearly measuring it wrong. 
              Maybe I need a new word. Rereading my arguments, I think I agree with you that many distinguished academics are 'limited' in their rationality, as they only seem to apply it to their own field. However, I think that while they are 'limited', they are still very, very good at rationality, within their area of expertise. Probably more rational in that area, then we can assume EY would be.
              Anyways, my main point is that rationality is not intelligence, which you agree with. One of your main points (if I'm understanding correctly) is that may celebrated intellectuals are limited in their rationality, which I agree with, and that rationality is more important that intellect, which I also agree with. 
              I believe, however, that even someone limited in their rationality can still be extremely rational within their field. I find it hard to believe that a talented scientist isn't being rational when they make their discoveries, considering how similar rationality is to the scientific method, and how much of what I've read about eliminating bias on Lesswrong and in stories, I've heard again in lectures given by professors of science. Rationality, as a methodology, is mimicked by a lot of other schools of thought also dedicated to solving problems.

              Essentially I disagree with the idea that EY, because of his impressive rationality, is automatically better than overall less rational experts in different fields, because lots of the fields I assume we're talking about (like science and engineering), require the development of in-field rationality to excel in. Essentially, I'm saying that even if someone's rationality is limited to their field, they can still be more rational within their area of expertise than we can assume EY would be, if that makes sense. 
              Like how a super specialised AI can still be better at maths than an actual general intelligence, even if the general intelligence is still, by most measures, objectively a better thinker (if that metaphor makes sense).
              ```

              - u/DaystarEld:
                ```
                >The actual comment I replied to was, if I remember correctly, talking about how even distinguished scientists can be limited in their rationality; and I think that saying that just because a talented scientist seems limited in their rationality means EY would be a better scientist than them, unfair.

                Sure, but that's not what I said :P What I actually said was "This does not by any means make them stupid or incompetent, only limited in things like their ability to apply epistemological rigor to their beliefs and carefully assess complex systems and social dynamics with minimal prejudice or bias."

                The real crux here seems to come down to what we define as Major Problems in the world. Since the original point I was arguing against, that there *aren't* many people smarter than EY working on them, and you think that within particular domain expertise there are, then defining what they actually are is important.

                If your perspective is that pure scientific research in fields like chemistry and physics and so on is the actual bottleneck for solving all the important and major problems in the world, then you're right. Award-winning scientists are probably pretty good at the epistemological rigor thing, within their own field.

                If my perspective is that there are far more major problems in the world than pure scientific research is currently trying to address (so, complex problems like reducing poverty and eliminated diseases and increasing access to healthcare and so on) then I think I'm right.

                Unless you *also* think that not just award-winning scientists but also politicians, NGO leaders, entrepreneurs, etc are smart enough in their limited-field rationality to be more capable than EY in identifying the problems in their fields and coming up with solutions, in which case, again, I see no evidence of this from what I know of the major players trying to tackle major problems in the world.

                Again, it doesn't make them dumb or incompetent. But it doesn't make them capable of actually outperforming a version of EY that was a domain-expert in their field. By my definitions, that makes them not *actually* more clever/rational/competent. The "better informed" part often goes with the domain expertise, but honestly, it really doesn't always. And again (again) that doesn't mean there aren't *good reasons* for that, like "they're too busy dealing with the politics of their position to actually inform themselves about what's actually true and useful to solving the problem they're working on," but that still doesn't justify the original claim.
                ```

          - u/fassina2:
            ```
            You are right and I believe EY would agree with you. His point from what I can tell has always been that rationality is useful and can make you better at a large number of activities.

            That's why he tries to spread it and make more people more rational. 

            He cites this as one of the reasons for writing HPMOR, he wanted more young people to learn about rationality so that they could fix other problems he can't fix himself because doesn't know everything and is not an expert at everything.

            He's only one man, and with that being the case, he's found that the best substitute to the shadow clone jutsu (as was alluded in this topic) was to teach other people the methods and let them build upon them with their own knowledge and abilities.
            ```

    - u/Sparkwitch:
      ```
      You don't see them because most of them aren't out making public noise. It used to be that spreading information was difficult because the technology to do so was awkward and expensive. Now spreading information is difficult because it's so cheap and easy that it's hard to distinguish signal from noise. 

      Like evolution, general societal advancement isn't a perfecting process. Improvements are invented and reinvented over and over until one mutation happens to reach widespread use, and those tend to favor the novel and the sexy.

      I'm not talking about politicians, yes, but not scientists or philosophers either. I'm talking about everyday schlubs like [this guy](https://equilibriabook.com/), curing his girlfriend's SAD without actually managing revolutionize anything. Things like this happen *all the time*, mostly by people who don't write books about it.

      Engineers, [in the broadest sense](http://www.kiplingsociety.co.uk/poems_martha.htm), are out there keeping everything running smoothly, innovating as necessary, and generally sustaining the status quo *by anonymously undermining it*. A few make the right sort of noise at the right time and their practices are adopted. The rest remain anonymous forever and once they're gone some other engineer has to reinvent the wheel.

      The accomplished and celebrated are so because they were good at self-promotion as well as being good at whatever else they managed. Compare Bill Gates to Linus Torvalds, and then to Dennis Ritchie. Compare Thomas Edison to Nicola Tesla, and then to William Stanley Jr. And every one of those people is more celebrated than most.

      The folks in charge of institutions are good at getting promoted, regardless of whatever else they do.

      We will never know the names of most of the "more clever and better informed" people beating their heads against the Greatest Problems because most of them won't succeed in anything other than local improvements, and most of them aren't trying to get noticed, and vanishingly few would achieve revolutionary change even if their ideas were adopted.

      They chip away at the stone, and we move a few micrometers further through the tunnel towards the light.
      ```

      - u/DaystarEld:
        ```
        Hmm. How to put this. 

        Part of me thinks you might be right? But more of me thinks your claim is actually unfalsifiable. Which makes it... not really convincing. 

        Like you can't ironically drop EY's example of what he did in as an "everyday schlub" as if it proves *your* point rather than mine. Not everyone who does things like that are going to write about it, but just assuming that it's no more special than what millions of people do constantly seems way too big a claim to make given common observation that most people *don't* actually independently discover cures to illnesses by logic and testing, or whatever other impressive feats keep "chipping away at the stone." You're presenting this as a *narrative* for how society functions and advances. Where is your evidence?
        ```

        - u/Sparkwitch:
          ```
          "Under any circumstances, remember, four-fifths of everybody's work must be bad. But the remnant is worth the trouble for its own sake."

          My evidence is that society isn't a cesspool of hopeless misery. As a general trend, on an individual basis, lives improve. When something seems gone wrong, people *try*. Not necessarily with the degree of logic and testing that Yudkowsky did, nor necessarily with the available resources but with whatever they have at hand and by whatever knowledge base they possess.

          Investigate most homes or workplaces and discover half a dozen gems of jury-rigged brilliance. The same sort of life hacks that pre-dated easy methods to distribute them. The majority of them don't do much more than keep things muddling through, but a few really knock it out of the park.

          If we all had to depend on the trickling output of a rare sort of genius, we'd have been doomed a long time ago.
          ```

          - u/DaystarEld:
            ```
            The goal posts have shifted again. The original claim was Big Problems, not every day problems found in "most homes or workplaces." Life hacks are cool and useful, but if all EY had contributed to the community was life hacks, he would not be nearly as appreciated as he is.

            I'm not sure why you think we're arguing about whether the majority of the people in the world need to be geniuses to survive, or to avoid life being a "cesspool of hopeless misery," but if that's what you thought I was saying then rest assured we both agree that's not the case :)
            ```

            - u/Sparkwitch:
              ```
              Adjusting zoom or projection on the map makes the goalposts seem to move, but they really are right where they used to be. I'm not entirely sure we disagree about anything. 

              In much the same way that it's all small stuff that we oughtn't sweat, the greatest problems engineers come into contact with are ultimately the Greatest Problems they solve. That's the slow chipping. Significantly compiled and iterated life hacks is indistinguishable from progress. All that cheating and yearning and working *as a whole*.

              I didn't mean that somebody in particular is more clever and informed about everything than Yudkowsky - though, to be fair, somebody probably is or has been - I meant that *everybody* is. It's a lousy narrative and, yes, an almost certainly unfalsifiable one. There's no protagonist, no antagonist, no beginning or ending. By the time positive change finally happens, it's not a big surprise... it's all but inevitable.  

              We lionize the individuals who take the last step, but they're only slightly more responsible for giant leaps than Neil Armstrong was for stepping on the moon. They stand on the shoulders of some giants, sure, but also the shoulders of millions of hardworking, hardthinking human beings.

              Genius is .01% inspiration and .99% perspiration and 99% everybody else inspiring and perspiring. Yudkowsky inspires me, as do others in the community, and "Big Yud" really was meant to be affectionate rather than mocking... but so far as I can see, FOOM doesn't work the way it does in his stories.
              ```

              - u/DaystarEld:
                ```
                >I meant that *everybody* is.

                Aaand you lost me again :P

                Sorry, but I think we really are disagreeing about something, and it's something rather important, so I'm going to go on a bit of a rant.

                I don't think your narrative is "lousy" in the sense you seem to mean it, as in unattractive, I think just the opposite: that it's far *too* attractive, just in a different way. It's doing the opposite of the "Genius Hero Saves The World" narrative, and I have even less reason to believe it is remotely true. 

                A snarky and sad part of me honestly wants to grab your head and turn it toward the world to stare at what's actually happening and be like... come on, friend. Have you MET the average person? Because as the saying goes, half of all people are dumber than that. 

                Yes, there's beauty and love and genius in unexpected small moments and individual hardships and the grand interwoven tapestry of society and so on. But no, that doesn't mean what you're saying is true. And I feel mildly annoyed that I have to point this out, because it feels vaguely like being gaslighted if someone tries to insist that everyone is "X positive thing" and I have to be the one willing to say the obviously-true-thing that makes me sound like an asshole, which is that no, they aren't. 

                Not everyone is actually smart, or clever, or whatever. Even taking into account individual specialties and skills, objectively no, they aren't. 

                Not everyone is actually physically beautiful "in their own way." Say what you will about subjectivity of beauty, we can actually measure this and in every meaningful way, no, they aren't. 

                Not everyone is actually interesting or meaningfully unique. Any combination of arbitrary and near-infinite features may be technically unique, but when it comes to determining how much attention people are willing to actually give each other, no, they aren't.

                And not everyone is actually contributing in some way to the solving of Great Problems. That just doesn't match my observations and experiences. Great Problems are Great because they are *hard* and life very rarely works like movie scripts where plucky salt-of-the-earth heroes solve problems that the smartest people struggle with. Small localized optimizations, yes. Worldwide problems, no. You need smart people for those, and smart people are *rare*. Even intelligent people are not as rare as actual, usefully smart ones. And "rational" ones are even rarer. I wish the world was different, but it's not. 

                If you're using the "shoulders of millions of hardworking individuals" thing to mean that those smart people need a whole society to make their coffee and beds and pencils and computers and shampoo and food and car and everything else that they use day to day, yes, of course, but that is absolutely shifting the goal posts from "People more clever and better informed than Big Yud etc etc," and this conversation would be resolved if you would just admit that instead of trying to find ways to make it still apply by changing the argument completely.

                Yes, *of course* we shouldn't mistake the popular and visible face of an accomplishment as the only person who contributed to it. But that's a strawman, and if you think the original goal post hasn't shifted because every tiny bit of inspiration and perspiration adds up to the big wins, that's just... wrong. A millions of people have spent literally billions of dollars on charities that accomplish nothing but help thousands of people have jobs that they can be proud of but which nudge the world toward an optimal place absolutely zilch. And that's being generous and not assuming they do more harm than good.

                Genius is NOT 1% inspiration and 99% perspiration. It's important, yes, massively important, but you know what? A lot of people are working their asses off every day and they're not doing anything new or interesting or important to anyone outside their own life, in any way that affects the state of the world or helps on-net. "Genius" loses all meaning if we reduce it to 1 part per 99ths hard work. That's not the world we live in.

                99% of everybody else is NOT inspiring. This is a false narrative that makes everyone sound like the heroes working together to save the world, and I get why it's an important weapon to fight against the also-false narrative that there's some Protagonist upon whose herculean singular efforts the world turns, but it's taking things way too far and avoiding the rather obvious truth:

                There are MANY protagonists. Each fighting in their own way, attributing their own genius, to their own causes. 

                But they are NOT common, they are NOT the majority, and if people think they are, if people think little optimizations that make their own life easier or maybe help out their family or friends or coworkers is "enough," then they cede and abdicate and excuse themselves from the actual heroic responsibility to take up the *really* hard work, of pushing themselves *beyond* tiny local optimizations into the actual, commonly-used-and-much-more-sensible-definition of the originally used phrase, "Big Problems," which are not solved by a billion people working hard in their own separate directions, but a relatively small handful of people working cleverly at the right time, in the right place, on the right problem, and not giving up until it's actually done, and the world is meaningfully different as the result of *their* genius and effort in a way that's intrinsically different than anyone else on the planet's own genius and effort toward their own local optimizations.

                If your main objection is actually this:

                >but so far as I can see, FOOM doesn't work the way it does in his stories.

                Then sure, of course not, because *magic isn't real.* 

                That doesn't mean FOOM doesn't happen on less exponential scales. It just means you need more of them, lacking such world-breaking force multipliers. Not one Protagonist that no one else can be, many protagonists that anyone might be... but not everyone.
                ```

                - u/Sparkwitch:
                  ```
                  Would it have helped if I'd capitalized Everyone? A whole rather than a universal singular?

                  I agree with more or less everything you say here, and continue to think that our difficulties are of perspective rather than fundamental. Worse, they're probably rooted in my own unclear or poetic phrasings.

                  I agree, for example, that perspiration for its own sake is often a waste. My point, and I conjecture Edison's, is that ideas are cheap. My addition to his quip is that genius is born more from its context than its singular inspiration. There's no Einstein without the crucible of physics thinkers and experimenters in late 19th century Germany. And if that set of schools and labs hadn't developed there, it might very well have developed somewhere and somewhen else and some other brilliant kid who in our world grew up thinking about boiler pressure maintenance or mechanical calculation or market forces or animal husbandry would have been imagining travel at the speed of light...

                  ...and been willing to do the work of proving it, instead of the work of becoming an inventor, or a farmer, or an entrepreneur.

                  >That doesn't mean FOOM doesn't happen on less exponential scales. It just means you need more of them, lacking such world-breaking force multipliers. Not one Protagonist that no one else can be, many protagonists that anyone might be... but not everyone.

                  I endorse that completely, and am quite content to end the conversation there.
                  ```

    - u/ishaan123:
      ```
      This, and also, can we please collectively agree cut it out with the taunting "Big Yud"? He's a real person. 

      I understand there are often different social norms for politicians and public figures (although i question whether there should be, i accept that there are). But while he has a lot of clout on this corner of the internet, he *isn't* a politician, or powerful public figure, nor has he hasn't hurt anyone or advocated for any very bad things. It's really not nice. 

      Personally I consider using playground taunts like that as a sign that the person speaking shouldn't be taken seriously.
      ```

      - u/causalchain:
        ```
        Before this comment, I never considered "Big Yud" as an insult, since all the contexts I've seen it in have been positive.
        ```

      - u/DaystarEld:
        ```
        Agreed, though I'm under the impression that "Big Yud" is as often a term of endearment as it is a pejorative? It's definitely seems hard to tell, sometimes, but yeah, that's why I stick to his name, or EY.
        ```

    - u/wren42:
      ```
      He's talking about \*all of history\*, and specifically people who have advanced their art in a meaningful way.  Unless you believe EY to be a genetic aberration unique to this generation (a vanishingly small probability, especially given his accomplishments), there have been and are plenty of people "on his level" working on pretty much every big problem.   he's not talking about leaders of institutions - he's talking about people who are inventing new maths and physics.  EY himself admits there are people above his level in several sequences.

      &#x200B;

      &#x200B;
      ```

  - u/KrakenSticks:
    ```
    what does FOOM stand for?
    ```

    - u/Sparkwitch:
      ```
      FOOM isn't an acronym, it's shorthand for [the Vingean Singularity](https://en.wikipedia.org/wiki/Technological_singularity): a hypothetical historical moment when superintelligence reconfigures technological evolution to the point that it's no longer comprehensible within the preexisting framework.

      I'm using it slightly more loosely than that, as the process of breaking the fictional world that rational fanfiction often aspires to.
      ```

- u/Nighzmarquls:
  ```
  I've mentioned this to people before. FOOMS don't ever look all that impressive from the inside.  They are normal progressions of events.  Humans were a FOOM event.  Out of nowhere the planet exploded with us.

  The only people that reliably seem to be properly awed by singularities are those being left behind.

  I personally do strive to write stories with asteroids.

  But its tricky to do right.

  The best i've seen to do it is either much like you've noticed write about the asteroid ands don't tell anybody. Which is what your two examples do. (Along with surprises)

  Or you write about the mice who cower. while suddenly the whole world vanishes and is remade around them. Which lets you do more standard drama.

  Which is generally what i've seen Charles stross do.

  I lile to blend the two personally. Although I think I've succeeded more with fan fiction because you can sort of use standards of the canon as a surrogate for the "dinosaur" perspective in the audience.
  ```

- u/SkinnyTy:
  ```
  I'm a huge fan of HPMOR, it was my first real intro to not just rationalist fiction, but scientific thinking and it changed the course of my life from being in a cultish religion and pursuing what my upbringing had taught me to pursue to becoming a scientist, which I am now working on my PhD in Molecular Biology so I can help fight the dragon that is old age. 

  I love that book. 

  But I HAVENT READ WAVES ARISEN so I had to stop reading your really interesting looking analysis when you gave the warning, and I can't yet contribute on your really interesting commentary. I vow to read Waves Arisen and return to comment on your fascinating thread as soon as I can.
  ```

  - u/Aretii:
    ```
    The Waves Arisen is a Naruto fanfic written by an author named Wertifloke, of whom no trace existed before or since.

    Consensus seems to be that it was written pseudonymously by someone known to the rational community under a different name. I'm >80% on it being Eliezer Yudkowsky himself.

    (Speaking of pseudonyms, this post reminds me of an old poster in this subreddit who went by PM_ME_RATIONAL_FICS. Similar discursive style.)
    ```

    - u/FenrirW0lf:
      ```
      I was really sad when PM_ME_RATIONAL_FICS deleted their account so I do hope that the OP is them reborn
      ```

    - u/xamueljones:
      ```
      >I'm >80% on it being Eliezer Yudkowsky himself.

      Personally, I think The Waves Arisen was written by someone in the rational community who is one of Eliezer's friends (most likely Alicorn IMHO) and had him act as a beta-reader. The unknown friend wasn't too comfortable or didn't want the publicity and decided to be anonymous and let Eliezer have the credit for writing it.
      ```

      - u/Makin-:
        ```
        I could see it being one of Eliezer's friends (same with the Erogamer), but why Alicorn of all people? She has an extremely memorable writing style and TWA isn't it.
        ```

        - u/xamueljones:
          ```
          >extremely memorable writing style and TWA isn't it.

          To be fair, TWA's writing style in of itself was extremely memorable (remember the jokes about 'plot toxicity'?) with no other story known to the rational community having a very similar writing style. If the author is already known by other works, then it is pretty likely that the author went to extreme lengths to write in a very different fashion to disguise their style. It's even stated as such in the very first [chapter](https://wertifloke.wordpress.com/2015/01/25/chapter-1/).

          > This work was created in part to explore a more haphazard approach to  writing, the results of which can only be evaluated in the light of  strangers’ feedback.

          Anyway long story short, Naruto in TWA has a strong ability for introspection and self-reflection that I feel is mirrored very similarly to Bella in Luminosity. There is no really good reason. Alicorn just someone who I know to be Eliezer's friend with a strong talent for writing and is at the top of a list with very few candidates on it.

          I just find it difficult to believe Eliezer wrote TWA while also writing the last few chapters to HPMOR. His hints about TWA makes a lot more sense to me if he was a beta-reader instead of being the author.
          ```

    - u/arenavanera:
      ```
      Why is it that people are so certain The Waves Arisen was written by Eliezer?

      It certainly seems possible, but the writing style isn't particularly like HPMOR.  It also doesn't have the same hyper-pedagogical feel that HPMOR, Three Worlds Collide, and The Dark Lord's Answer have.

      None of those things are super strong evidence in the opposite direction, of course, since I would never have predicted that A Girl Corrupted was EY based on those criteria.

      But what *is* similar enough between the two stories that so many people are very confident they're the same person?  Is it just that "rationalist fanfiction that isn't poorly-written" is such a small class, and that the timing works out?
      ```

      - u/Aretii:
        ```
        > Why is it that people are so certain The Waves Arisen was written by Eliezer?

        I mean, the short version is that I disagree with you on this:

        >the writing style isn't particularly like HPMOR.

        The pacing, the style of humor, the themes... it all reads like EY to me. Like, this bit, right at the beginning:

        >This work was created in part to explore a more haphazard approach to writing, the results of which can only be evaluated in the light of strangers’ feedback, so leave your thoughts in the comment section to cooperate in the acausal trade that is this story. That goes double for anyone who can discern why in their case it should go double. Defectors will receive an empty box as due reward.

        It's not didactic in the same way HPMOR is, but I feel that's also true for Three Worlds Collide.
        ```

    - u/SkinnyTy:
      ```
      Well I started reading, and I will eventually read it, but it is too long and I want to read this so I'm just gonna read this. After all, the most important part of a ~~secret~~ spoiler  is knowing that it exists.
      ```

      - u/thrawnca:
        ```
        This essay isn't actually all that spoilery, really. It has spoilers, but nothing that I feel would break your enjoyment of the story.
        ```

    - u/Ms_CIA:
      ```
      I got curious and did a little digging into some old threads from PM\_ME\_RATIONAL\_FICS. I'm now 100% certain that's EY's alternate account, and probably the writing experiment thing he mentioned in his comment. There's a lot of clues, but what convinced me is that he writes about HPMOR like he's the author, not like a reader (he tries to hide it but it's there). And they have the same sense of humor too. :)

      What cracks me up is he commented on his pseudo thread in his normal account saying how awesome it was:

      [https://www.reddit.com/r/HPMOR/comments/5ry7gm/contrasting\_attitudes\_towards\_the\_wise\_old\_wizard/](https://www.reddit.com/r/HPMOR/comments/5ry7gm/contrasting_attitudes_towards_the_wise_old_wizard/)
      ```

      - u/Aretii:
        ```
        100% is not a real probability :|

        Interesting thought, though. I wonder who else is secretly Big Yud.
        ```

        - u/bambamramfan:
          ```
          This seems very unlikely. PMRF was fairly credentialist, and in the end fled from controversy over a post along those lines. EY would neither write that, and if he was he would just post in on FB or LW or whatever. (I could go into their other differences, certainly I have my own suspicions about who PMRF is, but this is the biggest.)
          ```

          - u/Ms_CIA:
            ```
            Any chance you have a link to the post? I was only able to find a few when browsing google.

            You might be right, but from what I've seen the two posters sound and act eerily similar. So I'm curious, what are the other differences you noticed?
            ```

- u/CouteauBleu:
  ```
  Glad you found something you like, I guess?

  Honestly, I don't think there was anything deeply meaningful about Harry achieving immortality for everyone. He just happened to stumble, mostly through luck, on an immortality machines that its previous owners had locked away without ever using its potential.

  Real life generally doesn't have that kind of immediately-exploitable world-changing potential lying around. I think Twig is a better take on world-changing transhumanism.
  ```

  - u/696e6372656469626c65:
    ```
    > Real life generally doesn't have that kind of immediately-exploitable world-changing potential lying around.

    Well.
    ```

- u/PurposefulZephyr:
  ```
  I disagree with your examples.

  You took the concept of an outside-the-context event causing a world-breaking catastrophe. You even used a proper hypothetical example ('epic' fanfiction ending half-way).

  Those two aren't such examples.  
  Those are the stories that have their amazingly intelligent and brave protagonists go on their quest to change the world, and ultimately succeed, totally at that.  
  Both of them use the rules available in the world. Used cleverly and lucky enough, but that's it. Jinchuriki's, versatile and powerful jutsu, ton of magic spells and unexplored magical properties of the world... yeah, all of them are part of the world. Local world. No need to explore the deep space or other realms for those.  

  If we went with dinosaur theme, we'd see this kind of example by a new, super-proficient and incredibly aggressive organism taking over earth.  
  Improbable? Yes. Impossible, given what dinosaurs experience (in their collective lifespans if not singular lives)? No. New species and mutations emerge all the time (given large enough timescale).  

  What examples could work?  
  [The Metroplitan Man](https://fictionhub.io/story/the-metropolitan-man/) would work, if only partly. Superman breaks known physics in many ways, after all. It's not as catastrophic (especially given his minimal interference), but outside-context none the less.  
  [HP&N20](https://www.fanfiction.net/s/8096183/1/Harry-Potter-and-the-Natural-20) also works, if still on a smaller scale. He's not of this realm, and his abiltiies baffle anyone around him. Give a lvl 20 mage such a journey, and you'll see a *real* meteor impact.  

  My point is that meteor strike should come from *outside* what we know. Outside all the known rules and plots.  
  Like Naruto continues his uplift work but suddenly an army of nano-chakra-bots come and raze all his work because alternate universe Orochimaru decides it's a good idea to conquer the multiverse.  
  Or you have a rom-com-drama and somewhere after protag's first kiss Rick just teleports into his room then liquifies and sucks out his girlfriend's spine because her genes make for the perfect ingredient of a delicious coctail.
  ```

- u/vimefer:
  ```
  Thank you for brightening my morning.

  May your showers and your coffee stay forever warm.

  (Edit) "Hilarious fist" ? Come on, we all know that the T-Rex evolved tiny arms so it wouldn't wank itself into extinction.
  ```

- u/CannedRealm:
  ```
  I love Waves Arisen and The Erogamer. I think they were written by the same author. Does anyone have any recommendations of stories that may be by the same author that I may have missed?
  ```

  - u/rhaps0dy4:
    ```
    Yeah, [Harry Potter and the Methods of Rationality](http://www.hpmor.com/)
    ```

- u/tjhance:
  ```
  I dunno, I think it's worth noting that the prophecy ("the asteroid event") was actually *directly triggered by an action that Quirrell took.* In fact, it was an action which was intended to have some psychological affect on Harry. And Quirrell knew that Harry had great capacity for magic power.

  So yeah, learning that Harry has the potential to destroy the world wasn't something Quirrell had thought of. But it's not nearly anywhere an asteroid level of an event, out of nowhere. And it definitely wasn't an event that had nothing to do with the plot. It followed directly from Quirrell's own actions and Quirrell could see that.
  ```

- u/wren42:
  ```
  I went and read waves arisen based on this recommendation, prior to finishing the article.  From the first part of your article I had expected a more catastrophic event to end Waves Arisen.

  To me, the Asteroid Strike in HPMOR wasn't Harry fooming, it was the End of the World prophecy.  Everyone was tooting along playing their roles, learning magic and playing politics, Voldemort Voldemorting, Dumbledore Dumbledoring, until Dumdum checked the prophecies and discovered he had BIGGER PROBLEMS. There was an asteroid coming.  There was nothing they could do about it.  The world was going to end, full stop.

  This is the kind of threat I imagined and mentally categorized as "Asteroid Strike", though I know get what you mean by the outside experience for the rest of society for Harry and Naruto's fooms.

  To me, the climate change metaphor was the real kicker in Waves Arisen.  We can speculate that the techniques Naruto came up with may have in fact been utilized by the "Ancients" -- that they had seen the potential for munchkined superweapons and chakra factories using water ninjitsu, and that the resulting arms race flooded the planet. Or, maybe even more realistically, that it was merely the combined actions of a few hundred million ninja leveraging relatively ordinary powers on a global scale.

  This kind of scenario is the much scarier and more likely scenario in our world.  Not a single person coming up with a neat trick and taking power, but a lot of people driven by game theory and politics to defect and escalate until the world is destroyed.

  We've faced this dilemma with Nuclear weapons, which was survivable because the actions required are overt and the consequences obvious and direct.  We're facing it with Climate Change, which may not be because all it takes is for people to *keep living normally* \- keep using energy and technology and climbing the consumption ladder and growing technology and civilization, like we've done for millenia.  The million small acts are perfectly sufficient to destroy the world and kill everyone.

  And we will face it very soon with AI, which will be a combination of the two - a natural progression of incremental improvements, motivated by economics and politics, that result in a sudden and likely catastrophic FOOM.

  All it takes is for things to continue the way they are - for people to keep improving AI gradually, and then for one idiot CEO some day to hire some eggheads to build a big smart program and give it the instruction to ["maximize shareholder value".](https://archiveofourown.org/works/9402014/chapters/21285149)

  &#x200B;

  This is the real meaning of an asteroid strike IMO - either natural events or unintended consequences crashing down on everyone's head in a way that is completely unavoidable once it starts.  It's not a satisfying literary technique to have Naruto breaking the water clones to refill his chakra en mass for the final battle only to discover he's flooded the rest of the plateau by accident and everyone dies - but this is the real danger.
  ```

- u/crivtox:
  ```
  In all of these coments I have the impresion that there is probably some other word we could be using instead of asteroid strike ,probably the name of a trope in tv tropes , but I can't think of one right now .
  ```

  - u/xamueljones:
    ```
    [Outside-Context Problem](https://tvtropes.org/pmwiki/pmwiki.php/Main/OutsideContextProblem)
    ```

    - u/crivtox:
      ```
      Yeah that fits well  , didn't come to mind yesterday for some reason.
      ```

- u/JOEBOBOBOB:
  ```
  I think there is a great external shock at one point: when he gains the rinnegan, and so all elements, super-vaporizer thing, perfect control of his chakra, the power to extrude an indestructible black material at will, and whatever secrets the previous user had. granted, he's already more-or-less foomed by then, having gotten the chakra-factory thing up and running, but it's still a sharp change. 
  also, I don't get where the 'the ninjas worship television' and "and has come to reform the ninja world with…a deluge of cheap Japanese electronics," things- none of what he does is electronic that I can see, and what are the ninjas supposedly worshiping that's like television?
  ```

- u/Arancaytar:
  ```
  To be honest, I'm not going to read that long an essay without a summary, so I'm not sure where you were going with this.

  But for what you're describing at the beginning, Banks coined the term [Outside Context Problem](https://tvtropes.org/pmwiki/pmwiki.php/Main/OutsideContextProblem).
  ```

- u/None:
  ```
  > (I got an A in literary analysis in school. Well, I got an F, but I interpreted it as an A.)
  >
  (I also got an F in topology and tried interpreting that as an A. It didn’t work.).   

  This is by far the funniest thing I've read all day, and had I not written this message I'd be reasonably confident that it would be the funniest thing I read all day. Now that I've written this, some people would be interested in proving me wrong so if I (say that I) predict that I'll see something funnier, it would make it less likely for me to see something funnier and if I (say that I) predict that I would see something funnier, it would make it less likely to be so. Of course after saying the above people may try to outsmart me and do the opposite of what I claim to expect them to do in response to my action, but I'll want to outsmart them so this goes on ad infinitum. As such, I would not make any explicit predictions, but please send me funny stuff.
  ```

  - u/xamueljones:
    ```
    Sure, I've got a few good math jokes:

    >What’s your favorite topic in mathematics?  
    *Knot theory.*  
    Yeah, me, neither.

    &#x200B;

    >Why did the chicken cross the Mobius strip?  
    *To get to the other, um, …*
    ```

- u/HereticalRants:
  ```
  This is beautiful.
  ```

---

