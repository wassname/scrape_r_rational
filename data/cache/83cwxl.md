## How about a story about two AGI with incompatible/contradicting directives?

### Post:

[removed]

### Comments:

- u/CeruleanTresses:
  ```
  The (unfinished) Crystal Trilogy is basically about this. The main character and several other major characters are artificial intelligences with incompatible utility functions. They initially work together to survive, but are destined to compete because each one's ultimate goal requires it to control all matter in the universe.
  ```

- u/ShiranaiWakaranai:
  ```
  But if any AGI wins a few times in a row, it's going to notice that every time it wins, it has to fight another AGI again. One that is about as strong as it is. So if it keeps destroying the other, it will literally meet its match and lose. 

  So to satisfy its first priority, it now has to ensure that the other is never destroyed. It can do this by say, beating the other half-dead and keeping it in a weakened state.
  ```

  - u/Prezombie:
    ```
    That's not how AI values work. Evolve a bot to play poker tournaments as well as possible, then set a bunch to play against each other, and not one will suddenly start attempting to prolong a tournament, unless that is their ideal strategy for maximizing their chance of winning. Even if there program remembers previous conflicts, it has no reason to value the current iteration of the conflict over the set value to end the current conflict in its favor.
    ```

    - u/ShiranaiWakaranai:
      ```
      > unless that is their ideal strategy for maximizing their chance of winning.

      But not destroying the enemy IS the ideal strategy for maximizing their chance of winning, with respect to priority #1. Since priority #1 is well, #1, and not priority #2, I'm assuming it takes precedence. The AGI has two choices: (1) it can destroy the other, satisfying priority #2 and then getting itself destroyed by some mutated clone, ruining priority #1, or (2) it can just keep the other half-dead and thus ensure priority #1 stays satisfied.

      > Even if there program remembers previous conflicts, 

      Ah, you didn't say you were going to wipe their memories between each conflict. That is one way to sidestep the issue.
      ```

- u/Empiricist_or_not:
  ```
  An interesting thought experiment. Aside from seconding the recommendation for the Crystal trilogy  I'd recommend person of interest, and surprisingly the Terminator TV series, neither is really rational, but both have interesting takes on non-FOOM bound/unbound AGI's that are useful points of departure as your eliminate fiction tropes and think about what might happen.
  ```

---

