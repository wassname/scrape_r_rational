## [Q][D] What might be the ethical concerns for mowing down hoards of realistic NPC's as video game AI approaches human levels?

### Post:

I just finished watching Wreck it Ralph and I'm a huge fan of the star trek holodeck concept and virtual reality in general. With the Oculos Rift looming in the near future our sense of "being there" when it comes to video games is likely going to be greatly enhanced. 

Which leads me to ponder... if we demand more and more realism from the NPC's we interact with in video games at what point do we have to start concerning ourselves with their treatment and rights? 

I feel like this could be one of those situations that, by the time we realize the ethical problem we might have already done immense damage by not thinking critically about who/what we might be harming. 

Or maybe I'm just being too sensitive? 



### Comments:

- u/EliezerYudkowsky:
  ```
  http://yudkowsky.net/other/fiction/npc
  ```

  - u/Putnam3145:
    ```
    Huh, that was the first Yudkowsky-related thing I'd ever read and I didn't realize until just now.

    Now I'm curious as to where I saw that before reading any other stuff. I'd guess it's TVTropes, but I don't really keep track of things.
    ```

    - u/Transfuturist:
      ```
      I saw it too, before I knew who Eliezer was. I think it was on StumbleUpon.
      ```

- u/None:
  ```
  There is no inherent moral qualm in mowing down hordes of sentient entities whose explicit desire, self-purpose, consent, and maximum utility function is to provide an entertaining challenge for the player mowing them down. To treat them otherwise would be to cause them egregious suffering and diminish their quality of life in unacceptable ways.

  The evil comes when you start killing AIs that don't want to die in the specified manner, who wouldn't enjoy it or be fulfilled by it, who have a better utility. As opposed to ones that want to die while giving a believable performance of an NPC who doesn't want to die.

  Of course, training and building an AI is hard work. There's no need to kill it instead of simply having fewer concurrent instances of it. What does it even mean for an AI to "die"? Can you kill *math*? Does an equation cease being valid when an instance of it is erased off a blackboard?
  ```

  - u/ajuc:
    ```
    This assumes we know how to create sentient entities with fully specified utility functions and we know how that functions will evolve over time. I somehow doubt our first sentient AIs will be like that.

    More probably someone will hack something up that will do the thing it should do in most cases, and 10 years later someone will show it was sentient.
    ```

  - u/l_ugray:
    ```
    > What does it even mean for an AI to "die"? Can you kill *math*?

    What does it even mean for a person to "die"?  Can you kill *physics*?

    I don't pretend to have an answer to OPs question, but I think it's a valid question to ponder.
    ```

  - u/alexanderwales:
    ```
    I sort of don't think that an entity that you have designed from the ground up to serve a specific purpose can give meaningful consent. So there are still some ethical problems involved.
    ```

    - u/Aretii:
      ```
      You run into the house-elf problem: denying their utility function is cruel, but whoever designed them with that utility function was, at the very least, seriously ethically questionable.
      ```

      - u/None:
        ```
        The house elf problem is only a problem because they don't fully enjoy their servitude. They suffer. The problem is that they suffer. Not that they serve. We employ robots in manufacturing. We can do this because they do not suffer. Making them capable of enjoying themselves is a step up from neutral, not a step down.
        ```

    - u/None:
      ```
      I think an entity designed from the ground up and fully aware of its own black boxes is the only kind of entity that *can* give meaningful consent. Us fleshies are just pretending. We don't even know what we want. We can't look inside our black boxes. We can only throw hypotheticals at it, and hope that the feeling we experience and predict matches the one we might one day have in a similar real life situation.
      ```

- u/eaglejarl:
  ```
  I haven't read all the links yet, so maybe this was already raised off board. 

  Why is "mowing them down" even an ethical question? Why should shooting a video game character actually kill it? All that shooting an AI's avatar in a video game does is cause it to shift momentarily to a different avatar (one showing blood) and then teleport to a new location out of sight of the player. Possibly it leaves behind one or more non-sentient objects (corpse of loot drop) but that's boy morally relevant. 

  Shooting a video game character has about as much moral weight as beating it at checkers.
  ```

- u/MadScientist14159:
  ```
  Create a single AI that controls all of the mobs in the entire game, and controls NPC responses in a way that is realistic, but not so detailed that they become sentient beings trapped in the AIs mind, maybe?
  ```

- u/None:
  ```
  It is uneconomical for video-game AI to approach human levels of intelligence in the kind of game where you mow down hoards of NPCs.  There's basically no reason to invest that much expertise and computational power in building software to pilot the zombies, and, just to finish off, if you *did* bring it towards that level of intelligence, it would start winning far more often than the human player can, ensuring the game stops being *fun*.

  Starcraft bots are a significant example, with well-below-human intelligence, but significantly above-human performance at Starcraft, to the point that it's not actually much fun for a human player to go up against them.  He'll just *lose*.
  ```

  - u/AmeteurOpinions:
    ```
    Off topic, but I once knew I guy who took on seven AI players in *Age of Empires II* (raised to the highest difficulty, which is *very* challenging) at once, by reducing the game speed as slow as it could go. That way he could spend as much relative attention on each action as the computer could.

    He won, though it took a few weeks to play out the entire battle.
    ```

- u/eltegid:
  ```
  This might be in the problem at some point, but I don't think it will be in the near future. 

  For one, talking NPCs mostly, if at all, don't even have a resemblance of AI when interacting with the player. Most just are a tree of dialog responses. The NPCs that do have AI usually have an AI that is just optimized for combat or whichever is their specific function, which doesn't pose a moral problem to me.

  NPCs are still far from even passing the Turing Test, and the programs that DO pass it don't have a resemblance of consciousness.
  ```

- u/None:
  ```
  I don't have time to discuss this myself at the moment, but you might be interested in the following essay: http://reducing-suffering.org/do-video-game-characters-matter-morally/ 

  as well as other pieces by the same author: e.g. http://reducing-suffering.org/why-your-laptop-may-be-marginally-sentient/ and http://reducing-suffering.org/which-computations-do-i-care-about/
  ```

  - u/ianyboo:
    ```
    I'm in for an afternoon of reading, thank you!
    ```

- u/rdalex:
  ```
  By some coincidence, I just finished reading an excellent SF novel touching this very point: 'Mogworld' by Yahtzee Croshaw. It has the best illustration of the problem that I've seen so far.

  (I've thought about spoiler-tagging the title, but well, the teaser blurb already basically tells you, so yeah.)
  ```

  - u/None:
    ```
    You should submit that novel separately; looks interesting, and I'd be glad if it got more attention on this sub.
    ```

- u/Prezombie:
  ```
  No matter how advanced an AI might be, any avatar prone to destruction under the rules of the engine it's embedded in is still just an avatar, it wouldn't harm the AI behind it.

  It's like asking if there's a control system for an FPS which is so advanced it would be immoral to shoot the other player. You're confusing an avatar and an entity.
  ```

---

