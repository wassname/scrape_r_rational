## [D] Monday General Rationality Thread

### Post:

Welcome to the Monday thread on general rationality topics!  Do you really want to talk about something non-fictional, related to the real world?  Have you:

* Seen something interesting on /r/science?
* Found a new way to get your shit even-more together?
* Figured out how to become immortal?
* Constructed artificial general intelligence?
* Read a neat nonfiction book?
* Munchkined your way into total control of your D&D campaign?


### Comments:

- u/LieGroupE8:
  ```
  Alright, let's talk about Nassim Nicholas Taleb. If you're not familiar, he's the famously belligerent author of [Fooled by Randomness](https://www.amazon.com/Fooled-Randomness-Hidden-Markets-Incerto-ebook/dp/B001FA0W5W/ref=sr_1_1?ie=UTF8&qid=1497580146&sr=8-1&keywords=fooled+by+randomness), [The Black Swan](https://www.amazon.com/gp/product/B00139XTG4/ref=series_rw_dp_sw), and [Antifragile](https://www.amazon.com/gp/product/B0083DJWGO/ref=series_rw_dp_sw), among other works. I don't think Taleb's views can be fully comprehended in a single day, so I strongly advise going out and reading all his books.

  -----
  **Edit**: What I really want to know here is: of those of you who are familiar with Taleb's technical approach to decision theory and how he applies this to the real world, is his decision theory 1) Basically correct, 2) Frequently correct but mis-applied sometimes, or 3) basically incorrect?
  -----

  On the one hand, I suspect that if he knew about the rationalist community, he would loudly despise it and everything it stands for. If he doesn't already know about it, that is: I remember seeing him badmouth someone who mentioned the word "rationalist" in Facebook comments. He has said in one of his books that Ray Kurzweil is the opposite of him in every way. He denounces the advice in the book "Nudge" by Thaler and Sunstein (which I admittedly have not read - is this a book that rationalists like?) as hopelessly naive. He considers himself Christian, is extremely anti-GMO, voted third-party in the election but doesn't seem to mind Trump all that much, and generally sends lots of signals that people in the rationalist community would instinctively find disturbing.

  On the *other* hand...

  **Taleb the Arch-rationalist?**

  Despite the above summary, if you actually look closer, he looks more rationalist than most self-described rationalists. He considers erudition a virtue, and apparently used to read for 30 hours a week in college (he timed himself). I remember him saying off-hand (in The Black Swan, I think) that a slight change in his schedule allowed him to read an *extra* hundred books a year. When he decided that probability and statistics were good things to learn, he went out and read every math textbook he could find on the subject. Then he was a wall street trader for a couple of decades, and now runs a risk management institute based on his experiences.

  He considers himself a defender of science, and calls people out for non-rigorous statistical thinking, such as thinking linearly in highly nonlinear problem spaces, or mis-applying analytical techniques meant for thin-tailed distributions on fat-tailed distributions. (Example of when thinking "linearly" doesn't apply: [the minority rule](https://medium.com/incerto/the-most-intolerant-wins-the-dictatorship-of-the-small-minority-3f1f83ce4e15)). He loves the work of Daniel Kahneman, and acknowledges human cognitive biases. Examples of cognitive biases he fights are the "narrative fallacy" (thinking a pattern exists when there is only random noise) and the "[ludic fallacy](https://en.wikipedia.org/wiki/Ludic_fallacy)" (ignoring the messiness of the real world in favor of nice, neat, plausible-sounding, and wrong, theoretical knowledge).

  He defends religion, tradition, and folk wisdom on the basis of statistical validity and asymmetric payoffs. An example of his type of reasoning: if old traditions had any strongly negative effects, these effects would almost certainly have been discovered by now, and the tradition would have been weeded out. Therefore, any old traditions that survive until today must have, at worst, small, bounded negative effects, but possibly very large positive effects. Thus, adhering to them is valid in a decision-theoretic sense, because they are not likely to hurt you on average but are more amenable to large positive black swans. Alternatively, in modern medical studies and in "naive scientistic thinking", erroneous conclusions are often not known to have bounded negative effects, and so adhering to them exposes you to large negative black swans. (I *think* this is what he means when he casually uses one of his favorite technical words, "ergodicity," as if its [meaning](https://www.facebook.com/nntaleb/posts/10152335020578375) were [obvious](https://www.facebook.com/nntaleb/posts/10152867756623375)).

  Example: "My grandma says that if you go out in the cold, you'll catch a cold." Naive scientist: "Ridiculous! Colds are caused by viruses, not actual cold weather. Don't listen to that old wive's tale." Reality: It turns out that cold weather suppresses the immune system and makes you more likely to get sick. Lesson: just because you can't point to a chain of causation, doesn't mean you should dismiss the advice!

  Another example: Scientists: "Fat is bad for you! Cut it out of your diet!" Naive fad-follower: "Ok!" Food companies: "Let's replace all the fat with sugar!" Scientists: "JK, sugar is far worse for you than fat." Fad-follower: "Well damn it, if I had just stuck with my traditional cultural diet that people have been eating for thousands of years, nothing all that bad would have happened." Lesson: you can probably ignore dietary advice unless it has stood the test of time for more than a century. More general lesson: applying a change uniformly across a complex system results in a single point of failure.

  For the same sorts of reasons, Taleb defends religious traditions and is a practicing Christian, even though he seems to view the existence of God as an irrelevant question. He simply believes in belief as an opaque but valid strategy that has survived the test of time. [Example 1](https://www.quora.com/What-did-Taleb-mean-by-his-criticism-of-Richard-Dawkins-in-Talebs-Reddit-Ask-Me-Anything-Q-A). [Example 2](https://www.facebook.com/nntaleb/posts/10152548740888375). Relevant quote from example 2: 

  > Some unrigorous journalists who make a living attacking religion typically discuss "rationality" without getting what rationality means in its the decision-theoretic sense (the only definition that can be consistent). I can show that it is rational to "believe" in the supernatural if it leads to an increase in payoff. Rationality is NOT belief, it only correlates to belief, sometimes very weakly (in the tails). 

  His anti-GMO stance makes a lot of people immediately discredit him, but far from just being pseudoscientific BS, he makes what is probably the strongest possible anti-GMO argument. He only argues against GMOs formed by advanced techniques like plasmid insertion, and not against lesser techniques like selective breeding (a lot of his detractors don't realize he makes this distinction). The argument is that these advanced techniques, combined with the mass replication and planting of such crops, amounts to applying an uncertain treatment uniformly across a population, and thus results in a catastrophic single point of failure. The fact that nothing bad has happened with GMOs in the past is not good statistical evidence, according to Taleb, that nothing bad will happen in the future. There being no good evidence against *current* GMOs is secondary to the "precautionary principle," that we should not do things in black swan territory that could result in global catastrophes if we are wrong (like making general AI!). I was always fine with GMOs, but this argument really gave me pause. I'm not sure what to think anymore - perhaps continue using GMOs, but make more of an effort to diversify the types of modifications made? The problem is that the GMO issue is like the identity politics of the scientific community - attempt to even entertain a possible objection and you are immediately shamed as an idiot by a facebook meme. I would like to see if anyone has a *statistically rigorous* reply to taleb's argument that accounts for black swans and model error.

  Taleb also strongly advocates that people should put their "skin in the game." In rationalist-speak, he means that you should bet on your beliefs, and be willing to take a hit if you are wrong.

  To summarize Taleb's life philosophy in a few bullet-points:

  * Read as many books as you can
  * Do as much math as you can
  * Listen to the wisdom of your elders
  * Learn by doing
  * Bet on your beliefs

  Most or all of these things are explicit rationalist virtues.

  **Summary**

  Despite having a lot of unpopular opinions, Nassim Taleb is not someone to be dismissed, due to his incredibly high standards for erudition, statistical expertise, and ethical behavior. What I would like is for the rationalist community to spend some serious time considering what Taleb has to say, and either integrating his techniques into their practices or giving a technical explanation of why they are wrong. 

  Also, I would love to see Eliezer Yudkowsky's take on all this. I'll link him here (/u/EliezerYudkowsky), but could someone who knows him maybe leave him a facebook message also? I happen to think that this conversation is *extremely important* if the rationalist community is to accurately represent and understand the world. Taleb has been mentioned occasionally on LessWrong, but I have never seen his philosophy systematically addressed.

  [Taleb's Youtube Channel](https://www.youtube.com/channel/UC8uY6yLP9BS4BUc9BSc0Jww)

  [Taleb's Medium.com Blog](https://medium.com/@nntaleb)

  [His essay on "Intellectuals-yet-idiots"](https://medium.com/incerto/the-intellectual-yet-idiot-13211e2d0577)

  [His personal site, now with a great summarizing graphic](http://www.fooledbyrandomness.com/)
  ```

  - u/ShiranaiWakaranai:
    ```
    > He defends religion, tradition, and folk wisdom on the basis of statistical validity and asymmetric payoffs. An example of his type of reasoning: if old traditions had any strongly negative effects, these effects would almost certainly have been discovered by now, and the tradition would have been weeded out. Therefore, any old traditions that survive until today must have, at worst, small, bounded negative effects, but possibly very large positive effects. Thus, adhering to them is valid in a decision-theoretic sense, because they are not likely to hurt you on average but are more amenable to large positive black swans. Alternatively, in modern medical studies and in "naive scientistic thinking", erroneous conclusions are often not known to have bounded negative effects, and so adhering to them exposes you to large negative black swans. (I think this is what he means when he casually uses one of his favorite technical words, "ergodicity," as if its meaning were obvious).

    > Example: "My grandma says that if you go out in the cold, you'll catch a cold." Naive scientist: "Ridiculous! Colds are caused by viruses, not actual cold weather. Don't listen to that old wive's tale." Reality: It turns out that cold weather suppresses the immune system and makes you more likely to get sick. Lesson: just because you can't point to a chain of causation, doesn't mean you should dismiss the advice!

    NO NO NO! This argument is one of my worst triggers. It's my firm belief that this is biggest reason why the world we live in is the hellhole we know today. Let me break down this argument for you, he's claiming that if everyone takes some action X, X must be positive. If it was negative, people doing X would slowly die off from the consequences of X until no one does X. That sounds plausible, but it's only half of the story.

    The thing you need to realize is that for many actions X, X can not only kill you, it can also cause more people to start doing action X. There's an actual term that describes this process: natural selection.

    Given any system of objects that can produce (slightly different) copies of themselves, what kinds of objects will dominate? A naive thinker would go "OH OH I KNOW: **survival of the fittest**!" and then talk about how the objects that are strongest, the objects that are healthiest, the objects that take the least self-harming actions, would dominate the system over time. Oh happy happy world.

    The truth is, the phrase "**survival of the fittest**" may have been the single worst scientific marketing blunder in the history of science. And that's saying something since they make other kinds of shitty blunders like "global warming" all the time. Descriptions of scientific phenomena that give laypeople ideas that are completely off the mark. For example, the layperson that hears global warming thinks "oh no the earth is getting hotter everywhere", when actually its the average temperature that is getting hotter, and some places may actually become colder. And so you end up with politicians throwing snowballs around claiming that debunks global warming. **facepalm**.

    The same thing is happening here. Fittest, does **not** mean the best at surviving. That is part of it, but a much much larger part of it is best at **reproducing**. Frankly, if there's a way to trade half your lifespan for several times more children, natural selection would welcome it with open arms. For example: an impotent human with the healthiest habits in the world will be removed from the system in a generation. Meanwhile, all kinds of rapists, adulterers, playboys, gigolos, prostitutes and what not continue to linger in the system, even if they have a whole host of behaviors that tend to harm themselves. In a sense, rape and adultery ARE traditions. They are actions that a significant fraction of the population do and have been doing for eons past, and will likely continue to do generations into the future. 

    Are these actions positive? Do they help you survive? Hell freaking no. They are crimes, so you get caught by police and punished, and such punishments tend to reduce your lifespan significantly. And even if there are no police, these actions still earn people's hatred, and may then cause you to be murdered in your sleep. But they help produce children. Children with your genes. And while yes, environmental factors can easily cause the child to abandon the way of the rapist or the adulterer (so you certainly shouldn't demand children be hanged for the sins of their parents), they now have a genetic push towards them, as well as a push from every idiot that says "TRADITIONS ARE ALWAYS GOOD". And so rapists and adulterers continue to make up a significant fraction of the population. It's the miracle of natural selection! Woohoo (sarcasm)!

    Now you might be thinking, "well okay, I'll just stay away from the traditions that involve having sex then. Surely they must be all good for survival now?" Still wrong. Because you can be a gene protector even without having sex. Consider racism. Racism was (and probably still is in many places) quite literally a tradition. A whole set of traditions even. Traditions you might not even think are associated with racism, yet have racist effects. Racism, from a natural selection point of view, is extremely good. When you oppress and kill people who don't have your genes, people who do have your genes have less competition for resources. But is racism good for you on a personal level? No. Racism prompts you to fight. Fighting involves risk to life and limb. You could easily get yourself killed or permanently crippled in these fights. Yet it is still everywhere because of natural selection. 

    Natural selection rejoices in making suicidal idiots for its cause. Kind of like bees really. There are bees that don't reproduce at all, and basically perform suicide attacks on any creature that attacks their hive. You know, suicide attacks: bad for personal survival, good for gene survival! And these suicidal bees are everywhere. Truly a great tradition (sarcasm)!

    And the worst part is, actions can reproduce in ways other than genes. Memes are a thing. You see this happening in real life all the time: successful people go around writing books about the actions they took to become successful, and people follow those actions to try and also become successful. In a sense, religious wars are the meme version of racism. If you oppress and kill the people who don't have your memes, people with your memes have less competition. Natural selection and tradition prompts you to be the suicidal bee, sacrificing your personal wellbeing (along with the wellbeing of people who don't have your memes), for the sake for the people who do have your memes.

    Frankly natural selection just loves evil and self-harm. There's just so much stuff you can do for your genes/memes by being evil and suicidal that it's the overwhelming favorite of natural selection. Hence reality being the hellhole that it is today. 

    So the next time you see a tradition, or something everyone else is doing. Stop for a moment and think: do I know the logic behind these actions? Can I point to a chain of causation? Otherwise, there's a significant chance that chain of causation is some kind of suicidal evil that protects/generates genes/memes.
    ```

    - u/LieGroupE8:
      ```
      This is a strawman of Taleb's views, which I cannot possibly do justice to in a single post. I do not fully agree with Taleb, but his argument is subtler than "it has survived natural selection so we might as well keep doing it." Taleb has explicitly said that he makes exceptions to his arguments for any practices that infringe on ethics. He defends religious practice mostly on a ceremonial and aesthetic basis. So, for example, fasting and prayer are good, but killing apostates is definitely bad. He is against extremism and literalism.

      Your point on the trade-off between individual survival and mass replication is good, though.
      ```

      - u/ShiranaiWakaranai:
        ```
        > This is a strawman of Taleb's views, which I cannot possibly do justice to in a single post. I do not fully agree with Taleb, but his argument is subtler than "it has survived natural selection so we might as well keep doing it." Taleb has explicitly said that he makes exceptions to his arguments for any practices that infringe on ethics.

        That is good to hear, but is still problematic, especially because **everything** "infringes on ethics" to some extent. After all, ethics includes **lies**.

        For example, if you perform an action X, you either have to hide it or others will know you have performed X. If you hide it, that almost certainly will involve lying, infringing on ethics. If it's revealed, others will be curious why you perform X. Many will suspect that X has some kind of positive effect, since you are doing X and you haven't died or suffered significant harm from doing X. (Otherwise why would you still be doing X?) And so by doing X, you will be implicitly suggesting to others, that X is a good thing to do. But if you aren't sure that X is a good thing to do, then that is an implicit lie. It infringes on ethics to lead people to do something you aren't sure is good for them. 

        In effect, saying "obey some rule X unless it infringes on ethics" really says nothing at all, and is the kind of thing you say when you're tired of listening to people tell you how horrible rule X is yet still refuse to acknowledge that rule X is horrible. And it's utterly terrifying when I see "smart", "rational" people say stuff like this.

        A few years ago, I stumbled upon a rationality website where the author went on and on about his system of ethics and how wonderful it was. And then, he had a page that said "hey guys, I know this system of ethics sometimes tells you to kill people in certain situations. You should just treat those cases as exceptions, and always obey the system unless it tells you to kill people." 

        ...

        Are you freaked out by this? Because I certainly am. Any system of ethics that tells you to kill people in some situations is almost certainly going to tell you to beat people to an inch from death in some cases, two inches in others, three inches in yet more others, and so on. Which of these are exceptions and which aren't!? And why?! The author sadly, did not explain this.
        ```

        - u/LieGroupE8:
          ```
          Let me put it another way. In every decision, you can do one of two things: 1) Keep doing what you've been doing, or 2) Do something else. Taleb says you should have a strong bias in favor of (1), *unless* there is a strong reason for (2). The set of strong reasons for (2) includes ethical violations caused by doing (1). Taleb backs up his arguments with lots of math about complex systems and stochastic processes. The thing is, I don't know enough of this type of math to tell how much he is BSing (and I majored in math!)

          > Are you freaked out by this?

          Yeah, sort of, because the system sounds too ad-hoc to work.

          > Any system of ethics that tells you to kill people in some situations is almost certainly going to tell you to beat people to an inch from death in some cases, two inches in others, three inches in yet more others, and so on.

          This is also true of basically any plausible system of ethics, though.
          ```

    - u/DiscyD3rp:
      ```
      >Meanwhile, all kinds of rapists, adulterers, playboys, gigolos, prostitutes and what not continue to linger in the system, even if they have a whole host of behaviors that tend to harm themselves.

      Somewhat tangential to your main point, but I think it's incredibly unfair to include prostitutes in that list. Sex work is the opposite of bad or evil behavior and I grow tired of seeing people disparaging it so readily.
      ```

      - u/ShiranaiWakaranai:
        ```
        I apologize for that, I did not mean to imply prostitution was in any way evil like rape. I put it there as an example of self-harming behavior. Offering sex services is like painting a huge target mark on yourself for sexual violence, and that can have serious effects on your health and wellbeing. It is really not a good action to take for your personal survival unless all your other options are worse.
        ```

    - u/ArgentStonecutter:
      ```
      I wish I could upvote this more than once. This one sentence fragment encapsulates so many bad ideas that I wanted to reach through the Internet and slap someone.

      > > if old traditions had any strongly negative effects, these effects would almost certainly have been discovered by now, and the tradition would have been weeded out
      ```

      - u/LieGroupE8:
        ```
        This is a claim that can be operationalized and tested, perhaps via simulation. And note that Taleb is not talking about ethical badness, which he makes an exception for, but about badness in terms of individual death or adverse health effects.
        ```

        - u/ArgentStonecutter:
          ```
          I'm kind of surprised that you would complain about /u/ShiranaiWakaranai's post being a straw man when you're doing the same thing.

          See, the thing here is, you don't get to pick and choose what parts of religion other people (the ones that are propagating and actualizing these memes) are going to act out. So, sure, there's lots of things in religion that are ethically neutral or good but they're inextricably bound in with the evil and self-harming stuff.

          Or, put another way, if you simplify the problem space to religious traditions that aren't harmful, you don't get to use that to prove that religious traditions aren't harmful. Because you still have the harmful ones as proof that "old traditions with string negative effects" aren't "weeded out".

          And you don't need a simulation to test it, you can observe it in the real world.
          ```

  - u/LieGroupE8:
    ```
    Put here because original comment was too long:

    **Taleb the Libertarian Anti-Transhumanist**

    Taleb's political views are somewhat difficult to figure out. (Actually a lot of his personal beliefs are difficult to figure out, either because he forms no beliefs out of epistemic humility, or because he explicitly considers it a virtue to be opaque, to the great frustration of every rationalist who has tried to understand him. See his cryptic April 30th facebook post, ["Never explain why something is important"](https://www.facebook.com/nntaleb/?fref=nf). Notice how he stays true to this advice regarding the advice itself.). As far as I can tell, he is not a Trump supporter (because he voted third-party according to at least one interview), but he considers a lot of Trump's policies as a step in the right direction due to axing blanket legislation that acts as a single point of failure and a black swan attractor. Taleb's nonchalance in the presence of Trump is due to the fact that he (rightly) considers most news stories as noise with no signal which ultimately won't affect anything. [See this interview](https://www.youtube.com/watch?v=kKW0LbeiWio). He ignores the way Trump talks and claims that just looking at actions, he is not much different than other politicians (really, Taleb???). He despises Hillary Clinton, not only because he doesn't like her policies, but apparently because he considers her as utterly devoid of morality and skin in the game. He dislikes labelling himself, but I would guess that he is mostly a libertarian, believing that small local governments and redundant economies are more robust (and "antifragile") than large governments. 

    Toy example of a globalistic over-optimization that leads to non-redundancy and fragility: Country A has a comparative advantage in food production and country B has a comparative advantage in machine production. So B produces all the farming equipment and A produces all the food, the two countries trade, everyone is happy. Whoops, country A's regime collapsed in a brutal civil war - now everyone in B starves to death because they have no redundant farming economy of their own. But country C depended entirely on B for mining equipment, so their economy collapses too, and so on. The errors propagate until the whole world economy collapses. Lesson: interdependent globalism without local error-absorption barriers is a ticking time-bomb.

    I expect Taleb would dislike the rationalist community because he would consider us to be over-optimizers who have fallen prey to overconfidence bias, who are unaware of asymmetric payoffs, and who apply linear statistical thinking where it doesn't work. In other words, he would denounce us for talking like we're high-and-mighty empiricists while being too lazy to carry out actual experiments or learn the ultra-advanced theoretical statistics necessary to properly understand the data we have received. 

    If Taleb delved further into the rationalist community, he would likely commend some of our people on their willingness to bet on their beliefs and on their approach to scientific rigor, for rationalists have a philosophy closer to his own beliefs than he realizes. But he would still strongly condemn transhumanism. This is because he views risk-taking as a virtue and an inseparable part of life, and he views transhumanism as wanting to remove all risk from existence. If there is no *real* risk of death, then nothing is exciting anymore! Transhumanism just makes everything fragile and removes a critical aspect of the environment that we evolved to flourish in, or so he would argue.

    The most frustrating thing I find about Taleb (aside from his unnecessary combativeness) is that he can be very difficult to understand when he is making an argument. Sometimes he gives examples without explanation, simply saying that the general principle of the example should be clear. Other times he doesn't even give the whole example, but makes cryptic references and allows the reader to fill in the details. I wonder if he does all this on purpose - I remember him saying something in Antifragile about how you learn more from teachers who are hard to understand, simply because you are forced to pay more attention. It took me a while to comprehend his worldview, but I think I've accurately represented it.
    ```

    - u/InfernoVulpix:
      ```
      Has he actually said as much about transhumanism?  The goal is to remove the risk of death but life will still hold many, many risks.  You can still put your money on the line or do things that risk being a colossal waste of time or do things like enter romantic relationships not knowing if they'll work out.  Risk of death is a narrow subset of all risks out there and it's the just the one with the worst penalty for losing.
      ```

      - u/LieGroupE8:
        ```
        I can't recall him ever mentioning the term transhumanism directly, but in some places he seems to refer to that general set of ideas indirectly. I'm inferring what I think his reflex response would be.
        ```

    - u/None:
      ```
      > He dislikes labelling himself, but I would guess that he is mostly a libertarian, believing that small local governments and redundant economies are more robust (and "antifragile") than large governments.

      That comes across as weird to me.  I see "libertarians" as directly enabling the economy to over-optimize itself into extreme fragility, and discouraging the robustness that comes from social democracy.
      ```

    - u/buckykat:
      ```
      > This is because he views risk-taking as a virtue and an inseparable part of life, and he views transhumanism as wanting to remove all risk from existence. If there is no real risk of death, then nothing is exciting anymore!

      Handy thing about deathists is that they die off.
      ```

      - u/CCC_037:
        ```
        So far, evidence suggests that *everyone* dies off (minus a statistically insignificant sample who have not quite died *yet*)
        ```

  - u/suyjuris:
    ```
    I am not familiar with Taleb, but only commenting on the arguments as presented in your post.

    > He considers erudition a virtue [...]

    I do agree that knowledge is important.

    > (Example of when thinking "linearly" doesn't apply: the minority rule).

    I read the linked article, and found it devoid of insight, but rather a collection of anecdotes. Some seemed quite forced, compounded by the fact that it tried to argue multiple theses depending on the previous ones. The chain of logic went from obvious statements to false ones quite nicely.

    > If old traditions had any strongly negative effects, these effects would almost certainly have been discovered by now, and the tradition would have been weeded out.

    I do not agree with this argument at all. The length of time something has been around for is not a strong indicator of usefulness. Many traditions (e.g. not washing your hands) have survived for thousands of years, yet abolishing them has yielded the most substantial improvement's in quality of life. (Also note, that this argument is not falsifiable by presenting some currently ongoing tradition.)

    For any tradition to be continued, it is only necessary for public belief to support its continuation. This is a weak indicator of any actual effects, but due to the huge influence of cognitive biases not a strong indicator. The process which produces the best predictions of reality (that are available to us) is called science (by definition). Things, with potentially huge downsides, you need to investigate carefully (*including* a variety of sources, like historical data) and apply error bars generously. And after you have done so, and the results are in, *you update your probabilities and move on*.

    > Alternatively, in modern medical studies and in "naive scientist thinking", erroneous conclusions are often not known to have bounded negative effects, and so adhering to them exposes you to large negative black swans.

    Traditions are not known "to have bounded negative effects", only to have had bounded negative affects in the past (even that statement is generous). Everything changes over time, and even knowledge that hold true for a long time may become outdated. It is, of course, possible to extrapolate from previously collected data in a reliable fashion. This is also called science.

    > Example: "My grandma says that if you go out in the cold, you'll catch a cold." Naive scientist: "Ridiculous! Colds are caused by viruses, not actual cold weather. Don't listen to that old wive's tale."

    Actual scientist: "Let me do a study on this and get back to you."

    > Reality: It turns out that cold weather suppresses the immune system and makes you more likely to get sick.

    Actual scientist: "You're welcome."

    This is (obviously) arguing a straw-man, of course you should not be naïve.

    > Scientists: "Fat is bad for you! Cut it out of your diet!"

    Somehow I doubt that there were many scientists expressing that sentiment. (Feel free to drop the link to any paper you might have cited this from, however.)

    As far as I know, the evidence points in the direction of a balanced diet having no significant disadvantages (for an average person). Claims in the media tend to be exaggerated. As there is evidence that having a balanced diet has no significant disadvantages, and there is a lack of evidence for any change having advantages, being conservative regarding your nutrition is only rational (without any appeal to tradition).

    > For the same sorts of reasons, Taleb defends religious traditions and is a practicing Christian, even though he seems to view the existence of God as an irrelevant question. He simply believes in belief as an opaque but valid strategy that has survived the test of time. [...]
    >> Some unrigorous journalists who make a living attacking religion typically discuss "rationality" without getting what rationality means in its the decision-theoretic sense (the only definition that can be consistent). I can show that it is rational to "believe" in the supernatural if it leads to an increase in payoff. Rationality is NOT belief, it only correlates to belief, sometimes very weakly (in the tails).

    I agree with the sentiment expressed in the quote. Rational actions, by definition, are the one with the highest payoff. Neither the practice nor the belief of religion is necessarily incompatible with a belief in rationality. However, I find it unlikely that the *methods* of religion (a part of the beliefs) are effective (i.e. compatible with a belief in rationality).

    > The argument is that these advanced techniques, combined with the mass replication and planting of such crops, amounts to applying an uncertain treatment uniformly across a population, and thus results in a catastrophic single point of failure.

    The logic depends on these techniques, which have been studied extensively, being more uncertain than traditional agriculture in a changing environment. I see no reason to believe that more advanced techniques are somehow more dangerous, but also able to—coincidentally—hide this fact under investigation.

    > The fact that nothing bad has happened with GMOs in the past is not good statistical evidence, according to Taleb, that nothing bad will happen in the future.

    The fact that nothing bad has happened with traditional agriculture in the past is not good statistical evidence that nothing bad will happen in the future. Scientific research, however, is good evidence.

    > There being no good evidence against current GMOs is secondary to the "precautionary principle," that we should not do things in black swan territory that could result in global catastrophes if we are wrong [...]

    Doing nothing may also lead to disaster. There are no safe choices.

    > Taleb also strongly advocates that people should put their "skin in the game." In rationalist-speak, he means that you should bet on your beliefs, and be willing to take a hit if you are wrong.

    This is excellent advice.
    ```

    - u/LieGroupE8:
      ```
      > I am not familiar with Taleb, but only commenting on the arguments as presented in your post.

      Maybe I should have asked people not to comment unless they had read all of Taleb's books, plus his personal website and facebook posts. Not that your comment is bad (it isn't), but a lot of the stuff that people are bringing up is addressed very thoroughly in his writing. I assumed that more people here would have read Taleb on the general principle of reading lots of different viewpoints, so that they would be on the same page as me, but either I was mistaken or those people are not commenting.

      > I read the linked article, and found it devoid of insight, but rather a collection of anecdotes

      Yeah, that's one of the things that really frustrates me about Taleb. His arguments are filled with disjointed, half-baked examples.

      > The length of time something has been around for is not a strong indicator of usefulness.

      Eh, sort of. See the other comments here addressing this.

      > Actual scientist: "Let me do a study on this and get back to you."

      Taleb would defend the actual scientist here. But I have seen plenty of people who think they are smart act like the naive scientist.

      > Doing nothing may also lead to disaster. There are no safe choices.

      Simulated Nassim Taleb replies: "That's like saying that even regular driving carries a risk of death, so I might as well drunk-drive! It completely misses the point of *asymmetric* risk! Traditional agriculture does not end the world with any serious probability, because if it did, we would already be dead (this is the principle of ergodicity). GMOs, on the other hand, have not been tested for long enough to rule-out fat-tails."
      ```

      - u/suyjuris:
        ```
        > Maybe I should have asked people not to comment unless they had read all of Taleb's books, plus his personal website and facebook posts.

        That would be an unreasonable burden on the commenters and is unlikely to yield more useful comments. I am willing to spent a few hours reading an opinion I find flawed, but after some time there just is no expected utility in it. (At some point the probability of me being unable to understand the argument drops too low, compared to the probability of the author's argument being flawed. That is just a general heuristic.)

        Also beware of being in an echo chamber; people who have read all his books are likely to agree with him.

        > Eh, sort of. See the other comments here addressing this.

        I only saw others addressing the ethics of traditional behaviors. Mind dropping a quote?

        > Simulated Nassim Taleb replies: "That's like saying that even regular driving carries a risk of death, so I might as well drunk-drive! It completely misses the point of asymmetric risk!

        This is backwards. The point was *not*, that in an absence of safe choices the most dangerous one was preferable. But that risks have to be assessed and the assumption of a risk-free alternative does not hold.

        Applied to your metaphor: "There is no point in wearing a seat belt! I drove around for decades without one, and I'm fine! This means that not wearing a seat belt does not kill me with serious probability, since I would have been long dead by now. But who knows what might happen if I put it on? After all, it could cut me, maybe trap me inside the car, or provide a false sense of security. No, driving without is perfectly safe and will always be."

        > Traditional agriculture does not end the world with any serious probability, because if it did, we would already be dead (this is the principle of ergodicity).

        Citing [Wikipedia](https://en.wikipedia.org/wiki/Ergodicity): "In probability theory, an ergodic dynamical system is one that, broadly speaking, has the same behavior averaged over time as averaged over the space of all the system's states in its phase space."

        This is a simplifying assumption (when applied to the system earth), that does not hold in reality. (Just look at a graph of surface temperatures.)

        As I understand the concept (and please correct me if I am wrong) the argument goes like this: When a system is ergodic, a measurement of a probability over a long period of time automatically gives the probability of that behavior in a random state. Meaning that any tradition is automatically safe, since it has previously exhibited a probability of extinction in the vicinity of 0.

        In a mathematical sense, this is a correct deduction. But please note (some of) the implicit assumptions:

        * The earth is an ergodic system.
        * 100 years is a long time (the time we have been doing traditional agriculture without it causing an extinction).
        * The only way to measure extinction-level risk of technologies is by employing these technologies on a large scale.
        ```

  - u/artifex0:
    ```
    So, here's a question that I think is very relevant to Taleb: is it rational to always accept an argument that you can't fault, even if you suspect that the source of the argument is biased or untrustworthy?  I don't think that's a question with an obvious answer, but I'd argue no.

    Suppose you Googled a well-established conspiracy theory- 9/11 truthers, UFOs, whatever. You'd almost certainly encounter arguments and apparent evidence that you couldn't immediately debunk based on first-hand knowledge.  You could, of course, also Google facts and articles to debunk those claims- but if you consider only the facts and reasoning presented and not the trustworthiness of sources, doing so would appear to be motivated reasoning.  These conspiracy theories are built up from decades of motivated reasoning, so why should using the same method yourself produce better results?

    I think the answer has to be that the sources of these theories aren't trustworthy enough to support their extraordinary claims.  We know that the people who come up with these kinds of theories tend to rely on fact-gathering and rhetorical methods that introduce an enormous amount of bias; we know that their arguments are usually contradicted by more trustworthy sources; and we know that they're often not all that rational.

    So, is it rational to discount the arguments of conspiracy theorists on no other basis than that mistrust?  Maybe in a perfect world, we'd all have the time to independently test the arguments that can be tested, and the education to judge the arguments that can't.  In a world with limited time, in which we encounter vastly more claims than we can independently verify, however, I think that mistrust can be a valid reason for disbelief.

    Nassim Taleb appears, at least to me, to be an extremely intelligent pathological narcissist.  He's made a lot of extraordinary arguments, a small number of which I can find fault with, but most of which I can't.  I think he's my intellectual superior, in both education and intellect, but I don't find him trustworthy.  I know from experience that people who behave like he does have problems with self-delusion, and I don't think he does a good job of taking the ideas and criticisms of others into account.

    Is that mistrust sufficient reason to dismiss his arguments, even when I can't personally fault them?  Maybe not entirely- he's not some rocker-adjacent conspiracy theorist, and he could turn out to be right about everything- but I think it's sufficient reason to be extremely skeptical.
    ```

    - u/None:
      ```
      > is it rational to always accept an argument that you can't fault, even if you suspect that the source of the argument is biased or untrustworthy?

      The *always* part is a trivial no.  If a hostile superintelligence puts forward an argument I can't fault in, I try my best to behave as if that damned thing had never spoken to me.  Even changing my behavior in the opposite direction from where the argument points is most likely letting the interlocutor manipulate me.  This also applies well below "superintelligence" to people who just happen to have cached arguments I've never heard before.

      Arguments *just are* social manipulation.  That is their chief function.  That's *why* the discipline of logic, and thence mathematics, evolved separately from rhetoric.
      ```

    - u/LieGroupE8:
      ```
      > I don't think he does a good job of taking the ideas and criticisms of others into account.

      Agreed. He makes himself almost unapproachable in this regard, at least online. Dissenters in the comments sections of his facebook posts are ridiculed.

      > So, is it rational to discount the arguments of conspiracy theorists on no other basis than that mistrust?

      I don't think Taleb should be put in the same bucket as conspiracy theorists. Also, your question has an equal and opposite, namely: Is it rational to trust the arguments of someone established to be a strong rationalist even if you don't fully understand them?

      > I think the answer has to be that the sources of these theories aren't trustworthy enough to support their extraordinary claims.

      Taleb doesn't care about epistemology so much as he cares about decision-making, and the interesting thing is that his main arguments tend to mirror the idea of distrusting theories that can't produce extraordinary evidence. Namely, he argues that under many cases of real-world uncertainty, your "default" behavior should be tradition and well-established heuristics, and you should only depart from these if you have a very strong reason.
      ```

  - u/ShiranaiWakaranai:
    ```
    > Edit: What I really want to know here is: of those of you who are familiar with Taleb's technical approach to decision theory and how he applies this to the real world, is his decision theory 1) Basically correct, 2) Frequently correct but mis-applied sometimes, or 3) basically incorrect?

    I think you have answered this question yourself pretty well. He is basically correct but only in some cases, because he has a strong bias towards not doing new things. For example, when he promotes caution in doing things like genetic modification, that's great. Caution is always good. And he does correctly point out several dangers in scientific research like spurious correlations that are just the result of random chance, which is always important to watch out for. Yet he doesn't promote that same caution for old things, like greenhouse gases and climate change, because burning coal and what not is what we have already been doing for years, its old!

    Which is strange because if he was born about hundred years ago, back before we started burning up all the coal and oil and what not, his old-things-good ideology would almost certainly make him promote the same extreme caution against dumping strange chemicals into the air as he now promotes against dumping strange genes into plants. 

    The bias towards doing whatever old thing we're doing now instead of some new thing doesn't really make sense to me, because whatever old thing we're doing now was a new thing at some point in the (usually quite recent) past, and, by his own argument, the fact that this previously new thing hasn't catastrophically killed us all yet is no proof that it won't do so in the future.
    ```

    - u/LieGroupE8:
      ```
      > Yet he doesn't promote that same caution for old things, like greenhouse gases and climate change, because burning coal and what not is what we have already been doing for years, its old!

      I'm pretty sure he is a strongly in favor of curtailing emissions. Only 100 years of emissions is not enough to make the practice "old" - thousands of years would be better. And emissions have global effect, resulting in a single point of failure. It's not just about being old - one needs to consider the error propagation mechanics and the dynamic time horizon of the given process.

      > The bias towards doing whatever old thing we're doing now instead of some new thing doesn't really make sense to me, because whatever old thing we're doing now was a new thing at some point in the (usually quite recent) past,

      See my second reply to /u/suyjuris above.
      ```

  - u/KilotonDefenestrator:
    ```
    > or the same sorts of reasons, Taleb defends religious traditions and is a practicing Christian, even though he seems to view the existence of God as an irrelevant question. He simply believes in belief as an opaque but valid strategy that has survived the test of time.

    Religion is always bad because it promotes the meme of accepting things without proof, indeed without the *possibility* of proof ^1 . It is a harmful meme that opposes reationality and makes people vulnerable to other comfortable falsehoods. 

    ^1 : I can place a high level of trust scientific results because I understand the steps I need to perform to become a scientist and verify them for myself. For religion there is no possible path to verifying the supposed facts.
    ```

  - u/OutOfNiceUsernames:
    ```
    > He considers himself a defender of science, and calls people out for non-rigorous statistical thinking [...] He defends religion, tradition, and folk wisdom on the basis of statistical validity and asymmetric payoffs. [...] 

    -

    > [the Quora post](https://www.quora.com/What-did-Taleb-mean-by-his-criticism-of-Richard-Dawkins-in-Talebs-Reddit-Ask-Me-Anything-Q-A)

    -

    >What I would like is for the rationalist community to spend some serious time considering what Taleb has to say, and either integrating his techniques into their practices or giving a technical explanation of why they are wrong. 

    Wouldn’t this mean that any analysis or criticism regarding his views would have to come from people who have proven to understand statistics — and mathematics in general — without having strayed off into /r/badmathematics/ territory? And the arguments themselves would have to be based on stat\math related concepts, so essentially they’d be made by and for people who know their math?

    And if that’s the case, then I guess the ending request in your comment should also be to *first* prove that the commenter knows their math or go learn it (“BRB!”) and only *afterwards* make their opinions known regarding this mr. Taleb’s stances, in this discussion tree (or any future ones related to it).
    ```

- u/AmeteurOpinions:
  ```
  I've recently read a number of articles/posts/stuff which proclaim a general despair of the "culture war", "social media", "mainstream media", etc. One thing which can be agreed on, is that this problem is created an enabled by modern communications technology, whether you consider that the Internet, TV, radio, or printing press. 

  For the sake of assume this *is* is a technological problem (as opposed to an alien brain parasite), and that there *is* a technological solution to said problem. What does this solution look like? I suppose mass wireheading would solve it, but that's the most brute-force approach. Actually, no, the most brute-force solution is planetary extinction via de-orbited celestial body. We should try to come up with a somewhat less harmful solution. Our victory condition is a sufficient reduction in perceived negativity that people don't feel compelled to blog about public negativity.

  A few ideas to get started:

  You could ban media which exceeds some arbitrary limit of negativity. This would require control of media to enforce said ban, so that's out. 

  You could genetically modify people to be happier (CRISPR?) bit that would take multiple generations to achieve the necessary scale.

  You could create Social_Media_But_Better which has active or passive countermeasures against increasing negativity. More feasibly, invent such tech and get an existing media company to buy and integrate it.
  ```

  - u/alexanderwales:
    ```
    I would say it's more enabled than created by the media. There's a reason that clickbait exists, and it's that human brains are primed for it. Same with "if it bleeds, it leads", which has been a guiding principle of yellow journalism for a long time. Changing the human brain is (mostly) right out, unless you're a world-class writer/thinker who can sway people away from negativity, or you want to muck around in gray matter, which isn't technically feasible.

    Large companies like Facebook, Reddit, and Google are entirely capable of doing sentiment analysis and directing people away from the things that make them angry, upset, sad, etc. They actually do this, to a limited extent, but there are some rightful (and wrongful) free speech and bias concerns. It's more difficult to figure out which things make people angry/upset/sad for the right reasons, whatever those are, and to steer them away from things like righteous indignation or political action, but it's probably doable. If people knew (or found out) you would have to worry about evasion, which would be a whole problem by itself. I don't think it's really the right way to go because of the pushback it would get.

    A better method is probably just a change in the cultural zeitgeist so that people focus themselves on spreading positivity and warmth in the world, but I sort of doubt that's going to happen unless it can gain some countercultural traction. You see it a little bit in the "wholesome" subreddits, I guess.
    ```

  - u/Gurkenglas:
    ```
    For the last one: https://www.youtube.com/watch?v=rE3j_RHkqJc makes me think that the emotion that spreads a meme could be identified by graph analysis. Tagging each post with a corresponding icon would let people actively choose what emotions to spend their time on.
    ```

  - u/eternal-potato:
    ```
    What is the problem exactly? Being upset by negative news?
    ```

    - u/alexanderwales:
      ```
      No, it's being sad/angry/upset by overexposure to negative news out of proportion to how much that negative news actually impacts your life and/or reflects statistical reality. For a non-current example, things like satanic panic or "super-predators" are extreme cases. Mostly it's about people walking around being sad or angry or afraid because the multimedia landscape, and to a greater or lesser extent, societal forces, have incentives to make them that way.
      ```

    - u/AmeteurOpinions:
      ```
      I think the most general case is "advanced communications technology discourages cooperation instead of encouraging it."
      ```

- u/None:
  ```
  Advice and guides for overlearning academic material?  I want to be able to go back to coursework and get consistent A's rather than even a single B, without having taken the course previously.
  ```

  - u/LieGroupE8:
    ```
    Take the course "Learning How to Learn" on coursera. [Here](https://www.reddit.com/r/GetMotivated/comments/5950tm/text_i_just_finished_the_online_coursera_course/?st=j44qoq5x&sh=7280341a) is a link to a reddit post summarizing the content.
    ```

  - u/MagicWeasel:
    ```
    I use anki flash cards for all my classes, after each lecture I make them up. I get mixed results: very good recall for multiple choice questions (like, I can almost get the slides word-for-word), but writing long, detailed answers is a lot harder as rote memorisation doesn't help with synthesis. 

    However, there are some things it's perfect for: doctors who have to learn the names of all the bones in the hand and things like that.
    ```

    - u/None:
      ```
      Rote memorization is probably good for symbol sequences as well, so thank you!
      ```

      - u/MagicWeasel:
        ```
        Anki is very icebergian. It seems like a simple enough program but there are extensions, shared decks, etc all over the place.

        For example, as well as my studies and french vocab decks, I have decks for the nations of the world (identifying them on a map, identifying their flag, their capitals). It's pretty useful/useless knowledge. 

        Plus it's great on a quiz night: "What do Zambia, Kazakhstan, Papua New Guinea and Moldova have in common?" (they all have birds on their flags)
        ```

  - u/TimTravel:
    ```
    I'm not sure what you mean by overlearning but I've found that a logarithmic rehearsal schedule is effective for memorization. As a heuristic, if you have flash cards, instead of moving the card to the end after rehearsing it, move it back a number of times proportional to how confident you are that you'll remember it next time.
    ```

- u/_o_O_o_O_o_:
  ```
  I recently came across the concept of [Chekhov's gun](https://en.wikipedia.org/wiki/Chekhov%27s_gun). It's an old idea but this time when I read about it, it really appealed to me.
  ```

  - u/alexanderwales:
    ```
    What I find really interesting is that there's some counter play with the audience. The author doesn't introduce a gun in the first act unless it will be fired in the third act, but since the audience knows that then the gun firing in the third act becomes less unexpected/thrilling. So authors are in a way encouraged to leave unfired guns and red herrings laying around, but that undercuts the tightness of the plot.
    ```

    - u/InfernoVulpix:
      ```
      I've observed myself noticing Chekhov's guns before, and then almost entirely forgetting them soon afterwards as I follow the rest of the story.  The true value of a Chekhov's gun is in how easy it is to, when the plot moves to another scene, let the gun slip to the level of remembered factoid, at which point the use of it in act 3 not only comes by surprise just as if it came out of nowhere, but has *bonus* thrill due to the connection to the first act.

      Intellectually, the reader can review what's happened and conclude the gun's going to be used, but when you're immersed in the story it's really hard to keep that in mind in the moment as you approach where it's used, so it works out just fine.
      ```

    - u/mg115ca:
      ```
      If you want to talk audience counterplay, there's always [Schrodinger's Gun](http://tvtropes.org/pmwiki/pmwiki.php/Main/SchrodingersGun). It's mainly used in tabletop games, but long form TV series use it as well. When The Master was killed and cremated on Doctor Who, there was a shot of someone reaching in and grabbing his ring from the ashes. Russell T Davies didn't even know who that person was going to end up having been, he just wanted to leave a hanging plot thread for later use.
      ```

    - u/neshalchanderman:
      ```
      >So authors are in a way encouraged to leave unfired guns and red herrings laying around, but that undercuts the tightness of the plot.

      These two (red herrings, unfired guns) differ. By way of example Percy skulking around in Harry Potter and the Chamber of Secrets is a red herring but not an unfired Chekov's Gun. Act 3 finds the gun fired, the snag sewn: he has a girlfriend. 


      Things that draw our attention, but are not part of the main plot, can be either distractors that go nowhere and mean nothing (unfired guns), or part of some other story strand (red herrings). Red herrings need not undercut the tightness of the plot. Side stories may add to the main narrative by imparting context and nuance.


      Both can generate surprise, an unsureness as to how the story will unravel, but be careful not to overwhelm your reader with detail.
      ```

  - u/Terkala:
    ```
    I recently read a book that featured the concept heavily. Steelheart by Brandon Sanderson. From the beginning it makes clear that every small element of the first chapter will be pivotal in the final chapter. And it features a few explicit gun forms of checkhovs gun narrative elements throughout the story.
    ```

---

