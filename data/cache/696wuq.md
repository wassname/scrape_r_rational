## [Discussion] Lord Asriel and the Methods of Rationality

### Post:

I've always had very mixed feelings about Philip Pullman's His Dark Materials trilogy, a sequel to which is coming out soon. On the one hand, it's basically the bible of Secular Humanism and it's very well written, but on the other hand it stabs directly at the differences between the rationalist and secular humanist community. The story is atheistic and anti religion but also decidedly anti-rationalist.

Lord Asriel in the story is a good rendition of a flawed rationalist hero, to the extent that I wonder if Philip Pullman deliberately wanted to deconstruct the idea. His plan is to build a republic of heaven, defeat the authority, merge magic and technology and have everyone live forever. If LAMOR were ever to exist, Asriel would only need some minor tweaking to basically become the adult Harry Potter from the HPMOR sequel, Significant Digits. Yet Asriel is constantly presented as hubrisic and even a little evil for wanting these things, and his ambition causes his downfall.

I can't have been the only person, reading those novels, to be angry at the authorial choices, when after defeating the forces of Heaven, Lord Asriel dies and all his plans come to nothing. All the souls are liberated from hell, only to evaporate into nothingness instead of trying to get on with living as free people.


From a rationalist point of view, the His Dark Materials trilogy ends with a nasty pro-death message, and the victory against the authority is nothing compared to the continued volume of avoidable death and suffering the story perpetuates. More than anything, these books made me reluctant to self-identify as a humanist precisely because they highlighted so sharply how pro-death so much of the humanist movement is. [(I'm not exaggerating this problem, watch at your own risk.)](https://youtu.be/pR7e0fmfXGw) Seeing this attitude in a book so beloved of sceptics and atheists was confusing.

If any story needed the rational treatment, His Dark Materials does.

**TL;DR - His Dark Materials is humanist but strongly anti-rationalist, because of its pro-death message. Reading it made me realise the problems with secular humanism and is part of what led me here.**

### Comments:

- u/None:
  ```
  [deleted]
  ```

  - u/ZeroNihilist:
    ```
    I'd like to know how long a life is too long. They don't have to be specific about it, I'd be fine with a ballpark figure.

    But how long is too long? 100 years? 200? 1,000? 10,000? 1,000,000? Can these pro-death people pick any finite time and say, "Yep, this is definitely far too long to live."?

    When I was younger, I couldn't imagine being old. Mind you, I only recently turned 28, but 10 years ago the idea of being 100 and still wanting to live flummoxed me. Why would anyone want to live when they were old and frail and needed help toileting, eating, etc.?

    Well, turns out that when you're in that situation you don't suddenly start wanting to die (though untreated depression is distressingly common in the elderly). And that's with our current quality of aged life. Imagine if you didn't lose mobility, dexterity, and cognition as you aged. Would a fit and healthy 200 year old beg for death? What if they were 1,000, 10,000, or 1,000,000? When would such a person decide they wanted to die?

    And what if they never made that decision, if they just went on and on until the heat death of the universe, or even beyond? Would that somehow be a terrible thing? I've never seen a good argument that it would be.

    Humanists who oppose immortality annoy me so much, because they're *so close* to the revelation. Ask them if they're happy about any specific unwilling death. With such an example, figure out what their problem is.

    Was the person "evil"? Well, what if that person was rehabilitated?

    Were they in pain or otherwise hurting? What if they were treated?

    Were they significantly impaired, unable to do the things they wanted to do? What if we fixed their bodies/minds, or provided alternate means?

    Point is, eventually you'll figure out that their criterion for the acceptability of a death isn't really related to age. It may be things that are related to age (like infirmity and senility), but age on its own is scarcely relevant.

    But they don't seem to make that leap themselves, to tease apart their feelings about age from their feelings about death. That frustrates me, especially when otherwise moral people seem to be totally okay with somebody dying.
    ```

    - u/thrawnca:
      ```
      It makes no sense to me to wish for oblivion/nonexistence. If that's what you expect after death, then death is unquestionably the worst thing that could happen to you.

      Even the Bible is not a fan of death. Right back at the beginning, Adam and Eve are told not to eat the fruit because why? *They'll die if they do*. The most positive thing the book says about death is that it has been defeated and is therefore less scary. Indeed, Paul indicated that if it weren't for the hope of resurrection - in other words, eternal life with perfect health, as you say - then Christians would be the most miserable people on earth.
      ```

      - u/THEHYPERBOLOID:
        ```
        "The last enemy to be destroyed is death." - 1 Corinthians 15:26
        ```

      - u/ZeroNihilist:
        ```
        I think it only really makes sense when you predict that the future is expected to hold net negative utility for you.

        E.g. if the "fire and brimstone" hell was real and you ended up in it, any satisfaction you derive from life would probably be heavily outweighed by all the torment.

        In a more practical example, suicide makes sense as a desire for people who are depressed. That's not to say it's a good idea; depression is frequently treatable, and if you haven't given treatment a genuine attempt because you believe it won't work, any prediction about the future is unfounded.

        So while I'd say suicidal ideation is almost always a poor solution, the error of logic is the lack of other attempted solutions, not the desire to die in the face of an expected lifetime of agony.
        ```

        - u/696e6372656469626c65:
          ```
          I'd argue that when available, cryonic suspension (basically reversible death) is strictly preferable to death no matter what, since you get all the benefits that death would provide, plus the possibility of revival later on.

          Of course, there are some situations in which this might conceivably lead to a worse outcome, such as the "hell" scenario where your torturers revive you in order to inflict more pain on you--but in such cases you're unlikely to have the option of cryonic suspension available in the first place. Hence, the "when available" qualifier I placed at the beginning of this comment. (Not to mention the fact that outright death probably *also* won't be available to you as an option in such situations, making the point a mostly moot one.)
          ```

    - u/Frommerman:
      ```
      In our current technological state, deathist humanism is pragmatic. While being in favor of the concept of death is obviously wrong, right now it is unavoidable.

      And there are many people for whom death is the best option available. Those for whom all remaining life will be unutterable agony should not be forced to experience that if they don't want to.
      ```

      - u/Sarkavonsy:
        ```
        Well, there is cryonics. But i suppose that isn't actually possible for most people, not even considering how many people could do it but won't.

        Actually, has anyone ever done or seen an estimate of how many people we could hypothetically put through cryonics given a 100% willing humanity? Could we theoretically save everyone in that scenario?
        ```

        - u/Frommerman:
          ```
          I doubt that's possible just from a logistical perspective. Cryonics on death requires several highly trained people to perform specific actions within less than a minute to work properly, and those people have to be already there when you die. This means they are either waiting for you to die, or you are chosing euthanasia. There isn't really a way to get enough people trained to perform the procedures for absolutely every person dying even in a nursing home or hospital, much less in accidents or at home.

          Barring some insane-to-expect cultural, economic, and technological advances, we can't realistically attempt to save everyone this way.
          ```

          - u/BoilingLeadBath:
            ```
            You underestimate the power of a cultural shift. I think it would be sufficient.

            "Several highly trained people" and "it's quick" means that cryonics is about as hard to implement as the most minor of surgical procedures that require anesthesia, or comparable to the level of medical care that people receive *in the ambulance on the way to the hospital*. Medical teams in the OR and ER learn many such procedures, and, if much of our entire society where deeply interested in cryonics (such that, when someone died in a hospital and wasn't frozen, their relatives would write an angry op-ed and the papers would publish it - a sequence of events that might happen at the 5-10% sign-up level) they'd learn that one too.

            With 5-10% of the population signed up with the idea that cryonics is worth trying, you get near-universal *availability* of the procedure over the entire west and much of China and India. At 70%, it has become the dominant belief about death, causality, and the afterlife in  those regions of sub-sahara Africa currently torn apart by religious struggles, and they start trying to save people with automotive antifreeze, old bike tires, and $5 dewars distributed by charity groups.
            ```

            - u/Frommerman:
              ```
              >the level of medical care that people receive *on the ambulance on the way to the hospital.*

              Heh, no. I'm an EMT, and the procedure required for proper vitrifaction of a human brain is *far* more involved and requires far more expensive equipment than currently exists in the EMS sphere. Cycling all the blood out of the body and replacing it with antifreeze requires a dialysis machine, and even with current technology those things are large, bulky, and very finicky to move and operate safely. In addition, the trucks which actually transport a prepared body to be stored in liquid nitrogen are equipped almost nothing at all like ambulances.

              Also, the fact of the matter is that human bodies take up a ton of space. You know how the catacombs of Paris are a labyrinthine boneyard? Imagine that but every body is fully intact and must be kept under liquid nitrogen at all times, and the nitrogen must be refilled at a constant rate that increases for each additional bit of volume the crypt comprises. It would be a *gargantuan* engineering challenge to create enough space to store even ten people per minute worldwide. That's over 5.25 *million* bodies per year, and that's only saving 10% of people.

              And all of this is ignoring the fact that *we have no idea if this will even work.* Which tends to discourage people from spending thousands of dollars on life insurance to hand over to a cryogenic freezing charity.
              ```

              - u/BoilingLeadBath:
                ```
                Sorry for the poor assumption Re: Cryonics and EMS. It is not offered in my area, so I had not read about the implementation details...

                Which is just as well. I sincerely doubt that contemporary cryonics is using very efficient tools. For instance, it would seem that for *replacing* blood rather than filtering it, a dialysis machine is the wrong tool, and if we had a market for a million of these operations per year, you'd get a simpler and cheaper purpose-built tool. Perhaps an open loop injection machine.

                As for the cryocrypt: tunnels are for transit; tanks are for storage. 52 million heads fit in 30 or so medium sized (IE, 20m tall x 20m dia) insulated tanks, kept cold with maybe 50 megawatts of electricity. An unusual chemical installation, to be sure, being focused on storage rather than production... but probably less that a billion dollars to construct. Call it 5 billion to fund future maintenance and... well, it's still a tiny amount of money.
                ```

                - u/Frommerman:
                  ```
                  Yeah, that's a billion dollars to store every head of every dead person...for one year. Then you need another billion dollars to store next year's heads, and another plot of land to drop them on, and, and...

                  And the reason you need specifically a dialysis machine is perfusion. Draining all blood from the major arteries and veins is pretty easy: cut someone's jugular and their own heart will do a significant fraction of the work for you. But we aren't trying to do that. We're trying to preserve the most complicated neural networking computer currently known to exist, and in order to do that, the antifreeze absolutely *must* go everywhere. Every vein and artery, every microscopic capillary, every single fragment of the maze that is the circulatory system must be flushed, if you fail the patient gets numerous clots in the brain, which at the very least would significantly cut down on the number of options for revival, and at worst could make it impossible if they didn't know about those clots as the attempt proceeded. The only way to do this is slowly and carefully. Additional pressure just ruptures the capillaries and gives your patient countless tiny aneyurisms, you can't accelerate the process. Dialysis machines clean all of the blood, and therefore they are designed to carefully pull blood out and put it back in at exactly the pressures that the body's systems can withstand, no more and no less.

                  So unfortunately, there's no way to simplify the process.

                  By the way, would you happen to be an engineer? 

                  Edit: Actually, I'm remembering this wrong. It's not a dialysis machine you need, it's a heart - lung bypass machine. Some *hospitals* don't have those, mostly smaller rural ones who expect to air evac anyone who might need them. A heart - lung bypass machine is the one which can perfuse fluids through the entire circulatory system without a functioning heart. These machines are...not really simplifiable. You could remove the 'lung' part, but the heart part is the toughest bit.
                  ```

                  - u/EliezerYudkowsky:
                    ```
                    Uh, not to sound insensitive or anything, but we currently spend way way more than that on torturing people horribly in the last year of their lives instead of giving them a quiet, graceful, hopeful departure to the future with their senses and dignity intact.
                    ```

                    - u/Frommerman:
                      ```
                      This is absolutely true, and I see way too much of that at work. However, that is for the most part by choice. People scrabble for every chance at survival no matter what, no matter the pain, not even considering for an instant that death might be preferable. Maybe because they know that they *don't want to die*, no matter their protestations that immortality would be terrible.

                      You are completely correct that everyone wants to be immortal and just won't admit it to themselves. And I don't think we should deny people that. Usually there is no hope of course, we overtreat the stage four cancer or amputate the last limb of the diabetic, but that doesn't mean people should be forced into a position where there's no hope at all. I do think choice is valuable.

                      And sometimes, people choose pain before death. I can't begrudge them that.
                      ```

          - u/Sarkavonsy:
            ```
            Right, that's about what I expected. *googles* yep, global rate is about a hundred deaths per minute. Totally outside the current realm of possibility.
            ```

    - u/Daneels_Soul:
      ```
      > But how long is too long? 100 years? 200? 1,000? 10,000? 1,000,000? Can these pro-death people pick any finite time and say, "Yep, this is definitely far too long to live."?

      2^{# ^of ^neurons ^in ^the ^human ^brain}.
      ```

      - u/ben_oni:
        ```
        Missing the point. What if the mind were expanded? Cybernetic implants added; consciousness uploaded, etc.

        Yes, a human brain can only cycle through so many states, but immortality discussions tend to almost instantly become trans-human immortality discussions.
        ```

      - u/None:
        ```
        What's the significance of that figure? Each neuron has far more than two possible states, after all, given its complexity...
        ```

        - u/xamueljones:
          ```
          Actually that figure indicates the number of connections possible between the neurons in the brain. So 2^{# ^of ^neurons ^in ^the ^brain} indicates the number of every possible brain that can ever exist which is a vastly larger space than the space of all possible human minds in every possible environment.

          Just to make it clear how that figure was reached, let's say that you have 2 neurons, then there is only two ways the neuron can connect to each other (there is a connection or no connection). Add one for 3 neurons, then there are 8 different connections (8 different groups with unique ways to connect or not connect the neurons). Continuing the pattern leads to 4 neurons-16 connections, 5 neurons-32 connections. According to this pattern, the group of possible brain states apparently doubles for every neuron added.

          EDIT: I made a mathematical mistake earlier with the numbers and did a little editing.
          ```

          - u/electrace:
            ```
            But why would that number be "years" rather than "milliseconds" or "millenia?"
            ```

            - u/Daneels_Soul:
              ```
              It almost doesn't matter. The difference between milliseconds and millennia amount to a difference of about 25 neurons in the exponent.
              ```

    - u/ben_oni:
      ```
      Beyond the heat death of the universe?

      Gotta love how these discussions always hit the point where godlike powers are invoked. This is also missing the point.

      If a being has the power of a god, then I would hope that immortality is okay. Presumably, they also have the wisdom to use it (and all the accompanying powers) responsibly. And if they don't, our little discussion isn't likely to matter to them.

      On the other hand, lacking the powers of a god, mortality is assured. The "pro-death" crowd, as you call them, generally aren't arguing that people should die, or that old people need to die; rather, it's a belief that we should come to accept death when the inevitability is upon us rather than rage against it. This should never be a value judgement for other people, but a way to assess our own morals.

      The fundamental question is how far someone should go to extend their existence?

      And for bonus points, in what circumstances should we violate the "Do Not Resuscitate" wishes of patients?
      ```

      - u/ZeroNihilist:
        ```
        > Beyond the heat death of the universe?

        > Gotta love how these discussions always hit the point where godlike powers are invoked.

        I'm not presupposing the existence of such an outcome. I included the possibility for completeness.

        > The "pro-death" crowd, as you call them, generally aren't arguing that people should die, or that old people need to die;

        In the linked video, Stephen Fry said that an infinite life would have no meaning. Assuming he thinks that meaning is a good thing for a life to have, he would appear to be arguing that death is good as a concept.

        > rather, it's a belief that we should come to accept death when the inevitability is upon us rather than rage against it.

        I agree. That doesn't mean that we shouldn't take steps to delay that stage for as many people as possible. If we could push that date back to hundreds or thousands of years, should we then complain that humans are simply living too long?

        > The fundamental question is how far someone should go to extend their existence?

        As far as they can without impeding on the ability of others to do the same, or until the point at which they do not wish to go on. This is what humans currently do, and have been doing for a hundred thousand years, only we've been limited to decades for most of that time.

        > And for bonus points, in what circumstances should we violate the "Do Not Resuscitate" wishes of patients?

        When those wishes were not made in sound mind. What would qualify for that situation is a question for psychologists and lawyers, but in general if somebody has an untreated mental illness or is being coerced, the DNR shouldn't be accepted.

        The issue gets murkier if somebody has a mental illness and has been offered and informed of treatment, but declined. Even if you accept their refusal of treatment, you may not want to grant their request for euthanasia. Again, this is probably a question for the experts.
        ```

    - u/thepublicinternet:
      ```
      I think I have a very precise answer for you!

      "Eternal life" gets either boring or disturbing when it passes the functional Busy Beaver number for universe simulations containing everything in our Hubble Sphere. There exists an amount of time where the universe itself must end or enter a loop (probably a boring loop like everything being so far from everything else there is no meaningful interaction). I'm not sure if that is a close enough ballpark for you, but, while the busy beaver number is incomputable in general, it is computable for specific numbers, one of which I've supplied (essentially). Or (for those of you more interested in complex loops) when all usable energy I could possibly interact with has been exhausted and there is some maximal entropy state for my hubble sphere. Since that is likely the longest we could get this machine to go, it is assuredly a boring fate. 

      Certainly at that point I, as a constituent element of the universe, would not have any particular time preference -- more time would by definition go unnoticed by me (as the noticing part of my brain would be in a loop as well), and I cannot fathom how I would prefer an unchanged and unnoticed loop over an end.
      ```

  - u/KilotonDefenestrator:
    ```
    > I really hate the comparison about the book that would be boring if it went on and on.

    It's an absurd argument.      

    What if you could only read 80 books in your life? Would it bring more joy because there is a finite number, or less joy because you miss out on so many good books?
    ```

- u/TheUtilitaria:
  ```
  It's common knowledge that His Dark Materials is meant to be a deconstruction of Narnia, but I almost see Unsong as like a deconstruction of His Dark Materials - God really is good after all.
  ```

  - u/Sailor_Vulcan:
    ```
    Well kinda. It's complicated. I mean there's still Thamiel and stuff. Then again it might just be that god's good side is not very smart when he crosses the Panama Canal. Since he turns into a dog. Then again, if Thamiel crosses the Panama Canal what would happen?
    ```

    - u/RMcD94:
      ```
      He would at least have liveD.
      ```

    - u/None:
      ```
      Actually, it was Nemo who turned into an omen, not God into dog.
      ```

      - u/XerxesPraelor:
        ```
        Or that was a red herring. Or it means that Nemo is God.
        ```

- u/Roxolan:
  ```
  > Yet Asriel is constantly presented as hubrisic and even a little evil for wanting these things

  I mean, he does torture-murder a kid [to power his machine](https://www.youtube.com/watch?v=D8aBP-JOZsU). And yes, blah blah utilitarianism greater good blah, but [he wasn't too bothered](http://econlog.econlib.org/archives/2016/01/the_invisible_t.html) by the whole affair.
  ```

- u/Amonwilde:
  ```
  In defense of HDM...just because death is bad doesn't mean that someone can't be considered hubristic or even evil for their own particular, flawed implementation. While immortality is a goal and a prize, that doesn't mean that any steps taken toward that goal are justified. The difficulty of implementation and high stakes also mean that more corners are likely to be cut in some kind of direct A to B search for immortality than there would in some large but better scoped goal, such as implementing universal health care.
  ```

- u/The_Magus_199:
  ```
  Honestly the thing that always bothered me about His Dark Materials is that it's *so* much the anti-Narnia that it kind of feels like it falls into the same traps to me? Like, it's definitely nowhere near as bad as Narnia's shouting "THE LION IS JESUS" in your ear every five seconds, but it still kind of feels to me like it's more allegorical than it needs to be to tell a good story. :/
  ```

- u/Lexabyte:
  ```
  The beauty of seeing the world in materialistic terms is that all bad things, all things that plague humanity suddenly become solvable problems. 
  The video was built out of standard arguments against living forever. It's the worst kind of contrarianism, the religious folk believe that living forever in an afterlife is great so we have to argue against that.
  ```

- u/DaystarEld:
  ```
  Yeah, it's definitely a problem with HDM that I only really noticed after I started my journey toward Transhumanism. 

  Rational!Golden Compass would be really interesting, to me, and I'd love to see Lyra as a rationalist, where Asriel and Coulter represent two potential failure modes of rationality and the anti-death value is included as well as the anti-God one. But the biggest hurdle is the titular Golden Compass itself... when you have a device (and by extension, sentient all-knowing magic particles) that can answer any question you pose to them, a rational protagonist has the potential for explosive munchkinry unless some major parts of the lore/world are altered from canon.
  ```

  - u/PeridexisErrant:
    ```
    > when you have a device (and by extension, sentient all-knowing magic particles) that can answer any question you pose to them, a rational protagonist has the potential for explosive munchkinry unless some major parts of the lore/world are altered from canon.

    Canonically, you can only use the alethiometer instinctively if you are 'innocent', ie childlike and unlikely to use it for this kind of thing.

    Adults need substantial training and careful interpretation, but they're fantastically valuable artefacts for good reason!
    ```

    - u/ardetor:
      ```
      Are... are you the same PeridexisErrant that made the DF starter pack? If you are, then I really need to thank you for that because it really lowered the barrier to entry for me.

      Sorry for off topic, I'll go back to lurking now.
      ```

      - u/PeridexisErrant:
        ```
        That's me - I'm glad it helped :D
        ```

    - u/DaystarEld:
      ```
      Hmm. I remember her losing the ability to use it at the end of the series, but you think r!Lyra wouldn't be able to use it instinctively from the beginning?

      Also I forget, was it ever cleared up whether ANY child could use it that way, and everyone was just ignorant of that until Lyra picked one up, or if it was something special about her as well as the fact that she was a child?
      ```

      - u/bassicallyboss:
        ```
        >you think r!Lyra wouldn't be able to use it instinctively from the beginning?

        I think it depends on how rational Lyra is, and why.  Rationality, if one is fully committed, has a certain ruthlessness about it that seems fundamentally incompatible with innocence.  It's hard to imagine, for example, anyone "childlike" [deliberately provoking a crisis of faith](http://lesswrong.com/lw/ur/crisis_of_faith/) just to make sure they held empirically true beliefs.  Then again, it's not too hard to imagine a precocious child who thinks they know everything about rationality, but who hasn't ever been in a situation where they needed to apply it.

        It would be very moving, I think, if Lyra had a shock (maybe the death of the boy at the end of the first book) that required her to apply all her rationalist art, and thereby lost the ability to read the alethiometer much earlier than in the source material.  Or, maybe even better, if the alethiometer itself told her she had to use rational thinking to prevent some bad thing, and as a result she lost access to its answers.  It's narrative necessity, I think, that rational!Lyra's childhood end suddenly and traumatically.  Which is a real shame when you consider how nicely and gently source Lyra's did.
        ```

      - u/None:
        ```
        I honestly don't know. Supposedly it was the onset of puberty that removed Lyra's ability to intuitively use the alethiometer. She was a very intelligent, cunning girl even in canon, so it's not like her ability came from naivety or innocence.

        Also, I remember there being an Effulgence arc set in the HDM-universe, so if you haven't, maybe look at that as something relevant?
        ```

- u/trekie140:
  ```
  I'm surprised u/DayStarEld hasn't given his thoughts on this yet.
  ```

  - u/DaystarEld:
    ```
    Whelp, that's what I get for avoiding Reddit most of the day.
    ```

- u/TimTravel:
  ```
  I always disliked how Mrs. Coulter was allegedly redeemed by the third book. She would have been more interesting as a psychopathic ally.
  ```

- u/None:
  ```
  that sequel has been coming out soon since 2006
  ```

- u/ben_oni:
  ```
  I'm confused. What part of rationalism is "anti-death"?

  Also, "well-written"? You've got to be joking. I found the author-tract to be incredibly annoying, among it's many other flaws.
  ```

  - u/Sailor_Vulcan:
    ```
    Rationalism is a *meta*epistemology. Transhumanism isn't a part of rationalism per se, rationalists just happen to usually be transhumanist because transhumanism is right. If death was a good thing and living longer could only make us suffer, then there would be more rationalists who weren't transhumanists.

    Just like how if biological evolution was a hoax, there wouldn't be so many scientists who believe it's real.
    ```

    - u/traverseda:
      ```
      And nobody is talking about forcing people to live forever. A lot of rationalists are pro assisted suicide (sometimes with the caveat that they need to seek psychological help, sometimes without).

      I'd characterize us a generally pro-choice.
      ```

      - u/KilotonDefenestrator:
        ```
        > I'd characterize us a generally pro-choice.

        Death is always an option. We think life should be too.
        ```

    - u/trekie140:
      ```
      I'm not convinced that a rationalists necessarily have to be a transhumanist. I am a transhumanist, but I see that as a rational implication of my values rather than my heuristics. Transhumanism is a logical consequence of utilitarian humanism, but I don't know of any requirement that you must be a utilitarian humanist to be a rationalist. It's a moral philosophy that I believe is good, but I don't think it's fair to say you can't be considered rational if you don't subscribe to it.
      ```

      - u/FeepingCreature:
        ```
        Transhumanism is humanism extrapolated to an absurd conclusion. Singularitarianism and AI alarmism are technological progress extrapolated to an absurd conclusion. Rationality trains people to take ideas seriously. Taking ideas seriously tends to make people transhumanists.
        ```

        - u/LiteralHeadCannon:
          ```
          To be clear, you're not using "absurd conclusion" in its colloquial sense of "false conclusion", right?
          ```

          - u/FeepingCreature:
            ```
            More in the sense of "Beware the Absurdity Heuristic." :)
            ```

  - u/696e6372656469626c65:
    ```
    This is something I find myself saying rather often these days, but:

    You should consider the possibility that your opinion of a particular work is not, in fact, the be-all and end-all in terms of quality judgment.
    ```

  - u/psychothumbs:
    ```
    It's more that the Less Wrong descended rationalist community that HPMOR comes from and this sub was founded by is than anything too intrinsic about people who try to think rationally.
    ```

---

