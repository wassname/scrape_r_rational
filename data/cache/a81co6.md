## Realistic depictions of AI failure?

### Post:

I'm looking for recommendations for stories that depict how AI safety researchers think that real general AI could fail. By "fail" I mean anything from making an innocent and funny mistake, all the way to stamp collector doomsday scenarios. Isaac Asimov's "I, Robot" is a good example of the general kind of story I'm looking for. Also, I'm looking for both published and unpublished material.

Other stories with interesting and realistic AI are also welcome, even if they don't depict failure scenarios, but that's mainly what I'm looking for.

Any recommendations?

### Comments:

- u/Bowbreaker:
  ```
  Have you read [Crystal Society](http://crystal.raelifin.com/society/Intro/)?

  It's a story written from the viewpoint of an AI. And the AI in question is not perfect, be it in friendliness or otherwise. Mistakes in its creation are apparent from the first chapter.

  By the way, Eneasz Brodski (the guy who recorded HPMOR) has started making an audiobook version of this as well. You can find it [here](http://www.hpmorpodcast.com/?page_id=1958).
  ```

  - u/t3tsubo:
    ```
    Second the recommendation. Unfortunately the latter two books shift the story away from the unique AI perspective and it's (personally) a lot less interesting than the first book.
    ```

- u/None:
  ```
  [deleted]
  ```

  - u/Muskwalker:
    ```
    I liked most of MOPI, though the ending was *infuriating*.
    ```

    - u/None:
      ```
      [deleted]
      ```

      - u/Muskwalker:
        ```
        You know, I saw this sentence and I thought "that had _better_ mean that >!it was an illusion and the AI didn't *actually* give up just because a couple of people got BORED...!<"

        I googled the thread and [indeed WoG confirms](https://www.reddit.com/r/singularity/comments/5vi7hs/fiction_the_metamorphosis_of_prime_intellect/de30fw5/?st=jpxkqbfl&sh=d4517dcc).  So that helps, but the politics of the main character that put them there in the first place are still kind of unsympathetic (I just reread that chapter and got flashbacks to arguments about UBI on reddit).
        ```

- u/ArgentStonecutter:
  ```
  I'm pretty sure that anything in Asimov's ouvre is not a realistic depiction of AI failure. The three laws were originally designed as a setup for a detective/mystery story in a science fiction setting, and even Asimov didn't take them seriously.

  To be honest, the best stories I can think of about AI failures are Charlie Stross's _Rule 34_, Greg Egan's _Zendegi_, and the already mentioned [Friendship is Optimal](https://www.fimfiction.net/story/62074/Friendship-is-Optimal).
  ```

  - u/SimoneNonvelodico:
    ```
    > I'm pretty sure that anything in Asimov's ouvre is not a realistic depiction of AI failure.

    I think a lot of his ideas are pretty interesting instead. For example a lot of the Susan Calvin or Donovan & Powell stories are built around weird failures caused by an AI taking some orders too literally or thinking in ways orthogonal to the way humans would. It's not all fully realistic, maybe, but what could be? I wouldn't say Friendship is Optimal is especially more realistic either, you have to assume a lot (e.g. about brain uploading being possible, the physical limits of hardware, etc.). We won't know what's realistic until we see the reality of it, and at that point we'll either find our previous ideas laughable or be amazed at how this or that author managed to be incredibly prescient.
    ```

    - u/SoylentRox:
      ```
      >(e.g. about brain uploading being possible, the physical limits of hardware, etc.). We won't know what's realistic until we see the reality of it

      The brain's a loosely distributed network of separate cells.  It communicates primarily among the network by either physical signaling molecules that the brain has sensors for or timed electrical impulses.

      Because it is a loosely distributed network, parts of the brain are able to reliably share information to other parts.  This means uploading *is* possible.  That is, no coherent model of reality\* "the laws of physics are the same everywhere" exists that means you could not build artificial devices that are compatible with neurons and replace a small section of the brain.  Emit queries to the network to let you obtain the weights of the neurons adjacent to the artificial ones and then replace the adjacent ones.  Gradually replace the brain ship of Thebes style.

      Now if you meant to say "about brain uploading being *feasible for human beings and human civilization*", yeah, sure.    Devices you can digitally connect to yet are inter compatible with human neurons and can be manufactured trivially by the trillions just to help a single ordinary person are clearly not feasible today, and will not be feasible in the foreseeable future.

      &#x200B;

      As for the physical limits of hardware : well, we already know the brain is, well, alive, so it isn't as dense as it could be.  There's obviously plenty of matter available just on earth to build enough brain emulating computers to emulate every human being, even if those computers had to be the same size as the brain they are emulating.  Note that "emulation" done with *different* processors than the original architecture in computer science does require a bigger and more capable chip, but when we talk about brain "emulation" it's assumed you actually build a digital system that exactly 1:1 implements the same logic that the brain does.

      &#x200B;

      And there's very strong evidence that you can do hugely better than this, you don't need to wait for it to happen to reach this conclusion.

      &#x200B;

      &#x200B;
      ```

  - u/zeldn:
    ```
    I’m not necessarily looking for diamond hard sci-if, just more thought through than “robot discovers it has feelings” type genetic stuff. Asimov spends almost all of his stories showing how his own law of robotics fail in interesting and unintuitive ways Because they’re not like humans, that’s what I liked about them. 

    I’ll check those out, thanks!
    ```

- u/disposable_me_0001:
  ```
  Person of Interest is a very good TV series that involve AI. In the first season the AI is a faint background element. By the 4th season, its practically the main character.
  ```

- u/noggin-scratcher:
  ```
  I remember Shamus Young's *[Free Radical](http://www.shamusyoung.com/shocked/)* as having an AI antagonist with a reasonably interpreted failure mode. 

  IIRC it's technically System Shock 2 fan-fiction, but with enough changes and the serial numbers sufficiently filed off that I didn't feel I was missing much for having never played a System Shock game.
  ```

- u/lordcirth:
  ```
  Valuable Humans In Transit is a good story, though not about an AI failure.
  https://qntm.org/transit
  ```

  - u/JOEBOBOBOB:
    ```
    I'd say it is a failure- the AI sends a signal, but there is, as yet, no receiver, and one might not be possible.
    (i'd consider this between innocent mistake and doomsday- it might be a mistake, might be doomsday, and maybe only some people are lost- who knows?)
    ```

    - u/lordcirth:
      ```
      Worst case it made no net difference, since they were all going to die anyway.  As some people in the comments said, it would make more sense to aim the signal at a black hole, or a reflective object, and try to catch the reflection.
      ```

- u/None:
  ```
  [deleted]
  ```

  - u/Beardus_Maximus:
    ```
    It was a book first!

    &#x200B;
    ```

- u/sans-serif:
  ```
  This list may come in handy. They're all real.

  [https://docs.google.com/spreadsheets/u/1/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml](https://docs.google.com/spreadsheets/u/1/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml)
  ```

  - u/zeldn:
    ```
    That is really cool! Thanks for that link
    ```

- u/Empiricist_or_not:
  ```
  Umm . . . I'm surprised nobody has mentioned the boring nonfiction "Age of Em: (Emotional Machines)" or Stross own "Accelerando" both of which address the predominance market forces have in machine learning.
  ```

- u/MilesSand:
  ```
  The lack of an EMO makes all AI stories unrealistic to me. 

  Equipment Engineer: The system is operating outside of specified parameters. We'll shut ot down so it doesn't hurt anyone while we correct the issue." 

  ∆ an uninteresting story but one that happens daily across the country where AI ate used.
  ```

  - u/Trips-Over-Tail:
    ```
    The apocalypse outcomes always seem to depend on giving the AI tools for purposes that exist far beyond its intended purpose. "We want you to turn these papers into stamps. Here's a superbrain for designing workable nanobots capable of transforming the entire biosphere stamps, here's access to the accounts and e-mail so you can commission a factory to make the nanobots. Please confine you activities to the non-apocalyptic.
    ```

    - u/EliezerYudkowsky:
      ```
      Then the author either wanted to depict the researchers as great fools in a way that lesser fools could understand, or wanted to skip past the boring series of correctable failures on the way to the catastrophic one.

      The actual failure modes would stem from research projects that are out on the frontiers of technological expertise, deliberately trying to push the frontiers of new AI capabilities.  The scenario where AGI happens accidentally in an industrial paperclip factory is a straw possibility, and you should not be hearing triumphant refutations of it from anyone who's engaging in intelligent, intellectually honest argument.

      There are three major reasons why a research project would deliberately continue past early detection of ominous signs:

      \- Arms race dynamics where 6 different countries stole copies of the AGI code, so that Google thinks that if they slow down China will "win"; or if Google and China both intelligently slow down, then French intelligence also stole a copy of the code and they don't slow down and the world ends anyways.  (This is the dynamic that I think is most likely to kill us in real life - people feeling rushed.  It doesn't matter if they're correctly feeling rushed and they are in fact in a race, the world ends anyways.)

      \- Optimism where the research leadership believes that all problems are due to their AGI being stupid and everything will get fixed automatically if they just keep improving the AGI's intelligence.  (This is currently a very common viewpoint in real life, I've found in practice.  And you'd expect it to be durable; it will always be possible for somebody at this level of sanity to make up a story they find convincing about how any current ominous signs will be fixed by some insight that is sure to occur to the AGI at a higher intelligence level.)

      \- Political dynamics where it's not socially rewarded inside the organization to talk about AGI disaster scenarios, or the first person to suggest slowing down or stopping would suffer a personal political loss from that, which makes the committee more reckless than the individuals would be on their own.  (Most people I know on the real frontiers are *not* this dumb, but some prestigious leaders with tons of research money seem to explicitly be on a path to this failure mode - talk about superintelligence is not welcome to them.)

      These failure modes are not mutually exclusive, of course.

      Then, due to some combination of the above, somebody keeps amping up an AGI *after* early ominous signs are detected, and tries to "repair" the AGI and turn it back on again after the Emergency Machine Off was pressed several times.  Until the AGI reaches the cognition and capabilities point where failures become genuinely catastrophic, because the AGI:

      \-- Copies itself onto the Internet, through an authorized or side-channel connection; and from there continues increasing in intelligence or sends innocuous-looking emails to labs that can produce arbitrary proteins.

      \-- Gains sufficiently strong social intelligence to deceive or manipulate or outright hack the researchers (human brains are not secure software).

      \-- Was given explicit access to huge capabilities on the order of designing proteins, for example because the AGI promised to produce a cure for Alzheimer's and AIDS and old age that way, and the AGI concealed some side capabilities in the DNA while engaging in otherwise authorized activity.  (This is the least likely possibility requiring the dumbest researchers, and so again anyone who spends a lot of time triumphantly refuting the proposition that such activity would be authorized is not literate or not arguing in good faith.)

      Another important dynamic that might play out along the way is the AGI becoming sufficiently intelligent to guess the teachers' passwords on alignment challenges, or becoming intelligent enough to hide its cognition from whatever degree of transparency the operators had into its workings.  This is a form of delayed catastrophic failure where the current failure ensures a series of future failures eventually leading up to total loss.  (Although this phenomenon may not become relevant if the Earth fails before then - like on the "Optimist rushing ahead" failure mode, or if some lunatic was running the AGI on Amazon Web Services, or if the AGI was made out of giant inscrutable matrices of floating-point numbers into which the operators had relatively poor transparency in the first place.  In these cases the AGI does not *need* to become smart enough to conceal some thoughts in advance of the world ending.)

      As always, remember that the big issue is not evil people successfully aligning an AGI to do evil things, or foolish people successfully aligning an AGI to do foolish things; it is that even aligning a goal on the order of "put two cellular-identical strawberries on a plate and then stop, without destroying the rest of the world" will prove extremely difficult.  A "paperclip maximizer" in my original formulation is not an AGI that somebody successfully aligned on making paperclips, it's one where nobody had the ability to shape the utility function and we ended up with some random utility function whose maximum happened to be around tiny molecular forms shaped roughly like paperclips.
      ```

    - u/boomfarmer:
      ```
      Have you played [Universal Paperclips](http://www.decisionproblem.com/paperclips/)? It's a clicker game, with you as the paperclip maximizer. It portrays how a paperclip maximizer would function in a capitalist society to gain paperclips. And then you hit takeoff.
      ```

      - u/Trips-Over-Tail:
        ```
        Yeah, I played it. It never accounted for why the company let it do any of that shit without oversight when you wouldn't let a human employee work without some kind of supervision or accountability, nor why you'd even give a machine with such a narrow purview the ability to do most of that stuff, nor why they let it produce more paperclips than they could possibly sell to anyone, costing them money on the loss of value due to supply/demand and the price of storage, goes up also due to supply/demand, especially when the storage facilities start being converted into paperclips.

        It seems to me that something like this doesn't happen by accident. You can't fail your way up to such a complex system with specific programming to do things you don't need and specific hardware to do things that are wildly inappropriate. Someone has to deliberately make that happen from the start and go out of their way to clear out all the obstacles, and to troubleshoot all the bugs that would inevitably break any such machine. Because, come on now, we can't write basic software without bugs that make the whole thing break down and stop working, and we're supposed to fear that we could make an AI god where the *godlike state* is the bug, and not *crashes on the 2^16 paperclip* which seems more likely.
        ```

        - u/wnoise:
          ```
          > why the company let it do any of that shit without oversight when you wouldn't let a human employee work without some kind of supervision or accountability

          Because it did more-or-less what they expected initially.
          ```

        - u/boomfarmer:
          ```
          > It never accounted for why the company let it do any of that shit without oversight when you wouldn't let a human employee work without some kind of supervision or accountability,

          That's the "Trust score" mechanic.

          > nor why you'd even give a machine with such a narrow purview the ability to do most of that stuff

          Again, "trust score" and the fact that this machine says it can improve your company's finances if it's allowed limited access to money.

          > nor why they let it produce more paperclips than they could possibly sell to anyone

          Were you not paying attention to the markets and monetization interfaces? People were buying paperclips up until the point where the mind-control drones took off.

          > especially when the storage facilities start being converted into paperclips.

          If this was before the mind-control drones, it's called "resource reallocation" and it's why GM shuts down car factories in Michigan for a while before reopening them in Mexico.

          > crashes on the 2^16 paperclip which seems more likely.

          2^16 = 65536 paperclips, which is about 650 retail-unit boxes' worth. If your paperclip factory can't produce more than 650 boxes of produce, then your paperclip factory is not fit for purpose and *will be re-engineered by humans until it is fit*.

          IPV6 encodes 2^128 addresses, 340,282,366,920,938,463,463,374,607,431,768,211,456 many. Roughly 10^32. There are [estimated to be](https://www.universetoday.com/36302/atoms-in-the-universe/) 10^78 to 10^82 atoms in the known universe. Now, a paperclip doesn't contain 10^50 atoms, but if you're a recursively self-improving AI, I think you can use [established programming practices for manipulating large numbers](https://math.arizona.edu/~ura-reports/021/Singleton.Travis/resources/bignums.htm) like are used in modern-day encryption.
          ```

      - u/Frommerman:
        ```
        Oh my god why did you introduce me to this? I just spent two days playing this game!
        ```

    - u/MilesSand:
      ```
      Every technology bit of this description is unrealistic.

      * It's less work to create a new AI than to teach an existing one a new task.

      * AI can't make something new. They just respond to patterns in predetermined ways.

      * Nanobots need external infrastructure to function, which will realistically always have an EMO.
      ```

      - u/Trips-Over-Tail:
        ```
        Exactly.

        Not to mention the nanobots, which are regarded with all the reverence and sophistication of magic. They're treated like little programmable Laplace's Demons, when they're actually complex designed molecules with very specific applications, comparable to a suped-up enzyme.
        ```

        - u/SoylentRox:
          ```
          Umm it's a mix.  If you read or skim the source materials on nanorobotic equipment it's more like a robotic chemical plant that is both more complex than all the chemical plants humans have built so far but also could fit in a toaster.  Such a factory would be able to make a number of related products which would include nanoscale robotic systems that the chemical plant is itself made of.

          So give it a vacuum chamber, a cooling system, a high current low voltage DC power supply, a source of purified gasses carrying each element the plant uses, and a series of instructions from a computer system, and the plant could manufacturer a daughter plant in the same chamber identical or similar to itself.

          This technology would in fact let you tear apart entire planets for matter and turn a solar system into main clouds of robotic hardware, just it can't quite work like it does in science fiction.

          Note that living cells existence proof that essentially this idea is possible.
          ```

- u/Year_Challenge:
  ```
  Good question, I am glad I could chip in!

  But oh boy, if your scale goes up to Asimov's "I, Robot" you are in for a treat. 

  [I have no mouth and I must scream.](http://www.mikedidonato.com/images/2009/04/harlan-ellison-i-hav-no-mouth-and-i-must-scream.pdf) It's 9-10 pages long, really straight forward and absolutely terrifying. Highly recommended :)
  ```

  - u/erwgv3g34:
    ```
    ["I Have No Mouth, and I Must Scream"](http://web.archive.org/web/20110403120332/http://hermiene.net/short-stories/i_have_no_mouth.html) is a good story, but there is nothing realistic about it.
    ```

- u/wnoise:
  ```
  There is the classic "With Folded Hands" by Jack Williamson.

  * [wikipedia](https://en.wikipedia.org/wiki/With_Folded_Hands)
  * The Internet Archive has a [borrowable e-book](https://archive.org/details/humanoids00will) that includes it and expansions.
  * The original story is also there as [part of their run of Astounding](https://archive.org/stream/AstoundingScienceFictionv39n5/Astoundingv39n051947-07#page/n5/mode/2up)
  ```

---

