## [D] Monday General Rationality Thread

### Post:

Welcome to the Monday thread on general rationality topics!  Do you really want to talk about something non-fictional, related to the real world?  Have you:

* Seen something interesting on /r/science?
* Found a new way to get your shit even-more together?
* Figured out how to become immortal?
* Constructed artificial general intelligence?
* Read a neat nonfiction book?
* Munchkined your way into total control of your D&D campaign?


### Comments:

- u/rational_rob:
  ```
  So, there was a discussion a few days back about making a video series for the [Sequences](https://wiki.lesswrong.com/wiki/Sequences). I, for one, think that it's an obvious step (and I'm surprised nobody else has taken it). How much general interest is there in this? How do you think we should accomplish it?
  ```

  - u/ZeroNihilist:
    ```
    I haven't read them but if I'm right in assuming that they're essentially 26 essays, distilling them into videos is going to lead to either (a) a significant loss of content or precision, (b) long, wordy videos unlikely to appeal to the audiences we want to reach, or (c) a very large time investment to achieve the desired accuracy and brevity.

    That said, I think it would do wonders for disseminating the ideas into the wider population. A well-made series of videos that avoids coming off as preachy (difficult when covering a topic that can be summed up as "You're thinking like an idiot, here's how not to do that.") would be worth the cost of its creation.
    ```

    - u/trekie140:
      ```
      I'm on board with this idea, but I think that some editing may be in order for at least some articles to make them more appealing to a wider audience. I didn't care for Yudkowsky's promotion of the many-worlds interpretation of quantum mechanics, particularly his insistence that it wasn't just his opinion. I'm also one of the people who sided with Robin Hanson in their debate about AI, so EY came across as heavy handed about that too.

      What is guaranteed to be controversial, however, will be Yudkowsky's comments on religion as a concept. If we want more people to learn rationality, we probably shouldn't say that true rationalists are all atheists. We should still promote secular humanism, but articles like Outside the Laboratory will repel theists instead of encouraging them to hold more rational beliefs. EY is very smart, but his self righteousness will rub some people the wrong way.
      ```

      - u/DaystarEld:
        ```
        >articles like Outside the Laboratory will repel theists instead of encouraging them to hold more rational beliefs. 

        I think it's important not to insult people, but I don't think discussions about rationality should hold back the truth, and the truth is that cognitive dissonance from double standards in work/life are a very real psychological phenomenon.  It's the kind of thing that actually often prompts people to eventually look into their faith and realize it doesn't match up with their reason. 

        I'd like to ensure that whatever communication is done reinforces the idea that rationality is a spectrum, and that you're not "barred at the door" if you have religious beliefs. But that isn't the same as denying that all theological beliefs are irrational, and that it's impossible to go all the way to the positive side of the spectrum without reconciling that.
        ```

        - u/None:
          ```
          I agree. The point of half the Sequences was that holding theological beliefs is irrational - and that you can apply this to most of the spectrum of non-evidence based beliefs. It's not wrong to say *all* organized religion as people practice it is irrational. You can change those beliefs to something more rational, but at that point it stops being religion. Then again, it's really just an argument over definition, and we know how those go.
          ```

      - u/buckykat:
        ```
        Outside the Laboratory is crucial to the sequences. Without it, we'd get 'rationalists' like Ginny in GWSI.
        ```

        - u/DaystarEld:
          ```
          The Dark Valley of rationality, I think it's called? Where someone knows just enough to better defend their beliefs, but not enough to critically examine them? I had a conversation with a Young Earth Creationist who acknowledged that the reason they reject evolution is that it's antithetical to the Adam and Eve story, which is at the core of the whole original sin concept that makes Jesus's sacrifice make any sense, and since that MUST be true, evolution MUST be false.

          They had all the right data and understood the logical consequences of the data, but their priors still made them reject the outcome. So frustrating. It's like watching someone in a maze walk right past the exit because they're afraid of what might be outside.
          ```

  - u/DaystarEld:
    ```
    It might be best to, as a community, approach existing Youtube channels that are good at this sort of thing and see if they'd be interested in doing an episode here and there. One by CGP Grey, one by Veritasium, one by Smarter Every Day, one by Kurzgesagt, etc. If some crowdfunding is needed to sponsor an episode, that might be doable too.

    It might sacrifice uniformity in narration and art style, but it would also increase the awareness of the series, and it would be more likely to be taken up as projects by those youtubers, since it wouldn't completely derail them from doing their own work for months and months.
    ```

  - u/vakusdrake:
    ```
    That sounds amazing, because people are far more likely to watch a video you link them than read an article.    

    Plus people could stumble upon it on youtube, getting the information out to lots of people, who would otherwise never have encountered it.

    Hell, if you put good animation in the videos, maybe it would even become quite popular outside the crowd the sequences is currently read by.
    ```

    - u/rational_rob:
      ```
      I don't think animation would be good. In my experience, the worst way to talk about philosophy is to animate it - it makes things seem kind of unreal. It also puts a hard limit on how long you can make your explanation - the depth of your wallet (or, talent). If we went the minutephysics route instead of the Kurzgesagt route, and went with basic whiteboard drawings, it would be even harder to be taken seriously.

      I think the easiest way to do it would be CFAR-style (this topic came up in the thread asking about CFAR effectiveness, IIRC) where we have real locations and real people. The biggest problem with classroom learning, in my opinion, is the fact that it's hard to relate to real world concepts. When people are going into a video series naturally skeptical, that's the kind of image we want to put out. (think Veritasium)

      So, when discussing [Hindsight Devalues Science](http://lesswrong.com/lw/im/hindsight_devalues_science/) you might go to a WWII memorial or museum to make the point clearer.
      ```

- u/DaystarEld:
  ```
  Hey everyone, been a crazy few weeks, but I finally had a chance to continue working on the [AI game I talked about before.](https://www.reddit.com/r/rational/comments/56se39/d_monday_general_rationality_thread/d8m31oq/)  

  So far I've finished mapping out the turns as follows:

  There are 5 Phases in the game. In each phase, players take their turn round robin style (each player takes an action clockwise, repeatedly) until everyone has passed, and then the next Phase begins. If a player runs out of Action tokens, they are automatically skipped in later Phases.

  Players play with a screen that blocks some information from others, including how much money they have.

  **Phase 1: Funding**

  Every player collects funding from their research grants and resets their Action tokens, making them usable again. Any players that want to apply to new research grants are able to do so now at the cost of 1 Action. If two or more players try and apply to the same research grant, they must Test their machines to demonstrate their competence: the winner gets the card.

  Research grants are cards that give extra funds every turn as long as their conditions are being met. These conditions can be anything from "Gain 2 Funds per turn for 5 turns, then discard it. If you research User Modeling Level 2 by then, get an additional 5 funds." or "Get 10 funds. If you don't research Instrumental Convergence Level 1 by next turn, skip your Funding Phase."

  **Phase 2: Recruitment**

  Researchers equal to (n-1) are drawn from their deck and placed face up. Players bid on the researchers as a whole by picking up the amount of funds they are bidding in cupped hands to conceal the number, then revealing all at once.

  The player who bid the most gets first pick of the Researchers. The player who bid second gets second pick, and so on. Any player that did not bid gets to pay a Researcher's Minimum Bid cost to "rent" them for a turn. They are discarded at the beginning of the next Recruitment phase.

  Different researchers have different powers and benefits, but all give at least one extra Action point that can be used in some specific field, either Funding, Research, or Development.

  **Phase 3: Research**

  Players take turns using Action tokens and spending money to earn Research cards.  Some Research cards are the "blueprints" for Components for their AI. Components are needed for AI to function, and increase Risk. Some are also used to generate funding through patenting or research grants.  

  Other Research cards a Player might choose can improve their team's understanding of AI alignment in various ways which reduce Risk. These research cards tend to have levels, which can be upgraded during this phase as well for additional Action points and cost.

  The Research phase is also where espionage takes place. Once every player has passed, they may each secretly choose a different player they want to investigate. If two players investigate each other, both block each other from getting information. A player who doesn't investigate anyone automatically blocks anyone who investigates him. Successful investigations allow the investigating player to look behind their target's screen.

  **Phase 4: Development**

  Players use actions and funding to create Components if they have the required Research complete. Most Components are either core pieces to the AI construction, which increase Risk, or safety measures that reduces it, but some are ancillary technologies, like facial recognition software or motor-reflex control, which have commercial value and grant extra money or provide extra benefits.

  Deployment is also when most Sabotage cards might be used to throw wrenches in the other players' plans, either by messing with their research, getting their funding taken away, or feeding them false help in ways that look like real benefits until it's too late. Other Sabotage cards will be "instants" which can be used at other times, such as during the Recruitment or Funding phases.

  **Phase 5: Deployment**

  I'm still not sure if this should be its own phase or if anyone should be allowed to deploy their AI between phases. As I have it now, the opportunity to run your AI, either as a lab test or as a field test, comes now. 

  Players take all the completed Component cards for their AI and shuffle them together, then play a game of "Blackjack" against themselves to bring risk out of the -%. The better your "AI deck" is from all the extra components and research you've done, the higher your chances of a positive outcome. 

  If the players undershoot in a test, they gain a minor benefit. If they overshoot, the test is a bust and they get nothing. If they hit it exactly on, they gain a major benefit.

  If the players undershoot in a real deployment, they lose Action points on the next round. If they hit it dead on, they win or everyone wins, depending on what faction they are. If they overshoot, Everyone Loses, and the players refer to the chart described in [the first post](https://www.reddit.com/r/rational/comments/55o2ah/d_monday_general_rationality_thread/d8ckiqu/?context=3) about what might have gone wrong, depending on what components they're missing.

  This is just a basic overview of how the game would flow, and is will probably change drastically by the time the game is done. Any and all feedback welcome!

  [Next post](https://www.reddit.com/r/rational/comments/5cwd7l/d_monday_general_rationality_thread/da07x4x/)
  ```

- u/munchkiner:
  ```
  I am experimenting new techniques/tools/resources on myself, and I reached a good point to share the results with you. Probably you already know it all, but think about this as a way to start giving back to this awesome community.

  **Token Pomodoro**

  Everyone probably knows the [Pomodoro technique](http://lesswrong.com/lw/gp4/the_power_of_pomodoros/). Personally I'm using the [toggl](http://toggl.com) Chrome extension. It's mainly used by professionals for tracking time, but you can set it for pomodoro. I have set it so when the 25 minutes are passed it just shpows a popup without sound and continues tracking the time, so I have little distraction if I'm in the zone. Just racing against the time can be a huge motivator, but I am working on an upgraded version.

  Shortly if you plan multiple pomodoros during the day you can use physical tokens to track your progress. So for example yesterday I wanted to study german, and set 8 slot as a goal.
  Every time I completed a pomodoro I put on the table a new coin (I was lucky to have 9 of the same kind), until I finished the fifth and put them on a column with a golden one on top. Gotta hit those reward centers whenever you can! And apropos:

  **Gratification**

  It's much, much better to reward your brain toward a [positive habit](http://lesswrong.com/lw/cu2/the_power_of_reinforcement/) than a negative one. If you want to eat less junk food, it's more efficient to focus on learning to cook healthy recipes and establish an "incompatible behavior". So instead of fighting procrastination I have every day a few things I would like to do to push my long term goals, as learning a language or doing exercise. Also is nice to have some "unproductive" activities, like watching a series, ready when you need to relax, instead of just mindlessy browsing. 

  Remember to congratulate yourself everytime you finish an activity, even if you fall shorter than your initial plans. Guiltiness is your [enemy](http://mindingourway.com/guilt/). Yesterday I ended doing 5 slots instead of the planned 8, but it's much better than the usual 0, ans I felt really good. Of course then you need to analyze what happened to do better next time. Just don't plan thinking to be a machine. 

  **Fitness**

  I'm at level zero for physical ability. I even considered grumpily that working out was a waste of time in the past, compared to intellectual trials. Simply wrong. Just moving give you a great productivity and confidence boost, and is a great opportunity for socializing.
  I'm now doing the [bodyfitness routine](https://www.reddit.com/r/bodyweightfitness/wiki/kb/recommended_routine), that's a great start with a readyprogression on the exercises if they gets too easy. You can follow the videos and do probably everything at home. In my experience just starting is much better than waiting until you find the perfect program.
  I like so much the sense of improvement that I find myself searching a spot to do pushups at work or practicing handstand when at home.
  Also I have a park near home full of bars and rings, so I discovered the happyness of going there during a sunny day, feeling like a ninja doing training, and meeting other great people exercising there.

  I'm also trying to mantaining a [squatting position](http://placeofpersistence.com/30-30-squat-challenge-by-ido-portal/), but haven't results to share yet, if not that you feel good after.

  **Typing**

  > Why do many who type for hours a day remain two-finger typists, without bothering with a typing tutor program?^[source](http://lesswrong.com/lw/2p5/humans_are_not_automatically_strategic/)

  It's a shame form me, but I can't yet touch typing. I started in august [this program](https://www.typing.com) until half the intermediate program and then pretty much forgot about it. Since this weekend I'm forcing myself to 10-finger typing (also while writing this, so appreciate the effort!) to get to a decent speed.

  **Social skills**

  Everyone goes around the world with a continuos mental dialogue about his problems. If you forget for a moment about yours and ask someone about his life, and listen sincerely, you can skip all the boring small talk and get [close](https://en.wikipedia.org/wiki/How_to_Win_Friends_and_Influence_People) pretty fast.

  **Sleep**

  Sleep is important. Charging the phone away from my bed have the double benefit that I don't spend hours reading past a sane hour, and when the alarm rings I have to get up from bed.

  Also reduce at minimum the lights, even removing simple leds. Try to have complete darkness and throw open the windows as soon as you get up. *Ganbatte!*

  **Final words**

  I'm not used to writing so much, even less in English. Do you like this format? Think it could be useful? More time I spend here more I feel an itch for writing, and a blog could be a possibility, but still pretty feeble.

  Feedbacks (also on the writing) are really appreciated. See you on the [Discord server](https://discord.gg/5sutD3W)!
  ```

- u/gbear605:
  ```
  Does anyone have suggestions for daily rituals/habits/tasks that increase happiness? A few I've thought of so far:

  * Physical exercise

  * Meditation

  * Learn an instrument

  * Keep a diary

  * Dedicating x time per day to reading

  * Keep a list of things that happened to me that I'm grateful for
  ```

  - u/_fabien_:
    ```
    I see three of them:

    * learning to draw
    * learning a new language
    * cleaning up your space

    Though getting that many habits on at the same time would probably be overwhelming. The most bangs for my bucks for me has been physical exercise, meditation, a new language, and cleaning up.

    edit: forgot sleep, /u/munchkiner reminded me of it below. Sleep goes first (for me).
    ```

    - u/munchkiner:
      ```
      Happy to have been helpful! Yes, I wouldn't focus on more than 3 habits at a time, with one main and the others as support/slow burner.

      One great thing that gives instant happiness is partaking in social activities. Join a gym class or a painting club, doing something together makes you instantly part of a community.

      But the first thing for me is the sense of being on a path of long term self improvement.

      Also a diet without sugar and few pasta/pizza helps you against brain fog and spikes on your blood level.
      ```

  - u/MacDancer:
    ```
    I think you could reasonably add dancing to that list. It combines many of the benefits of physical exercise and artistic pursuits.

    I might be biased, though.
    ```

- u/Gaboncio:
  ```
  I've been doing some thinking recently and I've come to the fairly terrifying conclusion that I don't know how to estimate what a government's utility looks like. What does a government's ultimate goal look like? Is this even a meaningful question, or do I have to think about it in terms of the actual people involved? Is there a certain level at which it's possible to disentangle the individual officeholder's goals from the organization's, like we can for corporations? Halp
  ```

  - u/alexanderwales:
    ```
    I'm curious how you do it for corporations, as it seems to me that you run into the same fundamental problems.
    ```

    - u/Salivanth:
      ```
      In my understanding, the ultimate goal of a corporation is to make as much profit as they can. For a government, the ultimate goal isn't so cut and dry.
      ```

      - u/alexanderwales:
        ```
        A corporation can have a stated goal, and this can sometimes be to make money, but that's not really a given, especially for corporations which are privately owned. And having a stated goal doesn't *at all* mean that a corporation is working toward that goal, since the corporation is made up of a bunch of laws and a bunch of stakeholders, the former which work against the goal and the latter which have goals different from the nominal goals of the organization.

        Edit: That is to say, if you model corporations as having their stated goals, their actions will tend to make very little sense to you.
        ```

        - u/Salivanth:
          ```
          Regarding your first sentence, fair enough. I don't really know much about the goals of corporations.

          Regarding the rest of the post, while it's definitely true, that's also the more solvable part of OP's problem. If you know what the corporation's goal is, you can examine how the different goals of the individuals making up the corporation (get promoted, don't get fired, don't work too hard, whatever you like) affect the goals of the organisation itself. Hence why he said it was possible to disentangle the individual's goals from the organisation's when studying corporations.

          With a government, however, you can't do that at all unless you know what the end goal of the government is supposed to be. That seems to be OP's problem. You can't say "The goal is X, but Y seems to happen because of what individuals want, and that ends up not leading towards X" when you don't know what X is yet.
          ```

  - u/zarraha:
    ```
    I think the government's utility is supposed to be the sum (or average) of the utilities of all of its citizens.  A group of people with no government would form a government in order to protect their mutual interests and create laws that break up inefficient games like tragedy of the commons and force the higher outcomes.

    Now in practice you might nuance this with things like boundaries that restrict its size and type of activities it is and is not allowed to do in order to achieve these goals to prevent weird paper-clipping behaviors and also to prevent corruption, and maybe you would add some smaller weight to the utility functions of non-citizen humans.  But if you were to make a government with an actual utility function, the main function ought to look like a citizen utility maximizer with some smaller side terms.

    In practice I think it's actually just some average of the politicians' utility functions.  Each politician acts in their own best interests, and the citizens' utility functions are only bootstrapped into that by the desire to get reelected (and possibly some altruism on the part of some politicians).
    ```

- u/TimTravel:
  ```
  Does anyone know off hand what the going rate is to save a human life by giving the appropriate amount to the right charity? I feel like that's something I should generally know but it's a bit difficult to research.
  ```

  - u/DaystarEld:
    ```
    Look up the cost of saving someone from malaria. I think it's the most clear cut example where you can be sure that X dollars saves 1 life, though I could be wrong.
    ```

---

