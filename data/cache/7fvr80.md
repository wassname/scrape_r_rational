## [D] Monday General Rationality Thread

### Post:

Welcome to the Monday thread on general rationality topics!  Do you really want to talk about something non-fictional, related to the real world?  Have you:

* Seen something interesting on /r/science?
* Found a new way to get your shit even-more together?
* Figured out how to become immortal?
* Constructed artificial general intelligence?
* Read a neat nonfiction book?
* Munchkined your way into total control of your D&D campaign?


### Comments:

- u/callmesalticidae:
  ```
  (Headspace stuff, including an attempt to figure out how normal this is or isn't, because maybe other people are just describing the same stuff but in different terms)

  Sometimes I think that I'm rarely happy, and the best that I usually get is "alright, or not bad." 

  Other times, I think that I'm overthinking it all and that this is just how everyone normally is. 

  The impression that I get regarding how life is *supposed* to work: If happiness is graded from -10 to 10, a normal person ought to experience -10 about as often as 10, 5 about as often as 5, and so on, and that if this isn't true then something abnormal is going on. I'm not entirely confident that this is actually true but that's a large part of why I'm making this post, to compare experiences and try to figure out whatâ€™s actually going on with other people. 

  My best experiences are when I'm in a flow state, but subjectively that feels less "How other people seem to describe happiness" and more "Loss of sense of self." 

  Does any of this sound familiar to anyone else?
  ```

  - u/eternal-potato:
    ```
    I believe this is closely tied to how emotional you are in general. The more so, the more dramatic are the sadness/happiness oscillations. As somebody who spends about 98% of the time somewhere between 'mild annoyance' and 'mild amusement/contentment' I hestiate to describe myself as 'truly happy',  but likewise I am certainly not upset/sad/depressed either. Most things that would upset/cheer up a more emotional person are just kind of 'eeh, whatever', and more dramatic stuff is muted.

    No idea what 'loss of sense of self' is.
    ```

    - u/CouteauBleu:
      ```
      I think you're maybe mixing correlation and causation a little here, but yeah. That sounds about right.
      ```

  - u/holomanga:
    ```
    Person who's natural emotional state is "meh" and was also wondering whether I abnormally lacked true happiness here!
    ```

  - u/None:
    ```
    I'm interested to see what everyone else says, but I'm not a useful sample.  I live at about an average of -2 +/- 5.  Sometimes I go on vacation and forget about life for a while and it goes up to +5 +/- 3.

    My basic emotional makeup is a mix of, "The world is beautiful and I love people" with, "I will grind this ignorant *crime* of a civilization beneath my boot."  Yes, at the same time.
    ```

  - u/CCC_037:
    ```
    > Sometimes I think that I'm rarely happy, and the best that I usually get is "alright, or not bad." 

    This might be a definition thing. I spend a lot of time in a state of "alright, or not bad" or minor contentment; but I consider this to be a state of happiness. True, it's not "ecstatic", and it's somewhere on the low end of happiness... but I nonetheless consider it happiness.
    ```

  - u/ShiranaiWakaranai:
    ```
    Let's suppose that the average person only experiences happiness within the range -10 to 10, where having more than 10 requires you to be drugged, and having less than -10 requires you to be actually under torture. 

    Then I would say that having more than 5 happiness requires you to be delusional. To have the kind of mindset that thinks the world is beautiful, that society is just, or that a wise benevolent omnipotent being is watching over us. Because that's the kind of thinking you need in order to feel things like "true friendship", "true love", "true happiness", and "spiritual fulfillment", whatever the hell those are. 

    Personally, I fluctuate between -1 and 3 in my daily life. 3 is really my maximum because I never forget that my state of happiness is an artificial construct that I keep up to avoid the health issues associated with depression. I reach that level by being so engrossed in a story or video game that I temporarily forget about the cruel reality I live in. 

    Whenever I drop the pretense and think about reality, about how natural selection is a nigh inescapable law of logic that is trying and succeeding at killing us all in exchange for more progeny, about how sheer random chance can and eventually will ruin absolutely anyone for no reason at all, about how any powerful being watching over us is clearly horribly incompetent or malicious, about how most of the sentient beings in this world are so delusional that they will pursue strange concepts of happiness even at the cost of screwing over the rest of us, and about how even being depressed about it will hurt my health cause natural selection thinks unhappy people aren't fucking enough to be worth keeping alive, I sit pretty firmly at about -7 to -5. Which is definitely not healthy and so I quickly put back up my bubble of denial.

    On a happier note, I have never had issues about "loss of sense of self". The concept of some kind of "ideal self", like notions of "I'm supposed to do this with my life", or "this is what god designed for me", or "this is the meaning of my life" are essentially the delusions of delusional people who are so happy that they are inventing problems for themselves. Like when you beat a video game and then decide to try for a high score or a no-damage run or to complete every single achievement. You are artificially increasing the difficulty so you can find more challenge. But seeing as we live in a world where there are already countless life-threatening problems, why would you want to increase the difficulty more by insisting on completing the optional quests like finding out your "true self" or your "meaning of existence"? And those optional quests don't even have good rewards. It's not like finding out the meaning of life gives you +10 int or makes you immune to hunger.
    ```

    - u/CouteauBleu:
      ```
      > Then I would say that having more than 5 happiness requires you to be delusional. To have the kind of mindset that thinks the world is beautiful, that society is just, or that a wise benevolent omnipotent being is watching over us.

      Eh, [I think it's just biological](https://www.smbc-comics.com/comic/the-consolation-of-philosophy). I have pretty similar views, and I'd say I'm often pretty close to a 5.
      ```

    - u/holomanga:
      ```
      > It's not like finding out the meaning of life gives you +10 int or makes you immune to hunger. 

      It does if you then go to step 2 and figure out how to implement it in an AI!
      ```

      - u/None:
        ```
        *Spoilers!*
        ```

- u/traverseda:
  ```
  I am planning on wearing anti-corrective lenses when I'm at my computer, in an attempt to correct my myopia. This seems like a pretty obvious way to do that, and I am both surprised and confused that it's not common practice.

  In what ways does this go terribly wrong and ruin my quality of life?
  ```

  - u/gbear605:
    ```
    I presume you're discussing something like https://gettingstronger.org/2010/07/improve-eyesight-and-throw-away-your-glasses/ ?

    If so, then probably a combination of a lack of knowledge or confidence that it will work and a lack of motivation/time.
    ```

    - u/traverseda:
      ```
      I had not seen that, it was based on my own theory of how it should work, and some quick searches didn't turn up anything pertinent. I will have to read through the papers they ~~sight~~ cite.

      I was googling for entirely the wrong keywords.
      ```

    - u/jaghataikhan:
      ```
      Not going to lie, this just feels too good to be true (also pings some of my internal "the establishment is lying to you!" flags that tend to accompany contrarians/oddballs/etc who aren't actually *right*). 

      I *can* confirm lasik took me from like a -8 prescription to 20/10 vision, but I also know it wont last as I age. If this can help stave off some of the effects of aging now that I'm in my 40s, I'd be happy to try it out - let me know if it works for you?
      ```

  - u/GaBeRockKing:
    ```
    Huh, this seems interesting. I've been considering lasik, but I know it doesn't work long term.  Even if this only reduced my prescription, instead of eliminating it, it would be well worth it. Can you link me something that supports the usage of anticorrective lenses? I checked the article linked by gbear05, but would rather not rely on one source.

    Also, instead of using anticorrective lenses, would it be possible to just not use my glasses while at the computer, while being just close enough to the screen to be able to read the text, while far enough away for it to be significantly blurry?
    ```

  - u/sparr:
    ```
    Did you know that many years ago there was a product that you put on your eyes like a contact lens, to be worn while you slept, that would forcibly reshape your eyes to temporarily improve your vision the next day?
    ```

    - u/traverseda:
      ```
      Yes! That was a lot easier to google for.
      ```

- u/CouteauBleu:
  ```
  Help me out here.

  I was thinking about Eliezer Yudkowsky and HP:MoR the other day and I had this vague impression about them. I'm going to try putting it into words, and I'd appreciate if anyone can help me figure out what I mean.

  I feel like Eliezer Yudkowsky and MoR have this unique property, that I would call __incompressibility__, for lack of a better word. That property would be: they are not perfect, and someone can do better than them, but the only way to do better than them is to be more complex... or more smart, in some abstract sense.

  I'm really not sure how to put it. Basically, you can criticize MoR, but the only criticism that is valid is criticism that has *more thought* put into it than MoR itself? No, that doesn't sound right; you can put less though, but focus it more.

  A counter-example to that property would be a car without wheels. It can be an item of tremendous complexity, with immense thought put into it, but you only need non-immense thought to realize that the car won't be able to function very well.

  I guess a similar concept would be Pareto efficiency, but that's not it either.
  ```

  - u/Kinoite:
    ```
    Think of books in terms of their emotional 'payoff'.  What's the emotional highlight that you're going to remember in 10 years?

    Jim Butcher's Deadbeat is a "stand up and cheer" adventure story.  I think there was a mystery plot.  The world building is OK.  But you read the book for the epic moment where [deadbeat spoiler](#s " Dresden rampages into battle on a zombie dinosaur").

    Heinlein's Stranger in a Strange Land is an "idea" sci-fi story.  The characters do things.  But, the point of the book is seeing where Heinlein goes with his conceit.

    A romance novel might be about that moment where the male lead realizes he's utterly devoted the the female lead.  A horror story might be about capturing a feeling of creeping-dread that will stick with you long after you put it down.

    HPMoR's payoff was that it made me notice things.  The plot was OK.  The dialogue was often bad.  The impact was reading a story where the characters thought like actual people.  And, by extension, realizing how many stories relied on contrivance and stupidity to drive their plots.

    That feeling of reading worlds with actually-intelligent characters is the thing that makes me read rational fiction.

    Books written around a "payoff" need to nail their 1 outstanding aspect.  The rest of the writing can be anywhere from good to merely serviceable.  I think this is why the books seems "incompressible".

    If you change the core bit, you're changing the heart of the book.  Everything else is polish, since it's not why you were reading the book in the first place.
    ```

    - u/CouteauBleu:
      ```
      I think I see what you mean, but no, that's not what I'm after :)
      ```

  - u/CCC_037:
    ```
    > I feel like Eliezer Yudkowsky and MoR have this unique property, that I would call **incompressibility**, for lack of a better word. That property would be: they are not perfect, and someone can do better than them, but the only way to do better than them is to be more complex... or more smart, in some abstract sense.

    Hmmmm. I'm going to disagree.

    It is an excellent story, and it is going to be very very hard to improve, yes. But... there are flaws, which I feel can be fixed *without* going more complex.

    The most glaring of these is where [spoiler](#s "Harry manages to defeat the Dementor(s). *How* exactly the idea of the Earth floating in space allows him to do this is never clearly elaborated on, or, indeed, explained at all.")

    It's minor, I'll admit, but I feel that a proper explanation of that would result in a better story - and without increasing complexity.

    In other words, I think it is possible to do better while being only *equally* smart, not *more* smart.
    ```

  - u/xamueljones:
    ```
    I'm not sure what you mean, but I have a few guesses from my own experience with HPMOR:

    1) You could be talking about how there is no low-hanging fruit when it comes to quality. HPMOR has so much thought and detail put into it that there is no part of it which can be easily improved. Any improvements would require an author who is just as good or better at writing and explaining rationality concepts as Eliezer.

    2) Another thing you might be getting at is how every single bit of the story is essential. Remove any chapter and there will be holes in the plot. It's like how every word written is a crucial hint which are only obvious in hindsight. If someone tried to write the exact same story but shorter, they would find it very difficult. An accurate summary is very difficult (fortunately a good summary doesn't really need to convey everything that happened in HPMOR) and even readers who are given spoilers will still end up surprised. You can't describe the story very well without just telling the story itself.

    PS Sorry if #2 is too much word vomit, I'm about to go to sleep and just wrote down everything I could think of.
    ```

    - u/tonytwostep:
      ```
      > Another thing you might be getting at is how every single bit of the story is essential. Remove any chapter and there will be holes in the plot. It's like how every word written is a crucial hint which are only obvious in hindsight. If someone tried to write the exact same story but shorter, they would find it very difficult.

      I think we may be over-glorifying HPMOR a bit here. No matter how much you like it, it's reasonable to admit that (a) it has (at least a few) flaws, and (b) it has (at least a little) unnecessary cruft.

      Removing parts of the story may result in a *less enjoyable* story for you, but there are certainly small parts here and there which are not "crucial hints", and which wouldn't leave "holes in the plot" if removed. Eliezer even talks in his notes about how he thought parts of the story were awkward, or didn't like certain parts.

      I can't speak for him, but I wouldn't be surprised if there were parts he would remove/change, if he were to conduct a thorough edit of the work (similar to what Wildbow's been doing with Worm1)
      ```

      - u/xamueljones:
        ```
        Yeah, it was a little bit hyperbole, but I was just trying to guess what CouteauBleu is identifying. I agree with you that HPMOR is not so flawless in this respect.
        ```

  - u/None:
    ```
    I think that's just called being not-stupid.  Anything that's engaged at all with reality is like that: you can only knock it down by bringing more reality.
    ```

    - u/CouteauBleu:
      ```
      I... don't think so? You're definitely getting somewhere, and I think "not-stupid" is a good term for the concept I'm trying to outline, but there are thousands of ways to be engaged with reality, some of which can be knocked down with a lesser amount of reality.

      I was thinking about it, and it's more like... being level-N complete? Like, you're level-1 complete if you've considered all reasonable level-1 arguments, and you can only be "outmatched" by a level-2 argument or higher. That doesn't mean the person making the argument needs to be level-2 or higher; but the argument needs to be.

      Something like that, but less RPG-ish.
      ```

---

