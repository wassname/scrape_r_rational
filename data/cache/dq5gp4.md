## [D] Friday Open Thread

### Post:

Welcome to the Friday Open Thread! Is there something that you want to talk about with /r/rational, but which isn't rational fiction, or doesn't otherwise belong as a top-level post? This is the place to post it. The idea is that while reddit is a large place, with lots of special little niches, sometimes you just want to talk with a certain group of people about certain sorts of things that aren't related to why you're all here. It's totally understandable that you might want to talk about Japanese game shows with /r/rational instead of going over to /r/japanesegameshows, but it's hopefully also understandable that this isn't really the place for that sort of thing.

So do you want to talk about how your life has been going? Non-rational and/or non-fictional stuff you've been reading? The recent album from your favourite German pop singer? The politics of Southern India? The sexual preferences of the chairman of the Ukrainian soccer league? Different ways to plot meteorological data? The cost of living in Portugal? Corner cases for siteswap notation? All these things and more could possibly be found in the comments below!

Please note that this thread has been merged with the Monday General Rationality Thread.

### Comments:

- u/None:
  ```
  Prediction time! I'd like to register the following:

  We will see human level or greater General AI before 2030 (>=95% cumulative probability mass). This is an Inside View prediction rather than outside view.

  I'd like to know (a quick summary) of others thoughts about this. Do you think I'm crazy?

  I had another prediction which has been asked to be removed so I have removed it. Sorry /u/Badewell.

  Part of unedited post: (Unrelatedly, does anyone have the link for that Scott Alexander story about the Whispering Earring? I remember reading it a while ago and wanted to have a look at something)
  ```

  - u/None:
    ```
    You know, we've been discussing about EY writing under alias for a long time. But what about Eliezer, or other prominent writers, letting *other people* write under *their* name to make things even more interesting? Would that be plausible?
    ```

    - u/Roxolan:
      ```
      He kind of did that. He (in a facebook post I think?) said that anyone should feel free to write suspiciously Eliezer-like fiction and let people speculate that he's the real author. He will not confirm or deny said speculations.
      ```

      - u/None:
        ```
        No, I meant literally letting someone put his name on their story; think ghostwriting. But I guess that's comparable to a direct lie, which completely contradicts the whole premise. So yeah, forget it.
        ```

  - u/Norseman2:
    ```
    >We will see human level or greater General AI before 2030

    This seems pretty doubtful to me, but it's also not very well-defined. To get a clearer picture of what we're talking about, let's look at [tests for AGI](https://en.wikipedia.org/wiki/Artificial_general_intelligence#Tests_for_confirming_human-level_AGI[15][16]) and see which ones are most likely to be passed in the next 10 years:

    * Turing Test: I suspect we'll see a few AGIs pass in the next ten years. A bot that spouts frequently misspelled crap about antivaxxing, contrails, flat earth, Jewish conspiracy, pro-Trump, anti-immigration, pro-Russia shit, etc. is rather likely to be deemed a human. The bar for humanity can be set pretty low at times, so getting a bot to pass for a human is not necessarily very difficult.

    * Coffee Test: It seems quite doubtful that this will be passed in the next ten years. You'd need to be able to identify cupboard doors, open them to search for other things, identify water faucets and use them, identify coffee machines, identify coffee, etc. I suspect that with a prize of $100 million and ten years to get this done, it might be possible to get a robot that can succeed more often than not, but it would likely be a single-purpose machine rather than something that could also go and make a bowl of cereal.

    * The Robot College Student Test: This *might* be feasible for certain niches of study within the next ten years, particularly in subjects that use tests exclusively. However, completing all of the general education requirements would also entail things like English composition, which I doubt an AI would be able to pass in the next ten years.

    * The Employment Test: This has long-since passed. Industrial robots are already commonplace.

    * The Flat Pack Furniture Test: This *might* be passed in the next ten years. It seems unlikely, but I could potentially see stores selling flat-packed furniture and offering to have a robot assemble it in your home at some point in the next ten years. This can be aided by having a QR code on the box which provides a link to a set of instructions which are more machine-friendly, plus QR codes on the parts to help the machine identify them and follow the directions.

    * The Mirror Test: I could see this being passed fairly easily within the next ten years, since looking at objects in the infrared and ultraviolet spectrum could be done relatively easy by a robot, and this would make it much easier to distinguish mirror images from actual objects.

    * AGI IQ tests: For this, the obvious benchmark would be to reach or exceed an IQ of 100. I could see this potentially happening within 10 years, though I have very low confidence about the odds.

    Will a single machine pass all of those tests within ten years? It seems highly unlikely for now. We've definitely made some good progress since 2010, but not enough to justify saying that we'll have genuine AGI with just another 10 years. Right now, I suspect that AGI which could pass all of those tests is probably closer to 20-40 years out.
    ```

    - u/jtolmar:
      ```
      > Turing Test: I suspect we'll see a few AGIs pass in the next ten years. A bot that spouts frequently misspelled crap about antivaxxing, contrails, flat earth, Jewish conspiracy, pro-Trump, anti-immigration, pro-Russia shit, etc. is rather likely to be deemed a human. The bar for humanity can be set pretty low at times, so getting a bot to pass for a human is not necessarily very difficult.

      This has been done. 10 years ago, actually. I put a bot up on 4chan that passed as human, though some folks questioned its mental health. It was just a Markov Chain text generator fed with the topic it was responding to.
      ```

    - u/None:
      ```
      All good points. The only one I'm not so sure about is the Coffee test (ie. manipulating physical objects) as that's a very difficult problem in uncontrolled environments. Certainly doing it safely around humans in non-industrial settings should be done carefully.

      I still think basically all those tests will be substantially passed by a single or small collection of systems (such that those systems can be chained together to produce the outcome.) *Right now*, we have an explosion of complexity but as the tooling catches up to the tasks at hand, we are likely to get (due to compositionality) an "implosion of simplicity". I think you may be underestimating how long 10 years actually is and what we can achieve.

      In this prediction I'm just assuming current trends continue so no major wars, unrecoverable disruption due to climate change etc.
      ```

    - u/Roxolan:
      ```
      > Turing Test: I suspect we'll see a few AGIs pass in the next ten years. A bot that spouts frequently misspelled crap about antivaxxing, contrails, flat earth, Jewish conspiracy, pro-Trump, anti-immigration, pro-Russia shit, etc. is rather likely to be deemed a human.

      Turing didn't define an exact procedure for the test, so people have been trying to [munchkin it for cheap publicity](https://en.wikipedia.org/wiki/Eugene_Goostman) for a while.

      But from his [sample dialogue snippet](http://www.alanturing.net/turing_archive/pages/Reference%20Articles/TheTuringTest.html), the intent was to have a bot that could hold its end of an intellectual conversation and would cooperate with the examiners (who, in turn, must be educated and doing their best to ferret out the bot). [e: [Scott Aaronson says it better.](https://www.scottaaronson.com/blog/?p=1858)]

      At the very least, the human control ought to do that. So if only one of the two can string a coherent reply together, the bot will be easy to find.

      An AI that can pass a genuinely challenging Turing test, even if it's not a true AGI, could likely take over a ton of human service jobs - and finally realise the promised dream of everyone having their own personal assistant in their pocket. An AI that manages to be indistinguishable from a rambling madman... not so much.
      ```

  - u/Badewell:
    ```
    [Clarity didn't work, trying mysterianism](http://squid314.livejournal.com/332946.html), but if you don't have a live journal account here's an [archived collection](http://www.archive.org/stream/ScottAlexanderStories2017/ScottAlexanderStories2017_djvu.txt) that has it.

    Also, I can't check right now but I'm pretty sure that Groon has asked that readers not discuss speculation on who they actually are.
    ```

    - u/None:
      ```
      Awesome! Thanks for the link.

      I'll take that down then sorry.
      ```

  - u/Roxolan:
    ```
    > (At least) One of the works that Yudkowsky repeatedly hints he is behind is The Erogamer, the porn LitRPG/erotic novel that he keeps recommending.

    [Here's the relevant Yudkowsky post](https://www.reddit.com/r/rational/comments/9esous/the_asteroid_strike_unconceivable_threats_in/e5ssj4x/), and some [older discussion](https://www.reddit.com/r/rational/comments/9ft8w4/eliezers_latest_challenge_a_reverse_whodunit/e5z2bmj/). I tend to agree with that thread; The Erogamer reads *too much* like Yudkowsky to fit the bill. (I also really hope Groon is its own person because I want more such writers in the world.)
    ```

    - u/None:
      ```
      Agreed, which is why I say P(someone else) is likely higher, but of any single author P(Yudkowsky) is the highest.

      Despite that, it *really suspiciously* does fit *both* his writing goals *and* his writing style *and* all the meta-stuff around the story *also* fits "things which are consistent with things Yudkowsky would do." So, I would be very surprised if he was not involved at all *somehow*.
      ```

      - u/GeeJo:
        ```
        A year before Groon started The Erogamer, they wrote [Taylor Hebert, Harem Protagonist](https://forums.sufficientvelocity.com/threads/taylor-hebert-harem-protagonist.26134/). It's pretty clearly a first draft of the later work in terms of its premise, and also pretty difficult to see as an intermediate step between HPMOR and the Erogamer, in terms of its writing quality. 

        Which is to say that I'm *very* sceptical of Yudkowsky being Groon.
        ```

  - u/lupnra:
    ```
    My predictions tend to have a shorter timeline too but 95% by 2030 seems pretty high. What's your inside view reasoning?
    ```

    - u/None:
      ```
      Sure. So, the default situation which I expect to continue is pretty much Drexler's AI Services model or Christiano's prosaic AI (which pretty much describe the same situation to me.) In my estimate I'm conditioning on no globally catastrophic outcomes like war, asteroid strike and so on.

      The inside view argument has to do with the rate of building tooling vs the rate of building capability (or, current level of capability). I view "intelligence" itself as best understood "like" a rate, call it a learning rate or what you will. It can be measured, but is difficult to do so. See eg Chollet's recent paper [The Measure of Intelligence](https://arxiv.org/pdf/1911.01547.pdf) My view on intelligence is a little more nuanced than what Chollet writes, but I will re-express it more simply:

      Intelligence can be understood by seeing that there's at least two types.

      1. Accumulated skill and knowledge, from which your "instrumental rationality" ie, the thing which is most often *measured* by people seeking to measure the value of a system.
      2. The efficiency and speed at which you acquire this knowledge, *taking into account resource usage*.

      Chollet gives something like these two definitions. However, my view is that they are not unrelated. As a musician and gamer it is clear to me from personal experience that what you know (point 1) influences what you can learn, and how you learn (point 2). And the reverse is also true in an obvious way.

      Note: This is not the same thing as the "crystallised vs fluid" intelligence model. Both parts are fluid and changing in this case.

      Towards further understanding, it helps to make the observation from music that although there "appears" to be such a thing which many would describe as "repetition," in reality, when the context is taken into account, the best model for the situation is no longer "it is a repeat." This is due to the different *contexts* provided; there is a different before and after. Even when a pianist repeats one singular note, each individual note can be uniquely interesting in its own way.

      I believe humans are very powerful optimisation processes. Currently, we can on average be roughly quantified by our language-processing capacity when speaking out loud to each other. Recent research shows we have about 38 bits of "optimisation power" in this task (roughly corresponding to the Turing test). (Sorry, I can't find the page I had in mind for this, I believe I would have found it from Hacker News. Perhaps google?)

      This (~38 bits) was found across all languages studied - more information-dense languages tended to be said more slowly and less dense languages said more quickly, so the information transfer speed remained roughly constant.

      What this means is that the optimisation power of a machine able to pass the Turing test in full generality only need be about 2^39, (549 Gigabit); ie, it can choose a unique response during the timeframe of a conversation out of a 549 gigabit database (68 GB), where much of the database may be updated or not when the context (by which I mean conversation history and expected future conversation continuation) changes. We already have plentiful hardware capable of doing this. The engineering problem is only the specific algorithm which chooses the response appropriate to the context.

      Now, you may argue, but doesn't a human draw on a much larger pool of semantic concepts in order to generate this response out of (apparently) 2^38 ish possible responses within a few milliseconds, or even before the opposing speaker has finished? Well, yes, but we are not trying to solve *literally everything a human could do in one go* using AI. There's no reason to expect a single algorithm to generalise to a "full, FOOM superintelligence" when you design that algorithm for a specific purpose that doesn't include that.

      This is basically why I think most of the currently defined tests will be passed by automated systems roughly within a decade. It is BOTH the case that current tests are inadequate to capture what we really *mean* when we're talking about "artificial general intelligence," and so are relatively easy to pass by methods other than creating artificial general intelligence, AND that the default, prosaic continuation of current trends will predictably result in these tests being passed by 2030.

      My best estimation of the current situation is that everyone is pretty much following their microeconomic incentives. Therefore due to somewhat Molochian reasoning, I don't expect the situation to change drastically. Hence my prediction.

      Since nothing has to change in order for our capability (indeed, we already have it) to become able to pass many of the current "general AI" tests, the problem then becomes only a question of writing the necessary tooling and software to chain the various sub-solutionary programs into a working system. This is done at a slower rate, and mostly by smart hobbyists, so I expect this to be the bottleneck. However, smart hobbyists are smart, and 10 years is a long time. I feel confident in my prediction based on this.

      As a recent example, [John Carmack announced he is working on general AI](https://www.facebook.com/100006735798590/posts/2547632585471243/). This is the kind of thing which is expected by my model of the world in which this prediction was generated - as the problem becomes tractable in engineering terms, engineers will recognise this and focus on creating the tooling to solve it, purely because it is obviously an important problem.

      Because creating tooling "looks like" from the outside, "no progress," we may not see anything significant until 2025 or later. Nevertheless, that does not mean no progress is being made.

      What I also expect to happen however is for people like Francois Chollet to design better tests of intelligence which more fully capture what we really mean when we say the vague word. Of those expected, but unknown, future tests, I cannot guess at the likelihood of us developing systems to pass them, except if, somewhere during this process an actual general AI is developed which can do all intellectual tasks humans can do including AI research and program development. In which case, all bets are off. However, my prediction would still have come true.

      All of this has large implications for safety. If MIRI, FHI, OpenAI, etc do not work with and substantially solve many of the safety issues surrounding dissemination of such technology, we will end up in a world where "fake news" is *more common* than real news, and easier to produce. It will be easier to talk to a machine than to a human - real humans will not be able to tell the difference. Is this xkcd's "Mission Fucking Accomplished," or is this a horrifying dystopia? I don't have the answer to that question.
      ```

---

