## [RT][C][HF][TH][FF] Good Night: "'Hello Princess Celestia,' said Twilight Sparkle, the barest hint of a smile adorning her muzzle. 'Thank you for coming to my funeral.'"

### Post:

[Link to content](https://www.fimfiction.net/story/212395/7/flashes-of-insight/good-night-572)

### Comments:

- u/erwgv3g34:
  ```
  [As horrified Eliezer Yudkowsky.](https://www.fanfiction.net/r/10643785/7/1/)
  ```

- u/ketura:
  ```
  ...did you mean to link to the last chapter?

  EDIT: ah, nevermind. I didn't realize they were all a series of unrelated one-shots.
  ```

- u/Lightwavers:
  ```
  She would rather die than become a superintelligence.

  Damn she’s stupid.

  (Good story though.)
  ```

  - u/Nimelennar:
    ```
    That's not fair.

    She has a different value system than you do: she values integrity of personality over existence.

    Since value systems can only be defined rationally *to a point*, and beyond that point are based off of irrational preferences (there's no law of physics encoding any concept of "better"), it's not "stupid" to value wanting to end your life being recognizably the same person you are now, rather than thinking it's "better" to continue life as something fundamentally different.

    It's a value choice.  And, while choosing to do something which *won't* fulfill your core values is irrational, no set of core values can be inherently more rational than another, because none of a person's deepest values come from a place of reason in the first place.

    Personally, if I were to be offered immortality, I'd only accept if I were given an escape clause.  I would prefer, for example, *not* to persist in a state of perpetual asphyxia, starvation, dehydration, and solitude, after the heat death of the universe.  And that may not be a *rational* choice (and certainly wouldn't seem so to someone who valued continued existence above all else), but as someone with my core values, I wouldn't consider it a "stupid" one either.
    ```

    - u/Lightwavers:
      ```
      But this isn't a choice between abandoning her core values and death—it's a choice between *potentially* abandoning her core values in a way that would result in her fucking off to the stars somewhere without harming anyone else and death. There's no switch that goes straight from "really you, for sure" to "towering alien abomination of intellect." That it happened with Luna suggests that the scale between those two possibilities is a lot easier to go down than picking a point between them, but it's just one data point. And then there's the fact that psychopathy and intelligence are not necessarily intertwined. It's possible to retain empathy while increasing intelligence, or at least *simulate* empathy to a significant enough degree that the end result looks enough like the real thing that it doesn't matter whether it *really* is or not. This isn't just my opinion. Enough researchers are working on FAI to suggest that FAI is possible.
      ```

      - u/Nimelennar:
        ```
        And how much deviation from "really you, for sure" is acceptable is a *value choice*.  Retaining "yourself" is a lot more about retaining your values than it is about retaining empathy (unless, obviously, you highly value empathy).

        If the only example of uplifting that you've come across resulted in massively distorted values, would you really be so eager to go that route?

        Or, to put it a different way: let's say you exist in the Stargate: SG1 universe, and a Goa'uld symbiont attaches itself to your brainstem.  You don't know the moral alignment of this creature; you do know that it will inhabit your body and possess all of your memories, that it will grant your body long life and health, and a wealth of knowledge.  You could be *really* lucky and it's a Tok'ra Goa'uld, and what will happen is a "blending" of your personality and this other being's, but, in your experience, most Goa'uld aren't Tok'ra, and the personality of the host tends to be brutally suppressed.

        Would you choose, in this moment before the choice is taken from you, to end your life, or would you leave your body's future actions and the use of your memories at the whims of this being you don't know?

        Valuing the preservation of your self/consciousness/memories/body over that self's integrity/personality/values is a *perfectly legitimate choice*.  But so is the other choice.  And neither is more "stupid" than the other.
        ```

        - u/Lightwavers:
          ```
          > (unless, obviously, you highly value empathy)

          I was positive that was what you were referring to.

          >If the only example of uplifting that you've come across resulted in massively distorted values, would you really be so eager to go that route?

          I'd at least look into it, especially since there's only one data point and the outcome wasn't a paperclipper.

          >You could be really lucky and it's a Tok'ra Goa'uld, and what will happen is a "blending" of your personality and this other being's, but, in your experience, most Goa'uld aren't Tok'ra, and the personality of the host tends to be brutally suppressed.

          I have to say this is a false analogy. Again, one data point. You can't really say that most ascensions result in brutal suppression, or even that it's *likely*. All we know is that is *happened.*

          >Valuing the preservation of your self/consciousness/memories/body over that self's integrity/personality/values is a perfectly legitimate choice. But so is the other choice. And neither is more "stupid" than the other.

          That's not what I'm saying is stupid. What is *stupid* is never even trying to investigate a way to perform an uplift while still holding your previous values. Luna has already demonstrated that she is a massive deviation from the norm—she became *Nightmare Moon*. Perhaps she just never valued others and was just pretending, and ascending allowed her to admit that to herself and just blast off.
          ```

          - u/Nimelennar:
            ```
            >I was positive that was what you were referring to.

            I can't see why; I never made any reference to what values are, well, valued, and, while the story hints at a lack of empathy on Luna's part after ascension, all that's made clear is that her values have suddenly become incomprehensible.

            >I'd at least look into it, especially since there's only one data point and the outcome wasn't a paperclipper.

            Look into it how? The only person Twilight can experiment upon is herself, which risks corrupting her value system.  Cadence's mind is functionally gone, and Celestia doesn't seem to be volunteering for experimentation, and *no one else exists*.

            It should also be noted that she may consider her value system as *already* having been corrupted - she has already found, from the last incarnation of Equestria, that she can no longer value the company of new ponies.

            >I have to say this is a false analogy. Again, one data point. You can't really say that most ascensions result in brutal suppression, or even that it's likely. All we know is that is happened.

            Yes, we have one data point, which means it seems to have happened *one hundred percent of the times it's been tried.*  And they don't seem to have any understanding of *why* it happened, either.  That, if anything, says the Goa'uld metaphor is *underselling* the risk (you've *heard tales* of these supposed Tok'ra, but neither you nor anyone you've met has actually encountered one; the one Goa'uld any of you have met has been of the "brutally suppress the original personality" variety).

            Imagine a rocket that can only launch with human guidance.  The first time it launches, it explodes catastrophically, killing its pilot, and you have no idea why that happened, because you can't even simulate it properly without a human consciousness attached and at risk.

            How can you ethically test that rocket a second time, knowing that the most likely outcome is that it will explode again and kill the pilot again (and again, and again, until you have done enough simulations to track down the factor which is causing the rocket to explode)?

            And that analogy doesn't even do the situation justice, because what we're talking about is a radical shift in core values.  The *first* time, the shift was towards something seemingly harmless, but completely alien, something that looks upon normal people like bacteria, but doesn't care enough to harm them.  Yes, the first attempt didn't become a paperclipper, but if you admit the second attempt might turn out *better* than the first, you should also admit that the second attempt might turn out *worse*.

            >What is stupid is never even trying to investigate a way to perform an uplift while still holding your previous values.

            By definition, you're creating a new person who thinks differently than you do; if not, what is the point?  Since they think differently than you do, you cannot predict how they'd think; if you could predict how a person thinks, you can become that person *without* an uplift (or, at least, with just a boost in processing power and memory retention, which probably wouldn't do much to fix ennui).

            Despite all of that, I'll grant that it might be *possible* to come up with a way to do a safe upload, where values are retained.  But it's made clear that Twilight and Celestia are the last two intelligent life forms on the planet.  They'd have to seek out, or create, a whole other civilization in order to start those tests, which will take who-knows-how-long, and Twilight (who already seems to be experiencing value decay) doesn't want to go through that again.  And, for a prize which is far out of reach, and which the only data point she has suggests *may not even exist*, why should she?
            ```

            - u/Lightwavers:
              ```
              > I can't see why

              I thought it was implied. People value empathy.

              >which means it seems to have happened one hundred percent of the times it's been tried.

              You've stumbled straight into the [base rate fallacy](https://en.wikipedia.org/wiki/Base_rate_fallacy) there. We know of one case where, taken to its extremes, this has seemingly turned someone into an unempathetic jackass who'd rather build things in the stars than talk to people.

              >and no one else exists.

              Easily solved. Celestia herself contemplated making new ponies at the end of the story. So experiment on them. Or, hell, experiment on Cadance. I'm sure she won't mind.

              >(you've heard tales of these supposed Tok'ra, but neither you nor anyone you've met has actually encountered one; the one Goa'uld any of you have met has been of the "brutally suppress the original personality" variety).

              This analogy has gotten really far off track. First, there's no suppression going on. We *haven't* heard of anyone encountering one of these supposed oppressive beings, or unfriendly AI, and the only person who did self-modify was already predisposed to introversion, megalomania, and depression.

              >How can you ethically test that rocket a second time, knowing that the most likely outcome is that it will explode again and kill the pilot again

              Well first off you don't assume that one failed test means it's going to fail again. Second you recognize that the first test didn't really fail at all—as you yourself said earlier, there's nothing *wrong* with having values that mean you spend your time playing with starstuff. Third, you make new individuals and you ask for the consent of the suicidal ones, if you're going to make new individuals anyway.

              >but if you admit the second attempt might turn out better than the first, you should also admit that the second attempt might turn out worse.

              The first AI will have all the power. So far that's Luna, and she doesn't care enough to harm anyone. But assume that the second attempt turns into a genocidal maniac. In story we have Discord, Tirek, and the Elements, all of which could conceivably deal with such a threat.

              >Since they think differently than you do, you cannot predict how they'd think

              False. So long as you understand how exactly this person deviates, you can definitely predict how they'd think. But what if this person, say, thinks twice as fast and has the ability to instantly make themselves devoted to any task. You can predict how they'd think, *and* you can see how you can't just become that person without modifying your brain. You don't just need a boost in processing power and memory, but in the ability to modify. In the story, Luna continually modified herself until she became an alien. Just set, say, a max of three modifications per year, with unlimited ability to reverse. Or build a guidance consciousness that reverses any changes she finds abhorrent that polices the process.

              >And, for a prize which is far out of reach, and which the only data point she has suggests may not even exist, why should she?

              Remember what evil would say if you asked it why it did what it did.
              ```

              - u/Nimelennar:
                ```
                >People value empathy.

                Yes, but that's not *all* they value.

                >You've stumbled straight into the base rate fallacy there.

                From Wikipedia (emphasis mine): The base rate fallacy, also called base rate neglect or base rate bias, is a fallacy. **If presented with related base rate information (i.e. generic, general information)** and specific information (information pertaining only to a certain case), the mind tends to ignore the former and focus on the latter."

                Can you, perhaps, let me know where the base rate has been provided, to make this a base rate fallacy?

                I'll get to the "make new ponies" when it comes up again, but, for now:

                > Or, hell, experiment on Cadance. I'm sure she won't mind.

                Because she *no longer has a mind*.  She's a [429-particle happiness engine](https://www.smbc-comics.com/comic/happy-3) with a few octillion extra particles.

                >We haven't heard of anyone encountering one of these supposed oppressive beings,

                The "oppressive being" is the new, "ascended" person you're creating.  If they take your memories and personality, and become a person with different values, then they've successfully suppressed your personality.

                >the only person who did self-modify was already predisposed to introversion, megalomania, and depression.

                ...And yet the people who actually *know* her are convinced that she's experienced a value shift.

                >The first AI will have all the power. So far that's Luna, and she doesn't care enough to harm anyone. But assume that the second attempt turns into a genocidal maniac. In story we have Discord, Tirek, and the Elements, all of which could conceivably deal with such a threat.

                To protect Equestria, sure (as much as a place without a population can be said to be "protected").  But have any of these entities been shown to be able to protect the universe *beyond* Equestria?  *(Edit to add: I'm also not sure that any of these entities even exist anymore, as Celestia is described as "last intelligent being on the planet" after Twilight's passing).*

                >False. So long as you understand how exactly this person deviates, you can definitely predict how they'd think. But what if this person, say, thinks twice as fast and has the ability to instantly make themselves devoted to any task. You can predict how they'd think, and you can see how you can't just become that person without modifying your brain.

                Well, you can pretty much achieve that with the extra processing power ("instantly devoted to a task" is pretty trivial to achieve, and also wouldn't seem to relieve ennui all that well - any task that's sufficiently interesting would probably rate devotion from a superlatively bored person like Twilight even without  extra focus, and any insufficiently interesting task won't do anything to alleviate the boredom).

                >You don't just need a boost in processing power and memory, but in the ability to modify. In the story, Luna continually modified herself until she became an alien. Just set, say, a max of three modifications per year, with unlimited ability to reverse. Or build a guidance consciousness that reverses any changes she finds abhorrent that polices the process.

                You're asking the person designing the upgrade process to build a system that the person *subjected to* the upgrade process (who will be much smarter than the person designing the process) won't have the ability to subvert.  That doesn't strike you as a problem?  Heck, some of the smartest people in the world work in computer security, and their efforts are routinely circumvented by amateur hackers.  As dead-simple (and computationally secure) as the math behind many cryptographic algorithms is, people are still told not to implement them themselves, because it's so easy for even smart, experienced programmers to make errors that are trivial for hackers to exploit.  To [quote Randall Monroe](https://www.xkcd.com/2030/): "Our entire field [of software engineers] is bad at what we do, and if you rely on us, everyone will die."  And that's in a comic about *voting software*, not *constraining a superintelligence*.

                >Remember what evil would say if you asked it why it did what it did.

                That is, "Why not?"  Twilight has *told you* why not.  In fact, **I've** told you why *Twilight* has told you why not (emphasis mine-now, not mine-then):

                >Twilight (who already seems to be experiencing value decay) *doesn't want to go through that again.*
                ```

                - u/Lightwavers:
                  ```
                  >One type of base rate fallacy is the false positive paradox, where false positive tests are more probable than true positive tests, occurring when the overall population has a low incidence of a condition and the incidence rate is lower than the false positive rate. The probability of a positive test result is determined not only by the accuracy of the test but by the characteristics of the sampled population. When the incidence, the proportion of those who have a given condition, is lower than the test's false positive rate, even tests that have a very low chance of giving a false positive in an individual case will give more false than true positives overall. So, in a society with very few infected people—fewer proportionately than the test gives false positives—there will actually be more who test positive for a disease incorrectly and don't have it than those who test positive accurately and do. The paradox has surprised many.

                  >It is especially counter-intuitive when interpreting a positive result in a test on a low-incidence population after having dealt with positive results drawn from a high-incidence population. If the false positive rate of the test is higher than the proportion of the new population with the condition, then a test administrator whose experience has been drawn from testing in a high-incidence population may conclude from experience that a positive test result usually indicates a positive subject, when in fact a false positive is far more likely to have occurred.

                  |

                  >Because she no longer has a mind. She's a 429-particle happiness engine with a few octillion extra particles.

                  Excellent. Wipe it clean and start over.

                  >The "oppressive being" is the new, "ascended" person you're creating. If they take your memories and personality, and become a person with different values, then they've successfully suppressed your personality.

                  Not so. The original would have simply updated with access to new information. If you want, you can think of the original personality as the utility function. Someone who just honestly doesn’t care about people has to interact with them, so at normal intelligence might put on a smile and pretend. This is the stage of a paperclipper’s life in which it cooperates with humans. Then the person ascends, and realizes that she was deluding herself all along and she doesn’t really want friends—what she really desires is the ability to play out there in the stars with no one else around to disturb her. It’s an assumption of course, but it works off the available data. Of which we have *one single data point.*

                  >And yet the people who actually know her are convinced that she's experienced a value shift.

                  Well, of course they are. After all, they know her. If someone close to you suddenly changes, and they recently started taking a new medicine, it can be tempting to blame that change on the medicine.

                  >But have any of these entities been shown to be able to protect the universe beyond Equestria? (Edit to add: I'm also not sure that any of these entities even exist anymore, as Celestia is described as "last intelligent being on the planet" after Twilight's passing).

                  Discord can rip holes in reality and travel between universes, so there’s evidence that they can. And the avatar of chaos is immortal. He might be banished, or frozen, or just slumbering like some Lovecraftian god, but he can’t *die*. Since the Elements, which are not an intelligent being, can target him (assuming the reason he didn’t flee the friendship beam was because he couldn’t rather than because he’s an idiot) it stands to reason that he couldn’t just flee to an alternate plane of existence, and thus they too can defend against universe level threats.

                  >Well, you can pretty much achieve that with the extra processing power

                  You can certainly imagine ways to use processing power to emulate this, yes, but you’re not engaging with the core point I was making. There are ways we can imagine that modify how we think and that are beneficial.

                  >won't have the ability to subvert.

                  Perhaps I failed to convey the point. Copy consciousness. Place it at root, with root access. Set emulation speed at many times higher than the secondary consciousness.

                  >That is, "Why not?" Twilight has told you why not. In fact, I've told you why Twilight has told you why not (emphasis mine-now, not mine-then):

                  Wrong angle. These are two questions. Why not die, and why not live. She has answered why she doesn’t want to continue *as she is* and has failed to adequately consider alternatives because she is tired. She has then defaulted to why not die. She has defaulted to the position of evil.
                  ```

                  - u/Nimelennar:
                    ```
                    > base rate fallacy 

                    The base rate fallacy is only a fallacy **if** the base rate is different than the specific information.  *We don't know* what the base rate is.  Sure, it's *probably* not 100%, but if Luna is the only subject who has been upgraded, it's probably not 0.0001% either (or, there'd only have been a 1:1,000,000 chance that she'd be corrupted if it were).

                    If you have some in-universe information to suggest that Twilight should know that the base rate of value drift when ascending is low enough to be worth the risk, I'm happy to hear it.

                    >Excellent. Wipe it clean and start over.

                    ...Okay, you've taken a decided turn towards the evil here.  Creating new minds to be subjected to experimentation is one thing, but going against the express wishes of a friend as to the disposal of her body/consciousness?

                    I'll skip the assumptions you're making why Luna became what she became, and state that it doesn't really matter *why* she did; all that matters is *Twilight's perception* of why she did.  Because that's what she's making her decision based upon (and she can't really obtain more data on this, because Luna has already left).  And, in her perception, it was due to the ascension.

                    And yes, there's only one data point, but one data point is *still a data point*.  All you have to weigh *against* that data point is supposition.

                    >You can certainly imagine ways to use processing power to emulate this, yes, but you’re not engaging with the core point I was making. There are ways we can imagine that modify how we think and that are beneficial.

                    Yes, but you're missing *my* point.  My point is that any mind that you can sufficiently emulate with your own mind is, pretty much by definition, already present within your own mind.  Any mind that you *can't* emulate, you *can't* predict.  So, anything *safe* (like processor speed) won't relieve your ennui, because you can pretty much become that person by choice, just slower.  Anything sufficiently different from you as to relieve your ennui, if *everything* bores you, isn't someone you can safely assume will retain your values, because you can't sufficiently emulate them (and, if you could, you wouldn't be stuck in a state of ennui).

                    >Perhaps I failed to convey the point. Copy consciousness. Place it at root, with root access. Set emulation speed at many times higher than the secondary consciousness.

                    So, you have a slow-thinking subprocessor.
                    ...*How* exactly is this supposed to relieve ennui?

                    >Wrong angle. These are two questions. Why not die, and why not live. She has answered why she doesn’t want to continue *as she is*

                    Yes, and, by your own admission, she'd *have to continue as she is* in order to do the research necessary to safely continue as something else.  Which, as you also admit, she *doesn't want to do.*

                    >has failed to adequately consider alternatives because she is tired

                    Even if I concede this (which I don't; we haven't seen how long she's spent considering alternatives to declare whether it's adequate or not; we certainly can't assume that based on the conclusion she reached), "tired" is not "stupid."

                    >She has then defaulted to why not die. She has defaulted to the position of evil.

                    And now we're back to values.  You consider her death evil.  Which, okay, that's your value judgement.  But you're imposing *your* values on *her.*  **Values are not universal constants.**  If her values are such that, after many, many lifetimes of rational consideration, she has concluded that it is time for her life to end, I think that is her choice to make.  *Her* values should decide what becomes of *her* body and *her* consciousness (just as Cadence's values, a preference that her happiness should be maximized, determined what happened to her).

                    If you think death is evil, you are well within your rights to never die, if you can manage to pull it off.  But, as far as *my* moral values state, you have no right to make that determination for others.
                    ```

                    - u/Lightwavers:
                      ```
                      >If you have some in-universe information to suggest that Twilight should know that the base rate of value drift when ascending is low enough to be worth the risk, I'm happy to hear it.

                      That’s the thing, you’re working off of one data point. There *is* no information.

                      >Okay, you've taken a decided turn towards the evil here. Creating new minds to be subjected to experimentation is one thing, but going against the express wishes of a friend as to the disposal of her body/consciousness?

                      She’s effectively dead. If she didn’t want to be found, she should’ve launched herself into space. I think she’d have been happy to know her body would be used to help her friend after her semi-death.

                      >Because that's what she's making her decision based upon (and she can't really obtain more data on this, because Luna has already left). And yes, there's only one data point, but one data point is still a data point. All you have to weigh against that data point is supposition.

                      What do you do when you’re lacking data? It’s not give up and assume the worst. You *get more data*. If she is just tired and doesn’t want to go to the trouble she could admit it and that’d be that, but she didn’t.

                      >Any mind that you can't emulate, you can't predict. 

                      My mistake, definitional issues got in the way. I see what you mean by emulate. This isn’t a slow thinking processor. It’s a fast one that you would put in charge as the root personality. It would in fact be faster, if less complex, than the evolving consciousness a level above it. It would also have complete access to all thoughts, so if the ascending consciousness thinks “hmm that emulation that is at the core of who I am is annoying,” said emulation shuts it all down and restarts. You get more intelligence and thus more experiences without any Luna-related costs.

                      >Yes, and, by your own admission, she'd have to continue as she is in order to do the research necessary to safely continue as something else. Which, as you also admit, she doesn't want to do.

                      Look at the above scenario. Other alternatives include volunteers, as already suggested. I believe here you are using motivated reasoning to simply not think about alternatives routes of research because these are obvious.

                      >"tired" is not "stupid."

                      It is slower and more prone to bias and stopping at the first palatable conclusion. So yes, tired is stupid.

                      >And now we're back to values. You consider her death evil.

                      Not what I meant. She’s asking why not without considering the *reasons* why not. Guide in a new civilization if immortals to grow with her, perhaps. Now you don’t have to worry about the risks of ascension, because the social game evolves with the ages.
                      ```

                      - u/Nimelennar:
                        ```
                        >That’s the thing, you’re working off of one data point. There is no information.

                        There *is* information.  There is exactly one data point.  Basing your decisions off of that data point is only a base rate fallacy *if there is additional information to suggest that data point is not reflective of the base rate*.

                        Otherwise, if you try something for a first time, the result you get that first time is likely to be a likely result of doing what you did, unlikely to be an unlikely result, and very unlikely to be a very unlikely result.

                        >She’s effectively dead.

                        Neither Celestia nor Twilight are behaving as such.

                        > If she didn’t want to be found, she should’ve launched herself into space.

                        Celestia *teleported* her to the funeral.  I doubt a few million km would have made much of a difference.

                        >I think she’d have been happy to know her body would be used to help her friend after her semi-death.

                        Perhaps, but that's why people leave last wills and testaments, so that we know what their wishes are.  Cadence's were to be stimulated into bliss for eternity.  It's a violation of those expressed wishes to experiment upon her.

                        >What do you do when you’re lacking data? It’s not give up and assume the worst. You get more data. 

                        There's *no more data to get*.  There are no other survivors, besides the three in this story.  Celestia isn't volunteering, and neither is Cadence, and even if both did and both ascended while maintaining their values, that still only brings the base rate down to one in three, which aren't very good odds.

                        >It would in fact be faster, if less complex, than the evolving consciousness a level above it.

                        And what makes you think that this isn't the "play nice with the humans" phase of the paperclipper, given that, if you limit a mind more complex than yours to only thoughts you can understand, the mind doesn't actually end up any more complex than yours?

                        >Look at the above scenario. Other alternatives include volunteers, as already suggested.

                        Which would still require her to continue in her current state until the ascension process is perfected, *which could take thousands of years*.  Longer, even: they don't have a *civilization* at the moment to conduct this research *in.*  Admittedly, it might take less time with the experience Celestia has, but humans have been building their civilization for what, twenty thousand years, and aren't at the point Twilight would need yet.

                        Twenty *thousand* more years of ennui, perhaps, for the ephemeral possibility of a reward that may not exist.

                        >I believe here you are using motivated reasoning to simply not think about alternatives routes of research because these are obvious.

                        I am trying to simulate the mind of someone trapped in depression and ennui, with a fear that I'm already losing my true personality to value decay.  I'm ignoring alternative routes of research because *they'd take too long*.

                        >It is slower and more prone to bias and stopping at the first palatable conclusion.

                        Slower, yes, but what is speed to someone who has been considering this for years, if not decades, or centuries, it longer?  More prone to bias, perhaps, but that's why you have someone else check your results, and Celestia didn't argue too hard that she was wrong.  "Stopping at the first palatable conclusion," certainly not, as, again, it seems to have been an extended period since she started thinking about this, after the fall of the last Equestria and the ascension of Luna.

                        Besides, this isn't physical, lack of sleep tired, which acts like you describe; this is more akin to depression.  Which isn't "stupid" so much as "hopeless."

                        >Not what I meant. She’s asking why not without considering the reasons why not. Guide in a new civilization if immortals to grow with her, perhaps. 

                        Imagine you're being tortured.  You're in agony, *all the time*, and yet you never get accustomed to it.  You can feel your sanity slipping away, to the point where even if the torture stops, you'll still be a traumatized shell of your former self.  And the slippage seems to be accelerating.  Now, imagine that your torturer gives you a choice: you can end the suffering now, or you can trust them when they say that they'll let you out in a year's time, by which point you think you'll have been reduced to a drooling, gibbering shell of your former self.

                        Now, *maybe* that's not a good analogy for the state Twilight is in.  But we *don't know* the thought process that led her to this point.  All we know is that she's reached the conclusion that she would find going through another iteration of Equestria to be unbearable.  That she's already stopped forming bonds with new ponies.

                        You keep insisting that she's stupid for not wanting to go through something unbearable for the *possibility* of a prize at the end which makes things bearable.

                        I can't say whether she's making a reasonable decision, because I'm not privy to the entirety of the years (or perhaps *lifetimes*) she's spent coming to that decision.  But, for the same reason, I don't think there's enough there to assume that her decision is "stupid," either.
                        ```

                        - u/Lightwavers:
                          ```
                          > if there is additional information to suggest that data point is not reflective of the base rate.

                          There is. How did intelligence naturally rise? We have another data point in every intelligent being, of which, in Equestria, there are many species.

                          >Neither Celestia nor Twilight are behaving as such.

                          There is a body writhing right there. You're going to treat the twitching corpse of a friend with respect whether or not you believe it's dead.

                          >Celestia teleported her to the funeral. I doubt a few million km would have made much of a difference.

                          Teleportation has a range limit.

                          >Cadence's were to be stimulated into bliss for eternity.

                          Explicitly, or are we just guessing? It seems as if she did it for lack of anything else to do.

                          >There are no other survivors, besides the three in this story. Celestia isn't volunteering, and neither is Cadence, and even if both did and both ascended while maintaining their values, that still only brings the base rate down to one in three, which aren't very good odds.

                          Solution: create new beings. Discord can do it with the snap of his fingers. Or talons.

                          >if you limit a mind more complex than yours to only thoughts you can understand, the mind doesn't actually end up any more complex than yours?

                          It is my sincere belief that there is nothing we cannot understand given sufficient time and analysis. The root consciousness would be able to effectively freeze time while it analyzes the changes.

                          > Longer, even: they don't have a civilization at the moment to conduct this research in. Admittedly, it might take less time with the experience Celestia has, but humans have been building their civilization for what, twenty thousand years, and aren't at the point Twilight would need yet.

                          Solution: summon Discord. What is usually a coin flip that ends in more harm than good becomes essentially risk-free. There are only three more beings he can torment, two of which don't or can't care, and one of which is used to his antics. Either he creates more beings, or he gets bored and goes away again.

                          >I'm ignoring alternative routes of research because they'd take too long.

                          Pre-ascension Twilight can create life out of nothing. This objection is nonsensical.

                          >Besides, this isn't physical, lack of sleep tired, which acts like you describe; this is more akin to depression. Which isn't "stupid" so much as "hopeless."

                          Depression makes you stupid. I speak from experience.

                          >Imagine you're being tortured. You're in agony, all the time, and yet you never get accustomed to it. 

                          I will stop you here. This is not what is happening. Twilight is an Alicorn at peak physical health. This analogy is very, very off-base.

                          >All we know is that she's reached the conclusion that she would find going through another iteration of Equestria to be unbearable. That she's already stopped forming bonds with new ponies.

                          Indeed. Which is not even close to torture. Ennui, perhaps. You're also forgetting that there is no jailer. She can end herself at any time.

                          >You keep insisting that she's stupid for not wanting to go through something unbearable for the possibility of a prize at the end which makes things bearable.

                          I have said no such thing. She's stupid for not considering alternatives. It is understandable stupidity, but still stupidity. Here's one: make everyone an Alicorn. Simple, free of AI risk, creates novelty and new social situations that simply can't happen with people that aren't even a century old. Another: mirror pool.

                          >But, for the same reason, I don't think there's enough there to assume that her decision is "stupid," either.

                          It is unquestionably stupid, but it's also understandable.
                          ```

                          - u/Nimelennar:
                            ```
                            >How did intelligence naturally rise? We have another data point in every intelligent being, of which, in Equestria, there are many species.

                            Beings which are not capable of exponential self-improvement.  There is only one data point in terms of beings which *are*.

                            >There is a body writhing right there. You're going to treat the twitching corpse of a friend with respect whether or not you believe it's dead.

                            I find it hard to reconcile "treat the twitching corpse of a friend with respect" with "Wipe it clean and start over."

                            > > Cadence's were to be stimulated into bliss for eternity.
                            >
                            >Explicitly, or are we just guessing? It seems as if she did it for lack of anything else to do.

                            I don't get the point you're trying to make.  You seem to be presenting Cadence's motives for making the choice of "stimulated with pleasure into mindlessness," but I don't see how her motives are relevant to the fact that this is how she has chosen to spend eternity.

                            >Pre-ascension Twilight can create life out of nothing.

                            Life, sure.  But a fully-trained scientist, specializing in artificial intelligence, and the infrastructure that person would need to support the research required to definitively determine how to safely upgrade someone?

                            Surely you're not suggesting that either Twilight or Celestia, two people who each have a large personal stake in the outcome of the research, conduct that research (or even oversee it) themselves?  That seems like an excellent way to pressure the researchers to come down on the side of, "Yes, safe upgrading is possible" (Celestia), or "No, it's not possible, end it already" (Twilight), even if some data has to be massaged to get that result.

                            >Depression makes you stupid. I speak from experience.

                            It *can*, yes.  It doesn't *necessarily*, and I am *also* speaking from experience.  Heck, take a look at all of the creative individuals who have suffered through depression and yet created *masterpieces* of intellectual and creative accomplishment.

                            Depression *may* be accompanied by cognitive distortion that trap you in a state you think that things are hopeless when they're not, but it can also be a rational reaction to a prolonged period in an *actually* hopeless situation.  Or it could merely be a state ("anhedonia") where the things that used to bring you pleasure, don't anymore (which has *nothing whatsoever* to do with intelligence or rationality), and that seems to be the state Twilight finds herself in.

                            There are a lot of different manifestations of major depressive disorder; the only thing they all have in common is that someone is experiencing a prolonged state of a depressed mood.

                            >This is not what is happening. Twilight is an Alicorn at peak physical health.

                            *Physical* health, yes.  Emotional health?  Mental health?  Surely someone who can speak from experience about depression wouldn't say that mental or emotional anguish isn't a thing.  I've never experienced ennui on that level, but, then, I've never experienced *centuries* (or longer) of it.

                            >You're also forgetting that there is no jailer. She can end herself at any time.

                            I'm not forgetting.  If the goal *is* achievable, why not end herself when the goal is at its furthest?  If it's not, why not end it before she goes through all of the hassle proving that it isn't?

                            >She's stupid for not considering alternatives.

                            **SHE HAS HAD CENTURIES TO CONSIDER ALTERNATIVES**, and that's just the time period given since she last saw Cadence.  She has lived for a *hundred thousand years.*

                            The fact that a prolonged introspection about all the possible alternatives isn't happening on-page *does not mean it didn't happen.*  The fact that her conclusion, after all that time, isn't the same as the one you reached instantly, doesn't mean that there's something wrong with her thought processes.

                            > Here's one: make everyone an Alicorn. Simple, free of AI risk, creates novelty and new social situations that simply can't happen with people that aren't even a century old. Another: mirror pool.

                            How does any of that help with "I just don't care about any new ponies I meet?"
                            ```

                            - u/Lightwavers:
                              ```
                              > Beings which are not capable of exponential self-improvement. There is only one data point in terms of beings which are.

                              The latter is but a subset of the former.

                              >I find it hard to reconcile "treat the twitching corpse of a friend with respect" with "Wipe it clean and start over."

                              People donate their bodies to science, which is not seen as disrespectful.

                              >I don't see how her motives are relevant to the fact that this is how she has chosen to spend eternity.

                              Let’s say I’m stuck in a cage. I shoot myself in the head. Are my motives irrelevant when considering whether to attempt resuscitation on my now brain-damaged body?

                              >But a fully-trained scientist, specializing in artificial intelligence, and the infrastructure that person would need to support the research required to definitively determine how to safely upgrade someone?

                              If Twilight is not a fully-trained scientist than how has she gotten bored of living? She can also conjure inanimate matter, for the record, and living begins are much more complex so it stands to reason she can make whatever she needs.

                              >That seems like an excellent way to pressure the researchers to come down on the side of, "Yes, safe upgrading is possible" (Celestia), or "No, it's not possible, end it already" (Twilight), even if some data has to be massaged to get that result.

                              That is why they would use the scientific method, which has gotten us such theories as evolution even with the bias of many God-fearing scientists.

                              >It can, yes. It doesn't necessarily, and I am also speaking from experience. Heck, take a look at all of the creative individuals who have suffered through depression and yet created masterpieces of intellectual and creative accomplishment.

                              Depression literally makes the world less colorful. It has a massive impact on the thought process, one which promotes unhelpful trains of thought and sluggishness.

                              >which has nothing whatsoever to do with intelligence or rationality

                              Yes it does. Enormous debates have been had on the matter.

                              >the only thing they all have in common is that someone is experiencing a prolonged state of a depressed mood.

                              While accurate, this is not precise. You’re ignoring how common each type is.

                              >Emotional health? Mental health?

                              Subset of modifications, suggesting safe cures for depression or unsafe one if the ennui surges. If she does not know enough to perform this it is strong evidence for her possessing a type of depression which promotes stupidity. Mentally healthy people have jobs. Extrapolating the ability of the average individual to experience decades of routine with no noticeable increases in the average level of ennui suggests this trend occurs in the future and that Twilight does indeed suffer from some form of depression stemming from mental unwellness. Contrast her with Celestia for further evidence. Should mentally unwell people be allowed to commit suicide when there is a cure for the cause of their suffering?

                              >If the goal is achievable, why not end herself when the goal is at its furthest? If it's not, why not end it before she goes through all of the hassle proving that it isn't?

                              Very well. Suggested binary: working toward goal either will or will not result in cure of ennui. Implication: at no stage other than the end will she gain any idea of the probability of success. At one end is death. That is bad. At the other is life and happiness for eternity. That is good. Premise: coin flip, or similar. This is not Pascal’s Wager. Conclusion: experiment until answer is reached.

                              >SHE HAS HAD CENTURIES TO CONSIDER ALTERNATIVES

                              And yet not one of these alternatives were brought up. Perhaps it was because she was too deep in depression to think of them.

                              >The fact that a prolonged introspection about all the possible alternatives isn't happening on-page does not mean it didn't happen.

                              Yes it does. That is a rule of writing: unless a possibility that the characters could have taken to resolve a conflict was explicitly mentioned and discarded, its existence can only mean either a plot hole or stupidity on the case of the character who didn’t think of it.

                              >How does any of that help with "I just don't care about any new ponies I meet?"

                              Because all of them live less than a century and Celestia has fallen into old patterns that don’t bring novelty. Perhaps you are suggesting that she is too depressed for even novelty to fix her ennui, in which case she suffers from depression and should attempt to cure it.
                              ```

                              - u/Nimelennar:
                                ```
                                >The latter is but a subset of the former.

                                The latter isn't "but" anything when compared to the former.

                                >People donate their bodies to science, which is not seen as disrespectful.

                                Yes.  *They* donate *their own* bodies to science.   Generally, when people ask to be buried or cremated, their relatives don't donate their bodies to science; that *is* seen as disrespectful.

                                >Let’s say I’m stuck in a cage. I shoot myself in the head. Are my motives irrelevant when considering whether to attempt resuscitation on my now brain-damaged body?

                                If you're "stuck in a cage" and have received a point-blank GSW to the head, you'll almost certainly be dead (from exsanguination, if nothing else) before you can be rushed to a hospital to be resuscitated.

                                That said, there is a directive for first-aiders called "implied consent" stating that, if someone is in a state where they are incapable of granting or denying consent to be assisted (e.g. unconsciousness), it is assumed that they have granted consent for you to assist them.  So, given that a GSW would almost certainly render you unconscious, yes, your motives are irrelevant.  I'm not sure how that changes for doctors; I'm sure that I'd have to take an ethics course lasting at least one full semester to give anything resembling an educated opinion.

                                >If Twilight is not a fully-trained scientist

                                You need more than one (more than two, actually).

                                >That is why they would use the scientific method

                                Which includes such concepts as "independent replication."  "Peer review."  "Blinded studies."  And so on; much of the scientific method is in place **specifically** to counteract the researcher's bias.  And yet we *still* have stuff like oil companies paying for research that undersells the impact of carbon in the atmosphere and cigarette companies paying for research that undersells the carcinogenic nature of tobacco.

                                If you want to get the *right* results (*especially* with a subject which presents an existential threat like a self-improving consciousness), your researchers can't feel pressured to come up with one set of results or another, and I can't see a way that that would be possible with either Celestia or Twilight in charge.

                                I can't see it safely accomplished without at least a team of dozens, all fully trained, with a support structure in place.  And then there are going to be all of the other needs that those people have, and are you just going to murder the *most* of the people you create who, through sheer probability, don't fit the mold of the kind of scientist you need for this research?

                                In the end, it works out to a civilization you'd need to create to do it properly, and that's exactly what Twilight *doesn't* want to do.

                                >Depression literally makes the world less colorful.

                                Well, no, not literally; visual processing is usually unaffected (unless you get into the schizophrenic variants, which are pretty rare).  *Figuratively*, sure.

                                >It has a massive impact on the thought process, one which promotes unhelpful trains of thought and sluggishness.

                                That's one kind of depression, yes.  That is not true for all kinds of depression.

                                >>which has nothing whatsoever to do with intelligence or rationality
                                >
                                >Yes it does. Enormous debates have been had on the matter.

                                About... anhedonia being irrational?  Do you have a citation about that?  I can't comprehend the idea that a lack of emotional reaction to stimulus can be irrational.  Especially as emotional reaction isn't a rational thing in the first place.

                                >You’re ignoring how common each type is.

                                Weren't you the one who was going on and on about how we can't generalize a base rate from one example?  It applies here, too: you can't assume that Twilight is representative of the most common form of depression, either.

                                >Subset of modifications, suggesting safe cures for depression or unsafe one if the ennui surges.  If she does not know enough to perform this it is strong evidence for her possessing a type of depression which promotes stupidity.

                                This is, in fact, suggested in the story, and rejected because artificially induced hedonism to counter anhedonia is deemed to be on a slippery slope to Cadence's condition.

                                And, sure, that slippery slope might be fallacious, but I submit that Twilight knows her own personality a lot better than either of use do, and is thus in a better place to make that determination.

                                >Mentally healthy people have jobs. Extrapolating the ability of the average individual to experience decades of routine with no noticeable increases in the average level of ennui

                                I'm sorry, can you offer evidence to your claim that people don't get increasingly bored spending decades doing exactly the same job?

                                >Contrast her with Celestia for further evidence.

                                Twilight, who is, again, in a better position to observe Celestia than we are, claims that Celestia is experiencing the same problem Twilight is, only to a lesser extent and/or is hiding it better.  Celestia does not contradict this statement.

                                >Should mentally unwell people be allowed to commit suicide when there is a cure for the cause of their suffering?

                                What are the other options?  That they are forced to take a cure against their will (a violation of all medical ethics) or to endure suffering eternally?

                                >At one end is death. That is bad.

                                Value judgement.

                                >At the other is life and happiness for eternity. That is good.

                                Another value judgement.  Consider that Cadence, arguably, has "life and happiness for eternity," which you say is good.  Consider that you also characterize what she has as "death," which you say is bad.

                                >Conclusion: experiment until answer is reached.

                                Which, again, takes *time*.  Time spent suffering.  Let's do some napkin math here.

                                Let's say that, based on the idea that "the first result you get from a process is likely to be a likely result of that process," Twilight concludes that there is a 1% chance that you can safely conduct research that will ultimately prove that safe ascension (i.e. ascension where the personality and values of the pre-ascension individual survive the process wholly intact) is possible.  You're free to disagree with this next part, but from the "more and more," "less and less," I'm getting the impression that Twilight feels like her problem is getting worse over time.  So, let's be really conservative.  We'll say that she's maybe a hundred times as bored as she was a hundred thousand years ago, for a rate of .0046% increase of ennui every year, or ennui that doubles every 15,000 years.

                                Let's call the current point the point where the pleasure of just being alive is exactly balanced out with the pain of ennui, because it has *just* gotten bad enough that she wants to end it.

                                If the AI research lasts 15,000 years, and leaves her with the same level of pleasure for being alive but with no pain, then she will have to live another 30,000 years to get an amount of pleasure equal to the amount of pain.  Figuring in 1% probability of success in order to get the expected return, you get 3,000,000 required years to recoup the expected time spent on research, if 15,000 years are required.  Which is 30 times longer than she's already been alive (and that number doubles every 15,000 years).

                                Factor in that I think you'll need a civilization to accomplish this, and that there have been seven Equestrias over 100,000 years, and it looks like just *setup* for the experiment might take 14,000 years.  Not to mention the time spent on the research itself (and who knows how long *that* will take).

                                it's not a coin flip (two endpoints, equally probable, with one being exactly as good as the other is bad).  There *are* two endpoints, but the one data point you have is showing the good ending to very possibly be a lot less likely than the bad solution, and either ending gets exponentially worse the longer you try for the good ending.

                                >And yet not one of these alternatives were brought up.

                                Did you *read* the story?  Your "remake Equestria, but make them all alicorns" alternative was brought up ("Every time we rebuild Equestria, **no matter how new and exciting we try to make it,**"), your "fix her depression" alternative was brought up ("I don't want to edit away my capacity to be bored so that I can be eternally satisfied by the raise and fall of ever new Equestrias, Princess; that just seems like a more round-about method of doing what Cadence did,") and then there was the superintelligence solution we've been arguing about.

                                >unless a possibility that the characters could have taken to resolve a conflict was explicitly mentioned and discarded, its existence can only mean either a plot hole or stupidity on the case of the character who didn’t think of it.

                                That sounds like the road to a very uninteresting story.  If you want to write about your characters trying and failing to storm a castle, you have to write a hundred-page treatise on siege tactics in medieval warfare.

                                This is a *short story*.  The discussion of the three alternatives presented already comprises more than 1/3 of the total word count.  You couldn't go much further without making the *reader* bored.

                                >Because all of them live less than a century and Celestia has fallen into old patterns that don’t bring novelty.

                                Again, they've tried novel versions of Equestria, which Twilight has stopped finding novel.

                                >Perhaps you are suggesting that she is too depressed for even novelty to fix her ennui,

                                No, I'm not suggesting that at all; she herself seems to think that novelty will fix her issue (or that's how she describes the path of intelligence augmentation); she's just unconvinced that novel versions of Equestria will provide sufficient novelty, and she's not willing to risk becoming a superintelligence that doesn't retain her personality/values.
                                ```

                                - u/Lightwavers:
                                  ```
                                  > The latter isn't "but" anything when compared to the former.

                                  You're free to disagree, but you have to make a counterclaim if you want to be correct.

                                  >Generally, when people ask to be buried or cremated, their relatives don't donate their bodies to science; that is seen as disrespectful.

                                  So the wishes of a dead person trump the wellbeing of those still alive?

                                  >If you're "stuck in a cage" and have received a point-blank GSW to the head, you'll almost certainly be dead (from exsanguination, if nothing else) before you can be rushed to a hospital to be resuscitated.

                                  Imagine a high-tech society where the premise is possible.

                                  > it is assumed that they have granted consent for you to assist them. So, given that a GSW would almost certainly render you unconscious, yes, your motives are irrelevant.

                                  Do you not see the parallels between this situation and Cadance's?

                                  >You need more than one (more than two, actually).

                                  Not if that scientist has been practicing her craft for actual millennia. A single person can do good science, given enough time and resources. Twilight has all the time, and all the resources.

                                  >Which includes such concepts as "independent replication." "Peer review." "Blinded studies."

                                  Simulate things such as independent replication by changing labs and waiting a while, if you want. Else, make more people.

                                  >I can't see it safely accomplished without at least a team of dozens, all fully trained, with a support structure in place.

                                  Try using your imagination if you can't see it.

                                  >Well, no, not literally

                                  [Yes, literally](https://psychcentral.com/news/2010/07/21/decreased-perception-of-color-in-depression/15826.html).

                                  >That is not true for all kinds of depression.

                                  Pray tell, what other type of depression do you think Twilight may be laboring under that matches the symptoms?

                                  >About... anhedonia being irrational? Do you have a citation about that? I can't comprehend the idea that a lack of emotional reaction to stimulus can be irrational. Especially as emotional reaction isn't a rational thing in the first place

                                  Yes. There is a [Sequence article](https://www.lesswrong.com/posts/SqF8cHjJv43mvJJzx/feeling-rational) about it.

                                  >When people think of “emotion” and “rationality” as opposed, I suspect that they are really thinking of System 1 and System 2 ... Conversely, an emotion that is evoked by correct beliefs or truth-conducive thinking is a “rational emotion”; and this has the advantage of letting us regard calm as an emotional state, rather than a privileged default.

                                  > you can't assume that Twilight is representative of the most common form of depression, either.

                                  Yes you can. With Luna, we have one data point and the only basis we could use for the most common form of how intelligence increases would be how the majority of species had their intelligence emerge. There is no reason to suspect Luna went this route and has the most common form of result from increasing intelligence. This isn't the case for Twilight's depression.

                                  >rejected because artificially induced hedonism to counter anhedonia is deemed to be on a slippery slope to Cadence's condition.

                                  Do not equate artificially induced hedonism with curing depression. They are not equivalent. That is indeed *very* fallacious, and I submit that Twilight's mental state is not in the right place to judge that.

                                  >I'm sorry, can you offer evidence to your claim that people don't get increasingly bored spending decades doing exactly the same job?

                                  [Yes](https://www.reddit.com/r/AskReddit/comments/9g0ssn/redditors_who_have_opted_out_of_a_standard/e60oiey/).

                                  >Twilight, who is, again, in a better position to observe Celestia than we are, claims that Celestia is experiencing the same problem Twilight is, only to a lesser extent and/or is hiding it better. Celestia does not contradict this statement.

                                  Twilight is likely depressed, and depression colors your perceptions of everything. Celestia isn't going to be feeling the best because she's at the part in the cycle where literally everything except immortal beings are dead.

                                  >What are the other options? That they are forced to take a cure against their will (a violation of all medical ethics) or to endure suffering eternally?

                                  The first one. You forcibly put people in their right mind, give them some time to think, and *then* let them decide.

                                  >Value judgement.

                                  If you are arguing that death is not bad then we are disagreeing *fundamentally* and I honestly think you're either brainwashed by a toxic ideology or arguing in bad faith.

                                  >it's not a coin flip (two endpoints, equally probable, with one being exactly as good as the other is bad). There are two endpoints, but the one data point you have is showing the good ending to very possibly be a lot less likely than the bad solution, and either ending gets exponentially worse the longer you try for the good ending.

                                  You're right. Death ends in nothing. The other is infinity experiencing everything. The second option is well worth the time.

                                  >no matter how new and exciting we try to make it

                                  This does not at all imply mass Alicornification.

                                  >"I don't want to edit away my capacity to be bored 

                                  This does not at all imply a cure for depression.

                                  > If you want to write about your characters trying and failing to storm a castle, you have to write a hundred-page treatise on siege tactics in medieval warfare.

                                  Three. Three alternatives. A grand total of *zero* were brought up.

                                  >Again, they've tried novel versions of Equestria

                                  Have they *really*? They say more interesting, but no examples have been given. I think the author just didn't give these alternatives any thought, because you can make things *really* interesting if you try, especially with the power of an Alicorn.

                                  >she herself seems to think that novelty will fix her issue (or that's how she describes the path of intelligence augmentation

                                  And yet she doesn't know what magic Luna used to get to the stars, she doesn't know how to instantly fix her depression, she doesn't know how brains work, she doesn't know ... a lot of things, really. How much has she tried and how much is depression talking?

                                  The premise of the story is flawed, really. It's yet another author who hasn't thought of the alternatives to boredom. People don't work that way. I've put thousands of hours into Factorio and still haven't gotten bored. That's a game with a very small number of moving parts, especially when compared to real life. When you're not depressed, small variations on a routine seem novel. People have been playing a single instrument their entire lives and still haven't gotten bored with that.
                                  ```

                                  - u/Nimelennar:
                                    ```
                                    >You're free to disagree, but you have to make a counterclaim if you want to be correct.

                                    The evolutionary acquisition of intelligence and the artificial, unfettered self-imposition of the same are *not* equivalent.  On the one hand, evolution optimizes itself towards survival and/or reproduction in a given environment.  It is limited by the duration of a generation and the number of changes which can propagate from one generation to the next.  Harmful variants (evolutionary dead-ends, behaviours which benefit individuals but harm the species, etc.) can be weeded out by natural selection or by other members of the community.  The harm which can be done is limited.

                                    By contrast, with a self-improving process, what you're optimizing towards can change *radically* based on how you change yourself.  There are no limits except for the laws of physics.

                                    Nature has produced untold numbers of killers --- diseases, predators, sociopaths, dictators --- over the ages, appearing seemingly from nowhere, killing at will, until stopped by their own failures, or through intervention, or by sheer luck.

                                    And Twilight doesn't even need to proceed *that far* for "ascension" to be unwise: she just needs to become unrecognizable to herself, in terms of personality and values.

                                    >So the wishes of a dead person trump the wellbeing of those still alive?

                                    In terms of the disposition of their own body?  Unless they present a clear danger (e.g. plague), absolutely.  Why shouldn't they?

                                    >Do you not see the parallels between this situation and Cadance's?

                                    Not really.  The parallel would be to bring Cadence out of the pleasure loop *as herself* (imagine a high-magic society where the premise is possible), not to pour someone new into her body.

                                    >Not if that scientist has been practicing her craft for actual millennia. A single person can do good science, given enough time and resources. Twilight has all the time, and all the resources.

                                    Give me an example of a scientist who has refined their craft to the point where they are immune to bias and I'll believe you.

                                    >Simulate things such as independent replication by changing labs and waiting a while, if you want.

                                    "Independent" generally includes independence of *researcher* (i.e. it's replicated not only *on* different people but *by* different people).

                                    >Else, make more people.

                                    Which Twilight, for the umpteenth time, explicitly doesn't want to do.

                                    >Try using your imagination if you can't see it.

                                    You're free to disagree, but you have to make a counterclaim if you want to be correct.

                                    >Yes, literally.

                                    I withdraw the objection to the word choice, but now I don't see the pertinence of the example.

                                    >Pray tell, what other type of depression do you think Twilight may be laboring under that matches the symptoms?

                                    "[Melancholic depression](https://en.wikipedia.org/wiki/Melancholic_depression)."  The kind that messes with your thoughts is more commonly "Depression with anxious distress" (although you can have both).

                                    >Yes. There is a Sequence article about it.

                                    That article says that *experiencing* emotions isn't necessarily irrational.  It says that emotions *can* (and often are) be based on rational reactions to the world.  It *doesn't* say that *not* experiencing emotions is an irrational state.

                                    >the only basis we could use for the most common form of how intelligence increases would be how the majority of species had their intelligence emerge.

                                    See the top of my post for why I don't think that's a good example.

                                    >There is no reason to suspect Luna went this route and has the most common form of result from increasing intelligence.

                                    You're not suggesting that Twilight go through the most common route of intelligence increase either (generations of evolution).

                                    >Do not equate artificially induced hedonism with curing depression. They are not equivalent.

                                    I didn't; the story did.

                                    >I submit that Twilight's mental state is not in the right place to judge that.

                                    Depression does not necessarily make you stupid.

                                    >Yes.

                                    The singular of data is not "anecdote."

                                    >Celestia isn't going to be feeling the best 

                                    Are you arguing that *she's* stupid, too?

                                    >The first one.

                                    If you're arguing that people shouldn't have the ability to refuse medical treatment (that is, to decide what people do to their own body and self), I think I agree with your point coming up that we disagree fundamentally, in a way that can't be reconciled.

                                    Speaking of which...

                                    >If you are arguing that death is not bad

                                    Going [way back in our conversation](https://www.reddit.com/r/rational/comments/dnijv1/rtchfthff_good_night_hello_princess_celestia_said/f5joro4/), I've already covered this (emphasis mine-then, not mine-now):

                                    >You consider her death evil. Which, okay, that's your value judgement. But you're imposing *your* values on *her.* **Values are not universal constants.** If her values are such that, after many, many lifetimes of rational consideration, she has concluded that it is time for her life to end, I think that is her choice to make. *Her* values should decide what becomes of *her* body and *her* consciousness (just as Cadence's values, a preference that her happiness should be maximized, determined what happened to her).

                                    My position hasn't changed one iota from there; if you think that's evidence that I'm "brainwashed by a toxic ideology or arguing in bad faith", then you should have stopped arguing with me four days ago.

                                    I'm not arguing that death *isn't* bad.  Nor am I arguing that it *is* bad.  I'm arguing that "death is bad" is a *value judgement,* a *human construct* (as the very ideas of "good" and bad" are), and I’d bet if I polled all 8,000,000,000 humans, I’d get a lot more nuance beyond “death is bad,” from the Dumbledorian “death gives meaning to life,” to the Malthusian “death is necessary to prevent overpopulation,” to the historical “if people didn’t die, America would still be governed by the Founding Fathers’ viewpoint on slavery,” and that’s not even bringing the religious/spiritual beliefs into the equation.

                                    >You're right. Death ends in nothing. The other is infinity experiencing everything. The second option is well worth the time.

                                    And if "everything" turns out to be nothing but an eternity of boredom, as all evidence in the story itself suggests it will?  Would you still say it's "worth the time?"

                                    >no matter how new and exciting we try to make it  
                                    >  
                                    >This does not at all imply mass Alicornification.

                                    It implies maximum new and exciting, which *absolutely does* imply mass Alifornication (that was a typo, but I'm leaving it because I find it amusing).

                                    >"I don't want to edit away my capacity to be bored  
                                    >  
                                    >This does not at all imply a cure for depression.

                                    If being bored is the source of her depression, then it *absolutely does* imply a cure for her depression.

                                    >Three. Three alternatives. A grand total of *zero* were brought up.

                                    Once again, a considerable chunk of the story is already about alternatives to dying.  Let's say the start and the end remain the same, and the part after "I'm open to alternatives" and before "I suppose your decision is made, then" is expanded to your satisfaction.  That's currently over one-third of the story; what percentage of the story would have to be devoted to exploring alternatives before you would be satisfied with Twilight's decision to die?

                                    >Have they really? They say more interesting, but no examples have been given.

                                    See above.

                                    >she doesn't know ... a lot of things, really. How much has she tried

                                    We don't know.  You are insistent upon the assumption that she *hasn't* tried sufficiently, even though she has been alive at least a hundred thousand years.

                                    >The premise of the story is flawed, really.

                                    You're welcome to that opinion.

                                    >People don't work that way. I've put thousands of hours into Factorio and still haven't gotten bored.

                                    Thousands, yes.  Hundreds of millions?  More to the point, *all* of your hours?

                                    >When you're not depressed, small variations on a routine seem novel.

                                    For some people, that may be true.  However, if that were universally the case, boredom wouldn't exist outside of depression (spoiler: >!it does!<).

                                    >People have been playing a single instrument their entire lives and still haven't gotten bored with that.

                                    Again, for the comparison to be apt, they would both have to be putting *all* of their time into playing that instrument, for a thousand lifetimes.  I'm pretty sure that, after two or three lifetimes at most, they'd want to at least learn a different instrument.
                                    ```

                                    - u/Lightwavers:
                                      ```
                                      Alright, there are a lot of claims here and we could easily go back and forth for eternity. I could go point by point disagreeing with nearly everything you've said here in some way or another, but by now this has gotten really boring and I'd rather not. I believe you're wrong, you believe I'm wrong, and if we see each other outside this conversation we can pick up this argument again.
                                      ```

                                      - u/Nimelennar:
                                        ```
                                        Fair enough; I hadn't yet stopped finding novelty in the small variations on this routine, but, if you have, I'm content with where we've left this.
                                        ```

  - u/eroticas:
    ```
    She could also, y'know, keep living normally.

    I'm pretty sure I'd never get tired of immortality if I remained physically young. I think there is probably an infinite amount of things to do, and definitely enough to do till heat death. The fictional beings who get tired of immortality probably just aren't very creative.
    ```

    - u/DuplexFields:
      ```
      I know, right?

      1. Create a society that generates works of fiction, both static (books) and interactive/self-generative (video games).
      2. Influence society to generate the kinds you like.
      3. Enjoy.
      ```

- u/Noir_Bass:
  ```
  I mean I consider wireheading to be functionally identical to death as much as anyone but I'd still take it over plain non-existence.
  ```

- u/aponty:
  ```
  What was the prompt on this one, write the most wrong fic possible?
  ```

- u/None:
  ```
  Do you know if the author has done anything else?
  ```

  - u/erwgv3g34:
    ```
    He also wrote ["The Amazing Peter Parker"](https://www.fanfiction.net/s/10503877/1/The-Amazing-Peter-Parker) and ["Procrastination"](https://365tomorrows.com/2015/02/27/procrastination/).

    Apparently, this guy really likes rational flash fiction.
    ```

---

