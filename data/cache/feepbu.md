## [D] Friday Open Thread

### Post:

Welcome to the Friday Open Thread! Is there something that you want to talk about with /r/rational, but which isn't rational fiction, or doesn't otherwise belong as a top-level post? This is the place to post it. The idea is that while reddit is a large place, with lots of special little niches, sometimes you just want to talk with a certain group of people about certain sorts of things that aren't related to why you're all here. It's totally understandable that you might want to talk about Japanese game shows with /r/rational instead of going over to /r/japanesegameshows, but it's hopefully also understandable that this isn't really the place for that sort of thing.

So do you want to talk about how your life has been going? Non-rational and/or non-fictional stuff you've been reading? The recent album from your favourite German pop singer? The politics of Southern India? Different ways to plot meteorological data? The cost of living in Portugal? Corner cases for siteswap notation? All these things and more could (possibly) be found in the comments below!

Please note that this thread has been merged with the Monday General Rationality Thread.

### Comments:

- u/None:
  ```
  When my post is 11 hours old, the folks from [Deep in Pact](https://www.doofmedia.com/deep-in-pact/), the We've Got Worm-style podcast analysis of [Pact](https://pactwebserial.wordpress.com/), will start the recording of their last episode, where they discuss the epilogue. 

  Right afterwards, they'll begin a 24 hour livestream called [All Pact Up](https://www.doofmedia.com/2020/02/04/all-pact-up-details/), which will be livestreamed on their [twitch](https://www.twitch.tv/doofmedia), where they'll be collecting donations for a homeless kids charity.

  Their 24 hour marathon willl include fun stuff like a short game of Pactdice, live writing, fanart and fanfic reviews, Tarot readings, and a live interview with Wildbow himself. This is probably the first time Wildbow has ever done an audio interview, so I'm really looking forward to it.
  ```

  - u/Roxolan:
    ```
    So you're saying I have ~2 hours to read Pact if I want to hear a live Wildbow interview without risks of spoilers ðŸ¤”
    ```

- u/DangerouslyUnstable:
  ```
  I got drawn into yet another dumb argument about the teleporter problem (in case you aren't familiar, this is the idea that teleporters kill you and create an identical copy at the destination) in yesterdays post on the front page post about teleporters. 

  I'm curious about how the people in this community see it. Here is my view:

  Any coherent definition of life/identity that does not require a soul or other non-physical component will lead to one of 2 conclusions: either teleportation is not death or else death happens many times to every individual to the point that it is not something worth worrying about. 

  I come to this conclusion via the following:

  The main two arguments about why teleportation *would* be death is the fact that it isn't the same physical atoms/molecules and there isn't continuity of consciousness.

  However, physical atoms are being replaced constantly within the human body and I am sure there is complete turnover multiple times over the course of someones life. Under this definition, everyone is slowly dying all the time and will die multiple times across their lifetime.

  As for continuity of consciousness, there is decent evidence that consciousness is not continuous under any circumstances and that the appearance is a lie told by our brains. But even ignoring that (it certainly isn't proven), continuity is broken by comas, anesthesia, and, arguably, sleep. Meaning that, in the worst case scenario, continuity is broken multiple times every single day and under the best case scenario, almost everyone in modern society will experience it at least once or twice over the span of their existence. 

  I assume this community has a higher incidence of people who have thought deeply about this (probably much more so than I have) and would love any thoughts.
  ```

  - u/paradoxinclination:
    ```
    >However, physical atoms are being replaced constantly within the human body and I am sure there is complete turnover multiple times over the course of someones life. Under this definition, everyone is slowly dying all the time and will die multiple times across their lifetime.

    Personally, I think the rate of change is relevant. If you're okay with your personality slowly shifting over time, the same thing happening to your body is probably acceptable. But similar to how you wouldn't want your personality completely changing overnight, I wouldn't want my brain being replaced all at once, even if it was with an identical copy. 

    >As for continuity of consciousness, there is decent evidence that consciousness is not continuous under any circumstances and that the appearance is a lie told by our brains. But even ignoring that (it certainly isn't proven), continuity is broken by comas, anesthesia, and, arguably, sleep.

    I'd disagree with sleep being a break in continuity of consciousness, since your brain is still active to some degree. But yes, under my current theory of consciousness going into a coma or being put under via anesthesia would effectively kill me. 

    Also, the fact that many people would already be dying from the use of anesthetics isn't necessarily an argument against that being the case. If anything, it means you should be cautious to never use them on the off chance that your theory of consciousness is incorrect.
    ```

    - u/electrace:
      ```
      >But similar to how you wouldn't want your personality completely changing overnight, I wouldn't want my brain being replaced all at once, even if it was with an identical copy.

      I'm curious. If scientists discover tomorrow that due to quantum *mumble* multiple universe *mumble*, our atoms are, in fact, switching places with other atoms with identical properties from across the universe approximately every 3.49 seconds. 

      Would this bother you? I can say personally that this wouldn't even make me blink.

      > But yes, under my current theory of consciousness going into a coma or being put under via anesthesia would effectively kill me.

      Your brain is still active in a coma and under anesthesia. That's how doctors differentiate between "coma" and "death."
      ```

  - u/electrace:
    ```
    Both sides of the debate agree that the transporter gathers pattern information about your exact body composition by breaking it down into component parts, sends that information to another place, and then puts together new component parts to create the original pattern.

    The question is then framed as "Will the new person be *me*?" 

    Proponents of the "No" view could point out that throughout the entirety of your existence, out of all the things in the universe, "you" have shared one thing that everything that is *not* "you" does not share, active physical brain continuity.

    In principle, we could draw a bubble around your brain from birth, and move it around with you. Sometimes the bubble may expand as new neurons go online, or contract as they die off. But, we can all agree that consciousness lies inside the bubble. One could draw the path that the bubble takes from birth to death, and it would be one smooth curved line through space-time. Teleportation would destroy the brain, leaving a moment in time where the bubble could not exist, thus breaking the smooth line. Therefore, the new person would be a new birth of a new consciousness, and thus, not "you."

    Proponents of the "Yes" view could point out that throughout the entirety of your existence, out of all the things in the universe, "you" have shared one thing that everything that is *not* "you" does not share, a unique machine (the brain) that maps inputs to outputs.

    In principle, "you" are the end result of every input that you have received. When you see a red object, you experience the qualia of "seeing a red object", "knowing that red objects exists", "forming a memory of a red object", "experiencing emotions associated with red", "weakly associating the current emotional state with red" and "sending signals to the mouth to say things like 'that is red'". If you had seen blue instead, then your brain would have formed slightly differently. There is nothing in the universe that would process a red object in exactly the same way that you do. Teleportation would recreate your unique brain. Therefore, the new person is not a different consciousness, which would process information differently, and is thus, "you."

    Personally, I belong to the "Meh, whatever" view. Both seem like perfectly reasonable definitions of "you" in the same way that "something that causes an auditory experience" and "vibrations" are both reasonable definitions of "sound." To me, the teleporter problem is exactly equivalent to the old question about a tree falling in the middle of the woods. The answer is "depends on your definition."

    As such, the only thing that bothers me about the transportation problem is how people react to it. Being semantics, the answer to the "Is the new person you?" question should not inform your willingness to get onto the transporter platform. 

    Take the bird's eye view. At time x, there is a person at position y. At time x+1, there is a person with an identical everything at position z. The only question to consider from this formulation is "where is the better place for there to be a person?" Everything else cancels out.
    ```

  - u/Roxolan:
    ```
    > it isn't the same physical atoms/molecules

    An issue with the cell renewal argument is that neurons get replaced extremely slowly if at all.

    A [LessWrong Sequence](https://www.lesswrong.com/posts/7HMSBiEiCfLKzd2gc/quantum-mechanics-and-personal-identity) brought me to the same conclusion with a different argument: it doesn't make sense to attach personal identity to individual atoms because "individual atoms" don't actually exist. There's just an amplitude distribution as described by quantum mechanics.

    As for continuity of consciousness, it doesn't seem important to me? 

    What I want is for my next mindstate to be a consequence of my current mindstate according to a particular set of rules which define me. Whether the rules are implemented by brain chemistry, by a bunch of nanobots that scan my brain and reconstruct an identical one on Mars two hours later, or by a computer that accurately simulate the effect of brain chemistry (at arbitrary speed) doesn't seem like it makes a difference.
    ```

    - u/DangerouslyUnstable:
      ```
      While I find the "atoms don't exist" argument equally compelling (although probably harder to grasp for most people, even I don't really understand it, I just take it on face value from people who understand it better than I do), the fact that individual cells don't replace quickly says nothing about how fast the atoms in those cells replace. I am quite sure that even though a cell may not die/divide, the atoms that make it up are constantly cycling. It's basically the same idea scaled down a level. 

      And I agree with you completely about continuity not being that important *to me*. I was merely addressing the arguments that frequently come up. It sounds like you and I mostly see it the same way. I personally wouldn't consider teleportation death *even  if* no atom was ever replaced in your body and your consciousness was completely continuous throughout your life.
      ```

  - u/None:
    ```
    Honestly your conclusions seem self-evident and I doubt I'm going to say anything on the ship of Theseus problem that hasn't already been said.  So I think teleporters can raise far more interesting questions:

    What if teleporters work like scanners and 3d printers?  In that case, wouldn't it be interesting if there were innumerable copies of you living far away having experiences you'll never have because travelling at the speed of light is too hard for not light?  What if you have endless backups to fixed states in your life?  Can you ever really learn?  Do people die over thousands and millions of years because eventually they're not willing to part with memories?  Or they die because their definition of themselves becomes too fixed to change?
    ```

  - u/Amargosamountain:
    ```
    So according to you, a clone with your same memories and brain state is not a clone, but is actually *you*?

    This question is examined in some of Peter F. Hamilton's writing. We have the technology to backup and restore memories, so when someone dies in an accident, they get "re-lifed" in a new clone body with a clone brain injected with their old memories. In the books, the characters don't have a consensus about whether a re-lifed clone is actually the same person or not, but the society as a whole tends to accept it as true continuity. They call dying "body death".

    I felt that re-lifing someone is more for their friends and family than for themselves. I mean, if I die in an accident, I don't care whether my memories and personality live on in another body, because I, the only me that matters, am dead and I will not experience anything my clone does.
    ```

    - u/DangerouslyUnstable:
      ```
      If you define "you" as "you at time t", then the clone is as similar to "you" as "you at time t+1", yes. 

      According to my definition, "you", every second of every day, are a slightly different person than you were the second before. A clone is as close to the person who "made" the clone as that person is at a later time. (assuming crazy magic sci fi cloning with exact duplication including memories etc.)

      When you say "I, the only me that matters" as something separate from your memories, to what are you referring? What is the "thing" that "you" are that is not your memories? I argue that "you" is nothing *except* your memories (well...and the hardware on which to execute the personality built on those memories...but you get the point).
      ```

      - u/Amargosamountain:
        ```
        For me it's the fact that you could theoretically make a clone of me with my exact memories, and we could coexist together and we would be distinct people. If I'm standing next to my clone and you kill me, I'm dead (and for the sake of argument let's pretend my clone and my memories haven't started diverging yet). I don't get to continue my own experience through my pre-existing clone, so why would it be different with a clone created later?

        I can't conceive of *my* consciousness ever existing in any hardware besides *my* brain, Ship of Theseus concerns notwithstanding. That's a whole different argument :)
        ```

        - u/DangerouslyUnstable:
          ```
          But the "you" that died is just as different from the "you pre-clone" as the clone is. So yes, that individual died. But that individual is not any more related to the "pre-clone you" than the clone itself is. From the perspective of "you pre-clone", either one of you dying is equally bad, because those two individuals are both equally related to "you pre clone", and they are not different things. /u/gaberockking point about non-binary death is a good way of thinking about it.
          ```

          - u/paradoxinclination:
            ```
            >But the "you" that died is just as different from the "you pre-clone" as the clone is. So yes, that individual died. But that individual is not any more related to the "pre-clone you" than the clone itself is

            That's not really true, since the clone of you shares none of the molecules that made up your body whereas the 'pre-clone you,' of one second ago shares 100% of your molecules. You can argue over whether that matters or not, but it's a little disingenuous to state that there's literally no difference when there demonstrably *is.*
            ```

          - u/Amargosamountain:
            ```
            I think we're just disagreeing on what "death" means. I don't feel like the me of the past is dead, he just changed. Where you see distinct points (I assume) I see a series of points on a distinct line.
            ```

            - u/DangerouslyUnstable:
              ```
              I mean...yes, that is the point of the discussion...what is death. 

              That's a great analogy. They are points on a line. But it isn't a straight line (once you include cloning at least). It's an every branching tree (every time a clone is created, the line branches). Every single version of you feels like they have a direct connection to the original, and from the originals perspective, there is no differnce between the different branches.
              ```

        - u/None:
          ```
          >I can't conceive of *my* consciousness ever existing in any hardware besides *my* brain

          A way to bootstrap your intuition to see that it can would be to imagine a [gradual brain replacement](http://consc.net/papers/qualia.html) (your brain being slowly replaced by functionally equivalent small pieces of something else (like silicon transistors)). That makes it easier to see that it's the continuity of the pattern that's responsible for the continuity of the consciousness, and the persistence of a particular hardware/matter is only necessary to the extent to which it's necessary to maintain the continuity of the pattern.
          ```

    - u/GaBeRockKing:
      ```
      Death isn't binary. If you get hit on the head and lose an hour's worth of memories, have you died? Yes, but only by a few fractions of a percent. I care about myself in the immediate future more than myself in 20 years because 20 years from now I'll be fairly dead compared to current me, but given that I care for other humans, and me 20 years from now is the least dead variant of me to exist at that time, I care about them more than any other human. (For reference, I view other humans as being mostly dead versions of myself, who share some of the same brain structures, personality, and effectively equivalent memories, but lost a lot of my memories and replaced then with new ones, and they got massive brain damage they healed from.)

      So in a destructive teleport, the person at the other end is still me from before the teleport. The "me" that goes through the teleporting process will have maybe a few nanoseconds of subjective experience that will be killed, but given how prone I am to forget things anyways, that's a small price to pay.

      Though really, I'd much prefer to just have two instances of me. They would later diverge from each other, but from the perspective of pre-clone me, that just means there's a little less than 100% more me.
      ```

      - u/ansible:
        ```
        > If you get hit on the head and lose an hour's worth of memories, have you died? Yes, but only by a few percent. 

        I expect we'll need a new category of "partial murder" at some point.

        You killed some entity, and it was restored from backup, but missing some amount of time.  Maybe a few minutes of realtime would be a minor infraction.  

        But losing a couple years of realtime (during which you met the most awesome life-partner and had many adventures together) should face punishment closer to irrevocable murder (the normal kind in the year 2020).  

        Motive, as usual, should also play a part.  If the death was an accident that's one thing, if you did it to cover up some other crime, that's way more serious.
        ```

        - u/hyphenomicon:
          ```
          Future mystery novel where memetically induced Alzheimer's is the murder weapon.
          ```

  - u/None:
    ```
    [deleted]
    ```

    - u/lo4952:
      ```
      The way I see it, the specifics that create your "consciousness" so to speak are your memories of previous events. As long as you have those memories you are you, to the point where creating a clone implanted with your memories would create a second you for an instant, and then it would begin having diverging experiences and become its own consciousness. Looking at it this way, I see the physical destruction of your old body as equally important to your conciousness as say, forgetting why you walked into a room. Since your conciousness is constantly being interrupted by things like zoning out, daydreaming, sleeping, or even going into a coma, all of these things have the same effect as killing a body and recreating your memories elsewhere. With this idea, the subjective you that doesn't get to visit Seattle is probably not the you that woke up this morning, since at some point you got distracted, took a nap, etc. 

      Obviously not scientific, but my take on how I see it.
      ```

      - u/None:
        ```
        [deleted]
        ```

        - u/lo4952:
          ```
          I feel like I understand what you're asking, and correct me if I'm wrong, but no, I don't believe there would be a "shift" or "jump" or any kind of transition to allow you to continue your subjective experience on the other end. However without trying to be pedantic, I don't think its "another you" on the other end. Its just you, since there is only ever one you, and at this exact moment in time you are in Seattle. The "you" from before is gone, but not gone as in dead, gone as in how the you of yesterday is gone. 

          Additionally, I think this has a lot to do with what you consider your "continued subjective experience." In my opinion there is no way to determine whether or not it is a continued experience other than your own memories, so you could say that yes, since I remeber entering the teleporter and leaving in a new spot, that was all part of this single subjective experience I've undergone since my first memories.
          ```

          - u/None:
            ```
            [deleted]
            ```

            - u/lo4952:
              ```
              I think the difference in the way I see it stems from a continuous versus discrete perspective of self, or of what "you" is. In your example there are two continuous "selves" that while being the same person, are distinctly seperate, cut off by the act of teleporting.

              In my opinion there is only ever one "self" and that is the "you" at this exact moment in time. Any "you" before exists only in your memory, so what does it matter if there are gaps where you sleep, teleport, etc. From this perspective there is no you to die on the front end of the teleporter, since at that exact, frozen, moment in time "you" exist in Seattle, with a fresh memory of yourself just recently being wherever you were before.

              Thank you for the question, it's helped me clarify a few things to myself as well.

              Edit: Something Ive considered in my goal to put it in perspective is reframing the question to look at what would happen if instead when you entered the teleporter you went to sleep and someone carried you to the new location. When you wake up you are in the new location, and while you are in the "same body," with the whole "Ship of Theseus" thing and the probability field nature of atoms themselves I think the physical aspect is much less of a question than the conscious aspect.
              ```

- u/kraryal:
  ```
  I'm looking for a Naruto fanfiction that was recommended here a while ago.  


  The main character is a gender-fluid SI who gets tutored by Kakashi and in turn tutors Naruto and Sasuke. They think the MC is a bit of a paranoid nutcase due to the horrifying training.

  Near the end the MC is planning to prevent all death through some sort of munchkinry and runs the plan past Kakashi first. "See Kakashi, the problem is that everyone keeps dying." "Well," Kakashi thought, "that's certainly true."   


  Anybody remember it? Thanks very much
  ```

  - u/skiueli:
    ```
    This I think: https://forums.spacebattles.com/threads/kaleidoscope-naruto-si-complete.497083/
    ```

- u/Air_Ship_Time:
  ```
  https://docs.google.com/document/d/1BkgpQO8b_aaaG75m3cFQ-zcLd-UY6gFe5BtTR5qC3Gs/edit?usp=drivesdk

  I have here a list of powers granted by the GLOW. That is the light lanterns of the green lantern comic books use. One thing I noticed odd was that the green light offers no extra abilities while every other light grants something useful. 

  I was wondering if you might be able to think up a power for the green light? I have been considering just taking emotional manipulation immunity from blue and giving it to green. 

  This is for an alt power Taylor that has to take on scion. She gets a ring that can access all the colors.

  Thanks for any help.
  ```

  - u/Aabcehmu112358:
    ```
    The green lightâ€™s special gimmick is that it is the easiest to control and has the least mental deviation for using it.
    ```

  - u/ketura:
    ```
    If green is immune to manipulation it defeats green's one weakness, that willpower must be maintained.
    ```

  - u/sicutumbo:
    ```
    Construct strength for other colors is dependent on how much the user feels that that particular construct reflects the Lantern's emotion, but green has a constant strength that is only dependent on the user's current level of willpower regardless of what they are doing. Say that you have a blue ring. Healing others is pretty easy, shielding others means your shields are super strong, etc. But if you try to make a sword to kill the person attacking you or the people around you, it's not even as strong as steel, because in a lot of cases, killing someone doesn't inspire hope. Similarly, an orange Lantern can make really, extremely strong constructs to do things that they want to do, but if said Lantern is ordered to do something they're only somewhat interested in, their constructs are weak to the point of almost failing to do the job at all.

    But for green, willpower is fairly constant. Any construct you make is going to be as strong as any other given your current willpower, regardless of what those constructs are doing. Being ordered to do something you don't want to do or isn't something you're good at means your constructs are actually stronger, because you have to exert willpower to keep going even when you dislike what you're doing.  Green Lanterns don't need any training to maintain their emotion even when they're doing something unaligned with it, because nothing is unaligned with willpower or focus. Similarly, you can maintain focus a lot more easily than you can being angry at something for an extended period, when said thing isn't extremely important to you, or when it isn't immediately present. Green rings excel at intermediate tasks that don't immediately cause the relevant emotion.
    ```

    - u/Air_Ship_Time:
      ```
      Thank you, that helps me with writing a bit. Green is going to be the second light focused on after Indigo /compassion.
      ```

- u/chlorinecrownt:
  ```
  In Buffy, it's stated that orbs of thessala are used as paperweights, implying they're not particularly rare. These items are consumed in the process of re-ensouling a vampire, albeit with the drawback that "a moment of pure happiness" reverses the process. "Moment of pure happiness" appears to only refer to having sex with someone you really like. So you don't even have to be celibate, just only have sex with your second choice or lower.

  So why don't they use Angel to vamp Xander/Giles/Willow* and then quick soul them up before they do anything evil? In the short term, super-strong ally, in the long term, immortality.

  *If being a vampire causes issues with magic then this should probably be put off for a while. The semi-canonical "Fray" series suggests Willow is immortal anyway.
  ```

  - u/nohat:
    ```
    If your soul goes to a hell (or heaven) dimension it appears you may have a high rate of time dilation. You might not want to come back if its a heaven dimension, and you may suffer immensely otherwise (it also might be looked upon poorly if you die in the future). In general having a demon as a passenger (trying to grab the wheel) seems a bit risky. Also avoiding sunlight and true happiness sounds pretty annoying -- Angel's moment of true happiness is just one datapoint after all. On the other hand you can probably make a ritual that doesn't have that curse, considering spike found a way without that caveat, and the gypsies *were* actually trying to make it a curse.
    ```

---

