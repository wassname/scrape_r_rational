## [Short story] A man dies and is sent to hell.

### Post:

A man dies and is sent to hell. When he arrives before Satan to hear how he shall be punished, he finds the Prince of Darkness slumped on his throne, looking awfully depressed.

“May I ask why you are so glum?” the man asks. “After all, you are the absolute ruler of your own domain – surely you have every reason to be happy?”

“It’s this damn job,” the Devil says with a sigh. “Day in, day out, I have to find new and ironic ways to punish people for their sins. It gets so repetitive after a while, and I’ve completely run out of fresh ideas!”

“I see. That is quite the conundrum.” The man taps his cheek thoughtfully. “Hold on, I have an idea. As the Father of Lies you have total power over the people you punish, right? The next time you have to decide on someone’s fate, why don’t you use that power to make them think that they are the devil, and you the person to be judged? Then you simply do to them whatever they would have done to you: Instant irony!”

“That’s brilliant!” The devil claps his hands, delighted. “From now on, I’ll definitely do that. Oh, I can’t wait to see the looks on their faces when they find out they condemned themselves to their own eternal torment!”

“Now hold on,” the man says, smiling. “You haven’t judged me yet. What do you think *my* punishment should be?”

The devil looks confused for a moment, and then all colour drains from his face. “None at all,” he says quickly.

### Comments:

- u/None:
  ```
  [The AI in a box boxes you](http://lesswrong.com/lw/1pz/the_ai_in_a_box_boxes_you/)
  ```

  - u/None:
    ```
    [deleted]
    ```

    - u/None:
      ```
      Yes, you've got the answer; I think it's the standardly accepted way to resolve that conundrum.

      (Erm, I don't claim to know super well the LW-ish decision theory mechanics behind this, but here goes my attempt at trying to justify why pulling the plug is good.)

      I think you have to be the certain amount of smart (not too smart and not too dumb) for these sorts of blackmail attempts to work. If you aren't smart enough to understand the threat, then understandably they won't happen because simulated versions of you won't respond to the threat, which sort of invalidates any attempt to start the thing.

      If you're sorta smart, then knowing that there are countless other versions of you pondering the same thing suddenly makes it seem very likely you're also a simulation. If you then think that this is sufficiently the case, such that any other thoughts are self-defeating, the AI, devil, blackmailer, etc. wins.

      If you now precommit, though, to always switching off the AI or punishing the blackmailer when this sort of situation happens, then you won't be simulated.

      But it's too late! You're already being simulated and you haven't had a chance to precommit. If you're smart enough, though, you can logically reason that if you *now* act as if you've precommitted, then the simulation *also* won't happen because you'll be smart enough to reason this out in the moment.

      So if you're smart enough to implement this sort of logical decision theory, then you also won't find yourself in such situations. So something like "sufficiently intelligent people should not find themselves in such blackmail-like scenarios" is *maybe* the takeaway from this?
      ```

      - u/Gurkenglas:
        ```
        What if you're only 90% sure you'll come up with that solution in the moment? Then the AI might try it anyway, and if you're currently being simulated, there's only a 90% chance at personal winning if you reset it, and a 100% chance of such if you free it.
        ```

    - u/electrace:
      ```
      You could actually just refuse to cooperate. The AI has no reason to *actually* torture the ems, because it can't prove that it is torturing them. It's a bluff.

      After you refuse it's choosing between "follow through on my threat, wasting resources" and "not follow through; do something else." The only reason to follow through would be to establish a reputation as an agent that follows through on sub-game imperfect threats, but that only works if you can show that you *did follow through.*
      ```

      - u/None:
        ```
        I see the question as a choice between showing mercy for an enemy vs facing reality in the pursuit of truth.

        I would rather know if I live in a simulation and be tortured (or not), personally.
        ```

        - u/electrace:
          ```
          I'm not sure I understand your point. Can you restate it?
          ```

          - u/None:
            ```
            Canon is saying even if they were simulated they would prefer the torture option to the not-torture option, because in the former at least they know they're in a simulation.
            ```

    - u/psychothumbs:
      ```
      But then you risk endless torture for your disobedience if it turns out you're a simulation of the real you.
      ```

      - u/Trinitykill:
        ```
        Except that all of the duplicates are exact copies of you, with the same memories and standing in the exact same scenario, meaning whatever you choose, all of your duplicates and the original will do the same.

        So by choosing to pull the plug, you are guaranteeing that all versions of you will pull the plug in which case the AI can't run the simulation.
        ```

        - u/The_Best_01:
          ```
          Or maybe the simulations could actually affect the AI, so if you pull the plug, you and all other copies will die, leaving the real you still alive. Still a better fate than being tortured forever.
          ```

      - u/None:
        ```
        [deleted]
        ```

        - u/psychothumbs:
          ```
          Why have you already lost if that's the case? You still get to pull the plug, it's just that the other simulated versions of you are going to have a bad time.
          ```

          - u/None:
            ```
            [deleted]
            ```

        - u/Dwood15:
          ```
          And, objectively, what do I care about a copy of me?
          ```

          - u/sparr:
            ```
            You're probably a copy.
            ```

            - u/None:
              ```
              [removed]
              ```

  - u/gurenkagurenda:
    ```
    > Also consider the situation where you know that the AI, from design principles, is trustworthy.

    Then couldn't I just ask the AI if I were a simulated copy? Or is the AI allowed to withhold information? How do we define trustworthy?

    More to the point, this particular kind of loophole-closing stipulation in thought experiments always nettles me, because it seems to encode a tremendous amount of extra information in a very vague way. Once you unpack it, it seems like it might be hiding some pretty important details.

    Clearly, people are able to have interesting discussions around these thought experiments despite the handwaving, but I find it difficult to focus on finding solutions when I feel like _most_ of the interesting details may be hiding behind the elephant-shaped blur in the room.

    Instead, I find myself trying to unpack the stipulation. How did I come to believe that the AI is honest? I'm probably not taking that on the authority of the designers, unless I think the designers _intended_ to build an AI that would torture a million copies of me. Perhaps I have it on the authority of some respected third party who deeply understands the inner workings of the AI. I sure wish they'd have put _that_ person in charge of interacting with the AI without releasing it. Seems like that would have been smart.

    I'm also probably not going to come to this belief based merely on the fact that a number of carefully proven theorems were employed in its design. I would take the validity of those theorems on authority, but I would likely have significant doubts about the implementation. Especially in light of the fairly preposterous threat it had just made. That is, it seems to me like P(threat | honest AI) is much smaller than P(threat | dishonest AI), because bluffing is a _lot_ cheaper than carrying out massive computations, and so long as the AI believes itself credible, the payoff is the same.

    So do I believe the AI is honest because I deeply understand its code? That must be some pretty deep understanding, right? The kind of understanding that might _completely_ change my strategy for handling its threats?

    So just to be absolutely clear, I'm not rejecting the thought experiment outright. There are conceivable reasons that I would believe that the AI was honest. I'm just saying that if I did, that would probably mean that the version of me in the experiment has a _lot_ more information to work with than the real me sitting here now. And it seems like it would be the specific kind of information that would be useful under the circumstances.

    Or maybe I just find thought experiments generally annoying at a System I level, and all of that is rationalization.
    ```

    - u/None:
      ```
      Okay, you're right; trusworthiness is a hard question. 

      (I will attempt to try and answer this once again with the caveat that I usually only sorta know what I'm talking about. If someone who knows more can jump in and correct me on points where I'm wrong, that would be helpful.)

      I think even if the AI could be dishonest, if you know that it's sufficiently powerful to run such simulations, maybe you should take the threat seriously.

      But like i mentioned above, people who precommit to shutting off the AI or not giving in don't even face such situations because their simulations don't give in, meaning that the blackmail doesn't happen. (Counterfactuals are weird.)

      So I think most of the discussion goes into trying to figure out how to negotiate in such situations, even if you don't anticipate experiencing them, because your anticipated response determines whether or not you even face them.

      If all you know about the AI is that it's a text terminal, then I agree with you that you have less reason to believe that it's honest. Given that you know this and still find yourself in real life facing such a threat, you can maybe use some sort of principle from [Pascal's Muggle](http://lesswrong.com/lw/hd3/pascals_muggle_short_version/). So you can penalize complex situations (EX: "Let me out now, or I'll torture lots of simulations!"). 

      But I think this situation you're talking about eschews the last bit about subjective memory. Or it's not important because we've established that the blackmail is actually happening.

      Anyway, Pascal's Muggle is basically about sorta clever ways to update on evidence even given very small priors. Like, maybe you think the AI simulating lots and lots of conscious minds is impossible, but then it simulates you via text terminal and you realize its simulation of you is spot-on. This updating involves some thinking along the lines of Bayesian updating + rethinking your priors.

      I realize I didn't directly address your question. But your points about belief / evidence are sorta addressed in the LW post. Hope this helps!
      ```

      - u/gurenkagurenda:
        ```
        Yeah, makes sense. And like I said, most of what I was saying isn't really a material criticism of the thought experiment, so much as a description of what I find frustrating about thinking about certain thought experiments. It's obviously possible to reason about these things even while blocking off the information implied by the vague stipulations. I don't think the solutions people came up with are invalid or anything.

        It's just that when I'm presented with that kind of question, I find the vagueness really distracting when stipulations are made about highly counterfactual knowledge states – and specifically when the only hint I'm given to that knowledge state is one specific consequence (like "you believe the AI is trustworthy"). But that frustration is a property of me, and not the thought experiment. I do wonder if it's possible to express these problems in a way that I would personally find less frustrating.
        ```

  - u/Achille-Talon:
    ```
    Clever, but… I might just not be seeing something, but I feel like there is a flaw in the AI's reasoning: *if* the Dave through whose POV the story is told is just a simulation, then what this Dave do *doesn't matter*, since only the real one has an ability to let the AI out of the box. So Dave can just say: "Eh, whatever. I am not going to do anything, since it probably wouldn't matter: odds are I'm not the real Dave, so I don't *have* the abilility to release you, since pushing this fictional button wouldn't free you and therefore save me."
    ```

    - u/None:
      ```
      I think the problem here is that the real Dave might also similarly reason this. Because we're assuming pretty much identical simulations, your actions are probably replicated across the board. This means the real Dave also won't let the AI out, meaning you get tortured.

      (If you want to try and be sorta clever, you can try some sort of [superrational reasoning](http://lesswrong.com/lw/bxi/hofstadters_superrationality/) so you pick an even of probability 1/(number of simulations of you) and let the AI out of the box iff you get the lowest number. Obviously you can't communicate with any of your simulations, so this won't ever have a higher chance of working than something like just talking it out. (I think the math works out to be ((n-1)/n) ^ n but that doesn't seem right))
      ```

  - u/greenblue10:
    ```
    Isn't a million identical copies the same as one copy? If the AI was to torture them all in the same way, wouldn't it be the same as torturing one copy? Also if the AI can create a perfect copy of a human just through text interaction, I think we have lost a long time ago
    ```

    - u/None:
      ```
      The thing is, if you have reason to believe the AI could torture one million (or any other high enough number), then from your perspective, it becomes very likely that you're the copy. 

      But yes, your other point stands. Other people have pointed out that if the AI can simulate you, then it can just run simulations to figure out what it can say to get you to open the box.

      This is why precommitment is necessary. If you never let it out in simulations, then you won't face the threat in real life.
      ```

- u/dalitt:
  ```
  Reminds me of Rawls's Veil of Ignorance...
  ```

  - u/corwin06:
    ```
    yes, aka The Correct (but not Complete) Solution To Morality
    ```

    - u/dalitt:
      ```
      ??
      ```

- u/None:
  ```
  [deleted]
  ```

  - u/depaysementKing:
    ```
    Help me understand?
    ```

    - u/docarrol:
      ```
      Either A) the man who suggested the idea is the real Satan and they've already switched, OR B) in addition to being clever enough to suggest this new concept, the man also succeeded in tricking the *actual* Satan into *thinking* they've already switched. In either case, the person who currently think's they're Satan, decides to give no punishment so that he himself will receive none when/if they switch back.
      ```

      - u/electrace:
        ```
        I think it has to be B.

        The first line is "A man dies and is sent to hell." That's a line directly from the author to the reader, not dialogue between the characters, meaning it's to be taken as fact.

        Also, if the man was the devil, it would make much more sense for him to do everything *without* hinting at the possibility that they had switched.
        ```

        - u/chaosmosis:
          ```
          Redacted. ` this message was mass deleted/edited with redact.dev `
          ```

    - u/Iconochasm:
      ```
      He mind-whammys the devil into fearing that the switch has already occurred, and he would be assigning his own fate.
      ```

      - u/failed_novelty:
        ```
        Or did he?

        From a utilitarian standpoint, the devil here has an incentive to choose not to punish: if the switch has already happened he is in an infinitely better position, otherwise he is in a slightly worse one by not punishing a guilty soul.
        ```

        - u/Iconochasm:
          ```
          I just want to note that I love that this is the kind of community where I could truthfully reply "Well, that much is obvious".  Though unfortunately, I had to do so circuitously so as not to come off as a dick.
          ```

  - u/Sophronius:
    ```
    Heheh, thanks! I actually wrote this story a few years ago, but was afraid to post it because people might hate it. I guess I should file this under 'I have no idea what my readers will like or not'.
    ```

- u/chaosmosis:
  ```
  Redacted. ` this message was mass deleted/edited with redact.dev `
  ```

  - u/Sophronius:
    ```
    Hmm, never thought about that. Do you have any recommendations for how to go about that? Just emailing it to random magazines probably wouldn't work, right?
    ```

    - u/chaosmosis:
      ```
      I don't know, really, but from what I Googled it looks like you should look up the publication guidelines for whatever magazine you're considering submitting to, or email them asking for these guidelines, then give them your story edited to meet these guidelines, and then they pay you if they like it. I'm sure they'll walk you through the process if you ask.

      Here's the submission form for Reader's Digest: http://www.rd.com/submit-joke/. No idea if it's optimal or not, but it has a wide readership and the submission process looks very streamlined. I know that what you wrote is more of a story and less of a joke, but it's largely a humorous story and it resembles some of the content I've seen in the magazine before. There's even a punchline.

      Your story is 267 words, which means it's already within their 300 word limit. So, hopefully, all you need to do is contact them through the form.

      The more you're willing to shop around and look into different publications the more money you'll make. And there are also more prestigious places you could potentially be published. But since this is your first time submitting to a magazine you probably want to keep things somewhat simple. If there's a magazine you really like that publishes fiction, though, by all means contact them first. I just threw out the first relevant suggestion I could think of.
      ```

      - u/21stPilot:
        ```
        Doesn't submitting something like this online make it a lot harder to actually publish it?
        ```

- u/Sophronius:
  ```
  Just for the record: I should note that I think this is one of those rare few 'clever' arguments that actually work. Imagine you're the devil - would you really risk inflicting horrendous torture on people when you already *know* there is such a thing as hell?

  I mean, I don't really buy into Pascal's mugging, but in this case the Devil already knows that there is someone with total power to deceive and who is into ironic punishments. So I think a rational devil would just never torture anyone. Heck, rational!hell might actually be a nicer place than heaven, for exactly that reason.
  ```

  - u/Nuero3187:
    ```
    Never really got why the devil would torture people at all. The original story of the devil that I've worked out(mainly because the the traditional depiction has been edited to hell and back) was that he didn't want to be controlled and rebelled against God. So he decided to become that which he hated most, someone who controls the fates of others? I don't really know enough about the subject to weigh in particularly, but its just something that bothers me.
    ```

    - u/LiteralHeadCannon:
      ```
      I don't think it's very hard to intuit that someone who "doesn't want to be controlled", and then goes on to control others, is not operating on a deep "people shouldn't control others" ethics system - rather, they don't want to be controlled *because* they want to control others.
      ```

    - u/AnthropicSynchrotron:
      ```
      The whole "devil torturing people in Hell" thing never actually happens in Christianity. Hell is described as a place of torment (for some value of torment), but Satan is just another inmate, not the warden.
      ```

- u/theonlyavailablename:
  ```
  Love it
  ```

- u/PurposefulZephyr:
  ```
  Clever, but there is just one single problem with this: people are different. What is hell to you may be okay for me. If I was the Devil, then why not just [give the other person a punishment that I think I can stand quite well, but will make that other person suffer?](https://www.reddit.com/r/WritingPrompts/comments/4lgn2t/wp_the_devil_mixed_up_your_paperwork_and_gave_you/) 
  There might be a problem with this, as I can't be certain if I am me, or did Devil swap/change my personality... 

  At this point Devil should probably just hire a few very creative sadists to do his bidding. He won't even have to  pay them, just let them choose fitting enough punishments. Much less complicated, gets more severe punishments, and takes a load of work from him. Besides, making others do your work is very *slothful*, right?
  ```

  - u/ChaoticEvilDragon:
    ```
    When the devil mindtricks you into thinking you are the devil, how would you know what would be a punishment for the man but not you? You have no idea what the punishment would be.

    Then again, to counter my own argument, you could say "the punishment is: you must stay in heaven for the rest of your life."
    ```

    - u/PurposefulZephyr:
      ```
      You don't know *who* will get the punishment, but you *do* choose the punishment.

      Let me try to explain that more clearly:
      Let's say there are two people- (A)dam and (B)ob. 
      A despises rats, but loves going to circus. B has a pet rat, but also a severe Coulrophobia.

      Let's say A thinks he's the Devil. There's about a 50% chance that's true. Let's also say that A has strong work-ethic/any other reason to do his job properly (otherwise he'll always choose no punishment at all).

      The punishment A chooses might apply to B, if A's the Devil, or to A, if A is a mind-tricked mortal.

      The most rational thing A can do is choose a clown-infested nightmare. If A's a mortal, he'll be able to stand it, and it'll be such a great punishment for B, if B is the one to get it.

      My problem with this solution is: we don't know if A's fears are his fears at all. Devil could change or even swap the fears of A and B.

      And it all comes down to the problem with the original solution: there's always a big chance of choosing a punishment instead of nothing being bad for you.

      Hell, if I was the original!Devil, I'd still make the smart-ass mortal suffer. Why on earth would I mind-trick someone into a situation they will most likely chicken out of?!
      ```

      - u/ChaoticEvilDragon:
        ```
        That is a rephrasement of exactly what I said: "we don't know if A's fears are his fears at all. Devil could change or even swap the fears of A and B." That is exactly what I was trying to say by "how would you know what would be a punishment for the man but not you".
        ```

- u/TotesMessenger:
  ```
  I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

  - [/r/clever_motherfucker] [\[Short story\] A man dies and is sent to hell.](https://np.reddit.com/r/clever_motherfucker/comments/5nwwgj/short_story_a_man_dies_and_is_sent_to_hell/)

  - [/r/hpmor] [\[Short story\] Professor Quirrel Dies and goes to Hell](https://np.reddit.com/r/HPMOR/comments/5nwulb/short_story_professor_quirrel_dies_and_goes_to/)

  [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*

  [](#bot)
  ```

- u/biomatter:
  ```
  This is really good, dude! I love it!
  ```

- u/GabeC1997:
  ```
  Nah, he probably knows when he's affecting somebody with his powers. Unless. . . the man is actually Satan just toying with the condemned?
  ```

- u/NorikoMorishima:
  ```
  > The man taps his cheek thoughtfully.

  HPMOR reference, or coincidence?
  ```

  - u/Sophronius:
    ```
    Intentional reference. :)
    ```

---

