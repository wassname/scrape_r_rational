## [D] Monday General Rationality Thread

### Post:

Welcome to the Monday thread on general rationality topics!  Do you really want to talk about something non-fictional, related to the real world?  Have you:

* Seen something interesting on /r/science?
* Found a new way to get your shit even-more together?
* Figured out how to become immortal?
* Constructed artificial general intelligence?
* Read a neat nonfiction book?
* Munchkined your way into total control of your D&D campaign?


### Comments:

- u/ben_oni:
  ```
  Is it okay if I reformulate the Fermi paradox in a way that's more relevant to this sub?

  > Where are all the paperclip maximizers?

  That is, if UFAI is more likely than FAI, and a super-intelligence explosion is inevitable with any AGI, why hasn't the whole galaxy been converted into paperclips yet?
  ```

  - u/eternal-potato:
    ```
    When reworded in terms of existential threat like this, it becomes apparent survival bias is in play.
    ```

    - u/Gurkenglas:
      ```
      After accounting for survival bias, you'd expect the universe to be younger when we show up.
      ```

      - u/vakusdrake:
        ```
        I mean there _is_ actually an argument that on cosmological timescales we arose quite early in the universe. And of course you don't need us to have arisen first in the entire universe, just the first in our past light cone, so you may have some leeway here.
        ```

  - u/vakusdrake:
    ```
    I think it's also relevant that FAI are likely to want to grab resources as quickly as possible in a way that would look nearly the same as a UFAI (after all very few utility functions are going to care about not exploiting dead systems when that energy/matter can be used on other things) to an outside observer. And hell when you consider von-neumann probes then exponential expansion seems inevitable even without AI.

    So I guess my point is that the Fermi paradox is a problem pretty much regardless of what you believe (provided you don't believe in something crazy like the supernatural).                     
    Still I think as Isaac Arthur's great filter videos demonstrate, without any massive questionable singular great filters. You can still whittle down the probability of civilizations arising enough to make it plausible we are the only civs in our past light cone just using a great many smaller filters.

    Another interesting (if terrifying) idea, is that GAI's that end up becoming the only conscious mind in existence (whether through killing off their creators or having their creators eventually merge with them) are the norm. So were that the case, a GAI could have only one if not a handful of separate "observers" as it were so most minds that ever existed would actually be among the precursor biological civilization and thus we shouldn't be surprised to be in the majority of minds to ever exist.                          
    Actually I'm rather disturbed at how plausible that seems, especially given it would also set a great filter ahead of us, which is the worst possible case scenario..
    ```

  - u/ODIN_ALL_FATHER:
    ```
    While an interesting framing I don't think it fundamentally changes the problem. I prefer to view the Fermi paradox through the [The Great Filter](https://en.wikipedia.org/wiki/Great_Filter) and for reasons that mostly reduce my personal feeling of existential dread, I take it to be around the development of single cell life. To me this means that humanity has already passed the biggest hurdle and can go on to continue with eventually colonizing the stars.

    From there I make the assumption that that sentient life is extremely rare on the order of about ~1 planet developing sentient species per galaxy. As the distance between galaxies is extremely large I except sub-light speed travel between galaxies to be extremely difficult and it would explain why this galaxy hasn't been colonized already.

    Thus the reason the galaxy isn't just paperclips is the same reason we don't see aliens, it's just too rare.
    ```

- u/None:
  ```
  [deleted]
  ```

  - u/Noumero:
    ```
    You're talking about ***Outsider* [[novel](https://www.youtube.com/watch?v=5VMLijjQYIg&list=PLQ6wkJY2rtUBIWxchu7vCa3lK7E_T04Uy) | [discussion thread](https://www.reddit.com/r/rational/comments/43zh8p/rst_outsider_ep_16_first_contact_visual_novel/)]**.
    ```

  - u/LucidityWaver:
    ```
    I recall the video, vaguely, but I think when I saw it predates the monthly recommendation threads.
    ```

- u/MagicWeasel:
  ```
  Anyone want some accountability and pomodoro buddies? I just found this link to a lesswrong study hall via facebook: https://complice.co/room/lesswrong (lesswrong for those who don't know is a rationalist hub type place)

  I've been hanging around on it the past couple of days (probably won't be there today), and it's been very useful for motivating me to meet my personal productivity goals as a "commitment device". Admittedly I've only been using it two days but they've been extremely productive days.

  The "complice" website itself seems pretty good too (but expensive! 120 USD/year - I'm on a 2 week trial at the moment), but the study room is free at least. 

  Would be great to see some of you folks around. Over the next few weeks I'm hopefully going to put some 'doros on my day-to-day job (traffic engineer), my writing project (supernatural romance), and my new degree (nutrition).
  ```

  - u/callmesalticidae:
    ```
    Sounds interesting. I'd also like to hear how the paid features work out for you.
    ```

---

