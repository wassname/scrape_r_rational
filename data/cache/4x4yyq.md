## [Challenge Companion] Black and White Morality

### Post:

**tl;dr: This is the companion thread to the biweekly challenge, post recommendations, questions, thoughts, etc. below.**

[Black and White morality](http://tvtropes.org/pmwiki/pmwiki.php/Main/BlackAndWhiteMorality) is generally a trope that is avoided in rational fiction. From the sidebar:

> Any factions are defined and driven into conflict by their beliefs and values, not just by being "good" or "evil".

As such, I imagine that people will tend to go towards subversions or inversions of the premise, which is completely fine. I do generally enjoy stories that lead from the concept of good and evil toward a moral quagmire, especially if the protagonists are under the impression that they are Good and the enemy is Evil.

However, I think there's a place for pure evil and pure good in rational fiction. [*Unsong*](http://unsongbook.com/) describes evil at one point as "your utility function multiplied by negative one", which I think is a very interesting way of looking at evil, though it still leaves some ambiguity. For a normal story (in other words, not *Unsong*) you then get most of your mileage out of good fighting against evil in a rational way that lends itself to analysis on the part of the reader.

### Comments:

- u/ajuc:
  ```
  I love how conflicts are handled in Witcher short stories. There are a dozen or so short stories, are based heavily on conflicts between strong characters, where nobody was "good" or "bad", but everybody has different interests and different ways to achieve their goals (mostly because of their character and social position). The result was chaos and conflicts (usually solved by Geralt with sword and philosophy ;) ).

  Saga has much less of that, but the short stories collections (Last Wish, and Sword of Destiny) are highly recommended. I especially liked "The Lesser Evil" short story. It's not exactly rational fiction (magic has very few consistent rules, and there is a concept of "destiny"), but the handling of morality, conflict, and decisions made with imperfect knowledge is very good. Almost all characters are rational (given their knowledge and beliefs) and practical.
  ```

- u/Jiro_T:
  ```
  There are beliefs and values which amount to being evil.  "Jews are subhumans and I gain utility from getting rid of them.  The utility of Jews themselves doesn't count".

  *Indifference* can also count as evil.  "My goal is not to make you suffer, but I don't care how much you suffer in the process of achieving my true goal".
  ```

  - u/RMcD94:
    ```
    If you ask Hitler if he was evil he would not agree. In fact he would say the Jews are evil.

    Why is your definition of evil more accurate than Hitlers?
    ```

- u/LiteralHeadCannon:
  ```
  I don't think anyone in real life actually has a utility function inverted from anyone else's. But I do think there is real evil in the world. I wouldn't describe evil as an alignment, though, but rather a misalignment. It's a failure condition in forming an intelligence, analogous to failures to make a friendly AI. I think there is a single friendly utility function derivable from first principles, and that evil is a product of our failure to get to that good. (Compare and contrast learning disorders, where the effective route to getting things done fails to come together, rather than the proper goal failing to come together.)

  In short, evil is a failure of intelligence to assemble good.
  ```

  - u/derefr:
    ```
    > I don't think anyone in real life actually has a utility function inverted from anyone else's

    Reifying the concept of an inverted utility function would be a bit like trying to create a systemic definition of Opposite Day. If *you* have a hierarchy of instrumental goals that derive harmoniously from your terminal goals, you'd think the "anti-you" would have "anti-instrumental" goals that *oppose* their goals (i.e. an inversely-proportionate amount of akrasia to your own.) If *you* want to live to continue optimizing, you'd think your dual would want to die as soon as possible. Etc. It seems like a "naively inverted" utility function would necessarily be an *incoherent* utility function.

    (Which is too bad, because otherwise it'd be a pretty cool and simple way to procedurally-generate the villain in a create-your-own-character RPG.)
    ```

    - u/LiteralHeadCannon:
      ```
      Neah, systemizing an inverted utility function is pretty easy.  Just sort all outcomes from most-favored to least-favored.  The entity with a utility function opposite yours has the same list, but backwards.

      Of course no such entity exists, but it's a conceivable entity with simple properties.
      ```

    - u/ajuc:
      ```
      You have to resolve pronouns like "you" and "him" to invert utility function.

      Opposite utility function to "I want to have all the money in the world and don't care about anything else" isn't "I want not to have all the money in the world, and don't care about anything else", it's "I want Smith not to have all the money in the world and I don't care about anything else".

      So in fact 2 people that want to hoard all the money in the world have almost exactly opposite utility functions.
      ```

  - u/trekie140:
    ```
    I really like this because I've studied economic systems and have found much real world suffering to be caused by a failure to maximize good, but it's rare that the ones responsible could've known better. This perception of evil frames it as an internal struggle against your own faults, as well as humanity's struggle to overcome the faults we realize exist in our world.
    ```

---

