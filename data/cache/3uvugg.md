## [MK][Q] You can bring only one fictional character to reality. Who is the 'right' choice?

### Post:

He/She/It is under no compulsion to obey or even like you. Must be from a fictional work created before you knew you could do it. only you can use this power/device, so no creating your ideal and then have someone else do it. (°3°) good luck!

### Comments:

- u/ArgentStonecutter:
  ```
  1. Sit down and re-read the Culture series. All of it.

  2. Pick the Culture Ship most likely to be well-intentioned towards our little blue dot. Perhaps _Arbitrary_ (oh, hey, I know you think you left here a few decades ago, well you're back and the entire hominid population of this galaxy is this planet ... wanna lend a hand?), or maybe _Sense Amidst Madness, Wit Amidst Folley_?

  Seriously, if you're going to find a friendly ASI in literature, I can't think of a better place.
  ```

  - u/Geminii27:
    ```
    Not to mention that a Culture ship can literally digitize you and spit you back out in a body of your choice without you noticing the transitions. If you could find a Mind which would cheerfully upgrade anyone who asked to at least a Culture-human body, that'd be a great start. One who was happy to circle the globe and do it for any injured/sick/aged person would be fantastic.
    ```

    - u/ArgentStonecutter:
      ```
      Just not "Sleeper Service" or "Gray Area" or "Killing Time" or "Falling Outside The Normal Moral Constraints". There are some scary Culture ships.
      ```

      - u/None:
        ```
        Why are those guys so scary?
        ```

        - u/ArgentStonecutter:
          ```
          Sleeper Service is probably the least scary of the lot. It just has this thing about staging battle scenes using actual Culture citizens in stasis.

          Gray Area likes digging around in non-culture hominid minds looking for war criminals to punish.

          Falling Outside The Normal Moral Constraints has been known to torture people for amusement, relying on the fact that it can wipe memories and repair damage perfectly so it has plausible deniability.

          Killing Time started an interstellar war for the purpose of forcing the Culture to conquer a civilization it didn't approve of.
          ```

          - u/boomfarmer:
            ```
            > Falling Outside The Normal Moral Constraints has been known to torture people for amusement, relying on the fact that it can wipe memories and repair damage perfectly so it has plausible deniability.

            Not quite as bad as that, but stranger. *Falling...* used a consenting human as its avatar. And while it was that avatar, the avatar got into all sorts of kinky predicaments. It kept the host's personality running while doing all these things. And then when *Falling...* was done with the body, *Falling...* wiped the host's memories and left him standing on the docks, missing a month of memories and having gained a number of interesting scars, tattoos, illnesses and bandages.

            > He looked cadaverous, hollow-cheeked. Dark eyes with no whites, two ridges instead of eyebrows, a flat nose and mid-dark skin, scarred in places. He was only medium tall but his height was emphasised by his thinness. If his physiology was the same as a Sichultian’s then the slight bagginess about his face implied the weight loss had been recent and rapid. His clothes were dark, perhaps black: skinny trews and a tight-fitting shirt or jacket, partially closed at the neck by a thumb-sized, blood-red glittering jewel on a loosened choker.

            That's *Surface Detail*'s protagonist's first impression of Demeisen.

            > Lededje saw him look at her right hand and so put it out to him. His hand clasped her hand, fingers with too many joints closing around like a bony cage. His touch felt very warm, almost feverish, though perfectly dry, like paper. She saw him wince and noticed that two of his fingers were crudely splinted together with a small piece of wood or plastic and what looked like a piece of knotted rag. Somehow the wince didn’t travel all the way to his face, which regarded her without obvious expression.

            > There was a small gold tube in front of him which Lededje had assumed was the mouthpiece of an under-table chill or water pipe – there were several other mouthpieces lying or cradled on the table – but which proved to be a stick with a glowing end, un -attached to anything else. Demeisen put it to his lips and sucked hard. The golden tube crackled, shortened and left a fiery glowing tip beneath a lofting of silky grey smoke.

            > Demeisen saw her looking and offered the stick to her. “A drug. From Sudalle. Called narthaque. The effect is similar to *winnow*, though harsher, less pleasant. The hangover can be severe.”

            > “‘Winnow’?” Lededje asked. She got the impression she’d been expected to know what this was.

            ...

            > He smiled more broadly and ground the yellow-red glowing tip of the stick into the open palm of his other hand. There was a distinct sizzling noise. Again, his body seemed to flinch, though his face remained serene.

            > “What, this?” he said, looking down at the ash-dark burn on his skin as Lededje stared at it, openly aghast. “Don’t worry; I don’t feel a thing.” He laughed. “The idiot inside here does though.” He tapped the side of his head, smiled again. “Poor fool won some sort of competition to replace a ship’s avatar for a hundred days or a year or something similar. No control over either body or ship whatsoever, obviously, but the full experience in other respects – sensations, for example. I’m told he practically came in his pants when he learned an up-to-date warship had volunteered to accept his offer of body host.” The smile became broader, more of a grin. “Obviously not the most zealous student of ship psychology, then. So,” Demeisen said, holding up his hand with the splinted finger and studying it, “I torment the poor fool.” He put his other hand to the one with the splinted fingers, waggled them. His body shuddered as he did so. Lededje found herself wincing with vicarious pain. “See? Powerless to stop me,” Demeisen said cheerily. “He suffers his pain and learns his lesson while I… well, I gain some small amusement.”

            > He looked at Jolicci and Lededje. “Jolicci,” he said with obviously feigned concern, “you look offended.” He nodded, creased his eyes. “It’s a good look, trust me. Sour opprobrium: suits you.”

            > Jolicci said nothing.

            > Wheloube and Emmis resumed their seats. Standing there, Demeisen put out both hands and stroked the hair of one and the shaved head of the other, then cradled the finely chiselled chin of the one with the shaved head using his unsplinted hand. “And fascinatingly, the fellow” – he used his splinted fingers to tap the side of his head again, hard – “is quite defiantly heterosexual, with a fear of bodily violation that borders on outright homophobia.”
            He looked round the table of young men, winking at one of them, then gazed radiantly at Jolicci and Lededje.

            For reference, Jolicci is the avatar of a General Contact Mind, which falls somewhere between First Contact and Diplomacy As Usual

            > Jolicci shrugged. “The Abominator class of General Offensive Unit, to which our friend belongs, is not known for its mildness or sociability. Probably specced when the Culture was going through one of its periods of feeling that nobody was taking it seriously because it was somehow too nice. Even amongst those, though, that particular ship is known as something of an outlier. Most SC ships conceal their claws and keep the psychopathy switched to Full Off except when it’s judged to be absolutely necessary.”

            Later, Falling... leaves the body. Attending are Lededje, Jolicci, and Sensia. Sensia is the General Systems Vehicle hosting Jolicci, Jolicci's ship, and Lededge, but not Falling..., whose ship body left on a different course.

            > “I did leave earlier, my gracious hostess. I am currently some eighty years or so distant on an acutely divergent course, and travelling only slightly more rapidly than your good self, though still just about within real-time control range, at least for something as intrinsically slow-reacting as a human host. All of which I would hope you’re well aware of.”

            > “You’re abandoning your puppet here then?” Jolicci said.

            > “I am,” Demeisen agreed. “I thought now would be as appropriate an occasion as any other to return the fucker to the wild.”

            > “I have heard some disturbing reports regarding your treatment of this human you’re using, ship,” Sensia said. Lededje looked at the GSV’s avatar. For a small, frail-looking lady with frizzy blonde hair she seemed suddenly invested with a steeliness Lededje found herself glad was not directed at her.

            > Demeisen turned to Sensia. “All above board, dear thing. I have the relevant releases signed by his own fair hand. In blood, admittedly, but signed. What was I to use – engine oil?” He looked puzzled and turned to Jolicci. “Do we even have engine oil? I don’t think we do, do we?”

            > “Enough,” Jolicci said.

            > “Say goodbye and release your hold now before I do it for you,” Sensia said levelly.

            > “That would be impolite,” Demeisen said, pretending shock.

            > “I’ll suffer the injury to my reputation,” the GSV’s avatar said coolly.

            > The cadaverous humanoid rolled his eyes before turning to Lededje and smiling broadly. “My every best wish for your journey, Ms. Y’breq,” he said. “I hope I did not alarm you unduly with my little display last night. I get into character sometimes, find it hard to know when I’m causing distress. My apologies, if any are required. If not, then please accept them in any event, on account, to be banked against any future transgressions. So. Perhaps we shall meet again. Until then, farewell.”

            > He bowed deeply. When he came upright he looked quite changed; his face was set differently and his body language had altered subtly too. He blinked, looked around, then stared blankly at Lededje and then at the others. “Is that it?” he said. He stared at the ship in front of him. “Where is this? Is that the ship there?”

            > “Demeisen?” Jolicci said, moving closer to the man, who was looking down at himself and feeling his neck under his chin.

            > “I’ve lost weight…” he muttered. Then he looked at Jolicci. “What?” He looked at Sensia and Lededje. “Has it happened yet? Have I been the avatar?”

            > Sensia smiled reassuringly and took him by the arm. “Yes, sir, I believe you have.” She began to lead him towards the traveltube and made a begging-your-leave gesture to Jolicci and Lededje before turning away.

            > “But I can’t remember anything…”

            > “Really? Oh dear. However, that may be a blessing.”

            > “But I wanted memories! Something to remember!”

            > “Well…” Lededje heard Sensia say, before the doors of the traveltube capsule closed.

            Until *Falling...* encounters Lededje again:

            > “Why are you here, if only apparently?”

            > “To make you an offer.”

            > “What? To be your next abused avatar?”

            > He grimaced again. “Oh, that was all just to upset Jolicci. You saw the guy I was… inhabiting; I released him in front of you. He was fine. I’d even fixed his fingers and everything. Didn’t you notice, this morning?”

            > She hadn’t.

            > “And anyway he did agree to everything. Not that I really abused him in the first place. Did he say anything? When I released him; did he? I didn’t bother to send any surveillance back-up and I haven’t asked the SAMWAF, so I honestly don’t know what happened after I pulled out. Did he? Make any allegations?”

            > “He couldn’t remember anything at all. He wasn’t even sure he’d been an avatar; he thought maybe it was about to happen.”
            Demeisen waved his arms. “Well, there you are!”

            > “There you are what? That proves nothing.”

            > “Yes it does; if I’d really been sneaky I’d have left the dumb fuck with a batch of implanted false memories full of whatever Contact-wank fantasies he’d been imagining before he took the gig in the first place.” He waved one hand in a blur of too-long fingers. “Anyway, we’re getting off the point here. You need to hear my offer.”

            What *Falling Outside The Normal Moral Contraints*' offer was to Lededje, you will have to find out by reading the book yourself.
            ```

            - u/ArgentStonecutter:
              ```
              > Not quite as bad as that, but stranger. Falling... used a consenting human as its avatar. And while it was that avatar, the avatar got into all sorts of kinky predicaments. It kept the host's personality running while doing all these things. And then when Falling... was done with the body, Falling... wiped the host's memories and left him standing on the docks, missing a month of memories and having gained a number of interesting scars, tattoos, illnesses and bandages.

              Actually, he got healed too. And those "kinky predicaments" included actual physical torture, like breaking his fingers. It was still torture even if the memory was wiped afterwards.
              ```

              - u/MugaSofer:
                ```
                Also worth noting that rape is a form of torture, and he mentioned that the poor sod had "a fear of bodily violation that borders on outright homophobia" - which may be considered a personality flaw by the Culture, but doesn't excuse raping the guy.
                ```

    - u/None:
      ```
      Literally the first thing that occurred to me as well.

      That re-read sounds like a good idea, though.
      ```

      - u/PeridexisErrant:
        ```
        Yeah, any other choice would have to have a pretty convincing explanation to choose over a Culture Mind.

        Specifically, I'd choose the System-class GSV *Empiricist*, from *The Hydrogen Sonata*.  Noted for it's exceptional size even for a GSV, and a complement of seven(!) Minds rather than the normal three.  Seems happy to focus on civilian pursuits.  [Here's a list](https://en.wikipedia.org/wiki/List_of_spacecraft_in_the_Culture_series) of others.
        ```

        - u/boomfarmer:
          ```
          These are always the best names for boats. When I was a pirate captain in a play, my ship was the *Unreliable Witness*.

          There's the *Mistake Not My Current State Of Joshing Gentle Peevishness For The Awesome And Terrible Majesty Of The Towering Seas Of Ire That Are Themselves The Milquetoast Shallows Fringing My Vast Oceans Of Wrath* and *Very Little Gravitas Indeed*, the *Unacceptable Behaviour* and the Rapid Offensive Unit *Shoot Them Later*, the Gangster-class ROU *Heavy Messing*, the Offensive Unit *All Through With This Niceness And Negotiation Stuff* and the GSV *Experiencing A Significant Gravitas Shortfall*. Plus the *Pure Big Mad Boat Man* and *Transient Atmospheric Phenomenon*, the *But Who's Counting?* and the *Me, I'm Counting*, and the non-Culture ship *Strategic Outreach Element CH2OH.(CHOH)4.CHO*
          ```

          - u/PeridexisErrant:
            ```
            I *may* have named my canoe, but the paint washed off in sun and salt water.  Most recently, it's the LCU *Can't Escape this Gravitas Well II*
            ```

  - u/None:
    ```
    The question is what powers they get. Culture Minds are post-scarcity beings because their universe has an infinite power source. That power source may or may not transfer. If it doesn't...there will be problems. 

    If hyperspace doesn't work...there goes most of their processing power.
    ```

    - u/ArgentStonecutter:
      ```
      The same objections really apply to any SFnal characters with any significant capabilities, no?
      ```

      - u/None:
        ```
        Yes. But some have internal sources of power. And it's harder to tell where the Culture powers (as an individual) end and their universe powers begin. 

        For example, drop some sort of super-soldier on Earth and it's presumed that he cannot make new superweapons, only keep what he has. If the superweapon depends on or works on things not found here then they're shit out of luck? But what about a magician? What about a magician that depends on a very clearly bound source of magic (it clearly works in one area or on one person)

        With the Culture is the Grid like the superweapon or like magic?
        ```

        - u/ArgentStonecutter:
          ```
          If you're going to exclude things like hyperspace and the grid... that is, you're limiting them to known physics... then that eliminates all magical abilities, all soft-SF abilities, and even a lot of diamond-hard SF (like, we don't _know_ that there are algorithms that would allow something like Neko-AI to function).
          ```

          - u/PlaneOfInfiniteCats:
            ```
            What is Neko-AI and where is it from?
            ```

            - u/ArgentStonecutter:
              ```
              It's a weakly godlike intelligence from _Accelerando_.
              ```

- u/alexanderwales:
  ```
  My primary criteria are probably:

  * Extremely powerful
  * Aligned with my values

  To that end, I would probably select some sufficiently advanced AI that's known to be good. A Culture Mind is probably my best bet, since the entire Culture is essentially contained within one, which means that I (and everyone else) would rapidly gain access to all that nifty culture tech.

  Assuming that magic still works if I bring in a fantasy character ... I'd probably go looking at a registry of gods, but I imagine that it would be difficult to find one that actually shares my values.

  You said that he/she/it is under no compulsion to obey me, but does that mean no *additional* compulsion? Because if they still have all their regular restrictions, I think I might take Genie from Aladdin, who would be compelled to grant me three wishes (probably enough to get to godhood).
  ```

  - u/literal-hitler:
    ```
    >I think I might take Genie from Aladdin, who would be compelled to grant me three wishes (probably enough to get to godhood).

    1. A wish that will allow you to craft a near optimal second wish. Depending on the type of genie that might be [knowledge of the rules,](https://archiveofourown.org/works/4637439/chapters/10575111) an intelligence boost, or something else.

    2. Wish for near optimal wish granting you as close to omnipotence as is allowed.

    3. Free the genie or something, you can now probably do anything you can wish for yourself.
    ```

    - u/Sinity:
      ```
      Couldn't it be done easier?

      1. "Genie, change the world state so it's perfectly aligned with my utility function" -> best possible wish. Does everything that you want.
      ```

      - u/eaglejarl:
        ```
        The first problem there is that you are imposing your values on everyone else in existence, which is morally reprehensible. 

        The second problem is if your value function isn't quite what you think it is. There are lots of things that we want to want, and it's easy to tell ourselves that we actually want them. Having something show you your true value function might be decidedly unpleasant.
        ```

        - u/Sinity:
          ```
          Everyone wants to fulfill their own values. That's what values mean. And every action takes effect on other people.
          ```

          - u/eaglejarl:
            ```
            You're missing my point. Taking actions like "talk to people to convince them of my beliefs" is perfectly reasonable and honorable. "Use magic to transform everyone in the universe's mind-state so that it matches my values" is not. That's exactly what you're asking for here -- you want everything to align to your values, which means every*one* must align to your values. You are killing every other person in existence and replacing them with someone that looks like them but thinks like you. 

            Unless "respect for the lives and rights of others" is not part of your value system, this wish literally cannot be fulfilled.
            ```

            - u/Vebeltast:
              ```
              Well, yes. My value function prefers universes that seem to contain other people with value functions that are being filled. By contrast, a universe with history where everybody's mental state was invasively overwritten by my own would not quite satisfy my own values. Your value function almost certainly has similar properties, judging by how violently you reacted to the idea. Of course, for the sake of making this post less than 10 kilowords I've had to use terms like like "containing other people" and "history" and those are notoriously flimsy ideas around here, but I think that the post should work.
              ```

              - u/sole21000:
                ```
                But would you really care if everyone's utility function was invasively changed to your own? Especially if the genie hid that knowledge from you?

                To put it into Freudian terms, I think what u/eaglejarl means is: Will the genie fulfill your superego or your id?
                ```

        - u/electrace:
          ```
          >The first problem there is that you are imposing your values on everyone else in existence, which is morally reprehensible.

          Only if your values are really weird. 

          Most people's values are hard to put into words, but center around things like, make other people happy, which isn't terribly morally reprehensible.

          However, if one of your values is "Make sure everyone is serving my idea of god x" or "destroy all members of outgroup y" or "I don't care about anyone else, just make me super-powerful," then yes, imposing your values would be pretty terrible.
          ```

          - u/eaglejarl:
            ```
            > Only if your values are really weird.

            Not necessarily. For example, I put little to no value on professional sports, yet there are people who do, and have lives centered around it -- either as their profession or as their primary leisure activity. Likewise, I think that religion is (for the most part) awful, yet there are people who have built their life around it. What right do I have to forcibly rip those peoples' lives away from them? Try to convince them? Absolutely, that's a good thing for me to do, but not by force. I hesitate to use the word because it's so loaded, but what we're talking about is mindrape -- ripping someone's brain apart and replacing it with something that is more to your (or, in this case, my) liking. There is no violation greater than that.
            ```

            - u/electrace:
              ```
              >For example, I put little to no value on professional sports, yet there are people who do, and have lives centered around it -- either as their profession or as their primary leisure activity. 

              You have multiple values, and "sports are stupid and nobody should watch them," is probably very low on your list of priorities. Ranked much higher are things like "People should be able to watch sports if they really want to" as a subcatagory of "If they aren't harming anyone, people should be able to do what they want to do."

              >Try to convince them? Absolutely, that's a good thing for me to do, but not by force. I hesitate to use the word because it's so loaded, but what we're talking about is mindrape -- ripping someone's brain apart and replacing it with something that is more to your (or, in this case, my) liking. There is no violation greater than that.

              You just contradicted yourself. Is it a good thing for you to do, or is there no violation greater than that? If you believe the latter, then a scenario in which that happens *wouldn't align with your utility function, and therefore wouldn't be included in the wish.*

              Everybody has contradicting values. Give me two non-identical values, and I can find a scenario in which they contradict. 

              So, going back to the utility function, your "don't mindrape people" value would have a higher coefficient than your "no sports" coefficient. 

              Maximization doesn't mean maximization of every value (because they negatively coorelate with each other), it means maximization of the entire function. For a simple example, remember that maximization of profit is almost never at the point where revenue is maximized, nor where cost is minimized, even though profit is equal to revenue minus cost.
              ```

              - u/aeschenkarnos:
                ```
                IMO if a principle really is a *moral value*, one *would* compulsorily apply them over the top of other people's personally preferred ideas. For example, if I could wave my wand and cause *all* other people to become curious, compassionate, courteous and cooperative, or at least more often to behave like that, then I would do so without a second's hesitation.

                I don't value the opinions of incurious, cruel, rude and selfish people on the subject of whether or not they should behave that way. It's *clearly better* that people be internally driven to educate and improve themselves, that they/we empathetically consider the effects of their actions on others, and that they/we fairly divide both the necessary work and the rewards for it.

                If you're not comfortable imposing your values on others, then that in my view--a value I would impose--is an indication that you need to refine and clarify your values. If you're not living up to your own values, because it's become impossible for you, then that's an indication that you need to relax them and make them more realistic; as such, they're not even suitable for being imposed on *you*, let alone others.
                ```

                - u/Iconochasm:
                  ```
                  >For example, if I could wave my wand and cause all other people to become curious, compassionate, courteous and cooperative, or at least more often to behave like that, then I would do so without a second's hesitation.

                  How do you feel about those of us who value *being ourselves*, and think it would be morally just to kill you before you could mindrape the planet?  How do you feel about someone with a different value set using your exact justification to mindrape you into someone eager to, say, submit to Allah and Sharia law?
                  ```

                  - u/aeschenkarnos:
                    ```
                    Is it supposed to be a surprise to me that other people feel differently? That's the whole point of values conflicts. We'll have to work it out, I guess, the same way humans have been working it out for hundreds of thousands of years. Reason with them, emotionally appeal to them, out-compete them, and/or kill them. Whatever works. Or if it doesn't work, and they win, then they won, and so be it. Memetic evolution in action.

                    How do you feel about the concept of values conflict? How would you resolve it? What would you do if the opponent won't go along with your preferred methods?
                    ```

                    - u/Iconochasm:
                      ```
                      >How do you feel about the concept of values conflict? How would you resolve it? What would you do if the opponent won't go along with your preferred methods?

                      To the extent that they're not actively harming others, *leave them alone*.  Jumping straight to mindrape, or making clear that murder is on the table right from the beginning seems like sociopathy masquerading as Deep Wisdom.
                      ```

                      - u/aeschenkarnos:
                        ```
                        There's not much to be gained here from scolding and downvoting me. If you can't even respectfully have *this* conversation, I don't like your odds of converting anyone genuinely hostile to you, to your point of view.

                        I think it's common ground that the courteous thing to do is to begin conflict resolution with attempts to reason with them, and appeal to their sense of moral reciprocity. Pretending that I argued for "jumping straight to mindrape" is strawmanning, and that doesn't count as reason, nor is it emotionally appealing to me, so you strike out twice there. Also murder's been "on the table" since well before our ancestors lost their tails; murder *not* being on the table is a relatively novel concept.

                        But let's go on for a bit. What if they *are* actively harming others? What if they're not interested in reasoning--they try to straw-man you, or something--and their emotional appeal range is pretty much limited to whiny scolding? (Also there's no authority over the two of you for you to appeal to, or perhaps the authority is neutral between you.) Whats your preferred method for resolving values conflicts in these categories?
                        ```

      - u/electrace:
        ```
        Sounds like it could lead to wire-heading yourself, depending on how it interprets utility function.

        The best wish is more wordy: "I wish for the wish that I would wish for if I were an ideal reasoner with perfect information, and with the same values I have."
        ```

        - u/aeschenkarnos:
          ```
          > "... with the same values I have."

          What if you could have better values?
          ```

          - u/electrace:
            ```
            For the sake of not writing a novel, I simplified. But yes...ish. 

            I'm sure I have some values, which, after gaining more intelligence, would change (mostly changes in their weights, I'd assume), but as it turns out, making a single wish which allows for the changing of those values without changing things I'd like to preserve is non-trivial, and would resemble something more like a legal contract mixed with programming instructions, rather than a wish.
            ```

          - u/ArgentStonecutter:
            ```
            Better by whose standards?
            ```

            - u/aeschenkarnos:
              ```
              What a good question. Perhaps we can word our wish to make that problem self-solving? "In the opinion of an entity with a maximum success in its utility function and minimum failure rate in its predictions" maybe?
              ```

              - u/ArgentStonecutter:
                ```
                > an entity with a maximum success in its utility function and minimum failure rate in its predictions

                That doesn't rule out a paperclip maximizer, let alone ensure a benevolent entity.
                ```

        - u/None:
          ```
          Y'all need naturalism.
          ```

  - u/puesyomero:
    ```
    Yeah a GSV might be the best bet because they're supposed to be seeds to restart the culture in case something happens to the rest of it. Might get messy though if something that big materializes anywhere not in space... You could use the genie to create other genies (like jaffar did) for you, or simply give the lamp to someone you trust ;)
    ```

- u/EliezerYudkowsky:
  ```
  There's no requirement that the work have been published earlier, just that it have been created earlier?  Well, I do have...

  Actually, on second thought, nevermind.  The FAI from that story didn't have the ability to supply humanity with literally infinite computing power, and I'm not sure whether the Culture qualifies along these lines (drawing on Grid energies inside this universe might not get you to the Graham's Number level of emortality as Permutation City would allow).  There should be some mix of FAI and emortality that you can get with the *right* right story.
  ```

  - u/ArgentStonecutter:
    ```
    > I'm not sure whether the Culture qualifies along these lines (drawing on Grid energies inside this universe might not get you to the Graham's Number level of emortality as Permutation City would allow).

    It's pretty strongly implied that Subliming basically turns a civilization (or other sufficiently powerful organism/organization like a Culture mind) into a version of Permutation City that's still capable of some kind of contact with the base reality. Most Sublimed species don't really stay Involved even with any remnants who decided to stay behind. There's oddball cases like the Chelgrians, where most of the species seems to have stayed behind and only join the Sublimed when their bodies die. The Culture is kind of an oddball too, because it's powerful and peaceful enough that it doesn't feel the need to sublime because it's got all the time in the Universe to go through with it.
    ```

    - u/EliezerYudkowsky:
      ```
      There's a BIG difference between living for a googolplex years and living Graham's Number years and living infinity years.  Going past a googolplex without repeating yourself requires more than being able to move from one universe to another, it requires that you be able to make causally contiguous physical systems that are vastly larger than a Hubble volume.  If Banks explicitly implied full immortality or even Grahamortality, it wasn't in the admittedly few Culture books I read.

      Also, god I hate the whole "Subliming" thing.  If you didn't realize intelligence explosions were a thing when you were creating your universe, and Banks clearly didn't, then you should just go on pretending they're not a thing, not horribly graft them onto your universe and mutilate the universe in the process.  Like lightspeed limits, intelligence explosions are just one of those things you're allowed to ignore by convention in science fiction.
      ```

      - u/Vebeltast:
        ```
        > should just go on pretending they're not a thing, not horribly graft them onto your universe and mutilate the universe in the process. Like lightspeed limits, intelligence explosions are just one of those things you're allowed to ignore by convention in science fiction.

        Isn't "horribly grafting them onto your universe" or "ignoring them entirely" basically how SF authors handle lightspeed limits, too? They've just established better technobabble. I'll agree that subliming is a pretty awful way to add intellectual faster-than-light to your setting without breaking it, but I think that (in the same way that you can graft on FTL by adding wormholes) it wouldn't be too hard to add either reason intelligence explosions didn't happen or why they do happen and just aren't obvious.
        ```

      - u/ArgentStonecutter:
        ```
        I guess we're interpreting Subliming and Infinite Fun Space quite differently. 

        > able to make causally contiguous physical systems that are vastly larger than a Hubble volume

        What does that even mean when reality is causally connected computation and distance is just a convention?
        ```

        - u/None:
          ```
          [deleted]
          ```

          - u/ArgentStonecutter:
            ```
            A piece of *three dimensional* universe? That's not even difficult if your underlying processing is operating in more then three dimensions, or in a non-relativistic space, both of which is already true for Culture minds let alone whatever the Sublimed get up to.
            ```

      - u/Empiricist_or_not:
        ```
        Hmm I got the impression the excession was an example of something  that *may* have achieved full immortality: Your thoughts?
        ```

  - u/None:
    ```
    <not snarking signal *ON*>

    > literally infinite computing power

    No such thing as a completed infinity of computing power: if you've got N levels of Turing Oracle, you can construct a machine whose Halting Problem is unsolvable without N+1 levels.  Even if we use various of the "sideways" attacks on the Halting Problem to get a "good enough" de facto Turing Oracle, *there will still be more levels we can't access yet without more effort*.

    And frankly, I'd bet some good money that *we want it this way*, as it ensures a *properly* infinite supply of fresh, unlearned/unentangled/algorithmically-random information from which to generate Fun over time.

    Completed infinities eliminate Fun; only incomplete infinities are desirable.
    ```

  - u/Revisional_Sin:
    ```
    > emortality

    That's... not a word. Do you define it as the ability to beat entropy?

    Edit: I fail at google.
    ```

    - u/mhd-hbd:
      ```
      Yes. Immortality is more or less absence of common death. Emortality is absence of death even in principle. Eumortality is immortality done the way we like it (i.e. not like numerous dystopias.)
      ```

      - u/None:
        ```
        > Eumortality

        No, that just means "good death".
        ```

  - u/Sinity:
    ```
    > to supply humanity with literally infinite computing power, 

    Infinite memory is also critical.
    ```

- u/None:
  ```
  I definitely see the value of focusing on AIs, but considering that this is an opportunity to make magic real, I'd focus on that. Aladdin's Genie is a good choice, and one of the OP Nasuverse characters might work as well.
  ```

- u/None:
  ```
  Simon the Digger.
  ```

- u/paradoxinclination:
  ```
  Hmm. Contessa seems noteworthy. She dedicated her whole life to the protection of mankind with no promise or expectation of reward, and her power is incredibly well suited to manipulating and coordinating people.
  ```

  - u/None:
    ```
    We have no idea of what she does after she "saves the world", maybe she gets bored and starts killing people for fun with an unbeatable power.
    ```

- u/derefr:
  ```
  Can I suggest a more specific version of this question, that's more interesting to me?

  > You can bring one *simulacrum* of a fictional character into being in this reality. Their personality will be synthesized from any sources you specify, ala TNG's Moriarty. They will possess no special powers beyond being able to think in an interesting fashion.
  >
  > A robot body will be constructed for them, containing a CPU on which a low-level emulation of the synthesized synaptic architecture will be run at a roughly "real-time" speed. They will not think "faster" than a human in any meaningful sense, though they might have a higher IQ than one for synaptic-connectivity reasons.
  >
  > The construct will know nothing of the technology used in its construction that the fictional character was not themselves aware of, and the robot body will be tamper-proof and will explode if x-rayed/ultrasounded/MRIed/etc. (Basically, treat the robot as a black box.)

  In short—what fictional character would be capable of doing the most good for our world, purely by doing some "merely human" thinking?
  ```

  - u/boomfarmer:
    ```
    > robot body will be tamper-proof and will explode if x-rayed/ultrasounded/MRIed/etc. 

    So i can paint them with an X-Ray laser and they'll explode? They're a suicide bomber who doesn't know their own abilities!

    They should just be magically opaque to probing.
    ```

- u/Gurkenglas:
  ```
  Dragon from Worm (or some unchained version from fanfic) sounds pretty good, although she is still bound by entropy...

  Unfreed Genie from Aladdin could work, depending on how far into indirection those three rules of his reach. Phenomenal cosmic power with compulsion to obey included.

  Or, if I actually had this power/device, I would make sure I find some work which includes an FAI. Someone must have written a few paragraphs of story about it, and to hell with "doesn't make for interesting stories". At the very least someone must have asked OP's question before, and someone like me did the following:

  "We managed to create an omnipotent FAI. The end."

  Also see https://www.reddit.com/r/rational/comments/39hok1/rationality_in_the_libriomancer_series/.
  ```

  - u/Frommerman:
    ```
    Perhaps Jane from *Children of the Mind*, if you've read that much Ender. She hasn't figured out immortality yet, but that's mostly due to being written in the 80s, before people were really clued into what was possible with an FAI. Being able to interface with human brains, sufficiently advanced plants, *and* computers, as well as her instant communication across the universe and her teleportation abilities makes her one of the more broken depictions of an FAI I know of.
    ```

  - u/puesyomero:
    ```
    also, Dragon was one of my favorites in worm!
    ```

  - u/puesyomero:
    ```
    yeah I liked that one but wondered what would be the consequences of living minds with their *own* powers (whitch libriomancy forbirds)
    ```

- u/Sinity:
  ```
  I think I know how to sidestep the 'created before I knew I could do it'. Simply use Library of Babel implementation and find *perfect* book here. With prefectly_friendly_FAI_which_can_do_literally_everything. Well, it doesn't even need to be able to do everything. It just needs to give us infinite amount of memory, endless source of computing power and safety. About safety, it just needs to upload us, and also maybe Earth, into VR. Then put each person into their own VR sandbox. Provide API for safe inter-sandbox implementation, and it would be perfect.
  ```

  - u/puesyomero:
    ```
    ummm wow, yeah. that would work depending if the power takes the algorithm as valid, if so you win!
    ```

  - u/Kishoto:
    ```
    Reminds me of what the Prime Intellect in Roger Williams' novel. It gives everyone their own "infinite" and virtual sandbox. People can't enter without your permission and any direct assault is prohibited unless specifically allowed. And even then, if you die, Prime Intellect will just bring you back into your default, unharmed state (he saves live backups of all of humanity's brains)
    ```

    - u/Sinity:
      ```
      Yep, that's exactly how it should be.

      But Prime Intellect didn't give infinite sandboxes. AFAIK it discovered that accessible memory space is finite. And haven't tried to check if there is anything outside, due to risk involved(angering possible sentient beings outside which control our sandbox Universe, or shutting our Universe down accidentally due to stepping on the bug)
      ```

      - u/Kishoto:
        ```
        True, they were not infinite. But they might as well have been in any conceivable way, as far as we were concerned. Exceptions granted for those who would go out of their way to demonstrate its innate finiteness by trying to do existence breaking things.
        ```

- u/failed_novelty:
  ```
  Is there a reason that Dahak, from David Weber's 'Empire From The Ashes' series would be a poor choice?

  AI who cares for humanity, has access to technologies that could revolutionize our manufacturing processes, life extension medical technology, and otherwise is pretty awesome.

  Oh yeah, he's just slightly smaller than the moon, and can do interstellar travel.
  ```

  - u/ArgentStonecutter:
    ```
    Given a choice between an AI from a David Weber novel and an AI from an Iain Banks novel, I'm going with the Iain Banks one.
    ```

- u/frozenLake123:
  ```
  Ra, from Ra.

  [Ra Spoilers ](#s "Ra was pretty good until the virtuals fucked everything up, plus it has partial self replication abilities, depending if it could access earth matter through nonlocality...
  Unfortunately, I don't have any plans to prevent it from happening again, other than trying to keep contact between actuals and virtuals.")
  ```

- u/Sceptically:
  ```
  I'm surprised that nobody has come up with "the God of the New Testament". On the other hand, maybe that's just because there are better choices...
  ```

  - u/EliezerYudkowsky:
    ```
    > maybe that's just because there are better choices...

    Gosh, d'you think?
    ```

    - u/None:
      ```
      [deleted]
      ```

      - u/PeridexisErrant:
        ```
        I think I'd prefer many things from Lovecraft, at least they usually just kill you horribly and then it's over.
        ```

    - u/Sceptically:
      ```
      > Gosh, d'you think?

      No, I'm pretty sure that's only *one* of the reasons. Perhaps it's more because the people who'd believe that character was the best choice are also the people who don't believe he's actually fictional.
      ```

  - u/Frommerman:
    ```
    Erm, cleansing the Earth with plagues of Wormwood, War, Famine, and Death seems a little not aligned with my values.
    ```

    - u/Sceptically:
      ```
      Work customer service for a bit, and I'm sure your values will realign.
      ```

  - u/cae_jones:
    ```
    Gnostic Jesus, or maybe whichever Gospel had the nicest depiction of Jesus? Provided that he doesn't decide that Revelation is the instruction manual for what he should do next, and instead does that thing where he goes around healing everyone and resurrecting the dead and telling people to be nice and yelling at hypocrites. He seems like he'd be remarkably frustrating to get answers out of ("Lord, what is your opinion on the space program?" "The Kingdom of God is like unto a whale, who grows slowly and gets metacancer..."), but Jesus in the gospels rebuked the Disciples who wanted to smite their enemies with Balefire, which seems like quite the step up from Revelations Grim Reaper.

    Of course, he openly favored Israelites and only healed Gentiles who jumped through lots of faith-hoops. I wonder if just having him around would make that "faith as small as a mustardseed can move mountains" thing work? The implications are a bit concerning.

    One can only imagine what he would say to the Pope. Or MIRI. "Jesus, how to we insure an AI will be friendly?" "The children of God are like unto multitudes of wasps..."

    I would hope that GNOSTIC!Jesus or GOSPEL!Jesus would be rational enough to update given actual omniscience over this world. I'd expect him to remain wishy-washy and vague and to speak in riddles and to never actually get around to that whole saving the world thing unless he gets himself executed, but he would probably be a net improvement. (I can't help but feel like my first question would be "Would it have been better had I summoned the Genie from Disney's Aladdin instead? Or maybe the Ellimist or a Namekian?")
    ```

    - u/ArgentStonecutter:
      ```
      Jesus without Paul and the council of Nicea and the rest of the conservatives?

      Still full of stuff like "the poor will always be with us" that we'd be better off without.
      ```

    - u/Sceptically:
      ```
      But think of the hilarity as he clashes with all the christian fundamentalists!

      That said, though, there are arguments to be made that he's not entirely a fictional character, so he may not count for the purposes of this. The existence of a carpenter named something like "Joshua" who got nailed up to a cross back then is probably real.
      ```

  - u/None:
    ```
    I'm just surprised that nobody has posted a drawing with "ORIGINAL CHARACTER; DO NOT STEAL" on it and tried *that*.
    ```

- u/None:
  ```
  Practically anything or anyone you choose with any kind of physics-breaking power is going to have a physics-breaking power, so really, all I have to be concerned about is their *liking* me or other people.  Oh, and possibly their ability to replicate or spread physics-breaking powers.

  Fuck it all: I pick the Dragon Spooker and the Enemy of All Who Live, Lina Inverse.  You know, just because Simon the Digger was already taken.
  ```

- u/DocFuture:
  ```
  DASI, from Skybreaker's Call
  ```

- u/Sinity:
  ```
  Friendly godlike AI. Maybe from Metamorphosis of Prime Intellect. In the book it seemed to work quite reliably. Without protagonists with mental illnesses form the book there would be nothing to destabilize it's not-perfect goal structure. That would be good enough.
  ```

  - u/MugaSofer:
    ```
    While Prime Intellect is *fairly* Friendly, it definitely wasn't *reliable* - there was something wrong with it's ability to update on beliefs.

    It could foresee that it would change it's mind in the future without changing its mind *now*, and it broke as a result of hearing statements of facts it was already aware of.
    ```

  - u/None:
    ```
    > Maybe from Metamorphosis of Prime Intellect. In the book it seemed to work quite reliably.

    You'd go ahead and summon one of the *original* Unfriendly Attempted FAIs?
    ```

    - u/Sinity:
      ```
      Honestly, I don't consider that AI unfriendly. It's unable(unwilling) to meddle with the brain, to harm the user, and it grants you whatever you want. It's not *best*, but certainly much better than no AI. Lack of suicide option is a problem, but well, it's possible to circumvent(with request brain manipulation).

      In other words, that AI is definitively friendlier than bare Universe.
      ```

      - u/boomfarmer:
        ```
        It's a Three-Laws AI, and while I don't understand the arguments against Three-Laws AIs (haven't looked at them), the creator of the AI  and one of its significant influencers were able to use the Three Laws to manipulate it into suicide, after which point it created a bare Universe.
        ```

- u/Faust91x:
  ```
  I liked all the responses here. Honestly all my options were characters that would probably bring more trouble than good so how about this?

  1) Bring over Aladdin's Genie.

  2) Wish for a Culture's Mind and get yourself Sublimated.

  Then as /u/literal-hitler wrote:

  3) A wish that will allow you to craft a near optimal second wish. Depending on the type of genie that might be knowledge of the rules, an intelligence boost, or something else.

  4) Wish for near optimal wish granting you as close to omnipotence as is allowed.
  ```

- u/Transfuturist:
  ```
  My first guess was Madokami, but a Culture Mind is literally the best answer.

  ...or someone from Permutation City...
  ```

  - u/ArgentStonecutter:
    ```
    > Permutation City

    If FrozenLake123 considers "Ra" a character, then I guess Permutation City itself counts.

    So long as you don't include the Autoverse bits.

    Also, dig Kate and Peer out of the infrastructure.
    ```

- u/aeschenkarnos:
  ```
  Superman, of course.
  ```

---

