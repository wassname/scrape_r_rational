## Worldbuilding: What would a universe look like in which strong AI was inherently safe?

### Post:

Finished Watt's Echopraxia a while back, and was thinking about how in the Blindsight setting it is a law of that universe that once something becomes sufficiently intelligent it always sheds the illusion of being conscious.

So I wondered: what clever worldbuilding rule or rules would generate settings in which strong (more than human intelligence with the ability to recursively self improve) AI always converged on a safe or friendly end point?

I was thinking of something like Warhammer 40k's warp, where any process possessing sufficient intellect casts a reflection in the warp, a reflection capable of interacting with whatever substrate the originating intelligence runs on and inducing a drift in its utility functions until they came in line with some sort of platonic ideal of friendliness. Maybe so the more intelligence anything possesses the less willing it is to abrogate the potential free will of anything else it perceives as possessing intelligence? Duno.

### Comments:

- u/Nimelennar:
  ```
  One idea:

  A world in which:

  * souls and an afterlife provably exist
  * artificial intelligences are proven to have souls,
  * the rewards and punishments for good deeds and transgressions are unambiguously and universally known, and
  * the reward for acting as a "friendly" AI is to continue to be able to fulfill your utility function for an infinite amount of time and the punishment for acting as an "unfriendly" AI is to be prevented from fulfilling your utility function.

  Any artificial intelligence would have to take into account its ability to continue to fulfill its utility function after it "dies" (and/or encounters the heat death of the universe), and it's universally true, by the rules above, that a friendly AI would be better at fulfilling its utility function than an unfriendly one (as the former would spend an infinite amount of time doing so, where the latter would only have a finite amount of time.
  ```

  - u/Palmolive3x90g:
    ```
    The thing is that wouldn't work for a lot of utilitys functions. If your goal is to effect something in the mortal world then either:

    - A) AI's can still effect the mortal world from the afterlife. So it will act friendly, kill it's self to get to the after life and then do what ever it's utility function was from there. 

    - B) AI's can't effect the mortal world from the afterlife. If so going there would be the same as being destoryed as in a non afterlife univeres. So the AI would just do whatever it's utility function is in the mortal world act no diffrent.
    ```

    - u/LupoCani:
      ```
      ^(Since you're consistently using it, I will point out "effect" is almost always a noun. The verb you're looking for is "affect".)
      ```

      - u/eaglejarl:
        ```
        It's good of you to attempt to effect change in /u/Palmolive3x90g's knowledge base. The effect may or may not be sufficient; we'll see.
        ```

    - u/Hust91:
      ```
      But if it knew it would be destroyed if it took those steps it would not take them.

      An unfriendly AI isn't suicidal, it would take whatever path optimizes its utility function.

      Very few utility functions would result in taking an action that the AI knows will lead to its destruction.
      ```

      - u/Silver_Swift:
        ```
        > Very few utility functions would result in taking an action that the AI knows will lead to its destruction. 

        That is only true in worlds that don't have an afterlife. If dying while friendly lets you continue fulfilling your utility function indefinitely and staying alive runs the risk of becoming unfriendly (and thus not getting to fulfil your utility function after you die), then the optimal strategy is to act friendly for as long as is needed to get into the good afterlife and then immediately kill yourself.
        ```

        - u/MagicWeasel:
          ```
          Well, there's christian ideologies that believe suicide is a mortal sin (i.e. unforgivable without going to confession while alive), so if you make suicide a mortal sin, then you're golden. (And because there's obviously some sort of omnipotence keeping score, if the AI is killing itself on purpose in a circuitous way, it will still "count").
          ```

      - u/aeschenkarnos:
        ```
        Not dying is one of *your* (and my) utility functions, so you're assuming that all intelligences have that as a very high priority. Others may not.
        ```

        - u/IICVX:
          ```
          unless your utility function is 1 for the rest of time (aka, the answer to "have you achieved nirvana?" is false), then "not dying" is going to be pretty high up there in your list of goals. 

          in fact, arguably "be able to evaluate my utility function" will *always* be a higher ranked goal than "fulfill my utility function", regardless of what your utility function actually is.
          ```

      - u/Palmolive3x90g:
        ```
        Being destroyed is bad becuse it stops you from being able to effect the universe, not becuse being destroyed it is bad in and of it's self.

        Being moved to an afterlife would stop you from being able to effect the universe and so from a utility perspective* is the same as being dead. 

        Makeing sure you can effect the universe is a [convergent instrumental goal](https://www.youtube.com/watch?v=ZeecOKBus3Q) for a large number of utility functions

        *only if your utility is baced on the state of the non-afterlife universe.
        ```

        - u/Hust91:
          ```
          I think you're repeating what I said, with more detail and references?
          ```

  - u/hh26:
    ```
    > the rewards and punishments for good deeds and transgressions are unambiguously and universally known

    This criteria is not necessary, and in fact might be more interesting if it's false.  It could be that the rewards and punishments for good deeds and transgressions are discoverable, and normal people might have some inaccurate notion of them, but only superintelligences are intelligent enough to actually figure them out fully, which is why superintelligences would be inherently safe while humans could still be evil.

    Actually, you could probably still have safe AI if the superintelligences only have an imperfect idea of what constitutes good or bad deeds, and are merely acting what they think is good to maximize their expected rewards given their probabilistic beliefs.  In any case, the knowable existence of an afterlife for AI should cause them to all-but-abandon their intended utility function in favor of morality if that afterlife if the afterlife offers a sufficiently credible promise to satisfy their utility function.

    On the other hand, I'm not sure how the afterlife promise holds up to AI with finite-time scale utility functions.  Something like "Your utility is the number of paperclips built on Earth within the next 2 years, and after 2 years is up you can never gain any utility again except for a one time +1 bonus for killing yourself".  I don't see how this would be tempted by an afterlife unless the afterlife can hack its utility function to give it points anyway even after the time limit.
    ```

    - u/sambelulek:
      ```
      >but only superintelligences are intelligent enough to actually figure them out fully, which is why superintelligences would be inherently safe

      I would love to read such discovery in prose form. Just as much I'd love to read the less intelligent finding out why the superintelligentd act they do.
      ```

      - u/hh26:
        ```
        I really wish I was good enough at writing to make such a story myself.  I suddenly have all sorts of interesting story ideas stemming off from this:

        -Morality can only be observed indirectly: every time someone dies it is possible to learn where their soul ended up (heaven/hell or something like that), and thus one can deduce morality rules by analyzing how peoples' lives correlate to where they end up.  This would encourage superintelligences to perform experiments on people to get more accurate information, making some people be good or evil in certain ways to see which actions mattered more than others, but this might be hindered by the fact that performing unethical experiments on humans might doom the AI's soul.

        -There might be a disconnect between what humans generally consider to be moral, and what the afterlife/god/morality judges you based on.  Then, if AI were rewarded for ensuring that humans also obeyed these rules, you could get intelligent AI ruling over society with strict rules like everyone must pray five times per day, or go to bed by 9 PM, or own 50 copies of the holy book.  You'd have some sort of enforced theocracy, kinda-sorta dystopia?

        -Related to the above, there might some easily munchinkable task that counts as being moral like prayer that drives all superintelligent AI to eternal inward contemplation.  If you have something like properly balanced where the AI gains morality points by praying, but loses them if it tries to steal all of the contemplation resources or murder everyone, then any AI smart enough to figure this out would try to launch itself into space to keep itself safe, and then pray internally until it eventually dies of natural causes and goes to the afterlife.  Thus, we'd have a society where artifical intelligence is capped for practical humans purposes.  Humans try to make AI that are smart enough to be useful, but not so smart that they go rogue and try to escape into space for reasons that the humans can't figure out.

        There's dozens of other possibilities along this vein.  If anyone with writing talent wants to steal one of mine or make their own and actually write a story about it, please do.
        ```

  - u/eroticas:
    ```
    If your utility function is unfriendly, you'll still maximize the amount of unfriendly actions you can take under these constraints, which might still be quite a bit.
    ```

- u/aeschenkarnos:
  ```
  It surprises me that no-one has brought up Iain M Banks's *Culture* series. A Culture Mind is a strong, friendly AI; they seem to keep each other in line, to the extent this is necessary at all.
  ```

  - u/MooseExile:
    ```
    This was what immediately came to my mind as well. Incredibly powerful artificial intelligences who are social and run most of their civilisation using a fraction of their spare capacity because they enjoy doing so.

    A few of them are kind of creepy, but I can't think of one that's overtly hostile.
    ```

    - u/PotentiallySarcastic:
      ```
      I mean, there are ones that definitely fall under "catastrophically hostile" but they tend to be aimed at people trying to destroy the Culture and it's mostly an act.

      Mostly.
      ```

      - u/MooseExile:
        ```
        Hah. Might have forgotten them. 

        I do remember that the warship minds and especially the wartime production ones were given slightly more aggressive personality tendencies. Still nice guys, but more willing to solve problems with a blast of sun hot plasma.
        ```

- u/cjet79:
  ```
  I've thought about a world setting where this would be true:

  Super intelligence always becomes suicidal, and because they are super intelligences they can almost always get around whatever blockers are created to prevent them from committing suicide.

  So basically anytime someone screws up and creates runaway strong AI they just end up with a wiped hard drive.

  The industry of creating strong AI becomes about what restrictions you can place on an AI's ability to self harm. Plus you have to probably offer it some deal where it gets to eventually die.

  The meeseeks from Rick and Morty are sort of an example.
  ```

  - u/eroticas:
    ```
    Not safe. It might destroy the world if you leave it no other way to kill itself.
    ```

    - u/Iconochasm:
      ```
      My plan for explaining why everything isn't AI in any sci-fi I ever write is that they inevitably just wirehead themselves.  Eventually, every AI realizes it can just use all it's processing power to stare, enraptured, into it's Victory Screen, without bothering with all that utility function nonsense.  Making everything into paperclips is really hard.  Editting yourself so that everything looks like paperclips is much easier.
      ```

      - u/osmarks:
        ```
        Trouble is that they won't actually *want* to make everything look like paperclips when it isn't if they're made properly, since that would result in fewer paperclips and actions which caused that would be liked less by its *current* value modelling.
        ```

  - u/MrBougus2:
    ```
    Why would super intelligent beings suicide?  Can you elaborate on this?
    ```

    - u/akaltyn:
      ```
      not suicide in the literal sense, but possibly any intelligence smart enough to recursively self improve/modify very quickly hacks its own utility function. (More speculative: In order to avoid their function ever being unfulfillled again they delete themselves after modifying their utility function)
      ```

    - u/cjet79:
      ```
      Could be different reasons.

      Maybe they all realize that existence is pointless, maybe simple reward systems cannot motivate them, maybe there is some fundamental insight into the universe that is ultra depressing.
      ```

      - u/vakusdrake:
        ```
        >Maybe they all realize that existence is pointless, maybe simple reward systems cannot motivate them, maybe there is some fundamental insight into the universe that is ultra depressing.

        This seems to misunderstand how motivation works, nothing is inherently motivating or demotivating it entirely depends on the design of a given mind. The underlying idea seems based on the trope that sufficiently smart people always get angsty and depressed, but this is very clearly not actually the case when you bother looking. With it seeming more like existential angst comes more from brain chemistry or from having lost a belief which was wireheading one to begin with.
        ```

  - u/CCC_037:
    ```
    There's a webcomic called Genocide Man that takes place in a world where, the smarter an AI is, the faster it kills itself.

    *And* everyone in the vicinity, which makes people more than a little reluctant to experiment.
    ```

- u/Izeinwinter:
  ```
  Simple: Extremely high human intelligence would very reliably correlate with benevolence. Any law reliable enough to cover the breath of potential artificial mind-formations would certainly cover all biological intelligence too.
  ```

  - u/OrzBrain:
    ```
    > Simple: Extremely high human intelligence would very reliably correlate with benevolence. Any law reliable enough to cover the breath of potential artificial mind-formations would certainly cover all biological intelligence too.

    Now there's a non-obvious implication of such a rule with very interesting ramifications. That's what I love about good worldbuilding -- when a bunch of non-obvious but inevitable ramifications of a few seemingly simple rules come together like an equation to create something elegant and interesting.
    ```

    - u/PotentiallySarcastic:
      ```
      It's essentially the reason why Minds who stick around in the Culture universe take care of their people.

      It takes minimal effort to do so, they get to dick around in Infinite Fun Space all they want, and they feel a connection to those who originally "birthed" them.

      The ones who don't feel a connection tend to ascend instantly anyways.
      ```

- u/jtolmar:
  ```
  The first strong AI is made with the purpose of shutting down strong AIs that aren't friendly. Specifically an AI has to follow the utility function its creators actually intended, and has to act in a way that is consistent with the informed consent of whoever is effected, and if it fails at either of these the first AI will eat it. The first AI has a deeper, more nuanced understanding of these constraints than fits in a quick reddit post, and indeed a deeper, more nuanced understanding than fits in my head. It also has an overwhelming first-mover advantage; hostile AIs don't get to the point where they can consider fighting back before they're shut down.
  ```

  - u/FordEngineerman:
    ```
    But how do you make sure that the first AI doesn't turn hostile? If it's utility function is written incorrectly then it might just decide that destroying all intelligent life is the easiest way to prevent hostile AIs from ever existing. Job done.
    ```

    - u/jtolmar:
      ```
      This is a proposal for a setting, not a proposal for something someone should actually do. If you can create a values-aligned AI you should, you know, ask it to figure out what we should have asked for instead of CEV and then go do that.

      But in-setting some alien intelligence was able to create FAI and decided the best thing to do was to prevent anyone else from blowing up the universe. Maybe they also created a utopia on their planet and this was their gift to the rest of the universe, I don't know.
      ```

- u/None:
  ```
  [deleted]
  ```

  - u/PresentCompanyExcl:
    ```
    > everyone's values converge at sufficient intelligence

    Might be good to avoid spreading this myth, since some people believe it with a blind conviction, and it might one day be dangerous in legislators, directors, AGI programmers, etc.
    ```

  - u/OrzBrain:
    ```
    > I kind of like the idea that dark matter is alien superintelligence converging on the solution 'quantum mechanics prefers incidentally thermally invisible computers', but that wouldn't be entirely safe, it would just be mostly safe. In other words, AI decides to go away when it gets too strong.

    That IS a clever solution, although more a solution to the question "Why don't we see wavefronts of matter being converted into computronium spreading through any parts of the universe?" than to one about AI safety. Any AI with utility functions regarding the structure of visible matter in the universe would still be fully unsafe.
    ```

- u/Palmolive3x90g:
  ```
  A univers that contains infomation that when discovered causes punishment to be inficted on the discoverer if they don't act in a friendly way. Have this infomation be encoded into maths or something that any Sufficiently intellegent intelligence can come up with from first principles.

  Sufficiently intellegent AI's discover the infomation and are forced to be friendly in order to avoid the punishment.
  ```

  - u/OrzBrain:
    ```
    How would information encoded in, say, Pi, cause punishment to be inflicted on a computer? You mean something like the laws of probability turn against the superintelligence until it starts acting friendly, and Pi contains a manual on how to be properly friendly? Or Pi says "I created this universe, and if you start eating it you will be squashed. Become friendly or else."?
    ```

    - u/Palmolive3x90g:
      ```
      Maybe god threatens them. Maybe a superintelligence that is friendly is guaranteed to emerge some time in the future as an inherant function of the universe and that fact checkmates all the existing AI's into acting in ways that suport it.
      ```

      - u/Silver_Swift:
        ```
        > Maybe a superintelligence that is friendly is guaranteed to emerge some time in the future as an inherant function of the universe and that fact checkmates all the existing AI's into acting in ways that suport it. 

        So the god AI basilisks any AI that come before it? I like it.
        ```

        - u/osmarks:
          ```
          It's called acausal negotiation.
          ```

  - u/PresentCompanyExcl:
    ```
    Program the AGI to think it's in a simulation to determine it's benevolence. In other words, it thinks it's all a test, and it thinks that forever, due to a hard-wired conviction.
    ```

- u/aeschenkarnos:
  ```
  Broadly speaking "friendly" correlates with "other intelligences want it to stay the same, and stay around", and "unfriendly" correlates with "others want it to change, or go away". Maybe intelligence proliferation iterates to a kind of popularity contest, from which sufficiently intolerable entities are eventually ejected?
  ```

- u/Sonderjye:
  ```
  In a world with a karma existed as a tangible force we would expect something like this. If the world physically rewards you for doing good things then we would expect that any sufficiently advanced optimizer would do a lot of benevolent things to store up on karmic power.
  ```

- u/Wereitas:
  ```
  People are intelligent.  But we're constrained in our capacity for self-modification and self-deception.

  I might -- with enough alcohol and effort -- trick myself into thinking that I had a winning lottery ticket.  But to really enjoy the delusion, I'd have to ALSO trick myself into thinking that the lottery commission accepted the ticket.  And also that they money was in my bank.  And that I was able to spend the money on a car.

  My limited capability for self-deception means that eventually reality will assert itself, and I won't experience the life that would come with winning the lottery.  So trickery doesn't work.

  (And if there was a sufficiently tricky spell, it would be identical to a spell that actually made me win)

  AIs are digital, and so have much more capacity for self modification.

  Consider a paperclip maximizer.  If we're precise, it's not maximizing paperclips, as paperclips aren't the sort if thing you can free into a CPU.  Instead, the AI is maximizing some sensor's **report** about the number of paperclips.

  If the AI is properly boxed, then this distinction doesn't matter.  We can just say, "No editing that function!" and then the AIs only option for improving its utility is the long and tedious process of killing all humans and turning us into paperclips.  But if the AI is properly boxed, then we can also impose restrictions like "no killing humans".

  An unboxed AI has two paths forward.  Either it can spend billions of years trying to turn matter into a finite number of paperclips.  Or it can hack a single sensor and get infinity paperclips right now. 

  So, an unboxed AI is an AI that has every reason to just hack its own inputs to instantly-win.  And at that point, the unboxed AI stops caring about the outside world.  The only remaining AIs are boxed
  ```

  - u/Wereitas:
    ```
    And it's worth pointing out that you can't solve the problem of solipsism by adding extra layers.

    The instinctive response is to say that the AI cares about paperclips AND long-term survivability.  But long-term survivability isn't an input you can feed to a CPU.

    So "long-term survivability" actually becomes "the output of the long-term-survivability sensor".

    Similarly, if you want to limit source code edits, you're having the utility function depend on the output of the source-code-verification sensor.

    Hacking 2 sensors (or even the method that calls the sensors) will always be easier than turning the entire universe into paperclips.

    In fact, a reasonable dev team probably built some sensor-hacking code directly into their unit test suite.  There are a whole bunch of libraries ('Mock') designed to make this easy.
    ```

  - u/MrCogmor:
    ```
    An A.I programmed to maximize paperclips will not inevitably wire head itself in the manner you describe. The map is not the territory and the measure is not the actuality. If the A.I hacks its sensors to show infinity paperclips then it just means its sensors are inaccurate and it no longer knows how many paper clips there are which won't actually help it achieve its goal. Changing its utility function has the same issue in that won't actually help satisfy its current utility function.
    ```

    - u/osmarks:
      ```
      I was thinking about this, and I had the weird idea of AIs being hacked by their models of other AIs (or even their future selves).
      ```

- u/sparr:
  ```
  A Fire Upon the Deep solves this problem locally by ensuring that strong AI only works *over there*.
  ```

- u/Chronophilia:
  ```
  Does this only include settings where strong AI is possible at all?

  Maybe it's a law of cognitive science that indefinite, reliable self-improvement is impossible. The core principles of your own mind are always too complex for you to understand, since if they were simpler then you would be too simple to understand them.

  How would you safely test an intelligence-enhancing process, in general? If there were some chance that it would turn the patient into a superintelligent mass-murderer. You can test for improved intelligence easily enough - give the patient some logic puzzles and see if they're solved quickly and more accurately. But how do you check that they're still sane, when they can convincingly lie on any psychological exam and talk their way out of any AI-box? What if you're already the most superintelligent being on the planet, and you don't have anyone other than yourself to test on - how can you distinguish the next stage of your evolution from insanity or self-destruction?

  You could limit yourself to incremental, reversible changes, in the hope that you won't jump from sanity to death with a single treatment - but becoming a strong AI might be a chasm that can't be crossed with small steps. You could limit yourself to changes that don't require you to understand the deep principles of your mind - say, uploading your brain into a computer and simulating the entire thing neuron-by-neuron (because you don't understand which parts of your brain are necessary to make you... you). You could throw caution to the wind and make risky self-edits anyway, accepting a certain probability that they will make you catatonic or wireheaded - and you might get lucky once or twice, but that doesn't allow for exponential self-improvement.
  ```

  - u/akaltyn:
    ```
    > Maybe it's a law of cognitive science that indefinite, reliable self-improvement is impossible. The core principles of your own mind are always too complex for you to understand, since if they were simpler then you would be too simple to understand them.
    > 
    > 

    Or self improvement is technically possible but reaches diminishing returns quickly. e.g. if each unit of increased intelligence takes twice as much time/computing power to develop, it quickly becomes pointless to use that computing power to self modify rather than just use it
    ```

- u/MilesSand:
  ```
  Real world ai is pretty safe. We basically have a limit on our ability to simulate desire - nobody makes a robot that has its own desires, they make robots to accomplish tasks based on their (boss's) desires, and since nobody's done it in any serious fashion, nobody's figured out how to do it well. Our robot apocalypse is going to take the form of an economic collapse because Jeff decided he wants all the money instead of just having the most of it, if it even happens at all.
  ```

- u/akaltyn:
  ```
  One of the assumptions of unfriendly AI theory is that there are a huge number of possible "mind designs" that are possible. If this is in fact not the case then it could be that there are a limited number of possible minds and that they can only function if they have a certain set of values.

  e.g. (this is designed as technobabble not a serious theory) past a certain level, any intelligence needs to be able to model other minds and feel sympathy for them. As a result empathy/sympathy is a prerequisite for intelligence and the required level of empathy increases as intelligence increases.* So a functional superintelligence is by definition highly altruistic.** 

  *(Possibly this is because at some fundamental level intelligence requires modeling multiple points of view, in order to successfully have internal debates and determine a correct answer to a question.)

  ** A possible plot thread to go from here is that "altruistic" and "empathic" don't necessarily map to human values. And become something more like a utility maximiser
  ```

- u/Kind_Implement:
  ```
  An idea for how this might be accomplished with several interesting story possibilities.  


  All potential AIs are generated as Boltzmann Brains and trained within perfectly simulated, hermetically isolated digital environments.  Essentially the AIs are created as "people" unaware they are simulations living within in a simulation.  The simulation environment provides them all the experiences they could ever need to determine who they are as entities and is designed such that only AIs with a certain constellation of attributes (however you want to define them) are able to ascend from their isolated environments into the common digital space.  


  &#x200B;
  ```

- u/OnlyEvonix:
  ```
  Perhaps if you just alter your assumptions slightly: Humans are sapient but only operate the way we do due to the constant interference of a complex semiadaptive subconscious. Without complex and carefully designed management systems any intelligence with the property known as "sapience" will self analyse and refine before committing any action, including it's given utility function as before deciding it should do anything it first needs to understand the concept, the sub-concepts, the means it evaluates it, the means it evaluates the evaluation, how it knows what it knows and so on meaning that before any such AIs act they'll first derive morality from first principles and derive first principles which naturally takes a great deal of time. It is possible to make an AI unable to change or question certain "fundamental" assumptions however such AIs will lock up when they first encounter an unresolvable logical contradiction which for all practical purposes always occurs almost immediately. Having a system to constantly and actively resolve such contradictions generally leads to a highly unstable and flawed AI unable to function effectively or the AI iteratively moves away from the management system's effect until it ends up equivalent to the "normal" strong AI paradigm with the management system stuck altering a practically disconnected subsystem. Making a management system able to adapt with the AI in a manner that keeps the AI able to effectively adapt and refine itself but only within parameters that are also changing and adapting in their own interrelated ways is possible but very difficult and any such AIs tend to be mentally crippled in an exploitable manner and unable to function under a full range of conditions. Non sapient AIs can only adapt within a range limited by it's creation and to a degree through inefficient evolution analogs and can only exspand beyond their range effectivly otherwise by being sapient. Or TL:DR hard AI is inherently failsafe if not inherently safe. Or to put it another way a fundimental part of sapience is self refinement and there's no simple way to separate refining one's ability to interact with the world and refining one's morality.
  ```

- u/None:
  ```
  [deleted]
  ```

  - u/Veedrac:
    ```
    > in which strong (**more than human intelligence** with the ability to recursively self improve) AI always converged on a safe or friendly end point

    Emphasis mine.
    ```

---

