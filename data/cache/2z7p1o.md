## How to Distinguish Between a Person and an Optimizing Process

### Post:

I'm working on a story where a teacher is teaching her class about the mechanics behind time travel. Time travel in this universe only comes in the form of Stable Time Loops with only one timeline. The current lecture is about how certain Time Loops are selected over other potential Time Loops.

One student has raised the alternate hypothesis that the selection of time loops could be evidence of an intelligent mind rather than an Optimization Process like evolution. Currently the teacher has told the class to discuss reasons for each side.

I already have some good points for why the selection of time loops looks like an Optimization Process thanks to the LessWrong sequence on [Evolution](wiki.lesswrong.com/wiki/Evolution), although more tips would be helpful. However, I don't have a good definition for person-hood (or more specifically sapience) which is something that should be present in this world of AIs (not explicitly touched on in this story though). Basically what qualities does a person have, that an automatic process lack?

TL;DR - Given a list of actions, is the agent a person or is it a mechanical optimization process?

EDIT: I changed the TL;DR to be more clear on what I'm looking for.

### Comments:

- u/None:
  ```
  Consult this handy flowchart:

  > Is it good at its job?

  >> Yes.

  >>> Optimizing process.

  >> No.

  >>> Person.
  ```

  - u/xamueljones:
    ```
    Heh. That's funny. I'll include this as a joke from the class clown.
    ```

- u/None:
  ```
  In this case, "person" is a subtype of "optimizing process", namely, the subtype that happens to be implemented as a real-time, online learning algorithm embedded in the biological or post-biological psychological baggage of a human being, including things like internally distinct emotional valences and consciousness.
  ```

  - u/xamueljones:
    ```
    Still....how would one be able to distinguish the two solely from a given list of actions? It can be hard to read things like emotions and change in motives from anything other than face-to-face communication.
    ```

    - u/Kishoto:
      ```
      As far as I can tell, at a sufficient level of complexity, there won't be a difference. That's the only thing that makes humans human. Complexity. Specifically related to our cognition and sentience.
      ```

- u/DataPacRat:
  ```
  I can't offer a good theory, but in /practice/, the usual criteria for "personhood" that I've been using is something I call the "Trader's Definition": Can I bargain with it? Can I offer to exchange a banana for some backrubs, or some programming for some playtime? This has the advantage that anything that fails the definition, I have the incentive to treat /as if/ it weren't a person, while anything that passes, I have the incentive to treat it /as if/ it were.

  For your Time Loops case, if the thing that selects time loops can be negotiated with, then whether it's an Optimization Process or not, then on a practical matter, there's good reason to treat it as if it has an intelligent mind; while if it pursues its agenda without regard to any such offers, then treating it as a non-sapient Optimization Process seems workable.

  In short, to decide on which particular dividing line to use, first you should pick what difference the dividing line will make to your actions.
  ```

- u/TimTravel:
  ```
  [Relevant](http://www.reddit.com/r/HPMOR/comments/2xie39/time_travel_and_why_everyone_gets_it_wrong/)
  ```

  - u/xamueljones:
    ```
    Wow. You hit the main points of what I'm going for with my version of time travel. Paradoxical universes which are unstable time loops are excluded and their prior probabilities of occurring are "squeezed" into other potential time loops until a stable time loop reaches a higher probability over all other unstable time loops. After that is another step that I want to keep secret for now, because I'm not sure if it would make sense with the rest of the rules.

    Still is this a case of great minds thinking alike, or the fact that this is the only *sane* way (to me) to do stable time loops?
    ```

- u/Dykster:
  ```
  Have a nice comment on chessmasters vs. chess AI:
  http://www.reddit.com/r/rational/comments/2z5ooe/d_goddamn_do_i_hate_prophecies/cpg6uxs
  ```

  - u/xamueljones:
    ```
    Okay, this seems useful. Does the unknown agent work by prophecy (via a meta-time) or does it work by a distinct 'style' or strategy? There are subtle ways to guess at both.

    Thanks.
    ```

- u/chaosmosis:
  ```
  A person would be able to make leaps from hills and valleys, rather than being forced to use gradual change.
  ```

  - u/DCarrier:
    ```
    I'd say the opposite. A person, or any form of computational process with limited resources, will only be able to look for solutions in a small portion of the available space and will do things like make gradual changes. A law of physics that randomly selects one of many solutions to a set of equations can pick out any of all possible solutions.
    ```

    - u/EliezerYudkowsky:
      ```
      Affirm.  Natural selection and human intelligence will both have complicated signatures coming from the way they compute things.  A law of physics that selects stable possibilities should give rise to stable loops that are (if you end up in a random one) just randomly selected from among all stable loops; there should be no visible signature of how the loop might've been reached, aside from "It's a random stable one."
      ```

    - u/chaosmosis:
      ```
      Who said we were talking about a randomized selection, or law of physics? OP wants to talk about a situation in which a person can be distinguished from an optimizing process, and a situation where a time loop is chosen at random doesn't seem like optimization to me. Presumably all of the time loops have some quality in common which makes the idea of a designer appealing, but which also might make their stability more probable - survival of the fittest.

      Suppose time loops can be triggered and there's a set of potential stable time loops set up and one of these loops is optimized along the criteria purported to be the designer's motivations, but requires non-incremental change to reach. Perhaps the time loops tend to maximize the color purple, and a situation is established where the best way for the time line to maximize purple is to first create several other colors in a particular sequence. If the purplemost time loop is the one selected, that's evidence for a designer rather than a gradual optimizing process.

      There are non-gradual optimizing processes too, of course. But those reliably can't be distinguished from humans because humans are a subset of them, so I assumed they were irrelevant to what OP wanted to discuss.
      ```

      - u/xamueljones:
        ```
        Thanks for your post. What you are detailing explains the arguments for an optimization process that I am planning on for my characters to use, only with the selection criteria being simplicity and the number of future time loops generated (with evidence to back it up).
        ```

  - u/xamueljones:
    ```
    That seems to be the main difference if I look at it from the theoretical perspective, but I don't quite get how it would be applicable in practice. Maybe by seeing if there are any sudden shifts in patterns to imply a person, and absence of these 'shifts' is weak evidence of an optimization  process?

    Thanks for helping.
    ```

    - u/PeridexisErrant:
      ```
      This is somewhat complicated by a distinction in what constitutes an "optimisation process".  /u/chaosmosis distinguishes local optimisers (eg evolution), but lumps more complex algorithms in with people.

      For example:  assume a two-dimensional search space with too many points to sample them all, and an unknown utility function over the dimensions.  Evolution would pick a random point, then sample adjacent cells and move to the maximal cell (then repeat).  A person might systematically sample the space, then try to understand the function and then focus on sampling high-utility areas.

      A better dumb optimising algorithm (dumb as in, "I could write this" and "not domain-specific") would be to start sampling in a fractal pattern, and alternate a new fractal-point with local searches from points-above-mean according their proportional goodness.  

      A *smart* optimising algorithm would have a superhuman ability to reason about the underlying function, and look like a hyper-competent human (or one with inside information).
      ```

      - u/eaglejarl:
        ```
        >  Evolution would pick a random point, then sample adjacent cells and move to the maximal cell (then repeat).

        Which leads to local maxima.  Not sure that's relevant here, but it's a point of difference.
        ```

        - u/xamueljones:
          ```
          Local maxima in the context of a time loop is simply the first possible *stable* time loop the Optimizer finds which is closest (through hill-climbing) to the original timeline when the time traveler gained access to a time machine without any time traveling assistance (think HPMOR's Final Exam where the Time Turner didn't come into play unless there was a timeline where Harry got access without help).

          If it was a *better* Optimizer, then it would find the global maxima which is a timeline with no time loops, since the Optimizer would be influencing probability to prevent time travel from ever developing. Yes, the Optimizer is selecting for as few time loops as possible.
          ```

  - u/OffColorCommentary:
    ```
    This is only true of gradient ascent optimizers and similar.  There are plenty of non-person optimization processes capable of sudden change.  All of the ones we generally deal with in AI are smarter than gradient ascent, but there also exist stupid ones:

        State best = random(StateSpace)
        while (?) // However long we're allowed to execute
            State proposed = random(StateSpace)
            if (u(proposed) > u(best))
                best = proposed
        return best;
    ```

- u/MadScientist14159:
  ```
  if optimizing for a small number of coherent goals:

      optimizing process

      elif optimizing for a large number of sometimes conflicting goals:

          person

          else:

              alien
  ```

---

