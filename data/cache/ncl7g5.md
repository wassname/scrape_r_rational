## Running Hot (short story)

### Post:

[Link to content](https://sprague-grundy.github.io/running_hot/)

### Comments:

- u/nicolordofchaos99999:
  ```
  This is a very cool story! Wouldn't running your mind slower mean that entropic heat death comes that much faster for you, though? If I knew I was computationally immortal, I would not be willing to make that trade.
  ```

  - u/Veedrac:
    ```
    This is why you have starworks; the primary goal is to stop all your useful matter being on fire, and stick that in stable storage. Then the natural energy losses from that storage are just another factor in efficiency. If 99.9999%+ of your energy is on fire, entropic heat death comes sooner, and running hot is extremely important to make the best use of the timespan available. If, say, 90% of your energy use is wasted in battery leakage from a more stable source, it might be better to run slightly hotter and faster, just to up your efficiency. If only 0.1% of your energy use is in leakage, then running 10 times as efficient at 10% the speed will leave you running longer subjective time.
    ```

  - u/Auroch-:
    ```
    Theoretically, by disassembling all stars and waiting for the ambient background temperature of the universe to ~~warm up~~ (EDIT:) [cool down](https://arxiv.org/pdf/1705.03394.pdf), you can spend entropy more efficiently; the numbers I remember suggest that if you stop the civilization's runtime until very late, then run sims in that environment rather than the modern one, you get ~3x-5x the total subjective runtime.
    ```

    - u/sprague-grundy:
      ```
      Are you thinking of https://arxiv.org/pdf/1705.03394.pdf ?  I think that paper claims the same final result but with the opposite mechanism (i.e. that the background temperature will go down over time which will make cooling cheaper).
      ```

      - u/Auroch-:
        ```
        That looks right, my bad on the sign.
        ```

    - u/SvalbardCaretaker:
      ```
      There's been some complicated refutation of that concept a couple years back on overcomingbias. I don't remember the specifics though.
      ```

      - u/Auroch-:
        ```
        I haven't taken Hanson seriously for a decade now. He got stuck at some point and hasn't generated new ideas since. I don't think aestivation is relevant to the Fermi paradox but the computing argument is solid.
        ```

        - u/SvalbardCaretaker:
          ```
          It seems to my very layman view that he is still putting out a tremendous amount of thinking. The recent series on the Fermi Paradox is not TheGreat Filter Rehashed for example.
          ```

          - u/Auroch-:
            ```
            He has terrible underlying assumptions he hasn't budged on despite having their flaws pointed out repeatedly and having evidence come in which conflicts with him. This is obvious in AI and makes it obvious how his ideas are warped by unquestioned fundamental biases, mostly in the direction of assuming eternal competitiveness of ecosystems and the eternal disproportionate relevance of economics in other respects.

            Once you look for this it's glaring in everything else he writes. It's the [one mistake rule](https://thezvi.wordpress.com/2020/04/10/the-one-mistake-rule/): when you can see someone's model is making obvious, unarguable mistakes in one place, you should rule out trusting that model for anything else. It's not a *strict* rule for thinkers but it's a strong guideline.
            ```

            - u/SvalbardCaretaker:
              ```
              Can you point me to a good critique of his thinking modes? Esp the irrelevance of economics?
              ```

              - u/Auroch-:
                ```
                Economics isn't irrelevant. Every word in "eternal disproportionate relevance" was carefully chosen.
                ```

        - u/Veedrac:
          ```
          This is a pretty weird place to use an ad hominem, given the debate is purely technical and so very abstract. I assume the argument [is this one](https://www.overcomingbias.com/2019/02/aliens-need-not-wait-to-be-active.html), which seems pretty straightforward to me.
          ```

          - u/Auroch-:
            ```
            As I said, his assumptions are bad and so his reasoning is bad. It's not worth examining his argument for this, because it's bad on priors. There is no reason to expect he is correctly evaluating what technical aspects are relevant, and plenty of reason - including, e.g. the final paragraph of that post - to expect that his bad assumptions are at work.
            ```

            - u/Veedrac:
              ```
              The things you're criticizing him for don't even apply to this context. This is pure ad hominem: ‘I disagree with Hanson on X, so therefore he must be wrong on Y ≠ X.’ There's a place for ad hominem, but this isn't it.

              E: Surprised that *this sub* would downvote me for this.
              ```

              - u/Auroch-:
                ```
                I literally included an example of how they apply.
                ```

                - u/Veedrac:
                  ```
                  Unless I've missed it, that comment is not part of the paper, and not all that relevant to the core point being made, that it is not more thermally efficient to wait before using available negentropy. The paper does not rely on any economic arguments, or make any claim of what an alien civilization might want to do, beyond saying there is no physically mandated thermodynamic efficiency incentive to do it later.

                  Dismissing this on the back of Hanson briefly giving an example task that might incentivise early activity that you think is poorly chosen seems really weird. If the example were wrong it wouldn't invalidate the argument in any way, and honestly it's not some absurd example anyhow.
                  ```

  - u/Galap:
    ```
    I have always wondered about the following, related to this. Maybe someone with more specific math and physics knowledge can answer.

    is it possible to avoid entropic heat death by continuously slowing yourself down? The universe will never actually reach total equilibrium, but approaches it as a limit right? So if you keep slowing down forever, it would subjectively seem like you never hit the heat death.

    But, it depends on the relative strength of approach to different limits, like the Gabriel's Horn problem. Can you slow down fast enough to outpace the entropy issues? Which limit dominates?
    ```

    - u/meikaikaku:
      ```
      I think that that probably wouldn’t end up working out in your favor. You will have some nonzero entropy leakage over time even if you are running at arbitrarily slow speeds, so eventually the majority of your negentropy spending will be on that, rather than your increasingly slow thinking.

      From another angle, there is only a finite amount of negentropy in the universe. Even if you could use all of it, 100% efficiently, that will still put a hard limit on how much computation (and hence thinking) you can do. This then implies an unavoidable horizon where a certain amount of subjective time would use all the negentropy that exists, regardless of what rate you ran it at.
      ```

- u/ArgentStonecutter:
  ```
  Lockstep, by Karl Schroeder. >!They use advanced coldsleep to run their civilization a few months a century with subsapient AIs managing their worlds in between. The parent civilization has passed through singularities and ... gone somewhere... multiple times.!<

  A civilization like this is encountered in Diaspora, by Greg Egan. >!They use subsapient "Contingency Handlers" to watch for problems and contact the parent minds when they're discovered.!<

  Personally, in your scenario, I'd get volunteers to take realtime vacations and then return to the parent civilizations after 20 years or so while nothing happens in the real civilization. Heck, you'd be able to get people to pay for the rest.
  ```

- u/cthulhusleftnipple:
  ```
  > By comparison, near every computing technology ever invented works better the cooler you run it. With MOSFETs, you get smaller feature sizes, lower energy usage, and fewer errors to correct. 

  A pedantic criticism, but this isn't actually quite true. MOSFET technology, in particular, relies on higher temperatures to allow free charge carriers to escape their dopant atoms in order to provide bulk conduction. The essential nature of semiconductors is such that charge is not freely able to move without at least some activation energy. At T=0, they do not (generally) conduct.  

  Now, there are technologies that obviate this issue, mostly using III-V materials, and do run better the colder you get. However, MOSFET technology as it is generally used is not one of them.
  ```

- u/Nimelennar:
  ```
  This was a lot of fun.

  My only point of improvement is that you shouldn't use an acronym without defining it first.  Yes, I easily changed tabs and Googled what a MOSFET was, but until then, I didn't know whether you were referring to something real or not.  And having to Google it made the story less immersive; thankfully, that was near enough to the beginning that it didn't really affect my enjoyment.
  ```

- u/Kateryan035:
  ```
  I love this!
  ```

---

