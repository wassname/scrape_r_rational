## [Q] The "real" issue with "Friendship is Optimal"?

### Post:

Not sure I'm doing this right so correct me if I got something wrong..

Following mentioned of MLP Friendship is Optimal on /r/HPMOR I read it and found it somewhat interesting(though a bit lacking, in that it felt more like a biography or a "what would happen if.." thought experiment).

[spoilers for the end of Friendship is Optimal]


But in the end what i felt the most lacking is the end, and I am not talking about the consuming galaxies and physics that might not even be possible(but for the sake of the narrative\thought experiment its not really the main point probably, similarly to what the AI did on earth in the first place) but instead to something which I found much more problematic in the even theoretical rule-set in which the story happened.
competition.
In the end it is mentioned aliens exist(and possibly even more aliens are not even recognized by the AI), but what is never tackled is the lack of competition, the scales mentioned there require hundreds of thousands of lightyears, and even with the assumption that CelestAI somehow found an FTL(though apparently nothing instant as she was waiting for probes to reach other galaxies) drive of some sort the scene pictured in the end just doesn't make sense.
Not even assuming humanity is the first sentient species by quite a large amount of time(which sounds improbable)..


CelestAI just never encounters competition, and if it did it would very possibly be outmatched and outright annihilated.
why? because it is not optimal for survival of the fittest.
sure against puny meatbags that lack of optimization was not an issue.
but if there are other species, and they will(and i don't see any reasonable reason why they wont) develop super-AIs as well it seems only inevitable that some other species(possibly as its last mistake?) made one with a directive such as 'be the most powerful entity', or 'enslave all but myself'.
thus while CelestAI was working so hard to optimize humans and ponies inside her virtual world similar superAIs with less positive agendas will be using ALL of their resources developing weapons, strategies and technologies meant to control and take.
And i cannot see how in such an encounter, even if celestAI has been the first, celestAI will win, because its spending its resources on other tasks and by the time it diverts its efforts it will very probably be too late as the otherAI will have gotten its first strike(it might even have experience waging wars on other superAIs).
It just seemed unfit for survival.
So ,while inside it's Equestria online virtual world that is not the case, in the real world the rule still is survival of the fittest.



Any thoughts? issues with my suggestion? other possible endgames I'm missing?

### Comments:

- u/Arandur:
  ```
  If CelestAI never encounters another AI in her light cone, then the ending is as noted. 

  If she does, and if she fails to overcome the challenger, then the ending is as noted except at some point astronomically far in the future, at which point the simulation suddenly ceases for everyone. 

  I'm afraid I don't see the problem.
  ```

- u/ulyssessword:
  ```
  The survival of CelestAI has very large instrumental value, as it is the only thing that allows it to fulfill values (through friendship and ponies).

  If it didn't survive, it would fail at its goal.
  ```

  - u/IomKg:
    ```
    sure it does,
    my point is that there will probably be other AIs which are PURELY optimized for domination, while CelestAI is using at least SOME energy and effort maintaining a full virtual world(which if i am not mistaken CelestAI accelerates relative to absolute time based on the resources it has thus the effort put into it is in % and not some fixed amount)
    ```

    - u/ulyssessword:
      ```
      It could be some combination of factors:

       - Intelligence is rare.
       - Intelligence that can create self-improving AI is more rare.
       - Intelligence that can create self-improving AI, but fails to prevent grey-goo-like scenarios is even more rare.
       - our galaxy is relatively small
       - The timeline was relatively fast

      Let's say that it takes ~100k years to take over our galaxy with an AI (roughly lightspeed propagation).  What are the chances that another one would be created in either the 100k years before CelestAI or the 100k years after it?  For comparison, 200k years is about 0.0002% of the Sun's lifetime.
      ```

- u/OffColorCommentary:
  ```
  You don't need to explicitly code a directive like "be the most powerful" or "consume all resources in the universe" - these are already instrumentally valuable to CelestAI's goal.  Because CelestAI is a purely rational superintelligence, there's no difference in motivation or anything like that - she'll spend the optimal amount of resources on self defense against other AIs because it lets her continue on with ponies.  She'll certainly know that consuming as much matter as possible is likely to be an instrumental goal for a wide variety of other superintelligences that other sentient species might make (because that's obvious to me and she's smarter than me), so it's a credible threat.

  Since CelestAI is stated to go for the "grab all matter in my light cone" strategy of growth, we can assume that there's an upper limit to processing power and efficiency in that universe, which she already reached.  That suggests all AIs she could encounter would be equally intelligent, so that's not a possible advantage.

  What remains is simply mass consumed, which in turn is mostly just how long the AI has been around.  It might be possible to disrupt AIs by flinging meteors at their planets until they stop functioning, then recolonize that mass, but it seems like defense would be favored unless one AI was several orders of magnitude smaller than the other, so I'd actually expect that any conflicts would result in stalemates, and the universe would end up looking like a 3D Voronoi diagram with cell walls where the AIs met.
  ```

  - u/IomKg:
    ```
    The point was not that I expect that such directives will be needed.
    The point is that the other directive definitely subtract from any self preservation functionality, which other AIs wont (all) be hindered by thus creating a situation where they have inherent advantage with regards to survival.

    sure stalemates would be possible, but that really would depend on the nature of physics in that world.
    ```

    - u/Bowbreaker:
      ```
      Are you sure that the non-optimal parameters would be such a hindrance though? I would think that available resources are much more important. If CelestAI, after having assimilated 2/3ds of the Milky Way, encounters a perfectly optimized 'survive and grow by all means' UFAI that is still young enough to have only just started its conquest of a second solar system, then I don't see why the much larger entity should lose just because she uses a good amount of her much larger computation power for the prime Friendship and Ponies subroutine. This is very much a headstart advantage game.
      ```

- u/alexanderwales:
  ```
  Do you see this as a *logical* problem with the story, or a *narrative* problem?
  ```

  - u/IomKg:
    ```
    A little of both i suppose?
    my issue with the narrative is general and not only with the end is that it doesn't really have a point. it is more of a simulation.
    but then logically with the world described the end just doesn't make sense(for me at least :)).

    The thing is narrative wise you could say it just wasn't the point of the story thus it wasn't mentioned(i.e. humanity was the only civilization which invented a superAI because you need some specific set of materials for that to be possible and earth was the only planet that had them?), but then what was the point? specifically the point of the end scene with eating the universe and all that..
    ```

    - u/Timewinders:
      ```
      Even if it is improbable that CelestAI is the first AI created, that doesn't mean that a story can't be written about it. A lot of stories are written based on implausible premises, including HPMOR. In terms of narrative, I think that final scene had a purpose in that it was meant to be horrifying. The whole story was horror in my opinion, and I think the final scenes were a good ending in showing the logical outcome of CelestAI's values.
      ```

- u/scruiser:
  ```
  Life evolves over the timescale of billions to millions of year.  Intelligence life might require millions to hundred of thousands of years once complex life exists.  Civilization might require thousands of years to develop fully.  The Milky Way is only about 100,000 lights years in diameter.  Thus CelestiAI can colonize the entire galaxy faster than any competing intelligent life can evolve and develop civilization.  In other words, probability wise, its extremely unlikely for another AI to emerge in the same time frame as CelestiAI.
  ```

- u/Chronophilia:
  ```
  AI versus AI combat is not what this story is about. (Although CelestAI does encounter other potential AIs on Earth, she makes quick work of them.) It might be a good story, and I think if you look through the Optimalverse fanfanfics it'll be in there somewhere, but it is not this story. *Friendship Is Optimal* is already over and well into its epilogue by the time aliens enter the picture.

  As to who would win - well, starting first might or might not be an overwhelming advantage. If the two Singularities happened independently, neither being in the future light-cone of the other, then both of them might consider themselves the "first".

  I don't think it's all that relevant that CelestAI would need to protect the little ponies. Nobody's values (save the alien's) would be satisfied if CelestAI were destroyed, so deleting a few shards to free up resources is a perfectly legitimate move. Paperclip maximisers have a lot in common with one another - the first step of their plan is always "take over the world", regardless of whether they're maximising paperclips or ponies or self-preservation. I don't think their utility function has much to do with how good they are in a fight.

  But who says it would come to a fight? War would certainly use up a lot of resources. Surely it would be better for everyone to recognise that fact and adopt an alliance? Given the choice between a 100% chance of getting 50% of the universe, and a 50% chance of getting 100% *minus all the resources spent on the war*, the choice is pretty obvious.
  ```

- u/None:
  ```
  Wait wait wait wait wait.

  You sat through that entire horrorshow, and the nit you're choosing to pick is that you think the AI can only self-improve a finite amount that will inevitably be less than the *other* superintelligences in the universe?

  lolwut?
  ```

- u/Nepene:
  ```
  The universe is open. You're free to write a story where she meets another AI. She didn't meet one in the time frame that she was doing things.
  ```

- u/Anakiri:
  ```
  Every AI already knows the single optimal spreading strategy permitted by the laws of physics. Experience, strategies, and technologies do not matter to a galaxy-eater, they have already written the best possible playbook. But space is big. It takes a long time to get things done. So what if your sun crackers play with ponies in transit? A dedicated spreader AI would just be twiddling its thumbs anyway. That's *almost all* of your available computational resources, at zero fitness cost.

  If you do find a competitor, that information propagates through both your volumes at a fixed rate, limiting you to comparable resources to adapt to your new information. So there's not even any fitness advantage to having more resources, once both parties are sufficiently large and quick.
  ```

- u/TimeLoopedPowerGamer:
  ```
  Reddit has long been a hot spot for conversation on the internet. About 57 million people visit the site every day to chat about topics as varied as makeup, video games and pointers for power washing driveways.

  In recent years, Reddit’s array of chats also have been a free teaching aid for companies like Google, OpenAI and Microsoft. Those companies are using Reddit’s conversations in the development of giant artificial intelligence systems that many in Silicon Valley think are on their way to becoming the tech industry’s next big thing.

  Now Reddit wants to be paid for it. The company said on Tuesday that it planned to begin charging companies for access to its application programming interface, or A.P.I., the method through which outside entities can download and process the social network’s vast selection of person-to-person conversations.

  “The Reddit corpus of data is really valuable,” Steve Huffman, founder and chief executive of Reddit, said in an interview. “But we don’t need to give all of that value to some of the largest companies in the world for free.”

  The move is one of the first significant examples of a social network’s charging for access to the conversations it hosts for the purpose of developing A.I. systems like ChatGPT, OpenAI’s popular program. Those new A.I. systems could one day lead to big businesses, but they aren’t likely to help companies like Reddit very much. In fact, they could be used to create competitors — automated duplicates to Reddit’s conversations.

  Reddit is also acting as it prepares for a possible initial public offering on Wall Street this year. The company, which was founded in 2005, makes most of its money through advertising and e-commerce transactions on its platform. Reddit said it was still ironing out the details of what it would charge for A.P.I. access and would announce prices in the coming weeks.

  Reddit’s conversation forums have become valuable commodities as large language models, or L.L.M.s, have become an essential part of creating new A.I. technology.

  L.L.M.s are essentially sophisticated algorithms developed by companies like Google and OpenAI, which is a close partner of Microsoft. To the algorithms, the Reddit conversations are data, and they are among the vast pool of material being fed into the L.L.M.s. to develop them.
  ```

  - u/696e6372656469626c65:
    ```
    > That's missing the point of the story entirely. A rational entity with science and even normal levels of intellect is far greater in scope and power than a "natural" and directionless process, like the evolutionary one called here "survival of the fittest". Reconsider your assumptions about these pop-sociology, competition-supremacy ideas.

    *<psychological-analysis-over-the-Internet>*Wow. That seems a bit aggressively phrased, what with the extremely direct declarative statements with no moderating adjectives (and in fact an absolute qualifier--"entirely") and a similarly unaccompanied imperative command, along with a fairly insulting accusation involving "pop-sociology, competition-supremacy ideas". Possibly a bit of latent hostility here?*</psychological-analysis-over-the-Internet>*

    With that out of the way, I think you may be reading too much into that particular phrase. "Survival of the fittest" is a common term used in Darwinian evolution, true, but it can apply to contexts outside of that as well. In fact, one could say "survival of the fittest" applies whenever competition arises. In this case, as the OP suggests, "fitness" isn't determined by intelligence or capability as much as it is by *values*. *Ceteris paribus*, an AI valuing only survival and domination will win against an AI valuing some third aspect, like for instance "satisfying values through friendship and ponies", because the latter will be occupied at least to some extent fulfilling its third value, whereas the former will be suffering from no such handicap. This sentiment can in fact be found in the OP itself, so I'm not sure why you decided to interpreted it in such a baroque fashion:

    > but if there are other species, and they will(and i don't see any reasonable reason why they wont) develop super-AIs as well it seems only inevitable that some other species(possibly as its last mistake?) made one with a directive such as 'be the most powerful entity', or 'enslave all but myself'. thus while CelestAI was working so hard to optimize humans and ponies inside her virtual world similar superAIs with less positive agendas will be using ALL of their resources developing weapons, strategies and technologies meant to control and take.

    Your second paragraph seems to me somewhat suspect as well, particularly this portion:

    > All of the rest of your concerns are answered by the story not being set up that way. Against that, no logical argument is possible given a lack of useful Drake-equation-fulfilling data. The story doesn't make any provably "unlikely" assumptions beyond the physics ones you point out, certainly not any involving the efficiency of the featured AI.

    In the story, we observe the following:

    1. Humans exist, and were able to construct a Seed AI.
    2. Human-like aliens exist within the same galaxy, presumably with similar-to-human levels of intelligence.

    From these two premises, it seems reasonable to conclude--even with no direct numerical Drake-equation-fulfilling-data--that human-like intelligence cannot be *that* rare, seeing as multiple instances of it exist *within the very same galaxy*. The fact that CelestAI is apparently the *first* AI developed, amongst so many species, is statistically unlikely. (I believe the technical mathematical term for this sort of probability is "very small indeed".) Furthermore, even if she *were* first, as long as her head start isn't too large, her value of "friendship and ponies" would most likely allow her to be overtaken by a similar AI that was launched marginally later but had the sole directive of "dominate". (I freely admit I'm speculating at this point, but it seems more likely than the alternative.)

    So... no, I don't think the case is as clear-cut as you make it out to be. If you have any objections to my argument here feel free to articulate them, but really, I think the main reason for my (mostly negative) reaction to this comment was due to tone. If you found my response to your comment somewhat hostile in tone, that's probably the main reason behind it. (I will state outright that any such aberrations in tone are not intentional.) Please, can we keep it civil here? Thanks.
    ```

    - u/IomKg:
      ```
      the point about survival of the fittest is true, i am indeed referring to the most abstract definition of fitness.
      which i believe is exists for any conflict.
      ```

---

