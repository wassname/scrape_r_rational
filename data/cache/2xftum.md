## [D] I tend to dismiss hypotheses too quickly.

### Post:

Has anyone had this problem? Does anyone have advice in overcoming it? For several of the reveals in HPMOR I had originally considered the possibility but had dismissed it based on some reason or another. I'm not sure how to explain it. I tend to notice inconsistencies and suspend disbelief almost instantly, blaming the author instead of the characters. It's especially bad for mysteries, for obvious reasons. I tend to end up in a state where I've "eliminated" all possibilities I've come up with.

### Comments:

- u/None:
  ```
  It's good that you're noticing the pattern at all. The next step is to try to notice it immediately, or soon after it actually occurs. So you want to prime yourself to be on the lookout next time you find yourself in a situation where you may be discarding a hypothesis prematurely.

  Something that might work (its unnecessary for me but seems to work for others) is reinforcing the behavior of noticing by giving yourself a quick and easy reward. Use of a click counter may work, or treating yourself to a bite of chocolate. YMMV. Personally I immensely enjoy finding opportunities for self improvement, so the act of noticing things is itself a reward.

  (Edit: An important step I left out is to actively seek out concrete opportunities to trigger the unwanted behavior. This will speed up the process. You will know best how to do this.)

  Whatever you do, the goal is to get to a point where you routinely notice the behavior as soon as it occurs, and then as it is occurring. At that point you can transition from rewarding yourself for noticing, to rewarding yourself for implementing an alternate behavior instead of what you normally do.

  In your case something that might work is writing down the hypothesis and any perceived disconfirming evidence, along with estimates of your beliefs, and the strength of the evidence. Try to think of ways the hypothesis can still be true, given the perceived disconfirming evidence.

  Then over the course of the next few days (or however long it takes to get more evidence), you can revisit that hypothesis and adjust accordingly. This should help you to identify the sorts of situations where you too readily dismiss hypotheses, and become more mindful of that fact.

  Of course, nothing is solved until it is automated! Eventually the alternate behavior should become the new default behavior. With time and practice, you should internalize the process, and be less likely to dismiss hypotheticals out of hand.

  Hope this is useful! At this point I'd like to suggest further reading [here](http://agentyduck.blogspot.com/p/microrationality.html?m=1), and [here](http://agentyduck.blogspot.com/2014/12/how-to-train-noticing.html?m=) (which is where I got these ideas).
  ```

- u/tvcgrid:
  ```
  I have this problem now and then. My usual practice is to withhold judgement and kind of let the hypotheses pop like popcorn and then settle down. It's not like I would really have immediate off-the-cuff evidence that's strong enough to immediately eliminate most hypotheses... usually it takes a bit to even unpack new ideas and get them past my first few levels of cached reactions and responses. I still miss notes of confusion... but it's an ongoing thing.
  ```

- u/derefr:
  ```
  If I pick my tendency to do this apart *in this specific instance*, it's mostly a desire to assume that "they'd have thought of that, and everyone knows they'd have thought of that, so why would they go with the most (or second-most, or any-most) obvious solution?"

  The answer for me, that turns out to generalize *outside* of this specific instance, is giving more weight to "obvious solutions", and less weight to "solutions that assume some active intelligence stopped the obvious solution from working."

  Once you get used to this—and realize that other people don't default to thinking this way any more than you did—it also makes clear why some social engineering methods are so effective. Dress up as a security guard? Nobody would do that; it's *too obvious a solution!* 

  ...which is true, in a self-fulfilling way; people don't tend to guard against it precisely because most would-be criminals discard it as too obvious, and assume it would be the first thing their opponent would think to guard against.•

  Basically, this is a game theory problem, crossed with a game design problem. Assuming an intelligent adversary means assuming [yomi layers](http://www.sirlin.net/ptw-book/7-spies-of-the-mind) apply to your problem, because you're likely to be trying to outthink them just as they're likely to be trying to outthink you. Systems like stock markets also act like this: there is an economic incentive for the system to evolve away from anybody's ability to predict it.

  But for anything that *isn't* intelligent—and most things that happen in the universe are the result of dumb non-learning processes—you have to discard your impulse to think in terms of yomi layers, and actually look for the most obvious solution. Unless it has been fiddled with by some adversarial intelligence, the most obvious solution is also likely to be the solution that relies on the fewest things to go right, the easiest to implement, etc.

  There's a reason your lizard brain bubbles the "obvious" solution up as a suggestion to your mammalian brain first (which feels like something being "obvious" from the inside.) The parts of your brain that evolved when nature itself was the adversary think in terms of obvious solutions. Your higher brain functions try to predict other animals, or even other intelligent beings, and thus quash these suggestions; it's a useful flinch for social games, but it's not a useful flinch for generally-optimal strategy. **Do not try to out-think nature, for it is not trying to out-think you.**

  • (Also, security systems are *usually* built based on previously-attempted attacks, not imagined attack scenarios,  and something that is "too obvious" may have just never been attempted.)

  ---

  And I guess this also gets to the core of rational story-writing, doesn't it? In a rational (not rationalist) story, the universe acts like ours: it's a predictable, non-learning, "most obvious strategy dominates" system. The winner is the first person to have a sufficient idea, not the best idea.

  Part of Alicorn's *Effulgence* described a D&D campaign world: it was a horrendous and scary place to the Bells, precisely because there really *was* an intelligent adversary (the DM) at work, making the Most Obvious Solutions fail to work. I think this was meant to be taken as an indictment of a lot of other works, though; almost any work, to the degree that it is non-rational, has some force of "karma" that rewards deontologically-pleasing strategies when they come up against villains doing the Most Obvious Thing.
  ```

---

