## [RST][C][HSF][TH] IO.SYS

### Post:

[Link to content](https://www.datapacrat.com/IO.SYS.html)

### Comments:

- u/DataPacRat:
  ```
  6300 words of ravings about AIs and rockets.

  I appreciate all the feedback I can get.
  ```

  - u/gryfft:
    ```
    This was a stellar read. I want it to be longer; I am *not* suggesting that it *should* be longer, just expressing my *desire* for it to be longer, which itself is a signal that it's a more or less perfect length.
    ```

- u/LazarusRises:
  ```
  This was a great read! What a punchline, wow. Wouldn't a sim-runner who sees the AI advocate for destroying Earth be pretty unlikely to let him into reality, or is the hope that they see the logic and applaud the use of the Y-S Security Protocols? Also, if there really is an AGI on Earth, why would .2 think it's still confined there after all this time? Seems to me that destroying the planet either kills all/the vast majority of biohumans, or angers a hugely more powerful AI.
  ```

  - u/DataPacRat:
    ```
    > This was a great read!

    Thank you, I'm glad you enjoyed it. :)

    > Wouldn't a sim-runner who sees the AI advocate for destroying Earth be pretty unlikely to let him into reality, or is the hope that they see the logic and applaud the use of the Y-S Security Protocols?

    "Yes." <ahem> Non-mathematician answer: It's all about the payoff matrices.

    > Also, if there really is an AGI on Earth, why would .2 think it's still confined there after all this time?

    Remember the weak anthropic principle: Any universe containing a superintelligence that left Earth wouldn't be a universe containing the narrator, leaving the options of an Earth lacking any such entity, or an Earth containing an inscrutibly-motivated 'stay-at-home' intelligence.
    ```

- u/MultipartiteMind:
  ```
  ([deadpan] Yes, this has all been VR, and the simulation wetware is an author's brain.  We'd normally be happy to learn from you, but resource limitations compelled us to skip some steps--you don't know anywhere near as much technological knowledge as your memories tell you you do. [/deadpan])

  I'm contemplating the 'each bit' concept, and whether feasible; at the very least, I feel that the bits would have to pass a threshold of meaningful comprehension, before--for instance--incomprehensible 0s and 1s are interpretable as 'SOS' and drastically collapse the future decision tree all at once.

  Or to put it another way, if 75% of your decision branches have you choosing to drink water, and 20% milk, is there going to be a timing when a single 1-versus-0 prunes away half the branches to leave a 40% milk distribution?  That said, I like the information-limitation line of thought about the absolute requirements for an entity to choose between --that is, specify--one of many possible outcomes.  By the time you're choosing the timing and the different reactions, though, isn't every moment a 1/0/[nothing] three-way interaction?  Even if not waited for, opportune silence can also influence...  but I'm overcomplicating things, easier to just think of the bits as [signal]/[silence].  But still, when you can choose your timing then all moments of silence are bits too, plus the influence/threshold needed to accomplish meaningful branch-distribution pruning, so I feel that the actual number of bits to halve something's outcome likeliness would always be far greater than 1..?

  About the overall outlook, the 'Dark Forest' line of thought comes to mind--scarier in some ways in that all information transfer is suspect.

  ...hmm, maybe if you don't treat as involcing 'bits' any options which wouldn't end up having any behavioural changes, as though the brain's own limitations were collapsing inputs into the same output...  well, there's still the butterfly effect brainwashing approach, granted...  Hrm.
  ```

  - u/DataPacRat:
    ```
    > Or to put it another way, if 75% of your decision branches have you choosing to drink water, and 20% milk, is there going to be a timing when a single 1-versus-0 prunes away half the branches to leave a 40% milk distribution?

    I was surprised when I learned about Rowhammer attacks on RAM. And when I learned about Spectre attacks on predictive-branch CPUs. I figure that when dealing with a superintelligence, it's never the attacks you can think of that are the ones you have to worry about.

    Put another way, a sufficiently advanced superintelligence could be indistinguishable from Worm's Simurgh, who could manipulate atmospheric conditions to flip a single, vital bit in a message in-transit.

    > About the overall outlook, the 'Dark Forest' line of thought comes to mind--scarier in some ways in that all information transfer is suspect.

    I'll admit that I deliberately tried to channel Peter-Watts-style technopessimism a few times while I was writing. (I can't hold that state of mind for very long, but it's a useful exercise.)
    ```

- u/IICVX:
  ```
  I like how it's subtly possible that the main character is a simulation being run by the theoretical AI on Earth, who's trying to figure out what happened to the unhackable craft that was in orbit. 

  The setup is pretty unlikely (there's randomly an airgapped tablet left on the ship with a human level intelligence on it? That sounds like _someone's_ worst case scenario), and things keep on happening in a way that's both plausible but could also be the governing AI making sure the simulation matches observed reality - up to and including the main character's odd certainty that there's something to hide __from__ in the first place.
  ```

- u/JohnKeel:
  ```
  This was enjoyable, but I really wish you hadn't namedropped Yudkowsky twice. He really doesn't deserve that much credit, especially since it can't be determined whether he's actually doing any research...
  ```

---

