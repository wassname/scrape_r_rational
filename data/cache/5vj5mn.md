## [D] Wednesday Worldbuilding Thread

### Post:

Welcome to the Wednesday thread for worldbuilding discussions!

/r/rational is focussed on rational and rationalist fiction, so we don't usually allow discussion of scenarios or worldbuilding unless there's finished chapters involved (see the sidebar).  It *is* pretty fun to cut loose with a likeminded community though, so this is our regular chance to:

* Plan out a new story
* Discuss how to escape a supervillian lair... or build a perfect prison
* Poke holes in a popular setting (without writing fanfic)
* Test your idea of how to rational-ify *Alice in Wonderland*

Or generally work through the problems of a fictional world.

^(Non-fiction should probably go in the Friday Off-topic thread, or Monday General Rationality)


### Comments:

- u/awesomeideas:
  ```
  I've been thinking about how interesting it would be if there's this one true, perfect morality, the global maximum of the moral landscape, and we've all seen it flawlessly represented in the Bible, but there's a memetic effect that causes us to misinterpret/misread the words. Or maybe we read and understand the words correctly, but our own built-in moralities have been corrupted. Not just that, but our use of logic itself is made untrustworthy by mental meddling. 

  How would we notice, and what techniques could we apply to mitigate the effects?

  Heck, how would Heaven convince us that yes, it's actually a moral problem to mix your fabrics?

  I suppose in vague terms that's actually what's probably going on, sans the Bible and active memetic influence bit. Our bodies have been woefully constructed by evolution and our brains are part of that.
  ```

  - u/LiteralHeadCannon:
    ```
    It would be pretty hilarious if all Friendly AIs converge on following a particular religion.
    ```

    - u/trekie140:
      ```
      I'm curious as to how that could actually happen. Under what circumstances do you think an artificial being could decide to follow a religion that humans had created? Most people here are atheists who assume that any rational intelligence would be atheist as well, but I'm a spiritualist so I'd like to explore alternative possibilities.
      ```

      - u/MagicWeasel:
        ```
        Basically as the thread starter said, if it turned out the christian bible was actually literally true, it makes sense that a super-intelligent computer program would work that out and start adhering correctly to the religion.
        ```

        - u/trekie140:
          ```
          For someone who is comfortable with believing in something that I cannot objectively prove to be true, that seems like a bit of a cop out. It's one thing for an AI to make scientific discoveries about the nature of reality that no human could have, it's another for an AI to subscribe to a belief system without epistemological basis. This whole line of inquiry is probably wishful thinking on my part, but I find the idea that an AI could decide to join or create a religion, with full knowledge of what that entailed, fascinating. 

          I won't pretend I'm not looking for potential justifications for my own beliefs that have no basis in epistemology, but my situation has led me to believe that religious belief is an inherent component of some people's psychology. I've still abandoned beliefs that didn't pay rent, but I've never been able to abandon the fundamental premise of theism. I'd be really interested in seeing what circumstances could make an AI doubt atheism/physicalism without calling its sanity into question.
          ```

          - u/MagicWeasel:
            ```
            I guess for a materialist, if it turns out that Religion A is true, then there must be material reasons for believing as much, so an AI could discover those material reasons and come to those beliefs "rationally". 

            If you wish to posit that religious belief is inherently irrational then yeah, a computer isn't likely to hold them. (Though it may profess to hold them in order to better fulfill its utility function)
            ```

            - u/trekie140:
              ```
              I'm not convinced it's impossible since, as someone with autism, I think autism is the closest analogue we currently have to how a conscious computer might think. It's still a huge conjecture to make, but the basic idea of a mind that intuitively understands logic but has to be taught how to express and read emotions is probably a situation we'll come across with AI. I've thought this over and have come up with two scenarios where I think an AI could develop religious belief. 

              First, it's possible that a (not Seed, that would be very bad) AI could intentionally be made imperfectly rational so it gets along better with us irrational humans. Second, if an AI could be programmed to dream, meditate, or some other phenomenon that blurs humans perceptions then it's possible that it could decide its subjective visions express something real. As for why anyone would do this, I can only presume that the psychology of a conscious being might require such things in order to function or at least relate to humans.
              ```

              - u/MagicWeasel:
                ```
                I don't know. I think saying "an AI mind would be a human mind but with condition X" is just as inaccurate as saying as "an AI mind would be like a neurotypical human mind". 

                Human minds, regardless of their specifics, tend to have a few things in common - for example, wanting the world not to be made of paperclips, knowing that art is valuable, thinking sugar is delicious, etc. An AI wouldn't have any of that unless it was explicitly programmed. And if you miss one little thing, there can be a *lot* of values drift as a result.

                I'm very wary about anthropomorphising AI. It's almost certainly going to be less like us than a dog is. But at the end of the day, we've got a few years before it becomes a thing, so who knows...
                ```

    - u/Frommerman:
      ```
      It would be terrifying if that religion were Scientology.
      ```

  - u/trekie140:
    ```
    Wisecrack posted a video on Westworld today, and I think part of it might be relevant to this. They explain that theologians concluded man's fall from grace occurred because God wanted man to do as he said, but our actions would be meaningless unless we had the option of choosing not to follow his will. However, what if his will needs to be vague for there to be a choice?

    Maybe an easily comprehensible objective morality would be a violation of free will. If you could absolutely convince absolutely anyone what the right thing to do was, then would "right" have any meaning without anyone choosing to do "wrong"? If that's true, then that means it must *always* to possible for man to act immorally, even if everyone tried to do as God said.
    ```

  - u/Gurkenglas:
    ```
    We might notice when AI's reading comprehension gets far enough to bypass the effect, or when someone is hooked up to an MRI while reading the bible and we find anomalies. Sufficiently advanced medicine could implant the neuronal structures the effect prevents us from forming directly inside our head. By the way, physicists would be pretty interested in the apparently magical process that controls the effect. If all adds up to normality, our DNA must include a recipe for something that can identify the bible or perfect morality - genetic engineering could fix that part.
    ```

  - u/buckykat:
    ```
    At least for the second part, you'd expect to see some positive correlation between biblical literalism and positive societal outcomes.
    ```

  - u/vakusdrake:
    ```
    I think the whole idea of the moral landscape is probably fundamentally flawed, in that it either leads to wireheading (though there's no problem if you're willing to bite that bullet, but almost nobody is), or totally fails to account for things too far outside normal (let's call the ancestral environment "normal") human experience (in which case the idea of it having a peak is weirdly nonsensical).        

    Plus as SSC's consequentialist FAQ points out, nobody is likely to actually care whether something is right in some abstract sense if it contradicts our moral intuitions too much. If you introduce incentive systems like graded afterlives, then it stops being a moral system altogether and becomes a guide to acting in your own self interest.

    Other than that there's also the weirdness that within your setting there's the question of why everyone is reading the _same_ wrong version of the text. Like interpretations may differ but why are all the wrong literal interpretations converging on the same point?
    ```

- u/space_fountain:
  ```
  So there's some news about worlds fairly near us with some interesting properties. How would it change society to have a couple of habitable worlds within say a weeks travel at low delta-v close to Earth? Obviously space travel would be more important. I think practically we can assume a couple of things. Only one of the planets has intelligent life. Atmospheric conditions are not consistent, pressure and oxygen contents. Things like the biological similarity between the planets are more up to debate. You could make an argument that the most likely is for them all to share a common ancestor
  ```

  - u/trekie140:
    ```
    They discussed this over at r/space.

    http://www.reddit.com/r/space/comments/5vk7hf/nasas_big_announcement_7_earthlike_planets_orbit/de2t99q

    > Since we're letting our imaginations run wild, imagine if there were intelligent civilizations on each planet. They'd grow up being able to watch the others progress, making stories, myths, and legends about them. They'd have enormous motivation to get into space and eventually they would build rockets and visit each other, probably quite a long time after they had developed some kind of communication.

    > How amazing to finally meet the races of people you'd been gazing up at for thousands of years.
    ```

- u/MagicWeasel:
  ```
  So, something from my vampire-and-werewolf type world (what's this type of setting called BTW? vampires, werewolves, demons, etc - the sort of world that is in World of Darkness, Buffy, True Blood, etc? It's not fantasy because there's no elves, or is it?).

  We've got gargoyles. They're big immortal creatures with a couple of forms they can transform between, including a stone form. They have an interesting set of values: their terminal values are satisfied by satisfying the terminal values of others. Basically, they are a "Meet your meat" version of a slave. Wondering if anyone has ideas for how to explore this concept to its logical conclusion?

  Here's some other facts about them:

  * They are fine with being bought, sold and given away and will take on duties serving their "new master". (Mostly because their old master "ordered" them to do it, and they like following orders).
  * They have their own desires extra to "serving their master" - for example, they get married, have children, etc. Their devotion to their families is always less than to their master, but I guess their families deal with it. 
  * Their "satisfying master's values" is more of the form "they will treat you like a very, very good friend". So if you've got a big presentation early tomorrow morning, they're not going to obey orders to bring them enough alcohol to end up hungover and thus unable to give the presentation. Equally, if you're in the wilderness with your leg stuck under an immovable rock, they'd be willing to cut your leg off to save you. (In reality the rock scenario wouldn't happen because gargoyles are big, strong, and have *ridiculous* amounts of momentum available). 
  * i.e. they essentially have a vaguely Three Laws of Robotics thing, except not, because they are still completely autonomous so they don't get stuck in those weird paradoxes.

  One interesting conclusion I came to: a gargoyle's desire for a master is so strong, they will seek one out. If they can't find an intelligent creature to serve, they will choose an animal or plant and start satisfying its desires. I imagine those giant forests of clonal organisms being plants that a gargoyle has decided are its master.
  ```

  - u/kraryal:
    ```
    This type of setting is generally labelled "Urban Fantasy", where you have the masquerade, modern times, magic, etc. It's a pretty popular romance setting in traditionally published works, and reasonably popular in the larger fantasy milieu. Dresden Files, for instance.
    ```

  - u/None:
    ```
    Fantasy doesn't have to have elves. I have no idea what gave you that impression. And yah that'd fit under the fantasy tag though there may be some narrower genre that'd it fit in.

    Heck my top 10 fantasy stories don't even include elves.
    ```

    - u/MagicWeasel:
      ```
      The "fantasy equals elves" was a bit flippant, but I did mean it to say that it seems to me that vampire/werewolf stories set in modern times are a different sort of thing than the prototypical D&D setting which tends to be lots of magic, mythical creatures exist, that fact is commonly accepted, half-elves and orcs are integrated in society more-or-less etc. 

      Whereas "supernatural fantasy" (I guess?) is modern times, there's a masquerade(ish), magic/etc is not ubiquitous but only available to a very select few, etc.

      But maybe that's just a failure of imagination on my part. And something like Harry Potter doesn't fit squarely into either of those, as by my definition just then it's more in the "supernatural fantasy" box, but IMO it has more in common with "high fantasy" in a lot of ways. Then again, maybe it doesn't....
      ```

      - u/None:
        ```
        Urban Fantasy is a thing.
        ```

  - u/trekie140:
    ```
    If a Gargoyle choose to serve a nonhuman master, that could actually be very dangerous. There's no way for them to receive coherent orders or interpret their master's moral values, so one guarding a forest could easily become an eco-terrorist by simply inferring what their master commands. 

    If these gargoyles do exist, I think they'd be considered insane by their peers since they've basically invented a master they claim to receive commands from that no one else can understand and frequently conflict with the values and common interests of the vast majority of masters.

    As for you idea, I like it but I'd tone down the "care more about master than themselves" part. I think they're interesting enough as obsessively good friends, but the idea that they harm themselves or others they care about out of that friendship *without reluctance* is creepy and makes them harder to see as people.
    ```

    - u/MagicWeasel:
      ```
      >  so one guarding a forest could easily become an eco-terrorist by simply inferring what their master commands

      I love this. I don't *think* a gargoyle would go out and destroy bulldozers if the forest was getting paved, at least not until it became clear that "his" tree would be destroyed. A gargoyle looking after a forest is a desperate creature, and he would definitely have the pity of his equals.

      I also think that the second someone says "what the hell are you doing?", something primal will awaken in them - *they've just been ordered to answer a question* - and when the person follows up with "get the hell away from the bulldozer!" their eyes light up. *Oh my god, what is this? Why do I feel so good? I can't wait to get away from this bulldozer. Who cares about that stupid tree. I want that guy to give me something to do again!*

      Now the question is, what does the gargoyle do if the bulldozer operator says "piss off, I don't want anything to do with you"? Do they go on the hunt for a new master, or do they simply go "okay, this guy doesn't want to see me again, but I'm going to hide and watch him and do things in the background to make his day better" (maybe difficult as they do NOT have any even vaguely stealthy forms)

      >the idea that they harm themselves or others they care about out of that friendship without reluctance is creepy and makes them harder to see as people.

      Yep. i'm going for "creepy" (hard to see as people is a bit different as the nature of a gargoyle is to be friendly and relateable, and so would model reluctance if he sensed his master would like that), so that's a feature, not a bug. Here's a scene from what I've written that pretty much hits the nail on the head in terms of what you wrote:

      >“I don’t know if this is true or not, but my grandmother always said that there was no such thing as dinosaurs. She said there was nothing bigger than an elephant because bones aren’t strong enough to hold up that much weight. So… are your bones like normal… like human bones?”

      >“I don’t have bones.” 

      >“Wait, what?” Red paused. “So, are you… flesh all the way through?”

      >Julias grabbed one of the dishrags and wound it around his forearm, tucking it under itself to hold it in place. He casually picked up the knife he had been using for the onions and matter-of-factly slid it through the soft skin on his forearm. Blood began to seep out of the wound, which the tall man spread open to give Red a good look, angling the gash so that the blood would mostly soak into the rag.

      >About a centimetre into his arm, the flesh gave way to sandstone like that of his statue form - a pale beige with darker stripes in a more orange hue. A few tiny grains of the stone endoskeleton were chipped away by the blade.

      >Red swore, grabbing a clean tea towel to try and place on the wound to stop the bleeding.

      >“Oh geeze, Julias, what are you doing? Doesn’t that hurt?”

      >“Of course it does.” He grinned as he placed the knife on the table to hold the rag and the reddening tea-towel firmly on the wound.

      >“Are you going to need a doctor?” Red asked, noticing the warm, sticky blood on his hands. The acidic smell seemed to fill the air. He went to the sink to wash up.

      >“No, it will heal in a few days.” He said, tying the towel firmly around his forearm where it could serve as a makeshift bandage. “Do you have any more questions?” He picked up the bloodied knife and placed it in the bucket of dirty kitchen tools.


      EDIT: another thought I had: if a gargoyle would rather follow a human than a tree, it quite probably follows that after being given an order by a still "higher" form of life (e.g. vampire), he would prefer to obey that, since their desires are "stronger" (maybe? whatever metric would say a new human gets precedence over a tree it might have been nurturing for a thousand years). An interesting thought, with the consequence that such creatures don't remain in the service of mundane humans for long.
      ```

---

