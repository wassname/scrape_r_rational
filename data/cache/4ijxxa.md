## [D] Monday General Rationality Thread

### Post:

Welcome to the Monday thread on general rationality topics!  Do you really want to talk about something non-fictional, related to the real world?  Have you:

* Seen something interesting on /r/science?
* Found a new way to get your shit even-more together?
* Figured out how to become immortal?
* Constructed artificial general intelligence?
* Read a neat nonfiction book?
* Munchkined your way into total control of your D&D campaign?


### Comments:

- u/gabbalis:
  ```
  >Constructed artificial general intelligence?

  You know my parents came pretty close. They just forgot the artificial part.
  ```

- u/LiteralHeadCannon:
  ```
  All this GAI progress lately is *pretty spooky*, I've got to say. DeepDream is a nightmare.  I've felt pretty depressed lately because all of my brain's independent estimates indicate that the world as I know it will end before *a very conservative estimate* of 2030. What does my fanfiction matter, then? What does my original fiction matter? What does my education or pretty much anything I can do matter?

  Any advice for remaining a functional human being despite my knowledge that I will soon be either dead or something beyond my comprehension?
  ```

  - u/alexanderwales:
    ```
    AGI by 2030 is not properly conservative. That's 14 years away; as far as we are away from 2002 right now. I was rereading *The Singularity is Near* last week, and I really bought into a lot of it back when I was in college ... but that was more than a decade ago, and so much of it has failed to materialize, or has come only in the absolute weakest form that it could. (I also got a degree in computer science and worked for some pretty big companies since then, which also served to make me more skeptical about the rate of progress.)

    I think futurists *in general* overestimate, because overestimation is more sexy than underestimating. "The world will end by 2030" is a shocking statement, which makes it easier to spread around with clickbait titles. And it's not just futurists! Scientists and engineers tend to overpromise, or at least the more extreme promises tend to get spread around more. It's why you go to E3 and designers are talking about these lush, verdant, detailed worlds, but when it actually comes out, features have been lost and graphics have been downgraded. This is also why doomsday preachers always say that the world is going to end in the next few years, rather than a few centuries from now.

    (Conservative means different things when used in different ways, however. If I were part of a governmental body in charge of regulating AGI, my "conservative" would be the earliest possible date I thought it was feasible. If I were planning for retirement, however, my "conservative" would be much further in the future.)
    ```

    - u/None:
      ```
      As a fellow computer scientist... yeah.  At this point I've lost track of how many flagrantly unrealistic overpromises our project's upper management has made.  And that's for a new smartphone, not for something big and serious like AI.

      Deep learning is actually making large advances on vision and control problems, but it's also still finnicky, hard to architect, and error-prone on edge cases.  The probabilistic approach to cognitive science has explained a lot, but on the computational end, it has some speed problems and the languages aren't very good yet (especially for integrating with arbitrary real-world libraries).  Hell, prob cog sci hasn't even got complete explanations for some observed psychological and experimental facts yet.

      The future will arrive with maddening slowness.  Meanwhile, global warming is the only thing constantly ahead of schedule :-(.

      Fairest and Fallen, you are *such* a total douchebag.
      ```

  - u/None:
    ```
    The world can't end from AI yet.  I haven't published even my first theoretical finding, let alone started coding.
    ```

  - u/None:
    ```
    [deleted]
    ```

    - u/None:
      ```
      Thank you. This helped me quite a bit.
      ```

  - u/Frommerman:
    ```
    2030 is a wildly rosy estimate. Assuming Moore's Law keeps working (and there are those who think it won't), a $1000 computer will have the processing power of a human brain by 2045. Extrapolating back, we see that such a computer would cost over a million dollars still in 2030. Doable for some to do an upload at that point, but still too expensive, even assuming that we can develop a safe and consistent means of mapping and simulating a connectome before then. Your meatbrain is still going to beat the bots for a while yet.
    ```

    - u/None:
      ```
      > Assuming Moore's Law keeps working (and there are those who think it won't), a $1000 computer will have the processing power of a human brain by 2045.

      This kind of estimate depends strongly on how you're measuring the processing power of the human brain.  I don't think most estimates are very good, since they don't take into account that the brain is:

      * Natively stochastic: cortical micro-circuits are theorized to implement Markov Chain Monte Carlo algorithms, or something like them.

      * Natively parallel: we don't know precisely what sort of algorithm is used yet, but we think that spike trains encode surprisals, and so long-distance connections in the brain are some kind of message-passing of surprisals between different probabilistic models.

      So the brain ends up able to do *certain* things very quickly even while lacking a lot of serial processing power.
      ```

    - u/BadGoyWithAGun:
      ```
      > Assuming Moore's Law keeps working (and there are those who think it won't)

      It's already stopped. Intel officially cancelled Moore's law, they're now doing process shrinks every 3 architectures (ie, 2-2.5 years as opposed to 1.5 years). And that's just to stretch out the time before they literally run out of atoms to shrink - you can only make a silicon transistor so small.

      The fact is, barring a breakthrough in photonic computers, graphene, or other completely new substrates, in a few years (~2020) we will be at the point where we'll only be able to make silicon-based computer hardware more computationally powerful by making it bigger and require more energy.
      ```

    - u/Dwood15:
      ```
      Moores law has been agreed that it ended a while ago. Even the ceo of Intel in his recent 'moores law is still alive' speech refrained from mentioning the doubling of transistors
      ```

      - u/Frommerman:
        ```
        Transistor doubling isn't the only measure you could use, though. Cost is also a viable way to look at it, and though we can't really continue improving transistor density with current methods, we can make transistors cheaper. That is still happening.
        ```

        - u/None:
          ```
          In addition to everything /u/Dwood15 said, "Moore's Law" once referred to the clock speed at which processors could run generic serial programs.  Then it started to refer to how many parallel cores you could put on a chip, as clock speeds topped out between 2-3 GHz for affordable processors and 4GHz started to require increasingly advanced cooling systems.

          Now it's started to refer to stuff like power consumption.  It's great that chips are still improving at a regular pace - we all want to use less juice - but that doesn't mean they're improving like they once did.  For non-specialized applications where stuff like GPGPU computing doesn't apply, the exponential speedup in *how fast your average CPU-bound application can run* driven by chip development is firmly over.
          ```

        - u/Dwood15:
          ```
          That may be, however, Moore's law is typically associated with transistors, and my comparison was merely for the sake of performance in a desktop machine. At this point, we will not reach the mythical "power of a human brain by 2045" in a desktop pc (though you mentioned cash, I assumed standard PC as that's what the comparison is generally used by in sites like waitbutwhy), and that's the point I'm addressing. 

          As to the cost: think of it more like a logarithmic style curve instead of an exponential one. At one point, the cost will reduce to a minimum profitability level where Intel/hardware manufacturers will be unable to make cash if they make it any cheaper (though we aren't even close to that yet). Assuming no innovations in the hardware being produced, the processes to create the hardware can only be streamlined and improved so much so I would guess that we'll see the cost side of Moore's law slow down. 

          Also, Intel not having any competition from AMD on the desktop market isn't helping things either.
          ```

  - u/raymestalez:
    ```
    Well, you could focus your efforts on putting yourself in the position where you can take the most advantage of the upcoming AI/technology/etc.

    It may be hard for your brain to function and be motivated when facing death or incomprehensible things, but it loves to solve more short term and understandable goals.

    Until AI comes over and kills us all(or whatever), there's plenty of things you can do to be in the best position when it happens. You can be learning CS and try being a part of the research, or you can make money doing what you do best and then donate to things or invest in startups, to steer the world in the right direction. Or you can focus on becoming rich, because I bet that not every person in the world will be able to take advantage of life extension, BCIs, whatever comes in the future. You might as well strive to be one of the people who can.
    ```

---

