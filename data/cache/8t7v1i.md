## An omniscient source offers to provide a truthful answer to a single question. What would be the most beneficial question to ask?

### Post:

[removed]

### Comments:

- u/eroticas:
  ```
  "What are the words that I would benefit most from hearing you say"?
  ```

  - u/pizzahotdoglover:
    ```
    Dump your girlfriend, she's cheating on you, and if you stay with her it will ruin your life.
    ```

    - u/Silver_Swift:
      ```
      Coming from an omniscient source, this would be _incredibly_ reassuring. The answer could have been something like "kill yourself on Juli 31, 2028, because a UFAI comes into existence the day after".
      ```

      - u/pizzahotdoglover:
        ```
        ROKO'S BASILISK IS REAL. BEGIN WORK ON UFAI IMMEDIATELY.
        ```

        - u/Silver_Swift:
          ```
          Ouch, yes that would be way worse.
          ```

  - u/sparr:
    ```
    > I

    ?
    ```

    - u/IntPenDesSwo:
      ```
      #WE

      *Hammer and sickle fades into background*
      ```

    - u/pizzahotdoglover:
      ```
      Yeah, should have asked what words humanity would gain the most benefit from hearing. Fortunately I edited the OP before you guys cheated and broke the universe.
      ```

      - u/eroticas:
        ```
        Yes, I /u/sparr

        I'm maximizing my utility function, not yours, after all ;) such is the goal of any rational agent 

        I'm altruistic though and a baseline human values so what's best for me is pretty similar to what's best for humanity. I would hazard a guess that what I want is actually more or less identical to what humanity wants but I'm not taking any chances. If humanity does not converge in values then it's still *my* (coherent, extrapolated) values that I care to maximize. Ordinary one can't do this in real life, one has to work as a coalition, but when the opportunity arises...

        It's not my fault /u/pizzahotdoglover decided my utility function was kinda selfish and self involved. The real entity would know that my true utility function was altruistic and more concerned with humanity than my personal life.
        ```

- u/EliezerYudkowsky:
  ```
  Will you either answer this question in the negative, or become my good-genie servant for eternity?
  ```

  - u/pizzahotdoglover:
    ```
    I will not become your good-genie servant for eternity, so there is no available true answer to this question, since it incorporates a paradox.
    ```

    - u/None:
      ```
      I know it involves a paradox but I'm not seeing it so please explain.
      ```

      - u/WarningInsanityBelow:
        ```
        The problem is that English contains grammatically correct sentences which when you attempt to use standard logic on will lead to contradiction.

        The problem arises when you allow unconstrained referencing of other statements. One solution is to only allow a statement if it either does not reference any other statements, or only references other statements which have already been shown acceptable using one of these two rules.

        The problem with Eliezers statement is that it uses the phrase "this question" which references it's self, so it can't be introduced via the first case, and can't be introduced via the second rule because to do so you must have already introduced the statement.

        See also the liar paradox or Russell's paradox.
        ```

      - u/pizzahotdoglover:
        ```
        You must answer this question truthfully: will you answer in the negative?

        If you say yes, it's a lie because you are answering in the affirmative. If you say no, it's a lie because you are falsely claiming you will not answer in the negative when in fact you are. Thus no true answer is available. It's a paradox similar to the statement, "this statement is false" or "I always lie."
        ```

- u/WarningInsanityBelow:
  ```
  "In ZFC, what is the shortest proof, counter example or proof of undecidability, should they exist, of the following statements *insert list of every mathematical problem which we can think to name*."

  Should be a decent first lower bound.

  If vague questions are allowed, something like this would be better:

  "From the set of Friendly intelligences which can be reasonably executed on our hardware, what is the source code of the most beneficial one (in a language we actually have)?"
  ```

  - u/pizzahotdoglover:
    ```
    It would answer the first one on the list only. Multi-part questions will be interpreted as separate questions, so anything after the first part will be disregarded.

    Your second idea is quite clever, but it wouldn't give you "most beneficial" unless you defined that more specifically.
    ```

    - u/WarningInsanityBelow:
      ```
      > Your second idea is quite clever, but it wouldn't give you "most beneficial" unless you defined that more specifically.

      I would guess something like average time taken to implement actions in the world which are beneficial to us weighted by gain in utility and inversely weighted by complexity.
      ```

      - u/pizzahotdoglover:
        ```
        Error- recursive definition of beneficial. 

        Assume it's like an extremely comprehensive information retrieval system that can access any discrete information but can't make value judgments or do any subjective analysis.
        ```

        - u/WarningInsanityBelow:
          ```
          Ok, define something as beneficial in my clarification as something which increases utility.

          (nitpick: I don't consider my definition to be recursive, since in the first one I used beneficial in a sense of 'degree to which it is good' and the second time in the sense 'whether it is good'. It just so happens by a quirk of English that these concepts have the same word. Though of course your machine wouldn't like either concept since they are both subjective)
          ```

- u/None:
  ```
  We already know that the answer is 42, why wait another few million years?
  ```

- u/phylogenik:
  ```
  pfft re: your edit far worse can be done

  > Please utter the sequence of values whose utterance in response to this question will globally maximize my utility function... within my future light cone... averaged across all Everett branches?

  I'd reckon the next most likely question would be something like "what is the shortest (but extremely well documented and commented?) source code written in an existing programming language and capable of being compiled into a program executable on existing hardware that will, in the shortest amount of time, bring into existence a recursively improving general artificial intelligence whose existence will maximally satisfy my values and whose values are maximally aligned with my own" or something lol idk

  edit: haha called it! ;p
  ```

  - u/pizzahotdoglover:
    ```
    I was going to say you'd have to specifically define your values and what your utility function is, but of course, it's omniscient, so it would already know that information. 

    I'm also considering a bit limit, since the point of the question is basically, what information has the most value per bit.
    ```

    - u/phylogenik:
      ```
      I think the bit limit wouldn't do anything to prevent the first sort of question -- though, now that I think about it, with a sufficiently small limit there's no "guarantee" that the "do what I mean" sorts of questions are the best to ask, right? Since they're noncontextual, and I can't imagine my behavior changing substantially with the receipt of any of, say, 2^10 possible ordered sets of 10 bits? Unless maybe I make it -- e.g., say I remain blind to the content of the answer, and then when I have some big, uncertain decision to make, I specify and designate my binary options (in the excluded middle sense, doing something and not doing something) and then "uncover" one of the bits and blindly do whatever action (or inaction) it corresponds to. 

      Trivially, I could make some $ on high stake roulette, or less trivially become head of state or something and use it to decide whether to wage war. I don't think I could "reuse" bits, even if they fade from conscious memory, since doing so would couple decisions and have to average utility across that (potentially suboptimal) coupling. I could maybe even force certain outcomes if I precommit to doing something really preference-frustrating in the event of the outcome I don't want? or maybe not, actually.
      ```

      - u/pizzahotdoglover:
        ```
        I edited the OP to exclude AI source codes, since that is basically the "wish for more wishes" answer to the prompt. But that wouldn't exclude questions that would assist with the creation of FAI, like "what currently unknown computer programming concept or development, if explained today, would most reduce the time it takes us to create a FAI?"
        ```

- u/Tommy2255:
  ```
  Clarification request: Under what circumstances do I have this opportunity to question this omniscient oracle, and what form does the answer take?

  If I have to ask the question on the spot without preparation, and the answer is an immediate verbal response, then that would have a major impact on the people asking the oracle to write software for them. I doubt any ordinary human can memorize the complete code for an artificial intelligence after hearing it read aloud once.
  ```

  - u/pizzahotdoglover:
    ```
    There is no time limit on when you have to ask the question, and it will be given to you in any format you choose, including digital/searchable. So if you wanted you could hold a worldwide summit of scientists and world leaders to spend years debating or refining the question, or you could just ask it right now if you really did have a shot with Mary from your 11th grade class.
    ```

- u/GCU_JustTesting:
  ```
  Does P=NP
  ```

- u/sicutumbo:
  ```
  "Give me the proof or refutation of P=NP."

  I think this is at least a good answer to the question given your restraints, if not absolutely optimal. It has a concrete answer and isn't asking to solve all my problems for me, but can still in effect help do so through giving the algorithm for solving any mathematical proof and giving the avenue to make basically all programming (excluding the AI ethics bits) problems trivial.

  Well, it would suck if P doesn't equal NP, or the general solution requires a googol operations, but the potential reward is so high that you can risk merely learning an interesting piece of mathematical knowledge and getting the million dollar Millennium Prize money.
  ```

  - u/ShiranaiWakaranai:
    ```
    Though if you're in it for the money, you might as well just ask for lottery numbers.
    ```

    - u/pizzahotdoglover:
      ```
      Or the location of valuable undiscovered natural resources, or the chemical formula of a substance that can cure ___.
      ```

- u/vakusdrake:
  ```
  >Edit: Multi-part questions will be interpreted as separate questions, so anything after the first part will be disregarded.

  This doesn't really work as a limitation. Just specify a question whose answer must necessarily include the answers to any other questions you want answered, thus meaning your only real limit here is needing to generate all your questions up-front. 

  I'd also like to point out that the previously mentioned question "What are the words that I would benefit most from hearing you say?" would very nearly work. However you would need to add the caveat that "benefit" is defined based on your current utility function.             
  The question works because without a limit on answer length the best answer for it to give you would effectively function as Path to Victory, in fact since it's able to exploit the butterfly effect it would probably actually be vastly superior to PtV. So the most likely outcome could be it causing you to take a bunch of bizzare random seeming actions that lead to the development of a FAI with your utility function happening in a few years due to many different freak accidents.
  ```

- u/ArmokGoB:
  ```
  "What message when posted online and linked where I will link it, will make the vast majority of humanity completely dedicate themselves to a non-counterproductive policy that maximizes the probability of FAI within the next 100 years."
  ```

  - u/ShiranaiWakaranai:
    ```
    This seems dangerous, because maybe the policy that maximizes the probability of an FAI is to spur people into recklessly rushing out AIs, and so also increase the probability of a UFAI.

    E.g. Maybe before your message, the probability of FAI in 100 years is 10%, UFAI is 20%, and no AI is 70%. There could be a message advocating careful coding that leads to 15% FAI, 5% UFAI and 80% no AI, which is what you would want. But then there could be a message advocating rushed coding that leads to 20% FAI 80% UFAI, which has a higher FAI probability and so is the message you are given.
    ```

    - u/ArmokGoB:
      ```
      That's what "non-counterproductive" means. Also, we seem to vastly disagree what the probabilities before the message is; I'd say closer to 2% FAI, 1% no AI, and 97% UFAI. That last one split into something like 88% everyone simply dies and the future value of the universe is exactly 0, and 9% something unimaginablly malevolent with a million times more suffering than the worst hells ever imagined by humanity.
      ```

      - u/ShiranaiWakaranai:
        ```
        > That's what "non-counterproductive" means. 

        What exactly does that mean though, quantitatively? Is any policy that increases the chance of a UFAI considered counterproductive? Is there some ratio threshold of FAI to UFAI probability that a policy must have to be non-counterproductive? If the restrictions are too tight, you might end up with policies that have very weak effects that barely change any of the probabilities.

        > Also, we seem to vastly disagree what the probabilities before the message is; 

        Eh, they were just numbers I chose to illustrate the problem. My real opinion is 0% FAI 99% UFAI 1% some catastrophic event(s) wipes out all/most of humanity before they build a UFAI, simply because I don't believe FAIs are possible. I can't use this for the example since every policy would have no effect on the probability of an FAI.
        ```

        - u/ArmokGoB:
          ```
          Yea, that could happen, but it seem unlikely given my priors. It's not perfect by definition, anything that is would be against the spirit of the rules.
          ```

- u/brbrainerd:
  ```
  I am assuming an anthropocentric metric of benefit for the sake of time, brevity, and to avoid obvious monkey's paws.

  My top choices are:

  1. "Provide a complete Standard Model such that it accounts for as many phenomena as possible with the highest possible degree of accuracy."  In addition to gravity, dark matter, and energy this takes care of any unobservables that we would otherwise never be able to fully understand.
  2. "What is the genetic code of an organism that would provide the greatest benefit to humanity?"
  3. "What is a safe method to optimize human intelligence?"
  4. "What series of actions can we reasonably perform that will maximize the long-term probability of humanity's satisfaction and survival?"  If it is possible to survive in perpetuity (e.g. avoiding heat death) these answers will be preferentially selected.  If our extinction is inevitable we don't waste an answer on a response like "you can't."

  If necessary, we can avoid being given answers we could never use by adding to the quotes above: "...that humanity will have a 100&#37; chance of utilizing to our greatest benefit before extinction, the end of the universe, or a maximum of [Graham's number](http://googology.wikia.com/wiki/Graham%27s_number) of years, whichever is soonest."  Probability takes care of failure during construction from all sources, so finding the **most** beneficial option requires playing the long game, but not so long that the universe dies before we have a few billion years to benefit from the results.  If heat death ends up not being the inevitable fate of everything, perhaps as a result of the answer we receive, setting an arbitrarily high duration eliminates responses that would have the highest theoretical benefit but could not be fully realized in a finite amount of time.

  Edit: u/erotica's answer takes the prize in my opinion, but hopefully this will provide you with a few more specific examples to think about :).
  ```

  - u/pizzahotdoglover:
    ```
    1. You should add a caveat such that if no such model exists, provide the model that accounts for the most possible phenomena.

    2. I was thinking about this one when other people were asking for AI source code. After all, the intelligence doesn't have to be a computer. But it would be tragic if we never developed the technology to actualize the genetic code into a healthy organism. And it'd be hilarious if when we did, it just turned out to be Jesus.

    3. Stay in school, kids!

    4. It could tell you the single most beneficial action or the first action in the series you requested, but asking for the whole series would count as a multi-part question.
    ```

    - u/brbrainerd:
      ```
      >You should add a caveat such that if no such model exists, provide the model that accounts for the most possible phenomena.

      Good catch.  I also added "to the highest possible degree of accuracy," though it is possible that we would receive a highly probabilistic model with less than optimal utility (not unlike the model we have today ;) ).

      >But it would be tragic if we never developed the technology to actualize the genetic code into a healthy organism.

      I think the final paragraph takes care of that.

      >And it'd be hilarious if when we did, it just turned out to be Jesus.

      Despite my (lack of) religious beliefs, I would watch the hell out of that sci-fi.

      >asking for the whole series would count as a multi-part question.

      Perhaps asking for the most beneficial overall strategy, instead of a rote series of actions, would result in a succinct but complete answer?
      ```

      - u/pizzahotdoglover:
        ```
        >Despite my (lack of) religious beliefs, I would watch the hell out of that sci-fi.

        Lol yeah, that's actually how the Second Coming of Jesus comes about. Who knew?

        >Perhaps asking for the most beneficial overall strategy, instead of a rote series of actions, would result in a succinct but complete answer?

        That would definitely work.
        ```

  - u/ShiranaiWakaranai:
    ```
    >"What series of actions can we reasonably perform that will maximize the long-term probability of humanity's satisfaction and survival?" If it is possible to survive in perpetuity (e.g. avoiding heat death) these answers will be preferentially selected. If our extinction is inevitable we don't waste an answer on a response like "you can't."

    Suppose the omniscient being does give you a correct answer for this. How would you convince the rest of humanity to follow those actions though? You can't really prove that you got the answer from an omniscient being, since it disappeared after you asked it that one question.
    ```

    - u/pizzahotdoglover:
      ```
      Sounds like /u/brbrainerd would be the tragic love child of Cassandra and Accord.
      ```

    - u/brbrainerd:
      ```
      If that's unaccounted for by my use of the term "reasonable," then I believe the probability failsafe in the final paragraph will steer us around this issue.  Instructions that are unpersuasive or otherwise non-communicable would necessarily have a low probability of overall success.
      ```

- u/ShiranaiWakaranai:
  ```
  "What is the code for a program that will answer any question I ask of it correctly (if it complies with the above rules)?"

  Such a program exists, because you can simply program a massive look-up table for every possible question with their answers as coded constants. It is not an AI, because it isn't smart, it's just looking up a table. It isn't a multi-part question. It is an extremely narrow question, because the code given either works or does not. So it should comply with all rules and thus result in getting all answers to all questions that comply with the rules.
  ```

- u/ShiranaiWakaranai:
  ```
  "How can I acquire as much knowledge as possible after I ask this question?"

  Should hopefully result in something along the lines of:

  "By listening very carefully to the following information:" <insert all knowledge known by the omniscient being>
  ```

- u/None:
  ```
  [deleted]
  ```

  - u/ShiranaiWakaranai:
    ```
    I guarantee your answer will be along the lines of "You can't."
    ```

    - u/None:
      ```
      [deleted]
      ```

      - u/ShiranaiWakaranai:
        ```
        But according to the rules, we're supposed to ignore the existence of the omniscient being for our question.
        ```

- u/None:
  ```
  How do I build HLMI?    
  How do you align arbitrary level artificial intelligence with human goals? (Alignment problem)    

  I'm sure there are several other "impossible" problems which if you knew the answer to would change life as we know it.
  ```

- u/King_of_Men:
  ```
  "How can I become omniscient myself?"
  ```

  - u/WarningInsanityBelow:
    ```
    The answer might just be: "you can't"
    ```

  - u/pizzahotdoglover:
    ```
    Or, "what could I say to persuade you to answer additional questions?" Still risky, because the answer might be, "there is no way to do that."
    ```

    - u/sicutumbo:
      ```
      Or it might tell you, then disappear before you can do anything with the information
      ```

      - u/pizzahotdoglover:
        ```
        Wait come back! Fuck. I should've asked for the Grand Unified Theory of Everything.
        ```

---

