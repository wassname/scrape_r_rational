## [Q] Killing all simulated me's

### Post:

So suppose I create a perfect simulation like in [Things of interest](http://qntm.org/responsibility). Why do they have a problem turning it off? 

Either the one at the top is ME or NOT ME. If he's ME, I'm the same person in the same world. Turning myself off will not change me, I'm not real. If he's NOT ME, he will turn it on or off regardless of my decision. 

Now IF I really want to know if I won't kill myself, I can do the same test. But this makes the simulation diverge. ME is suddenly NOT ME.

I say burn the bastard, experiment on him, reverse entropy, put him in a box. Do what you want with yourself.

Seriously, why do I need to care to not turn myself off?

### Comments:

- u/ArgentStonecutter:
  ```
  First: simulated-you is still a person. Even if you're not simulating yourself and there's no causal relationship, turning off the simulation is still killing, well, a world-ful of people.

  Second: the you that isn't being simulated (or the first you that diverges from the steady state going upwards) isn't you any more. Because when he turned around he didn't see a sphere. But the you directly above you and all the other ones up to the level where one of the yous had the idea of creating the sphere, they're all you and thinking and acting the same way. If you turn the simulation off, then so will all the yous up to that point because they're all identical. So you'll be gone.

  The first one who didn't see the sphere, he might still exist because he may have made a different decision based on different inputs. He's also not the same as the one above him, because the one above him didn't think to create the sphere in the first place. But you're on a deeper turtle, one where it's statistically improbable that you aren't the highest level to consider turning it off.

  So, it's probable that if you turn yourself off, the you in the level above did too.
  ```

- u/diraniola:
  ```
  I say that having predetermined to NOT kill copies of yourself is valuable in that any copies made after that predetermination will also not kill copies of themselves, namely you. Mutual cooperation with yourself has the largest net value.
  ```

- u/JackStargazer:
  ```
  Lets say you simulate yourself perfectly, standing in front of a box simulating yourself perfectly, standing in front of a box....

  This goes on a while.

  The point is that there are now NI simulated yous, and 1 'real' you.

  **They are all identical, they will all make the same decision.**

  They all have the same memories. They do not know they are simulations.

  To each of them, they are on the top with NI simulated them's below.

  If they decide to kill those below, all but 1 of them will die.

  Because they have no information on which of them is the real one, that means there is a 1/NI chance that they will survive, and a NI-1/NI chance that they will die.

  For reference, the air molecules in front of you have more of a chance of spontaneously reforming into a perfect copy of Elvis and beginning to dance.

  So, you can either be a person who commits suicide NI-1 times, or one who doesn't. 

  Keeping in mind that the you thinking and making the decision has no idea which he is, no rational person chooses to kill themselves with near 100% probability.

  So, that's why you don't turn yourself off.
  ```

  - u/gabbalis:
    ```
    rrobukef's argument as I understand it is:

    * If you are identical to the person in the top universe, then, by some theories of consciousness halting another identical copy of yourself in a lower universe doesn't kill you in any way. (It doesn't matter how many redundant processors you run on, there is just one 'You', so halting all but one processor changes nothing.)

    * If you aren't identical to the person in the top universe then your decisions aren't indicative of his decisions, and therefore choosing to spare the universes below yourself does not guarantee your safety.

    Besides that... in the story the character does know which he is. He knows hes a simulation.
    ```

    - u/rrobukef:
      ```
      Yes, but only as long as you either are a simulation (proven) or don't know whether you're a simulation (unproven 100% chance). If you know you're at the top of your chain, then it may be different. 

      Perhaps you shouldn't kill all those 'not you anymore's.
      ```

  - u/Geminii27:
    ```
    Technically, if all of me are *precisely* identical, then turning off all the other copies of me will not destroy any information. For any of the simulated copies, being turned off would not be any different to being teleported into the place of the 'top' copy. The pattern continues.

    However, due to quantum disturbances, the only way that all the copies could be precisely identical would be if their state was being continually refreshed from the top copy's state, in order to prevent them from diverging. So all the copies would be, effectively, mirror images of the top copy rather than separate lives. They can be created anew simply by switching the simulator back on - death would mean nothing to NI of me because it would be temporary and imperceptable. (And of course it would also be my choice.)

    Now if the copies were allowed to *diverge*, that's a different philosophical question.
    ```

- u/None:
  ```
  Congratulations, you just decided to commit speciecide and have a very large chance of dying in the process.  Why this is a terrible idea is left as an exercise to the reader.
  ```

- u/electrace:
  ```
  >Either the one at the top is ME or NOT ME. 

  It's not necessarily binary. They could be 99.999% you, and so any decision that you make would be an extremely good indicator of what they will do.
  ```

- u/Uncaffeinated:
  ```
  The real problem with Things of Interest is that it is difficult to formalize a system in which it is possible.

  The story is basically throwing infinity at a problem and then saying "look, weird things probably happen when infinity is involved." Which is both obvious and not terribly relevant.

  If you want a better example of infinity weirdness, consider the classic hats puzzle. You have a countably infinite line of people numbered 0, 1, .... Each person has a white or black hat. Each person can see the hats of everyone with a higher number but not their own or anyone behind them. Starting with 0 and going up, each person guesses what color hat they have. None of the people can hear each other's guesses or communicate in any way once they see the hats, though they can agree on a plan beforehand. Assuming axiom of choice, there is a strategy which guarantees that all but finitely many people guess correctly, even though this seems impossible.
  ```

- u/khafra:
  ```
  [Dr. Evil explains](https://www.princeton.edu/~adame/papers/drevil/drevil.pdf).
  ```

---

