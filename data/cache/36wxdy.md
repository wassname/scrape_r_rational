## [D] Rational doesn't mean carebear

### Post:

I've noticed a bit of a trend in future societies.

In Three Worlds Collide, the future humans are basically little babies.

>"You want peace with the Babyeaters?"

>"Of course -" said Akon, then stopped short.

>The Lady Sensory looked around the table.  "And the Babyeater children?  What about them?"

>The Master of Fandom spoke, his voice uncertain.  "You can't impose human standards on -"

later:

>"The children don't die right away," said the Engineer.  "The brain is this nugget of hard crystal, that's really resistant to, um, the digestive mechanisms, much more so than the rest of the body.  So the child's brain is in, um, probably quite a lot of pain, since the whole body has been amputated, and in a state of sensory deprivation, and then the processing slowly gets degraded, and I think the whole process gets completed about a month after -"

>The Lady Sensory threw up.  A few seconds later, so did the Xenopsychologist and the Master.

Now, I'm aware that [TWC](#s "the confessor shows that actual humanity took precautions before handing their civilization to idiots.") Of course, the fact that the views of the author are not the views of the characters goes without saying.

[TWC](#s "This is tangential, but I'd argue that the civilization of TWC is like parents following their toddler children around the playground. This may be a normal interpretation but I haven't seen it in the comments")

There was a story posted here a while ago about the first human AI acausally bargaining with the first ever AI for protection and non-interference with all species that haven't developed an AI yet. As usual, this rested on a prisoner's dilemma situation which is resolved in the direction of cooperation. The fact that Dawn Hunters or Berzerkers are a valid outcome (an example situation: hmm, so I've acausally bargained and that resulted in protection. On the other hand, looking around I see that I have arisen quite early in the lifetime of the universe, so I have an excellent shot at being first. My creators did the calculations for time-until-universe-is-full and realized they'd need the whole thing, so I'll take steps to ensure that.) wasn't taken seriously.

I'm sure you can think of more societies like this.

Now, my main point: A society can be rational without being egalitarian or utopian. [TWC](#s "The superhappies are rational and big fans of xenocide, for example.") It'd be nice to see a little more variety.



### Comments:

- u/Sailor_Vulcan:
  ```
  What do you mean by "rational"? Also, what do you mean by "care bear" and "little babies"? The two parts that you quoted don't seem to be related to each other than the fact that they're talking about the baby-eaters. Although if by "little babies" you mean emotionally immature and overly sensitive, then what do you think of people who cry at funerals? Are they little babies too? Because what happened to the baby-eater children in TWC is worse than the holocaust, probably by at least an order of magnitude. In the story they commit genocide *against their own children* every year for the entire history of their species.

  If you meant something else by "rational", "little babies", and "Care Bears", you might want to elaborate what your arguments are more specifically and explain your reasoning. Just saying that the characters are little babies doesn't really tell us anything in particular about the characters, except that you think lowly of them.
  ```

- u/MugaSofer:
  ```
  I'm not sure why you think the future!humans are "babies". Because they feel empathy?

  Rationality is used to pursue your goals. In most humans, those goals are morality, so yeah, it's rational to try and build a reasonably utopian society.

  The Superhappies live in a Superhappy utopia, and the Babyeaters live in a Babyeater utopia. That neither of these bear anything but the vaguest resemblance to *human* utopias is *kind of the point of the story.*

  Similarly, Professor Quirrell is rational - indeed [HPMOR](#s "he's the original rationalist from which every other rationalist in the story derives"). But he doesn't strike me as a "carebear" by any stretch of the imagination. (Of course, to be fair, he's not really a society all by himself no matter how hard he tries.)

  In Eliezer's Brennan stories, the protagonists are all basically selfish, and their main goal is to gain power (albeit in a post-Singularity world of some kind.) *Signifiant Digits* is set in a post-HPMOR world where the protagonists rule most of the world, and it doesn't seem to be a utopia by any stretch. [The Girl Who Poked God With A Stick](http://squid314.livejournal.com/336195.html) is pretty much exactly what you're asking for.

  In fact, looking, the *only* examples that seem to fit are *Three Worlds Collide*, AlexanderWales' HPMOR epilogue, and MLP:FiO (which I haven't read, but going by descriptions.)
  ```

  - u/derefr:
    ```
    > I'm not sure why you think the future!humans are "babies". Because they feel empathy?

    Presumably because the average human living today doesn't feel much revulsion at, for example, the concept of animals eating other animals alive.

    Future humans would have to feel *way more* empathy than we currently do for *everyone* to have an impulse toward finding some sort of cooperative solution in prisoner's dilemmas with aliens.
    ```

    - u/MugaSofer:
      ```
      "Aliens" and "animals" occupy different moral categories in most people's minds, as any science-fiction fan knows. Learning about another tribe that is under a curse forcing them to eat their children each year would be more accurate.

      You're right about the Prisoner's Dilemma thing, but I think it's pretty clear in the story that most of the humans *don't* instinctively try to cooperate in the Prisoner's Dilemma with the aliens; only Akon does, because he's been trained as an Administrator and it's his job not to make stupid decisions. (And he's *right*, since the Superhappies would probably have shown up and doomed humanity if he'd defected.)
      ```

- u/None:
  ```
  Well, people are largely writing about what they consider rational *for human agents*, in fact, human agents who usually share a lot of acculturation with the author, so of course the author's views slip into the story.

  Of course, if you're asking what happens when the Affront *gets bloody clever*, the answer is that whole leaves of galactic space have to be sterilized to put a goddamn stop to what humans call "really terrible torture porn".

  Now, I wasn't really analyzing the sociology when I read *Three Worlds Collide* as much as I was admiring the trolling, but let me ask: what do you think "grown-ups" look like?  What would a civilization full of not-babies look like?

  Also, how old are you?  Just asking, because those of us actually in the "adult" age-group tend to find that, yes, 50% of the adult world around us, sometimes more, are total and utter babies.  It's one of those unpleasant awakenings to the facts of life.
  ```

- u/eaglejarl:
  ```
  Other people have addressed the empathy/rationality part of the discussion, so I'll take the AI part.  If this subreddit is seeing a confluence of rationality-means-nice among aliens and/or AIs, then it's probably because that's the unbroken ground.

  Until very recently, every depiction of aliens or AIs was negative -- Frankenstein, War of the Worlds, Dracula, Morlocks/Eloi, Berserkers, etc etc.  Part of the reason that Asimov's 'Robot' stories were (and are) so amazingly popular is that aside from being well written and enjoyable they were *different.*  They showed a world where robots could be allies instead of antagonists, and it was a new thought.

  Even today, the majority of AI stories give them antagonist roles.  Eagle Eye, Her (in the end), Transcendence (I think?  Didn't see it), the Terminator movies, the Matrix movies -- they all show AI as figures of fear.  MLP:FiO can be taken as uplifting or horror depending on your values.

  The worst of all, of course, is Jack Williamson's *With Folded Hands.*  That is the only book I've ever read where I physically shuddered after reading it; when I wrote [Baby Blues](http://www.amazon.com/Baby-Blues-David-K-Storrs-ebook/dp/B00V52XRIE) [paid link; just read the blurb] I used that as my model for how to achieve real horror.

  Again, if we're seeing a higher proportion of AI's / aliens as rational and at least somewhat friendly, it's because the other side has already been done to death.
  ```

- u/Farmerbob1:
  ```
  A rational society does not need to be utopian, I certainly agree with that.

  However, as long as you use the term egalitarian correctly, I have to disagree - for humans anyway!

  With exceptions made for children, the ill, injured or congenitally damaged, or the aged/infirm, who need help which might also include restricting some of their rights, everyone should have equal rights and opportunities.  Without this, you have a society that may devolve into a caste society, or slavery society.

  While a surface argument might be made for a rational society based on enforced social standing, I think it's fairly clear from the last few thousand years that such social arrangements are highly irrational, because the ones on the bottom tend to get upset and kill the ones at the top every now and then.  Courting that sort of social instability is just not rational, IMHO.
  ```

  - u/None:
    ```
    There's also the issue that actually-existing humans prefer living in predictable, high-trust social environments conducive to easy planning.  Slytherins are making Unusual Life Choices in real life, no matter how much fiction and certain forms of political hagiography glorify them.  Some level of egalitarianism is a necessary ingredient for that high-trust society: trust is almost always limited, and thus entails knowing that the power and incentive gradients between two people are not too steep.
    ```

---

