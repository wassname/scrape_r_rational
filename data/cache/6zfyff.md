## [D] Monday General Rationality Thread

### Post:

Welcome to the Monday thread on general rationality topics!  Do you really want to talk about something non-fictional, related to the real world?  Have you:

* Seen something interesting on /r/science?
* Found a new way to get your shit even-more together?
* Figured out how to become immortal?
* Constructed artificial general intelligence?
* Read a neat nonfiction book?
* Munchkined your way into total control of your D&D campaign?


### Comments:

- u/None:
  ```
  In the broad spirit of [here](https://www.reddit.com/r/slatestarcodex/comments/6z8grm/is_aphantasia_really_real_like_how_is_that_even/) and [here](http://slatestarcodex.com/2014/03/17/what-universal-human-experiences-are-you-missing-without-realizing-it/)...

  >I don't have a voice in my head, I have *four*, each of which speaks for a different part of me and who regularly talk to each-other socially.

  Now, that's a symptom of reading too much, and I know it.  I'm fairly sure I saw the study saying that people who read or write too much develop this and only this dissociative symptom.

  The universal human experience I think I'm missing though, is precisely dissociation.  I've never really stood back from my actions, even when they've been deeply irrational or under the influence of drugs or mental illness, and said, "That wasn't me."  I've never looked in the mirror and asked, "Is this really who I am?".  Drunk-me tries to help out sober-me because I'm the same person drunk as sober.

  This is more like my experience of the world, which is apparently so different from most people that it's worth noting as a character trait of *Granny Weatherwax*:

  >Most people, on waking up, accelerate through a quick panicky pre-consciousness check-up: who am I, where am I, who is he/she, good god, why am I cuddling a policeman's helmet, what happened last night?
  >
  >And this is because people are riddled by Doubt. It is the engine that drives them through their lives. It is the elastic band in the little model aeroplane of their soul, and they spend their time winding it up until it knots. Early morning is the worst time -there's that little moment of panic in case You have drifted away in the night and something else has moved in. This never happened to Granny Weatherwax. She went straight from asleep to instant operation on all six cylinders. *She never needed to find herself because she always knew who was doing the looking.*

  Emphasis mine.  I often wonder that I sort of fail to communicate what's going on in my life with others because I can't put my psychologically abnormal experiences into their frame of reference.

  So, uh, how *does* that work, to step back from your experiences and have some gap between them and "you"?
  ```

  - u/eternal-potato:
    ```
    > Now, that's a symptom of reading too much, and I know it. I'm fairly sure I saw the study saying that people who read or write too much develop this and only this dissociative symptom.

    How much reading is too much? Though I've hardly ever written anything, I estimate I read about 100k words of fiction a week on average, (ballpark, never actually cared to measure), but sometimes when I have nothing better to do I can read more in a single day. Yet I can't say I've ever experienced anything of the sort, let alone disassociation from it. I assume simulating fictional characters from behind the fourth wall doesn't count.

    > Early morning is the worst time -there's that little moment of panic in case You have drifted away in the night and something else has moved in.

    Is this _really_ how it is for most people? Sounds mildly terrifying.
    ```

    - u/callmesalticidae:
      ```
      > Is this really how it is for most people? Sounds mildly terrifying.

      This only happens when I have a really, really disorienting dream. Is that how it is for most people, and Pterry is exaggerating its frequency for effect, or does it happen more often for most people?
      ```

      - u/eternal-potato:
        ```
        Hm. For me dreams are like, well, really getting into a book or a movie. While I am engrossed in reading (dreaming) my internal narrative is kind of suspended and replaced by the narrative of the book (dream), but if I am interrupted by somebody (woken up) my narrative immediately resumes, with no disorientation or any need to remember who or where I am. 

        (I call my stream of consciousness 'narrative', but it is not really linguistic in the same way the book one is. I don't constantly describe what is happening, I just am.)
        ```

    - u/None:
      ```
      > How much reading is too much?

      Probably somewhere around the point where you've tried to step into the heads of people who do have mental conversations.  Nita Callahan and God-Emperor Shinji Ikari are probably the ones that made me start doing that.

      >I assume simulating fictional characters from behind the fourth wall doesn't count.

      Well my whole thing was that this seems to be a symptom of imagining other people a little too much, rather than an *actual* dissociative symptom.

      >Is this really how it is for most people? Sounds mildly terrifying.

      I know, right?  But on the other hand, I find most people's discourse about personal identity completely, utterly baffling.  Like, they *have a self-concept* apart from whatever they happen to be at the time.  I rarely develop one, and when I do, it's wildly divergent from reality, so it dissolves.

      Maybe if you had one of those, you'd have to ask yourself where it is when you wake up, like remembering anything else you know?
      ```

    - u/gbear605:
      ```
      > 100k words of fiction a week on average

      I read similar amounts, perhaps even twice or thrice that. I have also never had that kind of symptom.

      I occasionally will have moments where I look in the mirror and question myself being myself - whether the person I perceive myself as being is different from the person that others perceive me as being - but aside from that I have had none of the symptoms of dissociation that /u/eaturbrainz is talking about.
      ```

    - u/Tinfoil_Haberdashery:
      ```
      I've become so...empathetic, I suppose, to other people's issues that I've forgotten I'm not them. I've literally tried to get off at other people's bus stops after having a conversation with them where they described doing so. Weird feeling of jolting back to being me.
      ```

  - u/blazinghand:
    ```
    When I wake up, I'm completely aware of who I am and (unless I wandered off somewhere last night and forget) where I am. I have one voice in my head, and it is mine, though I can use it to "read aloud" other voices and so on. I can narrate other voices as well. I read a very large amount of fiction and a good amount of nonfiction as well and do not have multiple internal voices. It's possible I'm just less imaginative or something though.
    ```

  - u/traverseda:
    ```
    A close friend has jokingly referred to me as a "a spider hovering over a p-zombie".

    Do you have an internal monologue, when you're not specifically trying to format your thoughts for communication? Or I suppose when you're not reading. Can you visualize things?

    ---

    I suspect, in my case at least, that lack of disassociation is partly due to me not really making use of those internal feedback mechanisms. Serializing thought to text/speech is *slow*, and annoying, and I mostly don't do it except when I'm communicating with people.

    ---

    Reading through your links after posting this, and yeah, that.
    ```

    - u/None:
      ```
      > Do you have an internal monologue, when you're not specifically trying to format your thoughts for communication?

      Yes, several.

      >Can you visualize things?

      Usually I can visualize things in glimpses, or I can recall specific visual memories.  I can't make a continuous movie in my head the way I can recall sound continuously.

      EDIT: Actually, seems I can, it just takes more effort.
      ```

  - u/tomtan:
    ```
    I sometimes have a form of dissociation where I'm looking at the actions I'm doing like if the person performing them were someone else, a stranger that I find baffling and that I'm disappointed with. I'm an outside observer who tries vainly to influence my own actions. This usually correlates with periods of intense depression where I have to use a lot of will power to force that stranger to do anything.

    The feeling is a bit similar to Camus l'Ã©tranger or at least what I remember of it (I read it 20 years ago) and that feeling is relatively recent (probably triggered by my divorce), it also rarely last for more than 4-5 days at a time.

    When those period happen, I usually end up drowning myself in fiction, as a way to escape from reality and from this feeling, waiting for it to pass. But, yes, I wouldn't qualify this as being normal, nor would I wish it on anyone.
    ```

- u/LieGroupE8:
  ```
  Edit: See [my reply to ShiranaiWakaranai below](https://www.reddit.com/r/rational/comments/6zfyff/d_monday_general_rationality_thread/dmwfq4w/?st=j7hnugxd&sh=e01db591) for an overview of my endgame here...

  ---

  A couple of weeks ago, I made a post here about Nassim Taleb, which did not accomplish what I had hoped it would. I still want to have that discussion with members of the rationalist community, but I'm not sure of the best place to go for that (this is the only rationalist forum that I am active on, at the moment, though it may not be the best place to get a full technical discussion going).

  Anyway, Taleb has an interesting perspective on rationality that I would like people's thoughts about. I won't try to put words in his mouth like last time. Instead, the following two articles are good summaries of his position:

  [How to be Rational About Rationality](https://medium.com/incerto/how-to-be-rational-about-rationality-432e96dd4d1a)

  [The Logic of Risk-Taking](https://medium.com/incerto/the-logic-of-risk-taking-107bf41029d3)

  I'll just add that when it comes to Taleb, *I notice that I am confused.* Some of his views seem antithetical to everything the rationalist community stands for, and yet I see lots of indicators that Taleb is an extremely strong rationalist himself (though he would never call himself that), strong enough that it is reasonable to trust most of his conclusions. He is like the Eliezer Yudkowsky of quantitative finance - hated or ignored by academia, yet someone who has built up an entire philosophical worldview based on probability theory.
  ```

  - u/gbear605:
    ```
    Having read the two articles, I do not see anything that is antithetical to the rationalist community. I'd guess that you're thinking of claims like how Taleb does not think that science is useful for a lot of real-world problems. By his definition of science, I think Yudkowsky would agree. From what I can tell, Taleb's science is a specific subset of activities - academic science. Yudkowsky's science is "the ... kind of thought that lets us survive in everyday life." \[1] Science to Yudkowsky is figuring out that the red berries are dangerous and that if you put a dead fish by your corn seeds, the corn will grow better. Taleb's science, however, is only the search for absolute truth.

    This sentence \[2] by Taleb sounds like something Yudkowsky could have said in fact. Taleb speaks about how you need to focus on the instrumental value of activity, Yudkowsky's rationalism is about doing whatever achieves your goal ("winning")

    \[1]: http://yudkowsky.net/obsolete/tmol-faq.html#theo_conflict
    (An old page, but I believe that Yudkowsky would agree with this part of it)

    \[2]: https://medium.com/incerto/how-to-be-rational-about-rationality-432e96dd4d1a 
    "Your eyes are not sensors aimed at getting the electromagnetic spectrum of reality. Their job description is not to produce the most accurate scientific representation of reality; rather the most useful one for survival."
    ```

    - u/LieGroupE8:
      ```
      The antithetical part is that "beliefs" have nothing to do with rationality, for Taleb. There is no such thing as epistemic rationality, only rationality of decisions. So Taleb finds religion perfectly agreeable if it causes people to not die. Most "rationalists" despise religion, in my experience.
      ```

      - u/gbear605:
        ```
        I'd guess that this stems for Yudkowsky and most rationalists valuing truth for the sake of truth while Taleb does not. That's entirely a statement about personal preference, they just have different personal preferences.

        I doubt that Taleb would claim that epistemic rationality does not help with finding the truth, instead he would claim that it is useless because finding the truth is useless unless it has some other benefit to him, in which case it is part of his rationality of decisions.
        ```

  - u/ShiranaiWakaranai:
    ```
    Also, the more I read about Taleb's views, the more worried I become. His views are not irrational. They are quite logical, and the actions he advocate truly are the best ways to achieve his goals.

    The problem is his goals seem extremely susceptible to evil. 

    In "How to be Rational About Rationality", he states that his goals are about survival. Survival of the individual or the collective. And that any action taken that goes against survival is irrational.

    Does he not see the potential for evil here? There are plenty of ways to improve your own odds of survival by hurting others. Stealing their stuff, murdering the competition, turning people into slaves, etc. Similarly, there are plenty of ways to improve the odds of survival for the collective by hurting individuals: rapes to increase birth rates, dictatorships and blind obedience so decisions can be made quickly, culling the old and weak so they don't drag down the species, etc. etc.

    Now, last time, I was told that Taleb's philosophy has an exception: Follow the philosophy unless what it tells you to do infringes on ethics.

    But this doesn't even work because Taleb's philosophy promotes willful ignorance. It tells you to perform actions even if you don't know the reasoning behind them, so long as other people are also doing said actions. For all you know, these actions could be committing major ethics violations without your knowledge. Yet you aren't allowed to wait and investigate whether your traditions are evil before obeying. You have to obey them **now**, because to do otherwise is to risk the survival of the collective.

    It's really terrifying.
    ```

    - u/LieGroupE8:
      ```
      I'm going to respond to all your posts here, in one place. Just to tie things together, I'll tag the other people who responded to me (thanks): /u/eaturbrainz /u/696e6372656469626c65 /u/gbear605

      So here's my secret, ulterior motive for bringing up Taleb over and over: Taleb has intellectual tools that I covet for the rationalist community. We may not agree with everything he says and does, we may have different goals than he does, but if there are useful analytical tools that we could be using but aren't, we should greedily pluck them from wherever we can find them.

      Logic and Bayes' theorem are great and all, but as Taleb would point out, the formal structures navigated by those tools are not sufficient for a certain class of problems, namely, the problem of reasoning about complex systems. Of course, logic *constructs* the tools needed, because it constructs all of mathematics - but the direct application of modus ponens might not work out so well. Statements of the form "If A then B" for human-recognizable categories A and B will typically be useless, because by the nature of complexity, we can't get enough bits of shannon information about such propositions for them to be practically useful. Moreover, sometimes when it *seems* like this sort of reasoning is trustworthy, it isn't.

      For example, here's a mistake of reasoning that a starry-eyed young utilitarian might fall into:

      1) If something is bad, then we should stop it from happening as much as possible

      2) Wildfires are bad because they destroy property and hurt people and animals

      3) Therefore, we should stop as many wildfires as possible

      You might be thinking, "What's wrong with that?" But consider this: preventing small wildfires creates a buildup of dry brush and greatly increases the chance later on of a massive, even-worse wildfire. Thus it is better to accept the damages of small wildfires right away to prevent things from being worse in the long-term.

      More generally, Taleb argues: many people make the mistake of trading short-term bounded risks for long-term existential risks. Quite often, preventing short-term disasters just sweeps problems under the rug until they all collapse at once. For example, bailing out big banks instead of letting them fail just maintains the status quo and ensures that there will be another market crash from corrupt practices. Polluting the atmosphere to generate electricity in the short-term has long-term environmental consequences. Using plasmid insertion to create super-crops that solve hunger in the short term could lead to an ecological disaster in the long term (hence the GMO issue from last time).

      Talebs says: "Hey you guys. Stop naively applying modus ponens and bell curves to complex systems. Instead, here's a bunch of mathematical tools that work better: fractal geometry, renormalization, dynamic time-series analysis, nonlinear differential equations, fat-tailed analysis, convex exposure analysis, ergodic markov chains with absorbing states. It's a lot of math, I know, but you don't need to do math to do well, just listen to the wisdom of the ancients; practices that have survived since ancient times probably don't have existential risk. If you want to go against the wisdom of the ancients, then you'd better be damn careful how you do it, and in that case you'd better have a good grasp on the math."

      Regarding survivability: it's not that surviving is Taleb's terminal goal so much as it's a prerequisite for all goals. If you don't survive, you can't do the utilitarian goal-maximization that you want to do. Therefore, maximizing your long-term survival chances should always be your first worry. You can never eliminate all risk, but you can choose which kind of risk you want to deal with. Fat-tailed risk (like non-value-aligned artificial intelligence!) virtually guarantees that everyone will die, it's just a matter of when. Thin-tailed risk (like specialized or friendly AI) is survivable long term.

      So that's Taleb's general position, and I think a lot can be learned from it. That's why I recommend reading his books even if you don't agree with him. In the places where he is wrong, he is wrong in an *interesting and non-obvious way*.

      P.S. I feel like these ideas will not have their maximum impact here on a weekly /r/rational thread. Suggestions of where to put them instead are welcome. An overview of these things would make a great State Star Codex article, for example, if Scott Alexander decided to investigate. This is why I wanted Eliezer Yudkowsky to weigh in last time. Part of my confusion is *why isn't the rationalist community talking about these important issues and techniques? Does the community have good reasons for disagreement, or are they just unaware?*
      ```

      - u/ShiranaiWakaranai:
        ```
        >More generally, Taleb argues: many people make the mistake of trading short-term bounded risks for long-term existential risks. Quite often, preventing short-term disasters just sweeps problems under the rug until they all collapse at once. For example, bailing out big banks instead of letting them fail just maintains the status quo and ensures that there will be another market crash from corrupt practices. Polluting the atmosphere to generate electricity in the short-term has long-term environmental consequences. Using plasmid insertion to create super-crops that solve hunger in the short term could lead to an ecological disaster in the long term (hence the GMO issue from last time).

        But this mistake is what his philosophy also does. A lot of what he advocates is about keeping the status quo even if you don't know why. Going against the status quo is a short-term risk that he says you shouldn't take, even though keeping the status quo in the long term may be devastating. 

        The only way to prevent things from being worse in the long term is to actually think. Investigate. Analyze.

        Willful ignorance and blind obedience like Taleb advocates are recipes for long term disasters with short term gains.

        > just listen to the wisdom of the ancients; practices that have survived since ancient times probably don't have existential risk.

        I have discussed the perils of natural selection last time. Just because something is done a lot, doesn't mean it's safer. There are plenty of historical examples of natural selection leading to everyone dying. The very principles of natural selection advocate trading long-term advantages for short-term gains: half your lifespan in exchange for ten times the offspring now, create poison in your bodies which will eventually kill you in exchange for not being eaten by predators now, poison the environment in exchange for some boost to yourself now, etc. etc.

        I also find it very inconsistent that Taleb is anti-pollution, anti-fossil fuels. Burning coal and gas is just burning stuff on a larger scale, and burning stuff is literally one of the most ancient human traditions. People have been burning stuff since they were cavemen, despite all the environmental risks, because fire = energy. Whoever burns stuff gains a short-term advantage of light and heat. Even though plenty of towns and nomadic groups have probably burned themselves to death in accidental fires, and groups of cavemen have probably suffocated themselves to death in caves from all their fires sucking all the oxygen, the practice of burning continues because natural selection only cares about the short-term gains. This ancient tradition of burning stuff for short-term gains is exactly why we are paying the price today with global warming, and precisely why I keep advocating against "monkey see monkey do". 

        Don't just blindly copy, THINK.

        > Regarding survivability: it's not that surviving is Taleb's terminal goal so much as it's a prerequisite for all goals. If you don't survive, you can't do the utilitarian goal-maximization that you want to do. Therefore, maximizing your long-term survival chances should always be your first worry.

        This sounds dangerously like Knight Templar logic: **I AM THE FORCE OF GOOD. ALL WHO OPPOSE ME ARE THUS EVIL. ONLY I CAN SAVE THE WORLD, SO ONLY I MATTER!**

        Only making me more worried here (x.x)...
        ```

  - u/696e6372656469626c65:
    ```
    It seems to me that Taleb applies the same methods of reasoning used by rationalists, but he starts from a different set of assumptions. This doesn't seem particularly confusing to me, unless your confusion lies in why he chooses those assumptions as opposed to others (in which case he would probably reply "empirical evidence").
    ```

- u/Dwood15:
  ```
  Just went hunting with family in northern Canada, right in the Yukon for 2 weeks.

  We left tuesday 2 weeks ago, made it to canada, got off at whitehorse, then drove for 6 hours to watson lake, then took a bush plane into the middle of no where. The fourth day of the trip we hiked for 9 hours one way and posted camp. That night, i was walking like an old man. The next day, we hiked for six hours up a mountain. On the mountain was our quarry. Two mountain goats.

  Typing on a phone is obnoxious, more on Friday. I'm at Vancouver airpt right now, waiting for my plane to seattle.
  ```

- u/None:
  ```
  Just finished my recent project, an ebook on instrumental rationality!

  Link is [here](https://mindlevelup.wordpress.com/).

  It's got stuff on planning, habits, and some assorted heuristic-y stuff.
  ```

- u/gabbalis:
  ```
  Turns out, if you look really closely you can see stuff.

  Turns out, human social interaction is a beautiful dance with a dynamic flow.

  Turns out, you can enter a meditative state of artistic appreciation if you focus just so.

  Turns out, the most important part of charisma is putting every last ounce of your focus into reading the conversation.

  Turns out, getting your ears pierced is pretty sweet.

  Turns out, with the right nootropic stack you can see into your own malladaptive mental processes and rewrite bits and pieces.

  Turns out, you can approach that with just the right methods of meditative focus.

  Turns out, sex isn't actually about nerve endings.

  Turns out, multiplying 3 digit numbers together in your head is quite fun.

  Turns out, the sequences are pretty decent, I probably should've read them earlier.

  Turns out, what the FDA ain't made a ruling on, is pretty easy to buy online.

  Turns out, your greatest enemy is usually yourself, hiding yourself from yourself.

  Turns out, you can be whatever you want to be, if you can get the relevant hormones.

  Turns out, the best of the best can gaze into the souls of men and see what they can hardly see themselves.

  Turns out, humanity is insane.

  Turns out, I want to be an angel when I grow up.

  Turns out, the ballad of ancient earth is a grim one.

  Turns out, we fight to save the world.

  Turns out, you can be good if you try.

  Turns out, a bachelors degree in CS and 5 years relevant work experience in a testing lab your college owns can sometimes be enough to net you a 6 figure income out of college.

  Turns out, that only just allows you to break even when renting a 2 bedroom in Berkeley.

  Turns out, tulpas are a pretty neat mental tool.

  Turns out, the related discipline of hypnosis is too.

  Turns out, turning off a mental process for a while can sometimes be as useful and enlightening as building a new one.

  Turns out, you can churn credit cards for free air miles if you're careful.

  Turns out, You can't always generalize among minds.

  Turns out, you're all beautiful on the inside, but to see that you have to *see inside*.

  Turns out, the attrition rate is over 7,000,000,000 per century.

  Turns out, no mind deserves to have to suffer through this shit.

  Turns Out.
  ```

  - u/DaystarEld:
    ```
    Turns out, I enjoyed this :)
    ```

---

