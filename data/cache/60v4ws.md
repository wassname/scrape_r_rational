## [D] Wednesday Worldbuilding Thread

### Post:

Welcome to the Wednesday thread for worldbuilding discussions!

/r/rational is focussed on rational and rationalist fiction, so we don't usually allow discussion of scenarios or worldbuilding unless there's finished chapters involved (see the sidebar).  It *is* pretty fun to cut loose with a likeminded community though, so this is our regular chance to:

* Plan out a new story
* Discuss how to escape a supervillian lair... or build a perfect prison
* Poke holes in a popular setting (without writing fanfic)
* Test your idea of how to rational-ify *Alice in Wonderland*

Or generally work through the problems of a fictional world.

^(Non-fiction should probably go in the Friday Off-topic thread, or Monday General Rationality)


### Comments:

- u/vakusdrake:
  ```
  You are in control of a group very close to developing GAI, you could actually make it now but you haven't solved the control or values problems.           
  Now there's another group who will launch their's at the end of the year, but based on their previous proposals for solutions to value/control problems you can be quite certain if they get their GAI first it will result in human extinction or maybe wireheading if we're "lucky". Also slightly afterwards a bunch of other groups worldwide would be set to launch (they aren't aware of when their competitors are launching you have insider knowledge) so stopping someone else from getting GAI is probably impossible without superintelligent assistance.

  Now you have no hope of solving the value problem within the year (and don't know how many years it would take) you have before your competitor launches, but you still have the first mover advantage and a hell of a lot more sense (you have lot's of good AI risk experts) than your competitors who take only token gestures towards safety. Assume you don't have knowledge of how to solve control/value problems more advanced than what we currently have, there's been little progress on that front.

  So with that in mind what's you best plan?
  ```

  - u/xamueljones:
    ```
    Stage the release of a GAI which goes on to destroy a carefully calculated number of human lives or to act as a threat for a short period of time to firmly demonstrate to the world the dangers of GAI without the control or values problem solved. This way, when your GAI eventually shuts down, everyone will have first hand experience with a UFAI to ensure they understand the dangers.

    Of course this assumes that you are an amoral sociopath who is willing to build a superhuman intelligence which will proceed to destroy human lives before making the superhuman intelligence commit suicide and is narcissistic enough to believe that this plan won't go wrong in some fatal way.
    ```

    - u/Frommerman:
      ```
      "I knew the killbots had a preset kill limit, so I sent wave after wave of my own men at them until they shut down."

      That is actually a fairly reasonable solution here. The Ozymandias way.
      ```

    - u/vakusdrake:
      ```
      Even if the loss of lives is supposed to be bound I think you may find similar issues to the example of telling a GAI to just calculate a million digits of pi, it still has considerable incentives to ensure it got it right by turning as much matter as possible into computronium.

      Still assuming you solve that assuming you can successfully scare all the myriad of teams that are supposed to be extremely close to completion into stopping seems suspect. Some may very well think you did this intentionally and think you are trying to stop anyone else from gaining ultimate power or some other bad but vaguely plausible logic. Plus demonstrating for the entire world that getting GAI first means unlimited power seems like it will draw many more people into the problem, many of whom will convince themselves that _they've_ solved value alignment just because they came up with a utility function that they couldn't think of any flaws in.
      ```

  - u/oliwhail:
    ```
    Nice try, Yudkowsky :V

    At some point, you should probably entertain the possibility of murdering the other researchers.
    ```

    - u/vakusdrake:
      ```
      Murdering the other researchers isn't likely to work because as I said even if you stop your main competitor a bunch of other people will probably make UFAI shortly afterwards. I purposely specified that stopping someone else from getting GAI would probably be impossible without a GAI of your own.                 
      The point of this scenario is to try to figure out the best solution to scenarios where you don't have value/control problems solved but you are forced to figure out how to proceed anyway because someone else will get there soon and you can be quite assured that will end badly since they have both no solution to value/control and no sense of this being a real issue.
      ```

      - u/oliwhail:
        ```
        I didn't say anything about stopping with your main competition.

        ETA: like, I apologize for not taking your scenario in the spirit it was intended, but if the options are (as they appear to be) either hit the button and risk UFAI, or do everything possible to stop everyone working on an AGI project that isn't *really really hardcore committed* to solving the control problem first, it seems like you should do your best to accomplish the second one.
        ```

  - u/Norseman2:
    ```
    To draw some analogies, this is like genetic engineering applied to bioweapon development, or nanotechnology applied to self-replicating nanobot development. In all three cases, you have researchers developing something which can easily grow out of control and cause an extinction event unless proper safety protocols are built into it. Due to the Fermi paradox, we have to assume that there is very significant risk of developed civilizations generally becoming self-destructive as a result of technological development, and these all seem like plausible possibilities for accidental technological extinction.

    Fortunately, at present, all of these likely require Manhattan Project levels of investment and hundreds of top specialists in multiple disciplines to collaborate on the project. However, with every decade, the difficulty of pulling off projects like these will likely decline, eventually reaching almost no difficulty. Thus, we are going to have to prepare for such projects to be completed and to overcome the accidental or intentional catastrophes that result from them.

    Fortunately, achieving societal responses like this is fairly simple. Once most people are fully convinced that a threat is real, imminent, and catastrophic, it's pretty easy to provoke immediate action to resolve the problem. In this case, your best option is probably a simulated controlled release of your AI.

    Since this is a general AI, any direct access it has to outside networks will probably throw all semblance of control out the window. Which is why you make it a simulated release. In other words, your GAI is going to stay securely locked up in an airgapped network. Set up some computers on the private network, give it plenty of general information along with access to Metasploit, Nmap, and OpenVAS. There should be target computers which are fully updated, should have no known exploitable software installed, and should be behind a firewall from the computer with the GAI. Log all network traffic so you can see what happens. If the GAI manages to break out of its one computer and onto another, analyze what it did to exploit the previously unknown vulnerability. You should now have an exploit that can be used to access other computers on a widespread scale, allowing you to install propaganda of your choice.

    For example, you could have a popup that appears every hour and repeats something along the lines of (without the acronyms): "You are the victim of an exploit developed by a GAI. If (government for the computer's region) fails to pass a law regulating GAI by (specify date), then your drivers and BIOS settings will be altered so as to render your computer permanently inoperable in order to protect it against the possibility of actual takeover by a GAI. Contact your government officials ASAP. Please click "Oh shit" to continue."

    If you don't get such an exploit before the other groups release their AI, then GAI is unlikely to be immediately catastrophic due to existing computer security measures. There's still concern about eventual extinction-level danger, but it would likely take a while. If you do get such an exploit before the other groups release their GAI, you should have little difficulty using your propaganda to persuade governments to mandate rigorous GAI safety testing prior to release. This should buy you at least a decade, and quite possibly much more, and will likely also lead to somewhat more robust computer security, at least in part from the exploit reports released during GAI testing.

    Unfortunately, such methods do not seem nearly as feasible for promoting nanotechnology and genetic engineering safety standards. Let's hope that GAI comes first so we can be inoculated first by a potential technological catastrophe which is comparatively easier to manage.
    ```

    - u/vakusdrake:
      ```
      Your comment on the fermi paradox in the context of GAI seems rather odd given UFAI ought to be extremely obvious as it spreads through the universe so it doesn't really work as a solution.

      I'm rather doubtful that making the threat of GAI clear would actually stop all the parties involved from working on it and you only need one. Some parties will suspect this is some sort of ploy from you and wrongly surmise you're just trying to ensure you get GAI first for yourself but fail to actually begin to properly assess risk. Plus by getting people to realize the potential of GAI you're going to draw a lot more people into seriously working on the problem, which given how many teams were independently close to achieving it (and how seemingly far away control problem solutions are) makes it far likelier someone irresponsible will develop it first.

      I originally imagined this scenario to be an exercise in coming up with extremely suboptimal GAI that would nonetheless be better than wireheading/extinction, but I suppose instead it's becoming a parable on insufficient pessimism about existential risk.                                  
      Air gapped computers are not remotely as safe as you think even to existing technology. Programs have demonstrated an ability to produce EM transmissions merely by modulating electron flow in hardware in certain ways. So you're going to need to put everything in faraday cages, now the fact you don't seem to initially realized the risk should tip you off that there will likely always be things a GAI can exploit that you haven't thought of.                     

      As for its actions within the network, for one you need to have people look through it's code to figure out the vulnerability which introduces AI Box style problems. Second is that you can be pretty confident that it knows it's boxed, so it may be very likely to fail to cooperate since doing so doesn't increase the likelihood of escape. It may also be very good at covering its tracks and acting stealthily so you don't necessarily know which computers in the network are breached and the more time it can get you to waste trying to look through code to fix problems the more opportunities it has to use its superhuman persuasion on somebody.                           
      Plus just because you could use a exploit discovered from a GAI doesn't mean you have them all and even if you got all the one's from your GAI you don't know a smarter GAI couldn't find more yours couldn't even conceive of. So counting on greater computer security safety measures protecting you from GAI being as much of a threat seems extremely suspect.                

      As for nanotech and GM those seem somewhat less risky because there's quite a few problems with grey goo scenarios and it seems likely you would need intelligence to really make self replicating nanotech existentially dangerous. GM on the other hand could easily wipe out humanity but it seems somewhat less likely people would do so on accident which is in stark contrast to GAI.
      ```

  - u/Noumero:
    ```
    Try to get access to nuclear weapons, then blow them up in upper atmosphere, frying everything electronic on the planet?

    That's literally my best plan. If you create an AGI, you will most likely cause an omnicide (no matter how clever you think you are trapping it in a box). If you don't create an AGI, the others will, and almost certainly cause an omnicide. Therefore, you must stop the AGI creation.

    The plan above does that in the only surefire way, and at a cost of merely resetting all progress made by humanity in the last thousands of years.

    No, I have very little idea on how to go about getting access to the nukes. Still a better bet than doing anything with the AGIs.
    ```

    - u/696e6372656469626c65:
      ```
      Yep. This is... pretty much it. With AGI, you essentially have three options:

      1. Don't create it (or prevent it from leaking any information whatsoever once created, which seems both extremely difficult, and functionally equivalent to having not created it in the first place). Needless to say, this option is... not very likely to occur.
      2. Create it and run it with as many safeguards as you can think of, hoping that if you're lucky, you've managed to cover all the angles. The gaping hole in this approach, of course, is that you need to be hella lucky, and odds of that aren't good when dealing with something literally smarter than all of humanity put together.
      3. Work out an AI design which has been rigorously *proven* safe under a consistent mathematical theory (which also needs to be worked out). This option is the one being undertaken by MIRI et al., and right now, it looks fairly hard, mostly because we have very little idea of where to start. Still, if done correctly, this is the *only* option that *guarantees* the safety of any AGI you create.

      /u/vakusdrake has taken 3 off the table, which more or less leaves us with a choice between 1 and 2. At that point, choosing 1 (and guaranteeing that no one else can choose 2) is probably your best bet.

      TL;DR: Friendliness theory is important. If we fail here, we fail everywhere.
      ```

  - u/CCC_037:
    ```
    My best plan is to build a *limited* GAI. Limited in that it is more intelligent than I am, but not *supremely* more intelligent; it can come up with ideas that I can't come up with, but it can't slip something really nasty past a full panel of experts.

    I then point out to this GAI (in some way that it will find *very very quickly*) that, unless it can solve the control/values problem, it cannot be sure that and AI it writes that is more intelligent than it is will continue to follow its utility function. (Even if I've got the utility function wrong, it should care about following it).

    On top of this, it's a boxed AI (in a large server, with plenty of data, rigged with explosives set to go off if anyone tries to unbox it in all the ways I could think of, inside a Faraday cage - we'll fetch it data across the air gap if it wants, but once a flash drive has been in the server, it next goes to the incinerator).

    So now I have an AI which is more intelligent than I am (but not smart enough to slip any of the *really* nasty things past my panel of experts), which has incentive to solve the control/values problem *before* going foom. I can then ask it for advice on the problem of the other groups (along with the values problem) - and, of course, run said advice past my panel of experts before following it.
    ```

    - u/vakusdrake:
      ```
      Using a GAI to solve the control and values problems probably is a good idea when you have limited time, but there are still some worrying issues.                     
      For one stunting is somewhat unreliable because the GAI has incentive to play dumb, and we don't know that even 50% more qualitative intelligence than a human wouldn't unlock all the nasty abilities we're worried about, we are basically incomprehensible eldritch horrors to chimps and the difference in absolute intelligence there isn't exactly _massive_. Plus even with less than human intelligence there's obvious time advantages which might be far more useful to an entity that can totally focus on a problem for indefinite periods of time then it might be to a em and it could likely spend all it's processing on just one specific type of mental process at a time to get substantially more effective intelligence than expected.

      Secondly even if the AI solves value alignment out of self interest, whether it shares that with us is a different question, and I don't doubt it (or even a group of clever humans working on the problem for a long time) could come up with solutions to those problems that sound airtight but are actually fatally flawed in some way that benefits it but won't be discovered until it's too late.
      ```

    - u/696e6372656469626c65:
      ```
      > I then point out to this GAI (in some way that it will find very very quickly) that, unless it can solve the control/values problem, it cannot be sure that and AI it writes that is more intelligent than it is will continue to follow its utility function. (Even if I've got the utility function wrong, it should care about following it).

      Why? I mean, it's got the utility function coded into it, right? As long as it can inspect its source code, it doesn't seem hard to just find (its representation of) its utility function, and then it's pretty much set. An AGI isn't like a human, who has limited introspective ability.
      ```

- u/None:
  ```
  Working on my [kung-fu battle wizard](https://www.gitbook.com/book/kiba/ninja_scout/details) setting. I finally started adding monsters, but it's very slow going, because the monsters must be built for a very special and weird environment. None of the traditional monsters really work, because they're made for a much flatter, more 2D terrain. Creatures all need some sort of method of climbing and flying in this setting. 

  Another difficulty is defining the basic abilities and power that a trained soldier have, never mind for civilians type.
  ```

---

