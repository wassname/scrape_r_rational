## The Revenant Problem - A Philosophical Doom 2 Map

### Post:

[Link to content](https://www.youtube.com/watch?v=MiEYCXPI-qY)

### Comments:

- u/ArgentStonecutter:
  ```
  Can I have the option of tying down everyone who thinks the Trolley Problem is interesting on one of the tracks, so we never have to deal with this again?
  ```

  - u/FaceDeer:
    ```
    [My favorite version of the Trolley Problem](http://existentialcomics.com/comic/106).
    ```

    - u/MultipartiteMind:
      ```
      The '[Veil of Ignorance](https://en.wikipedia.org/wiki/Veil_of_ignorance)' concept mentioned sounds curious.

      If you poll people when they have established social positions, then even if you temporarily shroud their self-knowledge, they will be left with the self-serving prejudices (for their own group and against the other groups) that are inextricably(?) tied up in their understanding of social systems.

      If you erase enough of their understanding of social systems such that they don't have prejudices, or likewise poll them (as high-fluid-intelligence low-crystallised-intelligence minds as though created-from-scratch AIs, rather than nearly-thought-incapable newborns of embryos) before they've lived/studied/learned and gathered knowledge/understanding about social systems, what capability do they have to make meaningful choices between options presented to them?

      This seems to suggest a need for a means to either break through (and destroy) already-formed prejudices, to prevent the formation of prejudices while learning, or ideally both.  After that, a veil of ignorance would be helpful in preventing people from deliberately making immoral choices for personal benefit, in contrast with making thought-to-be moral choices which are for personal benefit.

      (An example that comes most quickly to mind is an actually-rich person who feels that financial support make poor people lazy and unproductive, or an actually-poor person who feels that keeping any money over certain thresholds would make rich people lazy and unproductive...  though I'm leery of mentioning any examples at all, both due to (the risk of prompting an irrelevant digression) and (my own acknowledged unfamiliarity with the issues actually involved in any such example).)

      ...Hmm, that is to say, I'm questioning how many self-serving votes are people deliberately making unjust choices (addressable by a veil of ignorance) and how many are them making sincere choices from the perspective of a biased worldview.  Though it does sound as though a veil of ignorance would only help, rather than harm (other than the specific instances of 'harm' that the voter afterwards realises they themself agreed to).

      ...beyond that, the concept of an analysis method that everyone can get behind...  or, if not utility-function-equalising, at least accurate models of outcomes...  though then we have the interesting issue of if, say, high-income people tended to end up deontological ('to watch as someone starves is tragic, to steal to prevent starvation is sin') and low-income people utilitarian ('a stolen loaf of bread to prevent starvation better than an extra loaf of bread held by one who has plenty').  Without people's ethical systems themselves being changeable, can people reach policy consensus?  What new information(?), distinct from self-interest, could convince people with disparate ethical systems to unify them?

      [Something that I sometimes ruminate on.](https://smbc-comics.com/comic/evil-ethics)  ...Come to think of it, I think I've been using the term 'utility function' even when contrasting utilitarianism with other ethical systems, so I think I need a better phrase.  Particularly, is there something one can call goal-directed-behaviour within an (arbitrary(/arbitrarily-designatable)) ethical framework?
      ```

      - u/crystal-pathway:
        ```
        The Veil of Ignorance is rather straightforward in the trolley problem.  You don't know whether you're pulling the switch, or potentially on the track.   The idea is that this can counter those biases you mentioned by forcing you to put yourself in the shoes of everyone whom a decision effects.
        ```

  - u/Menolith:
    ```
    On the condition that you can let them argue which side they should be tied on.
    ```

    - u/ArgentStonecutter:
      ```
      That would be cruel. In a hilariously funny "never go up against a Sicilian when death is on the line" sort of way.

      "Ah, but you're from Australia, and all Australians are criminals, so clearly I can't choose to be tied down on the trolley side, because a criminal would never flip the switch..."
      ```

  - u/sparr:
    ```
    Can you give some insight into why you think the trolley problem is uninteresting?
    ```

    - u/ArgentStonecutter:
      ```
      There's an interesting minor philosophical thought experiment in it, but it has been so overdone that people are jamming it in places where it has absolutely no applicability, to the point that it has basically no pedagogical use left.

      The only people who still care about it are the people who are so defensive about philosophy they're creating a veil of ignorance where you pretend you don't know if you've heard of the trolley problem before or not, and the people who are so stupid they think a Tesla should have a literal "howmanypeoplewillIkill()" function in it.
      ```

      - u/sparr:
        ```
        Why not a `howmanypeoplewillIkill()` function? Are you suggesting a smarter system that differentiates between maiming and killing and other types of injury? Maybe `howmanypeoplewillIkill()` returns a float, in death-equivalent-units?
        ```

        - u/ArgentStonecutter:
          ```
          > Why not a howmanypeoplewillIkill() function?

          Because properly programming an autonomous vehicle has to be about avoiding getting to that point, not about driving like a human and solving the trolley problem every time your software realizes it has fucked up.
          ```

          - u/sparr:
            ```
            > Because properly programming an autonomous vehicle has to be about avoiding getting to that point

            And just give up if you encounter a situation you couldn't avoid?

            > every time your software realizes it has fucked up.

            Ahh, spotted your mistake. You seem to be assuming that if the car needs to calculate how many people it will kill, it has [already] fucked up?
            ```

            - u/ArgentStonecutter:
              ```
              > And just give up if you encounter a situation you couldn't avoid?

              If you get to a situation you can't avoid, you've already screwed up, and the probability that you have any better options than "brake to a stop as quickly as possible" is negligible.

              > Ahh, spotted your mistake.

              It's not _my_ mistake.

              > You seem to be assuming that if the car needs to calculate how many people it will kill, it has [already] fucked up?

              Of course. Anyone else who has worked on real-time safety software will tell you the same thing. Safety systems are all about keeping well away from edge cases.
              ```

            - u/stale2000:
              ```
              No, it is because the self driving car solution to "I am in a bad situation that might kill someone" is almost always going to be "hit the breaks, and stop the car as fast as possible".

              And if your software ever "thinks" that it is in a bad situation where the solution is NOT "hit the breaks", then your software is almost certainly wrong, and is in some sort of false positive situation.

              Emergency car response is a problem of looking for horses, not zebras.  And in this kind of situation, programming in a zebra detection algorithm will almost certainly cause more deaths than it saves in 99% of situations.
              ```

- u/muns4colleg:
  ```
  Well, obviously the most ethical choice is to kill the five imps, you can even switch the teleport and switch it back so killing the five imps was an active action on your part. 

  They're imps, you gotta kill 'em. He should have used NPC marines instead.
  ```

---

