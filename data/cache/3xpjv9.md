## [D] Monday General Rationality Thread

### Post:

Welcome to the Monday thread on general rationality topics!  Do you really want to talk about something non-fictional, related to the real world?  Have you:

* Seen something interesting on /r/science?
* Found a new way to get your shit even-more together?
* Figured out how to become immortal?
* Constructed artificial general intelligence?
* Read a neat nonfiction book?
* Munchkined your way into total control of your D&D campaign?


### Comments:

- u/Vebeltast:
  ```
  Does anybody know why Spacebattles and Sufficient Velocity hate the Rationality meme-system? I haven't been able to get an answer out of any of them other than "Yudkowsky's navel-gazing cultish nonsense", much less a reasoned dissenting argument that'd I'd be able to update on. Did Methods of Rationality kill all their pets or something?
  ```

  - u/blazinghand:
    ```
    Rationality in general has a PR problem. People hear about it and based on whatever past experiences, dismiss it right away. Individual tenets of rationality, or even the whole hog, are accepted by people if you don't introduce them as rationality. You can put lipstick on this pig. 

    Of my friends, some hate the rationalism, and the one who hates rationalism the most is also the one who uses it the most. It's just a name / branding issue really. Stuff like the ideas in [Beware Trivial Inconveniences](http://lesswrong.com/lw/f1/beware_trivial_inconveniences/) or [The Toxoplasma of Rage](http://slatestarcodex.com/2014/12/17/the-toxoplasma-of-rage/) or whatever rationalist article, if presented without rationalism mentioned, are usually really popular. I can just take the idea, present it myself, and people will like it. It's hard to give them follow-up reading though.

    It's just a bad brand. I can't speak about SB and SV specifically, but that's just what I've observed.
    ```

    - u/alexanderwales:
      ```
      I remember talking to some people on LessWrong a few years ago about *why* the brand was a bad one and getting some combination of denial ("It's not a bad brand!"), obstinate refusal to see this as a legitimate problem ("It's a bad brand because we say things that are true!"), or placing blame on others ("It's the haters!"). It just convinced me that I wasn't likely to have a productive conversation on the matter. Same with the "cult" stuff, which is closely related.
      ```

      - u/None:
        ```
        My own pet peeve on that score: why is "the Sequences" usually (or often) capitalized? 

        For purposes of comparison, Christians like to capitalize "Old Testament" and "New Testament," "the Koran" is capitalized, etc.   

        **It's not a big deal**, and I suppose most people don't pay much attention to details like that -- but I've always found it a little creepy.
        ```

        - u/Vebeltast:
          ```
          I've observed a couple people as they read through the sequences. I think that it's capitalized like the other books because it has comparable power. If it's new to you, you can understand, and you it buy into it, you can build most of your personal philosophy around it. It's about as creepy as, say, an Asian person reading a Bible translation around the age of 20 and suddenly becoming a furious Christian.
          ```

          - u/None:
            ```
            If it was all placed in the context of traditional academic statistics and philosophy, it would seem a fair bit more commonsensical but a fair bit less Deeply Profound.

            Ironically, the thing I like most about this subculture is that we value the commonsensical and the natural *over* the Deeply Profound.

            It was Eliezer who said they're 85% non-original material, even though they don't cite much.

            We need way better introductory books.
            ```

            - u/Vebeltast:
              ```
              > Ironically, the thing I like most about this subculture is that we value the commonsensical and the natural over the Deeply Profound.

              See, that's the interesting thing that I've just realized: a lot of what we see as being common-sense is, to people who haven't seen it before, Deeply Profound. Like, the bit about death being bad: We see it as blindingly obvious. Death is simply a *bad thing* and I can't see anything in the laws of physics that demands it, and so I would prefer the universe where nobody has to die. But if you say all that to someone who hasn't thought about it, you end up Deeply Wise!

              Agreed on needing better introductory material. EY's single biggest achievement is that he put all of this in a single place and organized it so a single person can assemble it for themselves. It's an important achievement, but it can be duplicated more easily now that it's been done once. We just have to get it put in more places.
              ```

              - u/None:
                ```
                > See, that's the interesting thing that I've just realized: a lot of what we see as being common-sense is, to people who haven't seen it before, Deeply Profound. Like, the bit about death being bad: We see it as blindingly obvious. Death is simply a bad thing and I can't see anything in the laws of physics that demands it, and so I would prefer the universe where nobody has to die. But if you say all that to someone who hasn't thought about it, you end up Deeply Wise!

                Oh right.  As a group, [we're split into the five-year-old children and the meta-contrarians](http://lesswrong.com/lw/2pv/intellectual_hipsters_and_metacontrarianism/).  Oy.

                (Although the Second Law of Thermodynamics *does* seem to demand a heat-death of the universe *eventually*.  That's just not relevant to our timescales right now, unless you're searching for [Eternal Deep Truths](http://meaningness.com/preview-eternalism-and-nihilism) to solidify a worldview.)
                ```

        - u/Vicioustiger:
          ```
          I don't know what "the Sequences" are, but doesn't the "the" imply it is a name, making it a proper noun? Therefore always capitalized.
          ```

          - u/None:
            ```
            https://wiki.lesswrong.com/wiki/Sequences

            I'm not sure whether it is a name (the webpage I linked to above is titled "Sequences" but has both EY's collection of posts that are usually referred to this way with a capital S, as well as other sequences by different lesswrong-affiliated authors). 

            Regardless of whether it is a name, I still find it a little creepy to see someone told to "read the Sequences." 

            Just something off about that. Although:  I may be the one off here, I suppose creepiness is in the eye of the beholder.
            ```

            - u/Vicioustiger:
              ```
              I can understand that worry and comparison after reading some of the other comments here. The word cult has been used at least 5 times just discussing it, and when the entire point is to come to rational conclusion then anything with a cult connotation would seem off-putting.
              ```

        - u/blazinghand:
          ```
          I always assumed it was because Yudkowsky was planning on turning them into a book or something. [Prolegomena to Any Future Metaphysics](https://en.wikipedia.org/wiki/Prolegomena_to_Any_Future_Metaphysics) is capitalized because it's a title, even if it's a purely descriptive title.
          ```

          - u/None:
            ```
            You may be correct (and I believe he did turn them into [a book](http://www.amazon.com/Rationality-From-Zombies-Eliezer-Yudkowsky-ebook/dp/B00ULP6EW2)). Still, even so, "read the Sequences" sounds exponentially more creepy than "read Plato's Republic," no?
            ```

            - u/Rhamni:
              ```
              But in all seriousness, do read Plato's Republic. With footnotes.
              ```

            - u/blazinghand:
              ```
              I haven't actually run into anyone who's told me either of those things in response to a query so I can't say in context. Comparing "Read Yudkowsky's *The Sequences*" vs "Read Plato's *The Republic*", the latter sounds better, but this to me again boils down to a branding issue. If I wrote a book called *Modern Cognitive Science and You: Seventeen Easy Steps to Success*, even if it contained the same content, you'd have a real different experience recommending it to people. Same if a famous cognitive scientist wrote it and gave it a more professional title. 

              I'm sure it's not helped by rationalists suggesting it in a strange way, either. People in general don't know how to sell things. I doubt rationalists are an exception.
              ```

            - u/None:
              ```
              > Still, even so, "read the Sequences" sounds exponentially more creepy than "read Plato's Republic," no? 

              I think that depends on whether you know the actual content of Plato's *Republic*.
              ```

              - u/Rhamni:
                ```
                I mean, the eugenics stuff isn't even well run. A yearly rigged lottery? You don't think people will end up having sex outside of that?

                In all seriousness, he was a very thoughtful, intelligent man who lived in a society that thought slavery was ok and became the cultural capitol of 'Greece' by using money raised as tribute. Fortunately the main message is not that you should agree with him on every point. It's that you should collaborate with others, analyse arguments thoroughly, discard the ones that don't hold up, even if they come from him, and keep searching honestly for the truth.
                ```

                - u/None:
                  ```
                  > It's that you should collaborate with others, analyse arguments thoroughly, discard the ones that don't hold up, even if they come from him, and keep searching honestly for the truth.

                  And also that slave-taking is fine, virtue-ethics is a thing, all objects are mere projections of perfect Forms that live in a Heaven of Ideas, etc.

                  Frankly, I'm not willing to let any one thinker or group of thinkers claim ownership over basic critical thinking, in the same way that they don't get to "own" physics.
                  ```

                  - u/Rhamni:
                    ```
                    Fair enough, although I will point out that in Plato's imagined Republic, there are no slaves. There is a caste system, but all the material wealth stays at the bottom, while political power comes with forced asceticism and gender egalitarianism. Children are assigned caste independently of their parents, depending on how well they do in school (although the eugenics program suggests he expects most apples to fall near the tree). It's clearly far from a society I or others of today would endorse, but while the realm of the forms and all that jazz is plainly silly, the critical thinking was presented in a way that helped me become more interested in philosophy. Obviously Plato does not 'own' critical thinking, but he's an early master of it.

                    He is not in any way mandatory reading, but he was an excellent starting point for me, and I still enjoy reading a dialogue every now and then.
                    ```

    - u/Uncaffeinated:
      ```
      The problem with talking about "rationalism" like this is that you seem to be conflating multiple ideas. It's like, rationalism is about making smart choices, which noone can argue against, and then oh by the way, you're supposed to believe in evil AIs going FOOM and donate to Yudkowski now.
      ```

      - u/blazinghand:
        ```
        Good point! That's what I'm talking about.

        There's definitely some terminology problems here. "Rationalism" as it is used refers to a bunch of different ideas, some of which people like, and some of which people do not. This is exactly why, when you want to talk the things you want to share, you don't call it rationalism. 

        In a similar vein, when I try to introduce other things (like socialism or libertarianism or whatever charged idea there is) I don't call them by name. Names and labels hurt people's ability to be good about this kind of thing.
        ```

      - u/None:
        ```
        Also, "rationalism" means Descartes and "rational" has a tendency to be used as "Think what I tell you to!"
        ```

  - u/MrCogmor:
    ```
    I'd guess because they have had a number of obnoxious posters trying to encourage people to read less wrong or support the singularity institute. Probably also bits of appealing to Yudkowsky as an authoritative source despite his lacking of credentials the opponent would find meaningful.

    There was/is a personality cult around Elizier because of the halo effect and people creating an image of him as a person based only on his high rated and carefully crafted posts.

    While lesswrong has a number of transhumanist memes, the ones that are only really associated with lesswrong tend to be the weird and implausible ones like the A.I foom theory, worry over existential risk, roko's basilisk, cryonics and overuse of the word rationalist as an adjective.

    The community does resemble a religion in a number of respects. The meetups, solstice celebrations, the insular community, the weird beliefs and most importantly the appeals for money from the machine intelligence research institute (formerly known as the singularity institute)

    People do stereotype members of the rationalist community. They are wrong to write the community off as a whole but they do have a point. The people that do deliberately advertise that they belong in this community tend to be the obnoxious posters I mentioned earlier.
    ```

  - u/None:
    ```
    [deleted]
    ```

    - u/Vebeltast:
      ```
      ...Last time I saw them, maybe a month ago? Tim Josling was in the process of doing that when I poked the Less Wrong slack chat to see if it was interesting, testing how hard it was for random people on the subway to pick up some of the simpler ideas against bias. Maybe we're just hanging out with different rationalists?
      ```

    - u/traverseda:
      ```
      Last week my roommate was very pissed off at how hard it would be to run an independent study on bacopa monnieri, compared to quick and dirty trials we can run on other drugs.

      Yesterday I tried to make coloured-flame candles and used basic science throughout. I used basic science to figure out seam strength when bonding two pieces of mylar space blanket together just last week.

      We constantly use science to binary search though our 3D printers' problem space.

      Science is not what lesswrong brings to the table though. It's impossible to do any kind of engineering job without at least a basic adherence to the scientific method.

      A lot of the rationality techniques that I value most aren't just basic science though. When I did a CFAR workshop that was something that kept coming up, the cost of information and dealing with uncertainty.

      As an individual, you don't have the time or resources to test your questions against reality.

      Take, as an example, the question of what career to take, or which job offer to take. The scientific method won't help you here.

      People conflait lesswrong style rationality with science because ~~they~~we talk about science a lot. But science is only one tool in the toolkit, and although it's often useful in my day to day life, it's only useful when your claims are testable.

      The practical explanations of cognitive biases, cached thoughts, etc are really what make it a useful toolkit.
      ```

  - u/None:
    ```
    What is the rationality meme-system?
    ```

    - u/SvalbardCaretaker:
      ```
      The memplex of Xrisk,AI-Xrisk, effective altruism, human biases,bayesian calculation, and evopsych that originated on lesswrong.com. Eg. Harry Potter James Evans Verres style thinking.
      ```

    - u/Vebeltast:
      ```
      Basically anything related to Rationality as used here, utilitarianism, friendly AI, existential risk, etcetera. I've seen hostile reactions to the mere mention of biases, black swans, or recursive self improvement.I can't figure out why, either, because the reaction is never explained past "it's nonsense".
      ```

      - u/Nighzmarquls:
        ```
        Considering how much sufficient velocity and space battles pushes for starships that make very little sense, I'm pretty sure the "it's Nonsense" thing is not the real answer. But they as a total group might now know themselves.

        Incidentally I've started crossposting a story that is threaded with a whole bunch of rationality stuff to their forums and responses are pretty positive so I think it's likely that their actually more concerned with the 'dressings' of rationality being distasteful and not the actual core ideas.
        ```

        - u/None:
          ```
          [deleted]
          ```

          - u/Rhamni:
            ```
            While Eliezer's first post yelling at Roko was a very unfortunate, and the least calm thing I've ever seen him write, I think it's painfully clear that it's a non-issue E. and all of Less Wrong have no interest in talking about, but others like to bring it up again and again because it sounds silly, especially if you haven't read anything about all the stuff you need to read about for the idea to make sense.

            I have tried to give Less Wrong a chance a few times, but it doesn't capture my attention. I read a few sequences every now and then, I like this sub, and... well, that's about it.
            ```

          - u/Vebeltast:
            ```
            > It was reading about Yudkowsky, MIRI, and the reaction to Roko's basilisk.

            I guess that's sort of what I'm wondering about. Spacebattles et al. seem to be completely on board with 95% of the individual ideas if presented on their own - all you really have to do is rephrase them and post them in isolation - but if you mention that you got it from LW it's suddenly rejected. As if the argument's validity is somehow dependent on who came up with it first. "They're rejecting what they see as a cult" might explain that, though.
            ```

            - u/None:
              ```
              People don't like to be fed "hooks" where you start with seemingly commonsensical ideas and end up with radical, implausible-sounding stuff, at least not as Author Tract-y stories.  If you think your logic is airtight, you'll usually just talk to scholars.  If you want to write a story, you keep the weird-logic in the background and let people work things out on their own.

              People have very little reason to shift their fundamental, semi-metaphysical beliefs about the world just because someone's preaching at them.  In fact, it's rude and gets people mad.
              ```

      - u/None:
        ```
        "Black swans" are indeed a load of bullshit.  If your model (eg: Black-Scholes Equation) puts an extraordinarily low probability on an event (eg: demand-starved, debt-driven financial crisis) that other models (eg: conventional Keynesianism) called practically inevitable, and which has happened before (Great Depression), *it's just a bad model.*
        ```

  - u/Uncaffeinated:
    ```
    Well I can't speak for them, but I can say why I don't like it.

    At its worse, the community seems more like a cult than a group of people interested in overcoming biases and well thought out fiction. 

    For example, Friendly AI/Singularity stuff is just Rapture without the Jesus, AI-X Risk is Caveman Scifi for the modern age, Roko's Basilisk is Pascal's Wager with the serial numbers filed off (though at least noone takes that seriously) etc.

    For all its focus on being rational, there's a lot of outlandish ideas passed around without any critical thinking.
    ```

    - u/None:
      ```
      And this is why our cult leader's most under-appreciated saying is, "Beware things that are fun to argue about."
      ```

    - u/Vebeltast:
      ```
      > any critical thinking

      Perhaps the critical thinking is there you just haven't seen it being done? For example, it sounds like you're conflating [at least two of the different versions of the singularity](http://www.yudkowsky.net/singularity/schools/). I mean, a recursive self-improvement explosion is clearly a thing that could actually happen - we could do it ourselves pretty trivially if we didn't have all these hangups about medical research with psychedelics or if we dumped a spacex-sized pile of money into brain-computer interfaces - and the risk of unfriendly AI is obvious enough that *Hollywood* has been making movies about it since the 60's, though as always [the real deal would be much more subtle and horrifying](http://www.fimfiction.net/story/62074/1/friendship-is-optimal/prologue-equestria-online). I'll give you the initial response to the Basilisk, though; it's a non-issue now that people have realized that it's a wager and deployed the general-purpose wager countermeasure, but the flawed memetic form is still floating around causing problems.

      I can see how it would be extremely cultish if viewed from the outside, though. It's a large, obviously coherent system of beliefs, with a consistent core and an unusual but relevant and deep-sounding response for many situations, and that gives it the [seemings and feelings of deepness](http://lesswrong.com/lw/k8/how_to_seem_and_be_deep/) that you usually only see in religions. And then it comes down to whether your first impression suggests "Bible" or "Dianetics".

      Probably explains why 95% of it is well-received if delivered on its own. Without the rest of the large mass giving it unusual coherence and consistency, it seems like just an awesome idea rather than a cult. Which would kind of explain the success I've had directing unsuspecting people to just the sequences, since by the time they've gotten to critical mass they've bought into most of what they've read.
      ```

      - u/Uncaffeinated:
        ```
        I suppose this is a side tangent, but I'm fairly skeptical about the scope for recursive self improvement.

        First off, it's hard to make an argument that doesn't already apply to human history. Education will make people smarter, and then they figure out better method of education and so on. Technology makes people more effective and then they invent better technology, etc. Humans have been improving themselves for centuries, and the pace of technological advance has obviously increased, but there's no sign of a _hyperbolic_ takeoff, and I don't think there ever will be.

        The other issue is that it flies in the face of all evidence and theory. Theoretical Computer Science gives us a lot of examples where there are hard limits on self improving processes. But FOOM advocates just ignore that and assume that all the problems that matter in real life are actually easy ones where complexity arguments don't apply, somehow.

        Sometimes they get sloppy and ignore complexity entirely. If your story about FOOM AI involves it solving NP Hard problems, you should probably rethink your ideas, not the other way around. And yes, I know that P != NP isn't technically proven, but noone seriously doubts it, and if you want to be pedantic, you could substitute something like the Halting Problem, which people often implicitly assume AIs can solve.

        There's also this weird obsession with simulations, without any real consideration of the complexity involved. My favorite was the story about a computer that could simulate the entire universe, _including_ _iteself_ with perfect accuracy in faster than real time. But pretty much any time simulations comes up, there's a lot of wooly thinking.
        ```

        - u/None:
          ```
          I don't really know anything about these questions, but my first (and perhaps very naive) reaction to this: isn't the mere possibility that the takeoff could be very fast and the computational problems tractable something to be worried about? 

          For example, if you were 95% confident that one of your objections here would hold in real life, that still leaves a 5% chance of potential disaster.
          ```

          - u/alexanderwales:
            ```
            In *Superintelligence* Bostrom argues that medium or fast takeoff is more likely than slow takeoff, a sentiment which is echoed by a fair number of people on LessWrong. There was a recent article by Scott Alexander that said he thinks we live in a world where the jump from infrahuman to superhuman is going to be very fast.

            If the argument were "fast takeoff is unlikely but given the risks involved it's still something that we should take seriously" it would be a lot more palatable (though then it might read like Pascal's mugging). Unfortunately, I think there's also a tendency within the LessWrong crowd to first argue that FOOM AI is possible and then treat it as though it's probable, which doesn't do them any favors, especially given the lack of rigor applied to the question of probability.
            ```

        - u/Vebeltast:
          ```
          > but there's no sign of a hyperbolic takeoff, and I don't think there ever will be.

          My understanding is that you don't really need hyperbolic takeoff, or even a move move up the computational complexity hierarchy, to get hard a disastrously hard takeoff. All you really need is to move your intelligence off electrochemical computational platforms and onto semiconductors, which gives you something like a factor of 1e8 speedup. Then you accidentally raise a single hyper-fast serial killer without having an equally performant police department in place, and creating that equally performant police department is chicken-and-egg problem.
          ```

  - u/None:
    ```
    We *do* tend to act like a bunch of cultish autists.  Also, SpaceBattles is all about Truly Ridiculous Lulz: lecturing them about Rationality or Science is like lecturing Ashmodai on Torah.  Who are we to tell them to Stop Having Fun?
    ```

- u/Rhamni:
  ```
  I watched the movie Pan (2015) today. I enjoyed it, but it has one of the worst messages and most irrational villains I have ever seen. [Rant](#s "The evil villain conquers Neverland and enslaves it because he wants to live forever, and he needs tens of thousands of slaves to mine fossilized faerie dust out of the ground. While they do that, he searches for the hidden home of the faeries themselves. Late in the film he does find it, and it's a huge cave where faerie dust is more common than silicon is on earth. Faeries seem to just magically produce the stuff. So. What does the villain do? He takes a flying wooden ship, half a dozen flame throwers, and starts killing all the faeries. He goes through the dust fairly slowly, and since he doesn't share the stuff with anyone else the supply in the faerie cave would probably last hundreds of thousands of years, but still, *kill all the faeries?* At least be smart and try to catch some, don't just break out the flame throwers and start shouting about how you will at last be able to kill them all. There are a few other stupid things, like asking a boy if he's the magical child of prophecy sent to kill him, and then just sending him to prison after the first execution attempt failed because the kid could fly. Oh and faeries naturally live for thousands of years and naturally produce life extension powder, but among humans only villains want to extend their lives.")
  ```

  - u/SvalbardCaretaker:
    ```
    Thats what you get for watching big hollywood productions!

     Its fun to dream of a time where all the common media are suffused with the rationality-memeplex. Childrens books about planning fallacies and sunk cost fallacies!
    ```

- u/ulyssessword:
  ```
  What's a good way to go about giving to charity?  The way I see it, here are two parts to the question: how you choose a cause/organisation to support, and how you go about actually supporting it.

  For the first, one obvious answer among this group would be some form of effective altruism, and just leave it at that.  That leaves me with the question of what to do about groups that I'm personally involved in, or else are relevant only to the local area.

  I don't have a good or simple answer for the second half, other than to give money (the currency of caring.)  Beyond that,  do you guve a lump sum once a year? Wait for some matching opportunity? Automatic monthly ones?  Also, volunteering seems like a good idea for some things, mostly akrasia and community building.
  ```

  - u/blazinghand:
    ```
    I'm not a very charitable person, but there are a couple of causes/organizations I support. I mostly donate to feel good about myself, but also out of some basic duty to donate some nonzero amount of money. I donate about 0.5% of my gross income. Whenever I think about the money I donate, I feel proud of myself. It's also great to talk about. In terms of value for the money, donating to charity is a great way to make yourself feel good.

    I donate a small amount of money to [Wikipedia](https://wikimediafoundation.org/wiki/Ways_to_Give) every year. I do this because I think Wikipedia is great, and I get a lot of use out of it. Wikipedia needs (I think) about 3 dollars per year per user to operate, so I donate 10 dollars a year and feel pretty good about helping out one of the most useful tools at the level of "I'm doing my part, at least, and covering for a couple less fortunate people". 

    I also donate a medium amount of money to [Doctors Without Borders](https://donate.doctorswithoutborders.org/onetime.cfm), who do good. Givewell doesn't find them transparent enough to be a good idea to fully evaluate (compared to AMF or other charities) but gives them a [positive review](http://www.givewell.org/international/charities/doctors-without-borders). Doctors Without Borders is often involved in crisis areas, and also helps provide medicine and medical care throughout the world in underdeveloped communities. It's nice to donate to Doctors Without Borders and feel good about myself.

    My last donation is a political one, so you can stop here if you want. I donate a moderate amount of money to the American Civil Liberties Union (which I will not link, since it's political). The ACLU is an organization that defends the rights and liberties of Americans in court and by pushing legislature. Traditionally, they advocate for freedom of speech and religion, defending for example anti-war protestors. They also fought on behalf of the Japanese-American internees during WW2, and more recently for the rights of students, homosexuals, and the poor. They're also aggressively against the PATRIOT act, a set of laws that vastly increases the powers of the state and restricts civil rights in order to fight terrorism. I feel like the ACLU is one of the few big organizations fighting to keep America great and free.
    ```

  - u/None:
    ```
    Ask your group if they need money or labor more right now.
    ```

  - u/Gurkenglas:
    ```
    Perhaps you can publically commit money to the first matching opportunity that arises for a given rate and recipient. (Which is effectively matching with the reciprocal rate.) Although all these zero-sum moves in a game of charity are kind of silly, and I don't know how to mathematically tell apart "matching" and "taking hostages" and "proposing trade" and "blackmail".
    ```

- u/Rhamni:
  ```
  [SpaceX just landed the first stage of a rocket.](https://www.reddit.com/r/spacex/comments/3xs21y/spacex_on_twitter_the_falcon_9_first_stage/) Which is pretty cool, since that means you don't have to build a new one every single time you go into space. It's not gonna make space travel cheap, but it's going to bring the price down quite a lot.
  ```

  - u/gbear605:
    ```
    A Falcon 9 launch currently costs $61 million (according to wikipedia). According to reading on /r/SpaceX, the first stage makes up 75% of the cost. So yes, a rocket launch can now be down to $15 million. ULA, SpaceX's main competitor, costs the US government $380 million per launch. 

    To be fair, the cost for the government from SpaceX is $130 million instead of $61 million because of regulations and stuff, so a fair comparison would be $15 million versus $175 million. It's a *bit* of a difference.
    ```

- u/Nighzmarquls:
  ```
  I just read a very interesting abstract of recent research [here.](http://www.eurekalert.org/pub_releases/2015-12/ionr-psl122115.php)

  I consider my understanding of the topic amateur but this seems like potentially a really big deal for understanding brain structure.
  ```

- u/ulyssessword:
  ```
  Is there a word for "something like backlash, but without anything to backlash *against*"?

  The two examples that bring this to my mind are the actress playing Hermione Granger on stage being black, and Canadian Defense Minister Harjit Sajjan being Sihk and East Indian (I'm sure that there are more, and non-race ones as well.).  I haven't seen any criticism of them, but I've seen a lot of people loudly proclaiming that they're not only okay with it, but fully supporting it as well.
  ```

  - u/Seeworthy121:
    ```
    forlash?
    ```

  - u/LiteralHeadCannon:
    ```
    Sometimes I see the-thing-being-complained-about arise, in such a case... as backlash-to-the-backlash, and then it's used as evidence that the backlash was well-founded to begin with.
    ```

  - u/eaglejarl:
    ```
    > The two examples that bring this to my mind are the actress playing Hermione Granger on stage being black, 

    It took me a full minute to realize that you meant Hermione was being played by a black actress. I thought you meant that Emma Watson was performing in blackface
    ```

- u/rugurhfurghhrughh:
  ```
  http://strongfemaleprotagonist.com/issue-6/page-5-6/ cross post to /r/getmotivated
  ```

- u/Rhamni:
  ```
  So hey, if you ever wanted to build a rational dystopia, China has got a few tips for you. [Imagine Facebook, if the opinions and financial situation of your facebook friends made it easier or harder for you to get jobs, loans, etc.](https://www.youtube.com/watch?v=lHcTKWiZ8sI&feature=youtu.be)
  ```

  - u/SvalbardCaretaker:
    ```
    Its been done before! http://www.smbc-comics.com/?id=2286
    ```

    - u/Rhamni:
      ```
      That's neat, although the China thing appears to be actually happening.
      ```

---

