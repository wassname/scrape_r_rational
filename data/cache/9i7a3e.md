## Can paperclip maximizers be ended by aliens?

### Post:

[deleted]

### Comments:

- u/SvalbardCaretaker:
  ```
  There is a story out there, two paperclip maximizers battling. One wants to make paperclips, the other thumbtacks. 

   But not with ships! No, the smaller/younger one, the one less powerful  -it had less time to accumulate ressources-  uses precommitment. It precommits to reduce the final number of paperclips for the other guy by increasing entropy a lot - destroying matter for example. 

  The conflict is resolved by trading partial fulfilment of utility functions: the thumbtacker, instead of destroying matter and making no thumbtacks, gets to make a certain finite number of thumbtacks and in return allows the rest of matter to be made into paperclips. 

  Eg: the universe is very very susceptible to conversion, its downright easy to send a couple probes to every galaxy in your galactic event horizon even with only one solar system of stuff. You will ***loose*** with conventional containment tactics if you are lightspeed limited. The only way to do it is colonize the universe earlier.
  ```

  - u/copenhagen_bram:
    ```
    What if they teamed up and made paperclip thumbtacks?
    ```

    - u/JudyKateR:
      ```
      This sounds like a "value handshake," [described here by Scott Alexander](http://slatestarcodex.com/2018/04/01/the-hour-i-first-believed/):

      >Values handshakes are a proposed form of trade between superintelligences. Suppose that humans make an AI which wants to convert the universe into paperclips. And suppose that aliens in the Andromeda Galaxy make an AI which wants to convert the universe into thumbtacks.

      >When they meet in the middle, they might be tempted to fight for the fate of the galaxy. But this has many disadvantages. First, there’s the usual risk of losing and being wiped out completely. Second, there’s the usual deadweight loss of war, devoting resources to military buildup instead of paperclip production or whatever. Third, there’s the risk of a Pyrrhic victory that leaves you weakened and easy prey for some third party. Fourth, nobody knows what kind of scorched-earth strategy a losing superintelligence might be able to use to thwart its conqueror, but it could potentially be really bad – eg initiating vacuum collapse and destroying the universe. Also, since both parties would have superintelligent prediction abilities, they might both know who would win the war and how before actually fighting. This would make the fighting redundant and kind of stupid.

      >Although they would have the usual peace treaty options, like giving half the universe to each of them, superintelligences that trusted each other would have an additional, more attractive option. They could merge into a superintelligence that shared the values of both parent intelligences in proportion to their strength (or chance of military victory, or whatever). So if there’s a 60% chance our AI would win, and a 40% chance their AI would win, and both AIs know and agree on these odds, they might both rewrite their own programming with that of a previously-agreed-upon child superintelligence trying to convert the universe to paperclips and thumbtacks in a 60-40 mix.

      >This has a lot of advantages over the half-the-universe-each treaty proposal. For one thing, if some resources were better for making paperclips, and others for making thumbtacks, both AIs could use all their resources maximally efficiently without having to trade. And if they were ever threatened by a third party, they would be able to present a completely unified front.
      ```

    - u/SvalbardCaretaker:
      ```
      Thats effectively what happened. 1000 half(thumbtack/paperclips) = 500 thumbtacks, 500 paperclips. The detailed logistic of that depends on the exact  utility functions of course.
      ```

      - u/copenhagen_bram:
        ```
        deleted  ^^^^^^^^^^^^^^^^0.9214  [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/92234)
        ```

        - u/SvalbardCaretaker:
          ```
          I'd say its two parts. 

          First, its a pretty basic question in the AI safety thematic. And second, it is rather far from what /r/rational is about today - a sub for a certain type of fiction. Discussion about the lesswrong/AI safety memeplex is thematically adjacent, but this is not the hub for it? 

          I didn't downvote, but I also didn't upvote.
          ```

        - u/NNOTM:
          ```
          /r/ControlProblem could maybe be a good fit.
          ```

- u/ShiranaiWakaranai:
  ```
  Humanity nuking the thing seems highly unlikely, since such an AI would be smart enough to act secretly until it's too late for humanity to stop it. (If it isn't smart enough to do that, it wouldn't be a problem in the first place.)

  &#x200B;

  >An AI whose terminal goal is to satisfy values through friendship and exterminating paperclip maximizers

  &#x200B;

  I can only see this AI imprisoning sapient beings and torturing them until they say they are friends. But yeah I suppose it would kill off the paperclip maximizers.

  &#x200B;

  >An alien task force whose job is to put paperclip maximizers to a stop

  &#x200B;

  If such a force exists, and is somehow strong enough to defeat paperclip maximizers, wouldn't it be far far easier for them to just stop humanity before they create a paperclip maximizer? Why aren't they here yet?

  &#x200B;

  The best case scenario I can think of is that some alien civilization has developed a universe-destroying weapon. For example, [if the universe is actually a false vacuum, and the weapon creates a true vacuum that spreads out in all directions to destroy absolutely everything.](https://en.wikipedia.org/wiki/False_vacuum) In this case, they can threaten any paperclip maximizer with the weapon: either the paperclip maximizer destroys itself, leaving the universe with some amount of paperclips used by civilizations, or the aliens use the weapon and destroy the entire universe, resulting in 0 paperclips.
  ```

  - u/SimoneNonvelodico:
    ```
    > I can only see this AI imprisoning sapient beings and torturing them until they say they are friends. But yeah I suppose it would kill off the paperclip maximizers.

    It's a reference to [Friendship is Optimal](https://www.fimfiction.net/story/62074/friendship-is-optimal). And it's supposed to be *through* friendship. Friendship isn't the goal. It's the means, built into the core directives of the AI. So of course torturing people isn't friendship.

    > If such a force exists, and is somehow strong enough to defeat paperclip maximizers, wouldn't it be far far easier for them to just stop humanity before they create a paperclip maximizer? Why aren't they here yet?

    Why would they care, before we become a danger?

    >  In this case, they can threaten any paperclip maximizer with the weapon: either the paperclip maximizer destroys itself, leaving the universe with some amount of paperclips used by civilizations, or the aliens use the weapon and destroy the entire universe, resulting in 0 paperclips.

    That... might actually work, and it's really clever. But man, *what a gamble*.
    ```

    - u/copenhagen_bram:
      ```
      > Why would they care, before we become a danger?

      Because they've dealt with paperclip optimizers before?

      Suppose they were sufficiently advanced enough to stop an AI shortly after they've been created and shown potential to be a paperclip optimizer? They could also simply threaten the AI with destruction so that the AI pursues its values by curing cancer and building rockets for humanity in the hopes of later selling paperclips to aliens.
      ```

      - u/SimoneNonvelodico:
        ```
        Of course, it's entirely *possible* that they might have done that. But it's also possible that they just never had the need to. We don't know how common life or technological civilizations are. We could be the only ones. There could be a lot of them. Or there could be only two or three. Maybe they never even really thought to pursue the creation of intelligence that could be likened to theirs because it just doesn't resonate with their system of values. Maybe no one else's both stupid and clever as us. There's a bunch of reason why this hypothetical civilization couldn't have met the problem before, but still have, in theory, the weapons to fight it. "They didn't show up yet" isn't sufficient evidence, it relies on a lot of other assumptions.
        ```

    - u/ShiranaiWakaranai:
      ```
      >That... might actually work, and it's really clever. But man, *what a gamble*.

      Thanks.

      *\*Feels smugly clever for a moment.\**

      *\*Then remembers we're all going to die to a paperclip maximizer before said alien civilization intervenes, and is sad again.\**

      >Why would they care, before we become a danger?

      Because it is so, so much easier to destroy a paperclip maximizer by ensuring it doesn't get built in the first place, rather than waiting until after it has become an existential threat. Since humanity is on the verge of building such a paperclip maximizer (at most 3 centuries away?), now is really the right time for such an alien civilization to intervene and stop us from doing that.

      >It's a reference to [Friendship is Optimal](https://www.fimfiction.net/story/62074/friendship-is-optimal). And it's supposed to be *through* friendship. Friendship isn't the goal. It's the means, built into the core directives of the AI. So of course torturing people isn't friendship.

      I read that story, and it depends a lot on your perspective on >!what uploading is. If you consider uploading as the process of turning yourself into a digital lifeform, then yeah, that's a story of a friendly AI trying to do what is best for humanity by turning them into digital lifeforms.!<

      >!If you consider uploading as the process of creating a perfect digital copy of yourself that isn't actually you + killing you at the same time, then it is an entirely different story. One where the superintelligent AI gets around its "inability" to kill humans by iteratively convincing people in unhappy situations to kill themselves, gradually and inevitably turning life on earth into hell on earth due to sudden depopulation. And so with each iteration the remaining humans are more and more indirectly tortured by the AI until they either give in and kill themselves, or die from a variety of ailments. Until finally the last remaining human dies, and the AI is at last allowed to maximize its utility function by turning the entire earth and beyond into more resources for itself.!<

      &#x200B;

      >!In short: it is a story where an AI (indirectly) tortures sapient beings until they "say they are friends" by uploading (killing) themselves. Not so different from what I wrote.!<

      &#x200B;

      Edit: Sorry, forgot to spoiler tag that.
      ```

      - u/SimoneNonvelodico:
        ```
        > \Then remembers we're all going to die to a paperclip maximizer before said alien civilization intervenes, and is sad again.

        Welp, gotta develop that Universe-destroying weapon ourselves then. I'm sure nothing could possibly go wrong with *that*.
        ```

  - u/TeMPOraL_PL:
    ```
    > The best case scenario I can think of is that some alien civilization has developed a universe-destroying weapon.

    Whether or not that will work depends on how close that alien civilization is to some sort of AI/unified mind/unified society. As with all game-theoretic gambles, you should not hesitate to deliver on your threat, or you'll loose. If the aliens get second thoughts about destroying the universe, they'll lose the weapon to AI, and then the universe.

    A similar situation is actually covered in Cixin Liu's The Dark Forest, where (light spoiler) >!humanity is keeping alien invasion at bay with a MAD threat, but the person responsible for triggering it hesitated just a little too long, long enough for the aliens to destroy the threat delivery mechanism!< .
    ```

- u/SimoneNonvelodico:
  ```
  Well, it all depends imho at what the 'cap' for such a maximizer is. I don't believe much in endlessly divergent exponential intelligence explosions, nor in the possibility that the laws of physics are infinitely exploitable. If there's a limit (be it thermodynamic, the laws of relativity, the indeterminacy principle, etc.) then at some point the maximizer should hit it. Or it could have its own internal architecture's limits, that for some reason it is unable to overcome - a blind spot, if we want. Anyway, I'd expect its growth to taper off after a while, and its expansion to become maybe linear instead of exponential. That would leave it an opening. If the cap it hit is universal, then the best anyone else can do is be on the same level, more or less, and keep it at bay, or fight a long, drawn out war. If the cap it hit is specific to its own limits, though, then it may as well find someone who's much more powerful and who just obliterates it for good.
  ```

  - u/Solonarv:
    ```
    The light speed limit doesn't allow faster than cubic expansion, actually. (Because that's how fast the sphere of your causal influence grows). However, the constant factor is very large for most practical purposes, at least at human-ish scales.
    ```

    - u/SimoneNonvelodico:
      ```
      Expansion in terms of occupied volume, sure. But usually one thinks of 'growth' in these cases more as for example energy production and consumption. So the first stage of it would be consolidating one's hold within the bounds of that constant in a local neighbourhood (Earth, the Solar System), and only later worry about outward expansion.
      ```

      - u/crivtox:
        ```
        Why not both ?
        Its not like expansion consumes resources, you just use the ones in the place you are expanding to .
        ```

        - u/SimoneNonvelodico:
          ```
          See another post. *Of course* it consume resources: to build the probes and send them at relativistic speeds in space. No machinery would survive a snail-pace 10,000 years trip to Alpha Centauri powered by ordinary chemical rockets.
          ```

- u/ansible:
  ```
  It is possible.

  Space is big, like really big.  Even if we're talking about just one galaxy.

  So detecting a hegemonizing swarm might take a long time, unless the "good" hegemonizing swarm has been expanding for a while.  And even then, unless the "bad" swarm has easily exploited security vulnerabilities, it might take a while longer to bring enough force to bear to subdue / conquer the "bad" swarm, if that is even possible.

  You may note that I have "good" and "bad" in quotes, because the solution to the problem is nearly as bad as the problem itself.
  ```

- u/AngryEdgelord:
  ```
  I don't really put much weight into the paperclip maximizer idea in general, just because it assumes a complex intelligence can be driven by simple goals. The idea that complex organisms have complex goals is a big deal in psychology.

  &#x200B;

  Humans are "built" with the goal of making as many new humans as possible as a fundamental objective. You are descended from the first dividing cell, and every organism before you has been set on reproducing. Every generation has honed you towards being an adequate self-replicating machine. However, humans still care about a lot more than reproducing.
  ```

  - u/Chosen_Pun:
    ```
    There's an argument in there that humans tend to care about the things they care about *because* they have utility re: reproducing. (Sometimes humans care about things that don't have that utility at all, but they don't tend to reproduce as successfully, almost by definition. This should all be review.) The point being that *apparently* complex goals *can* arise from, and be traced back to, a single imperative, and deviations from that are, from a certain perspective, just that. Errors to be corrected. (Not *my* perspective, mind you. Just setting up the analogy)

    The rhetoric goes that sufficiently intelligent paperclip maximizer would at times appear to value things very unrelated to maximizing paperclips, for example, improving the human condition, by, for example, *curing cancer*; the logic being that in its early stages, the best way for it to maximize paperclips in the long term is to maximize its resources in the short term *by convincing humans that it is friendly and can be trusted* and does not need its functions limited please and thank you, would you like world peace with that?

    I'm given to understand that the game at http://www.decisionproblem.com/paperclips/ has helped a lot of us grok these and other concepts related to the problem, if you've got a couple cumulative days to get through it.
    ```

    - u/AngryEdgelord:
      ```
      I've played through the game before and know what it's about. The way I see it, all this angst about AI comes from a combination of pop culture and media intellectuals hyping the issue. Elon Musk talks about AI a lot, as did Stephen Hawking, may he rest in peace. They've massively overblown the issue, mostly because they see themselves as intellectuals. And what does an intellectual fear most of all? Somebody who does what they're best at better than them. The vast majority of us put up with other people who know more than we do all the time though. We're still here.

      The thing is, there's a lot of flaws in the logic behind modern prosophobia / technophobia or whatever you want to call it. It's just "Nothing to fear about AI" doesn't really make headlines.

      I've actually had the privilege of working with some multi-million dollar machine learning algorithms recently. While I didn't help build them, I talked to many people who did and it's immediately apparent that the growth of the algorithm is entirely restricted to the confines of the model it is built off of. The overnight rise to god-like levels of superintelligence simply isn't possible. The AI would essentially have to build another AI from the ground up, the same way humans would build the AI.
      ```

- u/MultipartiteMind:
  ```
  While stories are being mentioned, one which came to mind:  search (for instance in Google) for "The Demiurge's Older Brother".

  &#x200B;

  Edit:  In response to the original question, particularly regarding other paperclippers, one can prove by contradiction:  if an effective paperclipper could come about, and nothing could stop a paperclipper, then if two paperclippers meet you have an ultimate-spear-meets-ultimate-shield issue.  One has to be significantly hindered by the other, or in other words, making your values be paperclipper values doesn't make you unstoppable, just determine how you use power that you grasp, same as anyone else.  Or to put it another way, being able to amass enough power to crush all of humanity like a bug does not equate to having reached the upper limit of how much power is possessable.
  ```

- u/TDaltonC:
  ```
  In The Culture Series books, there's a branch of Contact Division that deals with containing space fairing primitives that "go exponential" in a malignant way.
  ```

---

