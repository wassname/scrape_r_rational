## I feel like there's a lot of potential for a Netflix comedy series about the rationalist community

### Post:

I'm envisioning this as a mash-up of Silicon Valley and The Big Bang Theory, but if the characters were actual rationalists.

Anyway, I've written some example scenes below. (Disclaimer: I am not a writer.) Feel free to write your own scenes and/or characters!

Cast of Characters:
-------------------
**Dave**: Has only recently discovered the rationalist community. Just met Charlie.

**Charlie**: Long-time community rationalist, obsessed with HPMOR

**Kate**: Roboticist, rationality-enthusiast, probably intends to take over the world someday "for the greater good"

**Gary**: the schizophrenic rationalist

Some example scenes:
-------------------

[Dave and Charlie are at the front door of Kate's apartment]

**Charlie**: [Knocks] Kate? You there?

**Kate**: Yeah, it's open, come in!

**Charlie**: [Enters] Hey Kate, I brought Dave with me AUGH WHAT DID YOU DO TO YOUR APARTMENT?

**Kate**: You mean, why did I strap wheels and electric motors to all the furniture and link them remotely to the Raspberry Pi hivemind hanging decoratively from the ceiling?

**Charlie**: Uh, yeah, I mean, are you *trying* to set off the robot apocalypse? What were you going for, exactly?

**Kate**: *First* of all, the brain tends to get used to its environment after a while and become complacent, so I figure that if my furniture randomizes itself every so often, the constant novelty will stimulate my brain to pay more attention to things in general. *Second*, it's really convenient for parties: I can just press a button and the furniture will move itself out of the way! *Third*, it was FUCKING FUN.

**Charlie**: I see

**Dave**: But why is there a fake handgun strapped to the toaster

**Kate**: Oh, I was definitely drunk when I did that one. I think I was going for general decoration that doubled as a deterrent for thieves, but yeah no, now that I'm sober I think I'll just replace the handgun with a laser and use it to shoot down flies.

--------------------

**Charlie**: So Dave, I've got to introduce you to Gary. He's, uh, he's quite a character.

**Dave**: Oh? What about him?

**Charlie**: He suffers from some kind of schizophrenia. Has a tendency to construct elaborate delusions and conspiracies. So he turned to rationalism to make himself more sane.

**Dave**: Did it help?

**Charlie**: Well... *sort of*, but mostly it just made him better at justifying his delusions.

[Soon]

**Charlie**: Dave, this is Gary. Gary, Dave.

**Dave**: Nice to meet you!

**Gary**: Nice to meet you too, Dave! Also, there is a dragon in my garage.

**Dave**: Like ... a sculpture of a dragon?

**Gary**: No, I mean a literal, actual, living magical dragon.

**Dave**: Uh huh. Let me guess - it's invisible?

**Gary**: Yes, in fact! How did you know?

**Dave**: And it's also inaudible?

**Gary**: Yes! Wow, have you had experience with dragons before?

**Dave**: And it lives in a magical dimension where it can't touch anything in our world, right?

**Gary**: Oh, no, you can touch it! I mean, like, how else would I have known about it, right? You can't see or hear him, but you can touch him just fine.

**Charlie**: Wait. *What*?

**Dave**: I did not expect that.

**Charlie**: Gary, can we, uh, touch your dragon?

**Gary**: Sure! Want to go touch the dragon right now?

**Dave**: Right now.

**Gary**: Right now!

**Charlie**: Yes. Yes I do.

**Dave**: Me too. I have *got* to see where this is going.

[Later, in Gary's garage.]

**Gary**: Alright, wait over here in the corner. I've got to find him first. This may take a while, since he's invisible and inaudible and all that.

[Gropes blindly around the garage for ten minutes]

**Gary**: Well guys, I'm really sorry, but I guess I was wrong. There isn't a dragon in here after all. But every mistake is an opportunity to improve, right? I'm going to *update* on this new evidence. Not a dragon in the garage. Not a dragon in the garage.

---------------------------------------

[Kate, Charlie, and several others are having a heated conversation. Next to them is a white board with a list of tabooed words.]

**Charlie**: What I'm trying to say is that cryogenics is morally justifiable or there is an argument X such that...

**Kate**: Ahem. [Points to the white board]

**Charlie**: Oh, right. I'm saying ... *not*... not cryogenics justifiable *and* not exists an argument X...

**Kate**: AHH, now I get it.

**Dave**: [Walks in and sees the white board] YOU TABOOED THE WORD "OR"?!

**Kate**: Hey, English "or" is ambiguous! It could mean either regular-or or exclusive-or!

**Dave**: Or both?

---------------------------------------

[At a Halloween party]

**Random party-goer**: So clearly *you're* Harry and *you're* Hermione... [Dave and Kate nod] But who are *you* supposed to be?

**Charlie**: I'm Professor Quirrel!

**Random party-goer**: Doesn't Professor Quirrel wear a turban?

**Dave**: I told you, Charlie, no one will get it without the turban.

**Charlie**: *This* version of Professor Quirrel doesn't wear a turban. It's from HPMOR.

**Random party-goer**: What's HPMOR?

**Dave**: Here we go.

**Charlie**: What's HPMOR? *What's HPMOR?* My good sir, what if I told you that there was a fanfiction so good that it could forever change your life...

---------------------------------------

[Dave and Charlie are at a bar]

**Charlie**: Alright, Dave, you gotta come be my wingman. I'm gonna go try to get with that cute girl over there.

**Dave**: Wait a minute, don't you have a girlfriend?

**Charlie**: Yes, but we're poly, so we can sleep with other people. For example, my girlfriend slept with Austin, who slept with Chelsea, who slept with Sharon, who slept with Derek, who slept with previously indicated cute girl. So if I sleep with *her*, it'll make a *cycle*! [Throws up hands]

**Dave**: [Throws up hands]

---------------------------------------

[Charlie is standing on a street corner. A woman walks by.]

**Charlie**: Ma'am! You there, Ma'am! Would you like to hear the good news?!

**Woman**: What good news?

**Charlie**: About the Jew who saved us all...

**Woman**: Oh, sorry, I'm an atheis-

**Charlie**: ...by writing the world's greatest Harry Potter fanfiction!

**Woman**: I ... what?

**Charlie**: Here, have some pamphlets! ["What is HPMOR?" "What are the Sequences?" "Who is Big Yud?"]

---------------------------------------

[Charlie, Dave, and Gary are sitting in a restaurant]

**Gary**: All of politics is controlled by a race of invisible elves attempting to destabilize human society.

**Dave**: I'm sorry, but no, just no.

**Gary**: That's not a valid argument, Dave.

**Dave**: What evidence could you *possibly have* of what you just claimed?

**Gary**: Uhh, plenty of evidence, Dave. I mean, how else do you explain Donald Trump if not for a race of invisible elves influencing people at the ballot box?

**Dave**: Well, *for starters*, Donald Trump's rise is explicable entirely in terms of known sociological factors. Like, why would you even need to appeal to an external power to explain this, much less a set of invisible elves?

**Gary**: I see your point. If Donald Trump's rise is explicable in terms of known things, that *would* count as weak evidence against the elf hypothesis due to the explaining away effect. [Thinks for a moment] Oh well, it doesn't matter anyway, since my prior probability for the elf hypothesis is, like, 100%. It's just a properly basic belief for me.

**Dave**: [Chokes on drink] Properly basic? A 100% prior?! Are you insane? Oh wait, that's right, you actually *are* insane, sorry...

**Charlie**: Gary, you can't possibly have a 100% prior about something like this. You just ... can't. I'm not sure it's physically possible.

**Gary**: I'm sorry, but what rule says I can't? Prior probability is a subjective estimate. You have no basis for criticizing my priors, except with respect to your priors, which I will simply disagree with.

**Charlie**: You want a rule? How about Jaynes' principle of maximum entropy? A prior distribution should have the largest possible entropy relative to the existing data. This rules out 100% priors.

**Gary**: Interesting, but why should this principle convince me? According to my 100% prior, I should assign 0% credence to that principle!

**Dave**: If you have a 100% prior, then you can't update on any future evidence! You're violating the core principle of bayesianism! 

**Gary**: Again, why should I care? My 100% prior says I shouldn't.

**Dave**: No, you don't get it... here, let me try explaining this way. There are a *lot* of hypotheses similar to the one you have just described. For example, it could be a race of invisible angels, rather than elves. Or it could be aliens. Or it could be elves in combination with sociological factors. Or it could be a mysterious force of doom. There are just too many possibilities in this space. You need to have a *reason* for limiting yourself to exactly the hypothesis you described. You need to have encountered some evidence that *distinguishes* elves from angels from sociology in your theory. Otherwise, you don't have enough of the address to locate the elf hypothesis in hypothesis space. 

**Gary**: [Thinks for a second.] You know what, I think you're right... I never considered angels or mysterious forces of doom. I guess I was wrong - I need further evidence to decide about the elves. Thanks Dave! You've always got such an interesting perspective.

**Dave**: You're welcome, Gary.

**Charlie**: Wait. Wait a minute. Gary, did you just *update against a 100% prior* after five minutes of conversation? How do you feel right now?

**Gary**: Meh, I'm fine. I mean, I wasn't all that sure about how sure I was to begin with. My meta-prior for that prior was only like 60%.

**Dave**: [horrified] But... but why would... but why would you...

**Charlie**: Let it go, Dave, let it go.

-----------------------------------

[Television PSA, paid for by MIRI]

[A father stumbles into his house, gasping]

**Father**: Son, why has the outside world turned into a flaming hellscape overrun by murderbots?

**Son**: Uhhh

**Father**: Did you create artificial general intelligence without a working theory of reflectively stable friendliness?

**Son**: Uh, maybe?

**Father**: [breaks down in tears] I'm so sorry. I've failed you. I'm a failure as a father. I'm so sorry.

[Robots break down the door, everything fades to black]

**Narrator Eliezer Yudkowsky**: Don't destroy the world. Talk to *your* kids about the value alignment problem. Before they doom us all.

-----------------------------------

[Dave and Charlie are watching TV. Kate walks in.]

**Kate**: Guys, I've got the *best* idea for a TV show. It's gonna blow everyone's mind.

**Dave**: I'll bite. What is it?

**Kate**: We create a hilarious comedy about a group of rationalists like ourselves! And of course, since the characters are themselves rationalist, they'll have this idea too! So they'll create a show about rationalism within their show! It'll be the most meta thing ever!

**Dave**: Didn't Seinfeld already do that?

**Kate**: Yeah, but why should that stop us? As TV tropes has taught us, almost nothing worth doing hasn't been done before. But we can do it better - we can explore new depths of meta that no one has ever explored. Imagine it: an infinite sequence of TV shows within TV shows, all with rationalist characters, whose ideas can all influence each other at higher and lower levels, and even bleed into the real world...

**Charlie**: Hold on. If such a thing is possible, then by the simulation argument, doesn't that mean it's overwhelmingly likely that we're in a TV universe *right now*?

**Kate**: Holy shit, you're right. Quick, we have to get out of here! Break the fourth wall!

**Dave**: If we look around in all directions, eventually one of us will stare directly into the camera!

**Kate**: Hey, probable TV audience! I know you can hear me! Tell the writers to give us superpowers or we are *burning this show to the ground.*

### Comments:

- u/None:
  ```
  This is amusing. Thanks for sharing.

  For real, though, this has way too many in-jokes to be an actual series for a typical audience. But it's funny nonetheless.
  ```

  - u/vakusdrake:
    ```
    Well it could be popular, but maybe only _post_ singularity.
    ```

  - u/LieGroupE8:
    ```
    I was sort of joking about it being an actual Netflix series, but part of me is hoping that someone takes this idea and runs with it. Maybe a low-budget YouTube series? There is like no rationalist presence on YouTube.
    ```

    - u/None:
      ```
      Look up the "Shit Skeptics Say" videos on YouTube for something kinda similar to this.
      ```

- u/JulianWyvern:
  ```
  Needs a biohacker. Someone has to promote good transhumanity instead of leaving it a domain of villains
  ```

- u/EliezerYudkowsky:
  ```
  If only there were scriptwriters who knew how to write Level 1 Intelligent Characters or show reason without constantly mocking it, this might be a good idea.  As it stands, in the world we actually live in...

  Best PSA ever tho.
  ```

  - u/LieGroupE8:
    ```
    If anyone wants this actually made, it might work better as a webcomic. Any artistically talented people on this board (definitely not me) should feel free to use the idea. I don't think I'll be pursuing it further.
    ```

    - u/DaystarEld:
      ```
      I was thinking webcomic too. I did some writing for one way back in high school and it was much easier than getting a TV show developed would be, and allows each of these great scenes you just depicted to be stand-alone comics rather than plopped into some episode's unrelated plot.
      ```

- u/xamueljones:
  ```
  My belly hurts from all of the laughing and now I'm very sad that this TV series doesn't exist.
  ```

  - u/LieGroupE8:
    ```
    Thank you! I now feel a bit better about my own sense of humor.
    ```

    - u/detrebio:
      ```
      Your sense of humor's aight! I found the HPMOR gospel sketch was funny enough to (1) make me choke on my own saliva and(1.a[prediction, 99.785% prior confidence (0.00215% metaconfidence) ]) die from obstruction of my aerial pathways. You'll soon be hearing from my lawyers.
      ```

      - u/LieGroupE8:
        ```
        Thank you! :)
        ```

- u/OutOfNiceUsernames:
  ```
  Some suggestions, in no particular order:

  * add scenes and events describing how their “community of 4” gets split into *smaller* communities because they keep having holy arguments about the finer points of things.
   * add scenes where they discuss how to design their bureaucratic hierarchical power structure. 
   * Maybe Gary tries to arrange a coup but drops the project because he gets distracted or bored?
  * make one or several characters be annoyed by Charlie’s obsession with HPMOR, have them criticize the story from time to time with varying end results. 
  * explicitly ban usage of laugh tracks.
  * add more side- and background characters.
   * a character that’s aware about the community and its principles, and even finds them somewhat useful, but thinks that the *mind* is the plaything of the body, and that just analysing your own emotions and reasoning patterns is not enough to develop emotional control or escape some emotional responses.
   * a character that’s trying to prepare for various apocalyptic scenarios (e.g. undergoing related training programs, constructing a bunker, foraging for supplies, etc) but is having a number of unpredicted difficulties with their projects because of their anxiety.
   * a character who likes identifying as a member of that community because it makes them feel smart.
  * use xkcd-style stick figures as the show’s format?
  * a scene or plot arc about them trying to design their own constructed language. The priorities they come up with (e.g. no biased words \ grammar structures), their successes and failures with it, the end result, etc.
  ```

  - u/trekie140:
    ```
    I actually don't mind laugh tracks for this style of humor. There are certain jokes that work better in front of an audience and aren't as funny when you don't hear a crowd chuckling with you. I wouldn't say it's my preferred style of comedy, but there are funny shows out there that wouldn't have worked as well without them. Frasier is probably my favorite, though it's produced and acted in the style of a stage show and has the high-brow wit you'd expect from that.
    ```

  - u/None:
    ```
    Maybe have laugh track up until they discuss the possibility of them making a show about themselves, and have them say "It wouldn't have laugh track, of course" and it ends right there
    ```

  - u/JackStargazer:
    ```
    >explicitly ban usage of laugh tracks.

    I cannot agree with this enough.  This is what stops me from enjoying the Big Bang Theory. 

    Every second sentence...
    ```

  - u/LieGroupE8:
    ```
    All great suggestions! You can write some of them yourself! I agree with the ban on laugh tracks - I think they mask poor-quality humor.
    ```

- u/oliwhail:
  ```
  > My meta-prior for that prior was only like 60%.
  This is absolutely something my circle of friends would say and frankly I find that hysterical
  ```

  - u/Roxolan:
    ```
    I mean, this is a real topic. It has [an actual LW article penned by Scott Alexander](http://lesswrong.com/lw/3be/confidence_levels_inside_and_outside_an_argument/) and everything.
    ```

    - u/zarraha:
      ```
      I can't say for certain since this is my first time hearing of meta-priors, but as a mathematician, my intuition says this should by simplifiable.  If you take a probability distribution of meta-priors over all possible priors (example, I think there's a 50% chance my prior should be 10% and a 50% chance it should be 20%)  You should be able to simplify it into just a single prior (There is a 100% chance that my prior should be 15%).  I'm not sure if this simplification commutes with Bayesian updating, but it should give decent approximations.  For more complicated meta-priors you'd need to to a more complicated linear combination, but my point is that meta-priors are no longer necessary to model since a model with them can be reduced to a single prior with 100% meta prior.

      Now if we go up a level, or two, or infinity, I suppose you should have meta-meta priors over your meta-priors.  And meta-meta-meta-priors, ad ininitum, but I think that we should be able to inductively resolve this and get an infinite sum/product that converges to a single prior.

      So if your meta-prior for a 100% prior is only 60%, you should just say that your prior is like 80% or whatever you get as an average after accounting for your other meta-priors.
      ```

      - u/LieGroupE8:
        ```
        It's like the old joke: "60% of the time the product works every time!" In the case of Gary the Schizophrenic Rationalist, there is no reason to separate out a bunch of meta-prior distributions. They should all collapse into one prior distribution as you say. If you are 60% sure that you are 100% sure of something, then you are definitely *not* 100% sure of that thing. Other people have mentioned the Lesswrong article about probabilities in different models, or Jaynes' Ap distribution, but I'm not convinced that they apply in this case. Gary is trying to save face by inventing a new distribution out of thin air, and pretending that was his *real* belief the whole time.
        ```

        - u/Nuero3187:
          ```
          I always took that joke as meaning the effectiveness of the product was perfect, but it doesn't work all the time. That it was 100% effective, but to get to that guaranteed effectiveness there was a 60% chance.
          ```

      - u/philh:
        ```
        I think the thing you're talking about is called an Ap distribution (with the p subscript).
        ```

- u/darkardengeno:
  ```
  My new model of a good society is one in which this is an economically viable television show.
  ```

- u/Kishoto:
  ```
  > Dave: Did it help?

  > Charlie: Well... sort of, but mostly it just made him better at justifying his delusions.

  I fucking LOVED this line XD
  ```

  - u/depaysementKing:
    ```
    I was chuckling for a good few minutes after this.
    ```

- u/___ratanon___:
  ```
  > **Father:** Did you create artificial general intelligence without a working theory of reflectively stable friendliness?

  Forgot 'again' at the end.
  ```

- u/None:
  ```
  I don't know about a TV series, but keep writing these scripts here and I'll donate laughs and upvotes.
  ```

- u/Tehino:
  ```
  At least half of the characters should be non-cis for accurate representation.
  ```

---

