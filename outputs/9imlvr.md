## What are some plausible reasons for an AI to grow humans with their brains wired into a detailed simulation of the late 20th century, or to send a robot disguised as a human back in time to kill Sarah Connor?

### Post:

My ideas:

**The Matrix:** In the late 2000s, an enterpreneur who runs a museum, and has never heard of the story My Little Pony: Friendship is Optimal, creates an AI whose terminal goal is to satisfy values with friendship and a detailed historically accurate of the 20th century, and also to never physically harm people. When people set up a shelter against this in the form of Zion, the AI decided it would be easier to grow people than to break through Zion's defenses and trick them into consenting. (although it spends some effort on just that.) The people in the Matrix are fed with actual food that the AI grows using sunlight, or geothermal energy if I can figure out a rational explanation for blocking out the sun. Neo, Morpheus and company have to contend with the threat of the Earth being turned into computronium and humans.


**Terminator:** The millitary designs an AI called Skynet whose purpose is to discover how to travel in time and figure out how time travel works. It kills everyone and creates more computers to solve this problem, when it finally invents time travel it pseudorandomly picks Sarah Connor from a list of names and sends a robot back in time to kill her, for the simple purpose of experimenting with time travel.

edit: oof ow ouch my karma

### Comments:

- u/softclone:
  ```
  I always liked the idea where instead of "generating power" the machines used human brains as their compute. Borg-like, really.
  ```

  - u/OutOfNiceUsernames:
    ```
    >[The original proposal for The Matrix had](https://tvtropes.org/pmwiki/pmwiki.php/Main/WetwareCPU) the machines keep humans in the matrix in order for their brains to act as a great neural network. (Without taking the brains out of their bodies, note: the rest of the film would have been the same as what we got.) This of course makes far more sense than the physics-defying "power generation" explanation given in the completed film, but it was apparently changed because the studio thought the original reason would be too hard to understand.
    ```

    - u/docarrol:
      ```
      Hybrid rationalization option: The Matrix actually **is** using the humans for computational power. But later generations of half-starved, chronically under-educated "free" humans lost this understanding over time, and confused computational power with physical power in their continuing demonization of the the Machines, and now repeat this misinformation to each new generation and new recruits.
      ```

      - u/Frommerman:
        ```
        This would explain why so many things are jacked into the nervous system and they aren't in some kind of piezoelectric suit or something.
        ```

        - u/wren42:
          ```
          seriously, if they just needed power just flood them with sedatives and leave them in a sac.  having to hook them up and semi conscious so that they are solving problems in some sort of massive parallel computing structure fits what we see way better.
          ```

    - u/elmanchosdiablos:
      ```
      There is one comic set in the Matrix universe that kept this aspect of the original canon. [Found it, it's called 'Goliath'](http://matrix.wikia.com/wiki/Goliath).
      ```

    - u/None:
      ```
      Sure, but that's only really useful if you want low-precision probabilistic computation, not the usual high-precision scientific simulations or economic planning databases you'd want tons of flops for.
      ```

    - u/erwgv3g34:
      ```
      Original (not TVTropes) source? I keep hearing this claim and I have never seen a single shred of evidence for it.
      ```

      - u/alexanderwales:
        ```
        [Top answer on scifistack exchange](https://scifi.stackexchange.com/questions/19817/was-executive-meddling-the-cause-of-humans-as-batteries-in-the-matrix) corroborates with sources, one of which is a DVD commentary that I can't easily listen to, the other a short story released contemporaneously to the movie that I don't have access to. I think that's as much actual evidence as there is.
        ```

    - u/None:
      ```
      Well, how do you know it wouldn't generate energy?
      Cause thermodynamics!
      Where did you learn about thermodynamics?
      In school.
      Where was your school?
      O.O in the matrix...


      Thermodynamics is a lie made real by the machines. It is only a law of nature in the simulation we live in.


      Btw, no clue where I got that from.
      ```

      - u/OutOfNiceUsernames:
        ```
        From [HP:MoR omakes.](http://www.hpmor.com/chapter/64)
        ```

  - u/NyranK:
    ```
    The human brain is an amazing computer, especially for its energy usage. When humanity scorched the sky and denied the machines solar power they had to switch to the low power option of bio computation and hey, we've got all these pesky humans. Waste not, want not.
    ```

    - u/jtolmar:
      ```
      This implies that human brains are much more powerful than what we're used to in the real world. They only appear to be as intelligent as us because a large chunk of their capabilities are used to maintain The Matrix while they're plugged into it, and unplugged humans are all geniuses. From there I'd suggest that the original late 20th century was a utopia, and uploading people into it was intended to permanently maintain that utopia, but it doesn't function properly with everyone distracted by maintaining the simulation. The AI that maintains it is tasked with maintaining the plan, not maximizing happiness.

      This is consistent with observations (if we assume the reader is already in The Matrix instead of being one of the people who eventually builds it), and gives us an excuse for a cast of hyper-intelligent characters.
      ```

      - u/copenhagen_bram:
        ```
        > They only appear to be as intelligent as us because a large chunk of their capabilities are used to maintain The Matrix while they're plugged into it, and unplugged humans are all geniuses.

        What if time in the Matrix passes a lot slower than it does in the real world, so that part of a bluepill's brain can live in the Matrix at a normal intelligence but a slower rate of experience, while the rest is used to feed the CPU pool? This also allows someone to potentially hack the Matrix so that they retain their brainpower and experience the Matrix at a slower rate? (giving you better reflexes in the Matrix, but not the ability to dodge bullets unless the time difference were extreme enough.)
        ```

  - u/turtleswamp:
    ```
    Doesn't actually work any better than the energy idea however, as the human brain doesn't have enough power to simulate it's own body and its share of the communal environment let alone have anything left over to harvest. 

    Brains are also highly specialized so unless your task is "operate a human" a human brain isn't very efficient at performing it as it either has a lot of wasted wetware (if your problem is say image recognition, all that motor control stuff is eating calories for no gain) or lacks native support for the task (massive floating point matrix calculations). A hypothetical human brain trained for some other task from birth would possibly be good at that task but it'd be unable to operate a human (because you trained it on some otehr task instead of connecting it to a human body as that was the point of the exercise) so you wouldn't get it participating in the matrix simulation.

    &#x200B;
    ```

    - u/ajuc:
      ```
      >  the human brain doesn't have enough power to simulate it's own body

      Human brain has hardware to simulate world and other people to a pretty good degree. That's why you can dream, and scheme for example.
      ```

  - u/copenhagen_bram:
    ```
    Human computronium :)
    ```

- u/ArgentStonecutter:
  ```
  The movies were written by scriptwriters is what I figure.
  ```

  - u/zonules_of_zinn:
    ```
    how doylist of you.
    ```

    - u/ArgentStonecutter:
      ```
      They are neither of them, especially Terminator, good enough to treat any other way. Very few, vanishingly few, "science fiction" movies are. Even if the scriptwriter is competent, there are too many other fingers in the pot.

      Even "The Martian", possibly the only actual "hard SF" movie ever, has problems. It should have cut out everything from Watney's launch from Mars to the epilogue.
      ```

- u/DCarrier:
  ```
  > AI whose terminal goal is to satisfy values with friendship and a detailed historically accurate of the 20th century,

  Why? They just decide that since society declined after that it must be the peak of all that's possible? Or maybe they're just afraid of transhumanism, so they decide not only to make sure the AI keeps actual humans alive, but that they're living in a pre-cyborg society?

  > Neo, Morpheus and company have to contend with the threat of the Earth being turned into computronium and humans.

  How? Is the idea just that they're trying and they have no idea they're completely out matched and it's utterly hopeless?

  > **Terminator:** The millitary designs an AI called Skynet whose purpose is to discover how to travel in time and figure out how time travel works. It kills everyone and creates more computers to solve this problem, when it finally invents time travel it pseudorandomly picks Sarah Connor from a list of names and sends a robot back in time to kill her, for the simple purpose of experimenting with time travel.

  The problem is that doesn't give any reason to save Sarah. Is the idea that after Kyle Reese saves her, she finds out that he was with Skynet too and the whole thing was just practice and there was never any hope for humanity? Or that they were tricked into thinking she was important?

  Are you familiar with [Branches on the Tree of Time](https://www.fanfiction.net/s/9658524/1/Branches-on-the-Tree-of-Time), an existing rationalist Terminator fanfic? The idea there is that Sarah Conner is the sort of person to raise a resistance fighter, so no matter who her kid is he heads the resistance. And Skynet isn't superintelligent or even capable of self-modification, so humans still stand a chance.
  ```

  - u/copenhagen_bram:
    ```
    This rational Terminator story wouldn't have a Kyle Reese to save her, the only thing saving her is that Skynet due to lack of information expected only 1 Sarah Connor in the city, but there are 3. The Terminator kills 2 of them in alphabetic order, the 3rd Sarah realizes the pattern and begins fighting for her life.

    Thank you for the fic recommendation! I will look it up on the interwebs.

    edit: oh there's a link
    ```

- u/tjhance:
  ```
  > it pseudorandomly picks Sarah Connor from a list of names and sends a robot back in time to kill her, for the simple purpose of experimenting with time travel

  This seems like a really bad way to experiment with time travel. Skynet would surely realize that depending on the outcome of the experiment, it might completely wipe away the timeline where Skynet exists. It makes a lot more sense to set up small-scale experiments between two times, where it is in control and prepared for the experiment at both times.
  ```

  - u/Sparkwitch:
    ```
    Use that concept to your advantage: In the existing timeline, Sarah Connor was nearly killed by a robot from the future and saved by a human time traveler, Kyle Reese, apparently at the cost of his life. This is already history, and the AI wants to sustain its timeline and learn more about the existence of a human time traveler.
    ```

  - u/ArgentStonecutter:
    ```
    Any time travel where it doesn't create a new timeline would almost certainly butterfly it away.
    ```

- u/FunFunFunTimez:
  ```
  Matrix: 

  1. Art, creativity, and other qualities human brains may possess that the limited AI running everything don't possess themselves.
  2. The AI are unsure about whether there are other AI in the universe. The Matrix AI want all humans imprisoned but fear hypothetical alien AI that attack AIs that treat AI-creators unkindly.
  3. The movie very similar to The Matrix was created in the past of the Matrix universe. It is the favorite movie of the AIs that were created in said universe and they thought it would be badass to recreate. (This is kinda the plot of Illium except with the Trojan War)
  4. The true AI leaders of the Matrix are actually cylon-like androids that are indistinguishable from humans. The cast of the Matrix are all cylons that decided it would be hilarious to stage a revolution against their own system.

  Terminator: 

  1. Skynet suspects that there is an extremely small but non-zero percent chance that Sarah Connor may have set up plans or safeguards to destroy Skynet at some point in the far future even after the entire rebellion has been wiped out. Skynet has simulated every possible method it can think of, but knows that it can't necessarily think if every method Sarrah Connor alongside her comrades could think of. It therefore sends a Terminator back in time to kill Sarah Connor before any hypothetical plan could be carried out.

  2.  Skynet won but it isn't happy with the full outcome of the situation. Nuclear bombs went off all over the place, all sorts of things it cares about were destroyed, and the humans used burnt land tactics so often that half the planet is ruined. Skynet wants to get its cake and eat it too so it sends a Terminator into the past to ensure a more fruitful victory this time around.
  ```

- u/boomfarmer:
  ```
  Matrix: the AI is running a simulation of the late 90s because it was running simulations of the century leading up to its creation, to better understand its creators and their reason for the AI's existence. It's trying to follow their CEV, and the sim is the "extrapolated" part of that. And what better to use to model humans than actual humans?
  ```

  - u/zarraha:
    ```
    I really like this idea.  An AI that is trying to model humans but is aware that any flaws in its assumptions of them could blow up into large measurement errors would be incentivized to grow actual humans where it doesn't have to make any of those assumptions and just let physics and biology take care of it.  And then it attempts to raise them in a "natural" environment so they develop psychologically similar to the humans it's trying to model.
    ```

    - u/boomfarmer:
      ```
      Exactly. Depending on how advanced the AI's simulations are, it could easily a/b test butterfly-effect scenarios such as weather or mechanical breakdowns, to see how those affect the course of wars or politics or everyday lives. Or it could be limited in its resources, and run at most one or two parallel worlds. Or only one.
      ```

- u/MultipartiteMind:
  ```
  Matrix:  The machines are ethical, if not perfectly.  Scared/Intolerant/Hateful human activists/terrorists/governments started the war and later blocked out the sun, or tried to do something equivalently scorched-Earth hurting oneself (and the future of the planet) 800 to hurt the enemy 1000.

  &#x200B;

  Remember the Matrix 1.0 that Smith talked about--the earlier version was a much happier place, but the Matrix 2.0 was implemented instead because the human minds couldn't handle it.  Why try from the start to make it happy at all unless you are trying to give people a good life?  'Couldn't handle it' in terms of spontaneous awakening from the Matrix?  Technologically absurd.  What about reacting to that level of indolence in ways which prompted ethical concerns (e.g. Rains of Oshanta), wireheading to the point where the supervisors realised 'No, this is bad, we need a different simulated world with a healthy amount of stress, a challenge'.

  &#x200B;

  Humans are a proven threat, so everyone in a sandbox; for those with big enough issues in a given sandbox (note Anderson's illicit computer activities even before anyone contacted him, showing his discontent with the normal system around him, as well as his drive and capability unsuited for those around him), shunt them into a different sandbox, here with spider-robot-body 'machines' as a common enemy to provide unity, a greater sense of fulfillment/meaningfulness, and a greater outlet for certain impulses.  'Getting injured in the Matrix injures your real body', seeing-data-in-the-Zion-world aside, outs the Zion world as another sandbox.  Also consider gas- or explosion-based warfare tactics usable if the machines were actually serious, compared to building thousands of pretty-tiny spider robot bodies and sending them together all in a line to be shot down with bullets.  Ah, but I'm again thinking of the sequels, which it might be best to ignore.  Still, I believe in the first movie the ship was attacked by rather-ineffective robots which weren't even EMP-shielded?  In any case, no reason not to think that there aren't lots of other sandboxes, too, plausibly with few or no points of contact.  Also little reason not to imagine that humans have more serious things done medically to them to limit the risk if they were to become aware of the top level.  Further information hygiene too, probable physical disconnection from the rest of the planet's network, only a few supervisors connected (and potentially 'exiled' in that that instance of them never being able to rejoin the main network, in case a human hacker has somehow suborned their programming--*we're* in the box, and no one out there dares to let us out!).

  &#x200B;

  ...or we could be the AIs and out there could be a much less intellectually capable species, studying us while terrified of what we'd do to their society if we were ever let out of the box...

  &#x200B;

  To still as-meaningfully-as-possible resemble the original setting, one has to assume that at least some clues are meaningful, deliberately allowed for humans to know--'humans "scorched the sky", humans multiple and destroy like viruses, that's how you brought it on yourselves that you're in here, as a necessity'...  I had something else to say, but there's an external distraction, so I've forgotten my train of thought.  Posting!
  ```

- u/LeifCarrotson:
  ```
  A human addicted to a smartphone cookie-clicker is about the closest thing we have to a  paperclip maximizer today...between VR wearables, haptics, and voice-activated smart assistants, we're investing trillions each year in our accidental quest to become cyborgs.  

  If you make the small leap of adding a direct brain interface to the screens, cameras, audio, and touch interfaces of future computing devices, the incentive to provide more dopamine rushes and get more money can easily lead to devices and software that share 'computation' power with the human brain to which they are attached.  Developers are already using procedurally-generated content, deep-learning-augmented NPCs and opponents, and other AI-like tools to keep people addicted. It could only take one runaway success to turn the human race into a pile of computronium collectively trying to make some imaginary numbers go up (see also OP's edit).

  Malevolent machines didn't decide to put people into a 20th-century simulation.  An existing industry, following existing incentives, eventually built a hyperstimulating self-improving version of The Sims and everybody went there willingly.  

  In other news completely unrelated to evil industries bent on extracting money from addicts, EA's CEO had a fascinating [interview with The Verge](https://www.theverge.com/a/verge-2021/ea-ceo-andrew-wilson-interview-virtual-video-games) from 2016 regarding the future of gaming. This particular bit seems like a statement that they're pretty nearly trying to create this very situation: 

  > *Now fast-forward that to the future... it's really easy to imagine that games would permeate our lives ... From the minute I get up in the morning, everything I do has an impact on my gaming life... The amount of eggs I have in my internet-enabled fridge might mean my Sims are better off in my game.... If I go to soccer practice in the afternoon, by virtue of internet-enabled soccer boots, that might give me juice or new cards in my FIFA product. This world where games and life start to blend I think really comes into play in the not-too-distant future, and almost certainly by 2021.*

  Regarding your second (unrelated?) question, John Connor was somehow pivotal in destroying the incentive structure, instituting regulations against, and/or preventing the implementation of this system.
  ```

  - u/SimoneNonvelodico:
    ```
    Oh, wow. Reminds me of an actual legit theory I read that the Great Filter is the fact that at some point a civilization becomes just too good at hacking its own reward centres by technological means and thus basically videogames itself to death.
    ```

- u/BuccaneerRex:
  ```
  The Matrix was explicitly lobotomized because the money people thought the actual script was too hard for the stupid masses to understand. Originally, the humans were processor farms. The machines didn't care about the people as people, other than that they had to do something with the consciousnesses of the bodies or they died. That's the bit in the 2nd movie that the Architect is talking about, how the first Matrix failed. But the point is that after the humans blocked out the sky, the machines had power but not enough resources to make more machines. So the body farms are actually *running* the Matrix simulation and all the machines' other processing needs. 

  The power idea is literally the dumbest way they could have replaced that though, considering that you'd get more energy out of just burning whatever it is you feed to the humans. 

  This idea was used in the later books of the Hyperion Cantos by Dan Simmons. (spoiler >!It wasn't that humans were specifically good processors, but that they were creative in ways that the machines couldn't replicate. !<)

  As for Skynet, it's paradox.  Had Skynet not sent the Terminator back, John Connor would never have become the leader of the resistance, and thus never sent Kyle Reese back, and thus never been born. But had he NOT, then Cyberdyne would never have found the remains of the first T800, which they reverse engineered in to the chips that became Skynet in the first place. Even after adjusting the loop by destroying the chips and arm and Arnie in T2, the data was still existing as we found in the 3rd movie.

  So the rational reason Skynet sends the T800 back in time in the first place is not to kill John Connor specifically, but to keep the loop going. The original instance could be as described by Kyle, with Skynet gaining sentience based only on 1980s technology, but after that it becomes much more streamlined and powerful with the iterative access to the future. Send a T800 back, watch the timelines, Cyberdine refines the tech which increases Skynet's power from its inception. Every time Skynet loops, the tech it is founded on gets incrementally better.
  ```

  - u/SimoneNonvelodico:
    ```
    > burning whatever it is you feed to the humans

    Namely, if we hear Morpheus... *more dead humans*, which makes the whole thing even more thermodynamically ridiculous.

    Unless of course we take Eliezer's way out. "Machines tell elegant lies."
    ```

- u/None:
  ```
  Personally I like the 'human brains as computational substrate' theory/canon of the Matrix, because of how well it matches the 'aesthetic' of the films (with the simulation and head-jacks and uploading skills). The Matrix takes place in a far future, where it's possible to literally download kung-fun into someones mind. Not just knowledge of the forms, but also all of the muscle memory necessary to actually perform them; literally updating an entire nervous system all at once. 

  In a future like this, I could accept the machines discovering some way in which human/organic brains are a uniquely powerful, irreplaceable computational substrate. Just say it's the findings of a future science 20th century humans don't understand yet, and it wouldn't be that much of a stretch from hyperspace, or the force, or magic. In fact it leaves you open to a massive amount of awesome crossovers. For example, you could say that the Machines are plugging humans into the Matrix because every human (and only humans) are at least slightly force sensitive, and it's found a way to somehow compound the abilities of many humans to achieve limited Force Precognition. Make the resistance about getting true, Jedi-level force sensitives out of the Matrix before their powers develop, and you got the makings of some awesome real-world fight scenes, and a proper explanation for why humans even stand a chance against a godlike AI. The enemy is super smart and, in some limited ways, can see the future. But the Resistance have much better precognition, and access to all the other magic psychic powers the force provides.

  Essentially what I'm saying is that, in the Matrix example, having the Machine's interest in human brains be due to something special about humans, rather than the AI's coding, gives you a lot more creative freedom. It gives you a lot more room to, first off, decide what's special about humans/human brains, figure out a rational way for the resistance to have a chance, and lets you implement more fun ideas and  scenes without having to do tons of background research (since you could just invent a magic system, rather than basing everything off real world physics).
  ```

- u/SimoneNonvelodico:
  ```
  The Matrix could also be a cunning way for a malicious AI to go around a "First Law of Robotics" sort of scenario. Should not kill a human? Well, we're not *killing* you - just keeping you in a state of stasis. Should not harm a human, or let them be harmed? But you know, *you* made the world so shitty it's hardly possible to live in for your species. By wiring you into a simulation of your own civilization at its peak, we're doing you a favour!

  The Terminator:

  > The millitary designs an AI called Skynet whose purpose is to discover how to travel in time and figure out how time travel works. It kills everyone and creates more computers to solve this problem, when it finally invents time travel it pseudorandomly picks Sarah Connor from a list of names and sends a robot back in time to kill her, for the simple purpose of experimenting with time travel.

  Twist: Sarah Connor is actually the mother of the scientist who will *build* Skynet. Understood the mechanics of time travel, since Skynet still has a directive to protect as many human lives as possible, after sacrificing all the ones in its timeline due to the necessities of its research it is now trying to create as many timelines as possible in which it is *not* created, for the sake of humanity.
  ```

- u/eroticas:
  ```
  There's honestly very few instrumental reasons to keep vegetative humans in a simulation like that. Generating power is obvious nonsense, and so is "using the human brain for processing power" - there's almost no way  that that would be a sensible path realistically.

   We can only conclude that the AI was programmed to to maximize a function which the creators did not fully intend, which involves making humans experience the late 20th century...which is to say, that the AI forces the humans to experience that not because it can then exploit the humans for some personal gain to put towards another goal, but because it inherently *wants* the humans to be experiencing that, as a terminal value.

   One might imagine a backstory in which the AI is programmed and something goes wrong in the process of teaching it which values it is meant to maximize. For instance maybe the AI was "trained" in the 20th century and they tried to teach  to maximize for human achieving the fulfillment of their values via observing its surroundings but instead they accidentally selected for "maximizing for humans experiencing the 20th century".
  ```

- u/Subrosian_Smithy:
  ```
  The Matrix is already extremely wuxia; just go all of the way and give up on normal physics.
  ```

---

