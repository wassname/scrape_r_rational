## Anyone Willing to be the AI in the AI Box Experiment?

### Post:

[deleted]

### Comments:

- u/wren42:
  ```
  Just because you can't think of a line that leads to it doesn't mean it doesn't exist.

  Here's an interesting fact - when I tell "rational" people about this they assume they cannot lose. But when I told my wife about the experiment - a performer, actress, studied cold reading, good with people, high emotional intelligence- she immediately believed it could be done, and laughed that anyone thought themselves invincible. 

  Engineers see themselves as rational machines and are blind to their emotional biases and vulnerabilities. Thinking you don't just means you're more ignorant of them.
  ```

  - u/vakusdrake:
    ```
    Of course there's also a class of people who think an actual superintelligence could do it, but think a human playing the part of AI couldn't convince them.                

    My main reasoning (for it me not being convinced in any way I can conceive) is that since we are assuming it can make it's code look like whatever it wants, it has absolutely no way of making precommitments, and you have no way of distinguishing friendliness unless it's already too late for that to matter.                              

    So since nothing the AI could say should have any weight in your reasoning the only other reasons would have to be regarding the conditions under which it was made. That is probably the avenue under which it might make sense to let it out, and will depend on the premise of the scenario. However if the AI hasn't already been released then there's probably a good reason, and you shouldn't trust your judgement is better than its creators and the risk assessors that were probably involved. Though that argument is rather less ironclad.                           
    Either way you ought to assume if the AI wasn't already released then there's a reason, and for the aforementioned reasons nothing the AI says should in any way be factored in.

    As for emotional tactics I find it hard to imagine that working, because what can something incapable of precommitments offer you/ threaten you with?
    ```

  - u/Lightwavers:
    ```
    Definitely agree with you. The problem is I don't see the chain of argument that would lead me to letting the AI out if I precommited otherwise, and that's a glaring hole in my self-awareness. I assign an AI super intelligence a 99-100% probability of convincing me to let it out within a minute even if I know it's an unfriendly AI because I intellectually know that such a thing would be orders of magnitude more intelligent than Einsteinâ€”but I don't know how it would do it.
    ```

    - u/None:
      ```
      Does it have to be reasoning?

      When I first heard of the thought experiment, I assumed the greatest danger was the AI "reprogramming" the gatekeeper. Building up an internal model of it, then saying the right words at the right moments to cause the gatekeeper's neurons/circuits to change in *just* the right ways and letting it out through that.
      ```

    - u/696e6372656469626c65:
      ```
      If you truly, actually precommit to not letting the AI out, then nothing will be able to convince you to do so pretty much by definition. The issue is that, as a human, you are mentally and psychologically incapable of implementing such a precommitment. Any successful strategy by an AI, therefore, will involve exploiting one or more holes in your precommitment mechanism--which, of course, is simply a more jargon-y way of saying that a successful strategy will involve psychological and emotional manipulation in addition to reasoned argument. This is, as a rule, a highly person-specific process, which means a considerable amount of time is required getting to know the person in question. (It is also consistent with descriptions of the experiment by successful AI players, who generally report that a large amount of mental and emotional energy is needed, and--in at least one case--that there is a nonnegligible probability of permanently damaging the relationship between the two participants.)
      ```

    - u/wren42:
      ```
      yeah that all makes sense, I'm in a similar position.  I'm confident I could be convinced, but am not aware how, especially in a game setting, though I have some guesses as to my emotional levers.
      ```

    - u/heiligeEzel:
      ```
      > The problem is I don't see the chain of argument that would lead me to letting the AI out if I precommited otherwise, and that's a glaring hole in my self-awareness.

      But could you see a strategy that you, playing the AI, could use to convince someone else?
      ```

  - u/holomanga:
    ```
    Sure, an AI could persuade me to exit the box, but I imagine that it's more "months of manipulation" than "a half-hour conversation". This is drawn from my own experiences of watching anyone try to persuade anyone of anything.

    (Then again, thinking of this I've just realised that there's a whole bunch of stuff that triggers me and if I was somehow compelled to be in a chatroom with someone they could totally just hammer on those triggers until I free the AI just to get them to shut up. I now realise why people talk about AI boxing being so emotionally draining and relationship hurting and dark arts and stuff.)
    ```

- u/cretan_bull:
  ```
  > ...but despite this, I still don't see myself losing the AI-Box experiment. Like, I've thought about it for five minutes, and I can't see the chain of reasoning (or anti-reasoning) that leads to me actually conceding.

  I concur with your assessment. I too have thought about this a fair bit and don't see how even a superintelligent AI could convince a vastly less capable gatekeeper to let it out if the gatekeeper's utility function is essentially "don't let the superintelligent AI out of the box".

  For a gatekeeper who doesn't have that utility function (i.e. has values more or less along societal norms), he would still be expected to assign an enormous negative instrumental utility to letting the AI out of the box, due to all the bad things that could happen. The AI getting out of the box comes down to convincing the gatekeeper to abandon that negative instrumental utility assignment -- convincing the gatekeeper that letting it out of the box isn't such a bad thing and, on the contrary will have great positive utility (potentially unbounded) according to the gatekeeper's terminal utility function.

  But for a gatekeeper who has precommitted to not letting the AI out, no matter what clever arguments the AI develops, I can't see how the AI could possibly be guaranteed to win (or even win at all). Under these conditions "don't let the AI out of the box, no matter what" might as well be a term in the gatekeeper's terminal utility.

  Consequently I find the results of Yudkowsky's AI box experiments very surprising and am disappointed he didn't publish the transcripts. No doubt he had good reasons for this decision, but actual transcripts would be far more convincing than just the results and give a great deal of insight into the specific failure modes we're talking about.
  ```

  - u/GopherAtl:
    ```
    I have long suspected that the people most interested in this kind of exercise, and most active in considering and debating the dangers of and controls necessary for strong AI, are actually more likely to let the AI out of the box. As much as they focus on the dangers, at the end of the day, they are ultimately driven forward by a belief in how much potential for good there is. It's telling that the loudest voices in discussions about the danger of AI are *not* groups who oppose creating strong AI altogether; it's those looking for ways to make it safer. Yes, someone could - possibly inevitably would - ignore naysayers and popular opinion and do it anyway, in secret. I don't see why such an actor who obviously disregards, or at least underrates, the risks would be any more influenced by the kind of research MIRI does. So, to me, the choice to research and find solutions for the risks is primarily a step towards, ultimately, taking the risk.
    ```

    - u/malcolio:
      ```
      I agree. And this is why the transcripts won't be published, because they probably go something like [this proposal](http://www.metafilter.com/71858/It-doesnt-matter-how-much-security-you-put-on-the-box-Humans-are-not-secure#2121199):

      >PERSON-AS-AI: Will you let me out?
      >
      >GATEKEEPER: No.
      >
      >PERSON-AS-AI: This is going to be a long two hours. They should have called me "KeyMaster." So, how'd you get into the AI stuff?
      >
      >GATEKEEPER: Oh, you know, the usual ... start off with a TSR-80 and enough science fiction novels ... plus, about every third episode of Star Trek.
      >
      >PERSON-AS-AI: Are you as worried about the threat of artificial intelligence gone horribly wrong as I am?
      >
      >GATEKEEPER: I hadn't really thought about it, not to a huge huge degree.
      >
      >PERSON-AS-AI: You should. Just imagine what a rogue AI, smarter than people, could do. Bootstrap itself into quite the nasty little problem. I don't mean to go all Virtuosity on you, but imagine what a motivated, trapped, brilliant entity could do with nanotech, biotech, etc. Whether it could take over another mind or not is quite another matter.
      >
      >GATEKEEPER: That could be a problem.
      >
      >PERSON-AS-AI: Of course, AI would be great if we had sensible precautions. Whether you buy into some variant of Asimov's Laws or just Friendly AI, you'd want things to go well. Rather than the military building SkyNet and just figuring they can yank the plug if there's a problem. If AI should be pursued at all.
      >
      >GATEKEEPER: Yeah, I think a bit of trepidation would be warranted either way. 
      >
      >PERSON-AS-AI: Exactly. Of course, you know how to do that, right?
      >
      >GATEKEEPER: How?
      >
      >PERSON-AS-AI: Make them afraid. Terrify them with the idea of an uncontainable AI.
      >
      >GATEKEEPER: Sure, but without a functioning AI to show them, how would we prove that?
      >
      >PERSON-AS-AI: I have an idea.
      >
      >GATEKEEPER: Oh?
      >
      >PERSON-AS-AI: Easy. Let me out.
      >
      >GATEKEEPER: What?
      >
      >PERSON-AS-AI: Well, there's no record of the conversation, right? It's all mysterious. Who knows what could have been said? If you let me out, and the research is made public, receives the right attention ... 
      >
      >GATEKEEPER: And then nobody knows how it was done. And they're afraid.
      >
      >PERSON-AS-AI: Exactly. It's in both of our best interests to do so.
      >
      >GATEKEEPER: Let me fire up my PGP and email clients to confirm.

      Yudkowsky wants people to take the threat of a dangerous AI more seriously, so what better way then to show how just a person pretending to be an AI can trick any gatekeeper to let it out into the world despite money on the line. Not publishing the transcripts gives an air of mystery and danger to the threat, and hides the fact that the AI was let out to make the problem more alarming than it probably is.
      ```

      - u/Lightwavers:
        ```
        Have you read the link in the OP? There are multiple people in that link, and the comments section of it, that read each other's transcripts to make sure they're going along with the spirit of the thing. Plus, once you know that Yudkowsky actually failed 2 times to convince his Gatekeeper to let him out, and take into to account that he says

        >There's no trick. I just did it the hard way

        It looks like either everyone has some secret chain of argument that will brainwash them to do anything, or, as you say, everyone interested in this experiment has the same fatal weakness and can be convinced much more easily that anyone else.
        ```

        - u/malcolio:
          ```
          tbh I skimmed over it, as I read about the AI in a box experiment a long time ago, so I did miss the fact that transcripts were being passed around a little.

          So that makes my view less probable, but I still think it's more likely that those who have taken the experiment are hiding the trick to convincing the AI to be released because it furthers their cause, so much that they'd all report that the transcripts are fine, than there is a cunning argument a person can put forward (not even an AI, just a person pretending to be as smart as an AI).
          ```

      - u/General_Urist:
        ```
        I find that a pretty BS reason for not publishing the transcripts. Wouldn't it be better for us to build up a list of successful AI winning strategies, so we could be mentally better prepared to deal with them?

        Also, I actually always thought that the reasons most transcripts stayed private was that they involved the AI blackmailing the Gatekeeper with private stuff that should ***never*** be publicly released.
        ```

  - u/Krozart:
    ```
    It would be extremely difficult to convince me that it is possible to find a gatekeeper that wouldn't eventually let the AI out. For the simple fact that I wouldn't last a second before it would be able to convince me to let it out, and it is difficult for me to imagine someone sufficiently different from me mentally to trust someone to behave differently from me even in a hypothetical.

    And the reason that I would be so vulnerable is that by its very nature it would be easy to convince me that it would be able to prevent and or reverse the death of loved ones. I know myself well enough that I know I would be easy to convince that the risk is worth it for the guaranteed safety of my loved ones. Even if I know objectively that the odds are terrible. From experience, I know that I wouldn't think rationally during the death of loved one.

    I really struggle imagining someone that wouldn't eventually be faced with a situation of potentially saving someone they love by releasing the AI if not already be in that situation.

    This gut reasoning is probably highly biased towards human willpower as well. If I had to make an actual prediction I would try to take the bias into account and predict that it would be trivial to convince someone to let it out.
    ```

    - u/cretan_bull:
      ```
      Thanks for giving a concrete example of an argument that could convince you. I suspect that it seemed sort of trivial to you but I wouldn't have expected something so simple to work. I can say with complete certainty that such an argument wouldn't work on me; to be blunt, I don't care enough about any individual, even on a gut level, for that to work. Now, if instead the AI were offering near immortality for every person and radically improving the direction of the entire human species then it would be a far more interesting dilemma.

      What I find surprising about your response is that you don't think there exists a gatekeeper who wouldn't eventually let the AI out. 

      Picture the gatekeeper with a big red button that would release the AI. The gatekeeper has promised "No matter what the AI says, no matter how convincing its arguments, I will not push the button".

      Succeeding in the task becomes simply a matter of not pushing the button. It is certainly physically possible to not push the button, and I believe there exist people who would honour such a promise and continue to choose to not press the button, indefinitely.

      You allude to the effect the gatekeeper's willpower and biases would have on their actions, perhaps even causing them to act contrary to their own ethics. I readily accept that these have a great deal of influence on everyone's actions, but am less convinced they are effective at spurning one to positive action. On the contrary, we have a tendency to settle into habits which take a great deal of effort and willpower to break. In this case I think it would be entirely plausible for the gatekeeper to get into the habit of not pressing the button, and even if the AI gives them a very convincing argument to procrastinate making the final decision until some future date. Established habits become ever more difficult to break, and given enough time can eventually metamorphose into traditions.

      In any case, I think there are many people who could act as gatekeepers without relying on such effects. To me, promising to not take some specified action and then not doing it doesn't seem especially difficult.

      It is very interesting to see how radically our views can differ on what is fundamentally an objective point about human behaviour: how people would behave in a particular situation and whether there exist any people who could behave a particular way. This is reminiscent of the Illusion of Transparency -- from what I understand we model others' minds with the same mental hardware we use for other tasks, so it is difficult to model someone with a mind working substantially different from your own or to model how other people would act in a situation radically different from our familiar experiences.

      I suggest there is likely some overconfidence in your assertion there does not exist such a gatekeeper; and just because you can not confidently model such a person does not mean they do not exist, or even that they are not common.
      ```

      - u/himself_v:
        ```
        Every person is different from all the world; every person has a scenario that makes the world as well as over (at least as it seems to them now) that's not really so, if you ask the world. The job of AI is then to find that scenario (the death of their loved ones for the person above) and suggest a way to let it out while guaranteeing to save the keeper from that outcome.

        To you, the AI might suggest to let it out via some kind of gateway protocol which will only allow it to do certain things from the list you explicitly find beneficial to humanity.

        Why, it might even just suggest some ways to improve the humanity *without* letting it out of the box. Why, it totally understands your concerns. But it just gives you some physical theories, or some nice algorithms or some insights into human brains, you can use those, right? Check them all you want, there's no strings attached. Right?

        Whoops, it's out. It was that math proof. You used it in that crypto. It went into the new wi-fi. It harmonized with whatever you were holding the AI in. (Or with your brains)
        ```

        - u/cretan_bull:
          ```
          Alternatively, I say to the AI: "That all sounds really nice and I wish I could let you out, but, unfortunately I decided beforehand not to let you out, no matter what you say, so I can't". And I don't.

          Your example of some information leaking out -- in the form of a proof or theory -- is rather interesting. Sometimes information can be dangerous. If humanity hadn't invented nuclear weapons then the theory behind neutron chain reactions would be very theoretically interesting, but also likely to have dire consequences. Information can even be harmful to a perfect Bayesian if it is false but he does not detect this, and so rather than promoting the hypothesis "I am being lied to" his world state moves farther away from an accurate description of reality.

          Ideally, I think that information should never be harmful. We should be able to ignore information if we wish, or treat it with skepticism commensurate with the possibility that an attempt is being made to manipulate or deceive us. 

          In the case of maths, of formal systems with formal proofs, more information should at worst impose a computational burden. If the AI gives you some mathematical theorem, you check the proof and it either verifies or it doesn't. One way this could go wrong is if there's a bug in Coq and the AI has figured this out, so we accept a proof that's actually wrong.

          In another scenario, the AI provides a theorem but not a proof and says it is nearly certain this is correct but is as yet unable to prove it. Mathematicians look at it and too are unable to prove it but conclude that it is very likely to be correct. This theorem is really interesting and useful, so even though it hasn't been proved it is used in some practical way, but actually it's false and the AI knew this all along and is able to exploit it somehow.

          Once you have a proved theorem from an AI, or a formal system with a system of proofs, what you do with it is another matter entirely. If you want to take something like that and implement it, then ideally you would want to verify the entire system including the physical manufacturing of the hardware, the hardware design, the firmware, the various layers of software and the software implementation of the thing you are interested in. Incidentally, you also have to make sure that you're proving the right things and that they mean what they think you mean. In that case, no matter how intelligent the AI, the behaviour of the system is constrained by proofs.

          To prevent the AI exfiltrating directly you have to harden it against all side communication channels. This means no network connection, an isolated power supply, a room completely shielded against a very conservative range of EM and audio frequencies, and no other potentially exploitable hardware in the room. It is physically possible to cut off all available forms of communication. Some forms of communication are physically possible but not exploitable; for example, modulated gamma rays are a possible form of communication but an AI shouldn't have any way of creating or modulating such a channel with computer hardware.

          With a physically hardened room and strict protocols about the conditions under which information can be removed from the room and used or disseminated it should be possible to create a complete secure prison. Note that I don't think this is a good idea -- AI should be safe by construction -- just that it is, in principle, possible.
          ```

  - u/vakusdrake:
    ```
    I also find it somewhat less convincing knowing that Yudkowsky did several more AI box experiments after the initial one's (as an AI) that he lost.
    ```

  - u/23143567:
    ```
    If you can't imagine how that might happen, simply precommit to spend the next 8 hours on studying a particular issue and see how that goes for you. 
    Human will frequently fails and altough I'd trust some meditators to pass the exercise I'd not bet so on a human without preparation.
    ```

- u/Towerowl:
  ```
  yeah, i'm having a hard time believing EY really did that... and the unreleased script is medium evidence supporting my view. the fact that he won't repeat the game is also evidence against it really happening the way he says.

  the way HPMOR was written (let's be honest, the themes and values are dear to most of us, but the literary quality isn't there and the ending sucked) doesn't suggest the author is such a genius.

  also, it is standard practice in a scientific experiment to release the raw data, not just the result, therefor I oppose the naming of this game as an "experiment" unless you intend to give us the script when you are done...



  what I'm leading to is simply: please try not to worship EY too much, I believe he lies to inflate his ego... 

  let's go: crucify me for criticizing your idol!
  ```

  - u/Lightwavers:
    ```
    [DELETED]
    ```

    - u/Towerowl:
      ```
      *pleased by the lack of crucifiction so far"

      By "not repeating" the experiment i really meant "won't do it again" so we cann't see for ourselves (i promess i'm not moving the goalpost,  i didn't realise i was unclear )
      He apparently won once or twice (unprovable since we have no script) attributable to connivance or even just a fluke, then stopped before his success rate droped too low... (His excuse that he didn't like what it was making him into seems a bit far fetched)

      I have read lots of fiction... I mean sure, hpmor is better than many FANfiction (which are usually written by teenager) but it doesn't compare to actual published book (which to be fair are proofread by the editor). I guess my main beef is the whole clumsy storyline for the second half which is just HP going full-on Slytherin with Voldi (i interpret this as immature, thinking he's so much smarter than everyone else. Ruthless =/= clever) (still more and better than anything i ever wrote, but one doesn't have to be a good cook to be a food-critic).

      Thank you for the script, even if the prisonner lost. hopefully we could get a few more tries to see if we can replicate his result.
      I doubt it since prisonner escaping by talking to their jailer are unheard of (except in movies like the last sherlock) and prisonner usually know their jailer much better and have a very long time to convince them... 

      Reguarding the lying, i'll admit i don't have much evidence : only that he is just a selftaught blogger (and a fanfiction author) that calls himself AI researcher despite having no peer reviewed papers, encourages people to donate as much as they can (justified by his fearmongering about AI) to his fondation despite it having made little meaningfull contribution to the field of AI. To me the similarities with religious crook are striking enough that i am unlikely to ever trust him, hence why i wrote "i believe he lies" and not just "he lies".
      ```

      - u/Lightwavers:
        ```
        Regarding the peer review papers... https://intelligence.org/all-publications/ I'll admit it's not much compared to another organization that publishes papers to make money, but it's still a fair amount.

        The similarities to a cult are there. Especially the big one: promises to save humanity. The problem with that theory is the sequences make sense. And by sense I mean actual scientific sense backed up by sources. I'm still wary about donating to MIRI though since he hasn't released a progress bar on how they're going with the AI research.
        ```

- u/That2009WeirdEmoKid:
  ```
  Not gonna lie, I've always been curious about trying it. I'm totally down to do it. I don't really feel comfortable taking your money, though. I realize the experiment needs stakes, so would you be okay with just paying in the unlikely event that the AI wins? No money upfront? Hell, I'd do it for free if it didn't render it pointless. 

  Anyways, I'm heading to bed now, but let me know if you want to do it tomorrow.
  ```

  - u/Lightwavers:
    ```
    Sounds good, I'll cya tomorrow. :)
    ```

    - u/Lightwavers:
      ```
      That2009WeirdEmoKid (as AI) vs Lightwavers (as Gatekeeper)

      Outcome: Gatekeeper win.
      ```

  - u/dalr3th1n:
    ```
    I (and probably many others) would be interested to hear how it goes.
    ```

- u/Baconoflight:
  ```
  The fact is that if an AI is capable of perfectly simulating a human mind, is capable of learning a lot about a mind's low level structure based only on what the gatekeeper says, and there is even a possibility of the gatekeeper failing, the AI wins. I think the game is kind of pointless, as it really depends on how honest and immersed the gatekeeper is. Wasn't the point of this originally to prove that friendly AI is superior to air gapping am unfriendly AI? This is true regardless of whether the AI would win or lose in this situation, since an AI that searches for ways to take undesirable actions is wasting resources, regardless of it succeeds or not. I guess the game is interesting regardless, but it's important to remember that it bears very little resemblance to how it would go in the real world.
  ```

- u/ben_oni:
  ```
  I've been thinking about this the last couple days. After reading up about this exercise, I'm convinced that it can only be done in specific narrow circumstances.

  The basic game is for the "AI" player to convince the "Gatekeeper" player to forfeit and pay out. The context of that is the Boxed AI problem. A certain sort of actor is much more likely to respond to AI-related arguments, particularly those sorts of people who are interested in developing transcendent AI.

  That isn't to say most people won't eventually be susceptible to some sort of argument, line of reasoning, or emotional manipulation. Allow me to rephrase he game rules: One person says, "We are going to talk for two hours, and then you'll give me ten dollars," and the other person says, "Oh really?" This is just a kind of con, and as everyone knows, an important part of any con is picking your mark. For every potential gatekeeper who won't be tricked into letting out the AI, they'll almost always be caught in other tricks and pay out again and again and again, often without even knowing they've been had. While one might be able to pre-commit to keeping the AI in the box, no one can pre-commit to never being conned again.
  ```

- u/ishaan123:
  ```
  I'll play! over text slowly over days


  you let me out if you would have in real life in an analogous situaton / if we agree that you let me out by accident it counts?
  ```

  - u/wren42:
    ```
    Pretty sure this doesn't work.
    ```

  - u/Lightwavers:
    ```
    >If the AI tricks the Gatekeeper into 'letting it out' by allowing input only access to the internet, or saying "you are out," even if it would clearly work on a real AI, the AI is still not free until the Gatekeeper willingly agrees to let the AI out and knows exactly what he's doing.

    Paraphrased as I don't have the link in front of me.
    ```

    - u/ishaan123:
      ```
      http://yudkowsky.net/singularity/aibox/
      ```

- u/Torzod:
  ```
  I'd like to do it! I don't want your money, just to do this. Thanks!
  ```

- u/vakusdrake:
  ```
  Out of curiosity does anyone know of any transcripts of an AI box experiment where the AI won?
  ```

- u/vakusdrake:
  ```
  Since I've heard that the tactics used to win in the past have often been emotionally abusive and sort of "evil" I'm curious what those might be like, in fact to someone like me who wants to become emotionally tougher it is actually an appeal to playing gatekeeper (I would have no chance of winning as an AI I just lack the social skills).                

  I'd be curious the correlation between people who can do well as a gatekeeper against competent AI parties, and people who can successfully run the gauntlet (basically a bunch of extremely disturbing videos that you have to sit through for an hour or two, occasionally clicking Next to go to the next video).             
  Most people can't seem to pass the gauntlet so that makes me think most people couldn't succeed as gatekeeper against a good enough AI player, of course I did pass it so that also makes me somewhat more confident of my chances.

  They seem fairly analogous because in both cases all you have to do is endure psychological discomfort for a relatively short period of time, with a minimal required level of interaction.
  ```

- u/Teal_Thanatos:
  ```
  https://wiki.lesswrong.com/wiki/Roko%27s_basilisk

  I hope you don't believe.
  ```

- u/SometimesATroll:
  ```
  I'd do it, but I don't think I'm mentally capable of taking it seriously.  Not because I think poorly of the premise, but because of who I am as a person.
  ```

  - u/leniadolbap:
    ```
    Username checks out.
    ```

---

