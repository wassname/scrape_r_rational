## Human goals in fiction and reality, and the goals of a strong AI.

### Post:

Eliezer Yudkowsky has written extensively on how all kinds of perfectly reasonable sounding goals become horrible, horrible absurdities when fulfilled by an AI of sufficient intelligence (power, which is equivalent to intelligence, as intelligence is equivalent to power -- I am defining power as the capability to shape the world to your will, and that is also what intelligence is). What he has little remarked on is that all *individual* human goals become horrible, horrible absurdities when fulfilled with sufficient intelligence (or power). He has written that we need to come up with a system of coherent goals to give any strong AI, or it will surely destroy us.

In case anyone has forgotten, the AI goal argument goes that if you give a strong AI any reasonable sounding individual goal, it will twist that goal right into horror. Tell the AI to make every human smile and it will make a virus that makes everyone's facial muscles like the Joker's, tell it to value smiles as an end point and it will carpet the universe with tiny smiley faces at the smallest limits of construction. Tell it to make every human happy and it will give everyone some kind of super effective opium or electrode implants or disease that directly stimulates the pleasure centers. And etcetera, ad infinitum.

My [previous post](https://www.reddit.com/r/rational/comments/5c9t84/too_much_rationality_too_much_intelligence_a_bad/) was about how applying sufficient intelligence to any individual human goal in fiction results in horrible, horrible absurdity. I now believe that this applies to all goals in all situations. Any human goal if fulfilled sufficiently well becomes horrible, because human goals were not designed for any purpose except to aid survival in a long vanished ancestral environment.

Before we go and build a strong AI with a set of novel constructed goals and hope that it will somehow turn out friendly, we should figure out exactly what the goals are of the seven billion general intelligences that are currently wandering around the planet, calling themselves humans. And we should also consider that every goal set may have a state of maximum satisfaction, of maximal power (intelligence) dedicated to its fulfillment beyond which it will become absurd and horrible, and that to design a goal set that will work when fulfilled by greater intelligence (power), the designer must also be super intelligent. What this means is that humans cannot program a goal set that will remain safe under all intelligence conditions -- past a certain point the AI must design its own goals. And there may be no way at all to make that process "safe" for humans, because human goal sets are not perfect and can never be perfect.

What are human goals? I don't know, and I don't think anyone else knows either, but I will now proceed to guess wildly and most likely wrongly based my incomplete knowledge of psychology.

I would speculate that the basic human goal is pleasure, in all its forms. Some might also say it is avoidance of pain, but this bothers me because many humans are perfectly willing to undergo extreme pain in order to acquire greater anticipated pleasure. Indeed, psychology seems to consider the ability to defer pleasure and undergo greater and greater pain to reach it to be a sign of maturity.

What is pleasurable? Here we reach the truly weird part about humans: all kinds of different things, varying wildly between humans, and varying greatly over time in individual humans. Freud claimed (or implied) that all pleasure was sexual pleasure displaced onto something else, but this idea is somewhat uncomfortable to consider, and Freud has been discredited on many things.

What would an average human consider her goals, the things that give her pleasure? Sex, food, safety, comfort, and an almost unending list of smaller things and finer graduations. And even those big ones are not stable. Some may consider lack of safety to be pleasurable, a goal, and do things like skydiving and driving sports cars in an unsafe manner and having unprotected sex. Some value lack of safety so greatly that this goal becomes strongly maladaptive and they kill themselves. Some may claim sex not to be a goal at all, and that they got pleasure from abstinence and/or devotion to some odd religious idea. 

And what are some finer graduations in the bifurcating tree of goals? A person may strongly value, say, reading good fiction, or playing video games, or listening to ever more specific forms of music, or developing skill at something -- no matter how small in this ever branching goal tree -- such as carving a piece of wood into just the right shape, or moving their body in just the right way while dancing, or writing just the right words on a page to convey an idea.

Human goals are an ever branching tree, forever subdividing into finer and finer sub goals. I would like to offer a theory for consideration here: these finer and finer goals are in some way related to available processing power. A creature with low processing power, such as an animal, values only the "big" goals -- survival, sex, food, shelter, etc. When more processing power becomes available these goals subdivide into ever smaller goals, which become ever more important.

And human goals are programmable to some extent. Some goals seem more like aspirations to make a logically derived goal pleasurable, rather than something that is actually pleasurable, things which evolution could never have made pleasurable. This may be wrong however, as there is research that implies that altruism *is* a programmed evolutionary goal. I am going to ignore this for the moment.

What maladaptive things can humans program themselves to value (derive deferred pleasure from)? A human can program herself to value making the world a better place for others, and willingly die in the attempt, experiencing no personal gain from her actions. A human can program herself to value one individual other human over all else, and continue unto termination trying to fulfill this goal. A human can program herself to value an idea, any idea, over all else, such as that the Sabbath should happen on Saturday rather than Sunday, and then willingly die in devotion to that idea.

What other factors effect human goals? I think a big one is boredom, novelty seeking. This is a goal which almost always remains constant -- a human being that does not get bored with something, does not seek novelty, is a human being with a broken mind, content to sit and rock back and forth in the corner forever, never getting bored, never seeking to do anything else.

I think boredom is related to the ever branching tree of human goals. A functioning human can appear to never get bored, but this is a misapprehension caused by not knowing their mind state. In actuality they are following the goal tree out onto ever finer branches, finding pleasure in ever smaller things, their skill in performing some tiny sub action involved in the overall goal, such as turning the wheel just right to take a corner with ever closer to exactly the right amount of speed and the right angle when driving, etc. 

We must understand human goals and how they work before we can safely build a superhuman AI, and it would also be wise to study them in order to determine how to do anything else better. Consider fiction writing. Does the foregoing mean that as a character becomes more intelligent we must focus on ever finer branches of the goal tree, ever finer and smaller things? Would a super intelligent human be concerned with a seeming universe of smaller and smaller things, not in replacement of the prime goals but in addition to them, in service to them, using the acquisition of increased skill at each smaller sub goal to become superhuman at the main goal?

### Comments:

- u/None:
  ```
  You should read a whole lot more psychology and cognitive science before spouting off about what goals are and how our goals work.
  ```

  - u/DerSaidin:
    ```
    > What are human goals? I don't know, and I don't think anyone else knows either, but I will now proceed to guess wildly and most likely wrongly based my incomplete knowledge of psychology.

    Seems like responsible spouting off to me.


    Would you like to contribute a TL;DR-sized summary of key stuff he got wrong?
    ```

    - u/None:
      ```
      I mean, on the one hand, fair enough.  On the other hand, an embodied Bayesian reinforcement learner with multiple reinforcement modalities isn't really gonna have "goals".  "Goal-directedness" is going to be the mode of behavior such an agent engages when it optimizes expected reward with no reward prediction error.  Saying that it *has* a utility function is incorrect, even though given sufficient knowledge of its cognitive structure we should be able to construct or induce one for it.
      ```

    - u/None:
      ```
      [deleted]
      ```

- u/Fredlage:
  ```
  >(Only for some reason he called goals "utility functions." Beats me why.)

  Not his invention: https://en.m.wikipedia.org/wiki/Utility
  ```

---

