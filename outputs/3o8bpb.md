## "If you give Frodo a lightsaber..."

### Post:

[deleted]

### Comments:

- u/EliezerYudkowsky:
  ```
  Depending how similar Clippy's psychology is to my own, and to what extent we're both buffered against the obvious possible ill effects of a speedup, I think we both boost a lot and split the universe between our utility functions.  But only if Clippy is similar enough to me that we have common knowledge that it's a *symmetrical* Prisoner's Dilemma.
  ```

- u/NotAHeroYet:
  ```
  double my thought-speed. the paperclip entity is me in mind and human in body, and unless i'm vastly underestimating myself, i couldn't turn the world into paperclips with that, or even significant nations. It will flood the market with paperclips at this point, but it still hasn't reached the dangerous point. When it dies, People can turn the paperclips back.
  ```

  - u/kaukamieli:
    ```
    It can think and plan. It will probably start with an oil business or something, to get money to make everything paperclips.
    ```

    - u/NotAHeroYet:
      ```
      Yeah, but it won't have the money just yet, and even if it does, it'll need a lot more skill to plan the world's destruction. at the most, it'll be a successful CEO with more... eccentric spending habits. I mean, it might get further, but I doubt it. The demand for paperclips or the supply, depending on how it operates, might skyrocket, but it won't be world-ending or even harmful enough that I would feel morally obligated to turn-down this offer. Now, exponentially boosting it would be foolish, but doubling, trippling, or even quadrupling should be safe.
      ```

- u/fdar:
  ```
  Why is everybody so concerned with nullifying the paperclip entity, to the point they're willing to sacrifice themselves to stop it?

  Even humans that decide their goal is to kill as many people as they can don't usually manage to do that much damage, how much damage would a human-level intelligence paperclip maximizer do?
  ```

  - u/sparr:
    ```
    > how much damage would a human-level intelligence paperclip maximizer do?

    If he's as smart as the average redditor thinks they are? A lot.
    ```

  - u/ishaan123:
    ```
    Given a supernatural thinking speed boost? I should think one could make a ton of money and spend it malevolently, at the very least. (Or, you know, just buy paperclips...) 

    (Not that my answer attempted to nullified it)
    ```

  - u/Chronophilia:
    ```
    Human society is a fragile thing and a clever sociopath can do quite a lot of damage sometimes. Especially one with literal superpowers.

    Though I take your point, no human is going to be turning the universe into paperclips any time soon.
    ```

    - u/Yasuda1986:
      ```
      Even more so since sociopaths are generally good people like everyone else.
      ```

      - u/LiteralHeadCannon:
        ```
        I was going to question this until I realized that I was coming at this from a virtue ethics perspective even though I know that's usually incorrect.
        ```

- u/ishaan123:
  ```
  We trade: Clippy can have its section of the universe for paperclips (which shall naturally be far away from earth) and I will have mine.

  If we ever get to that point. In the mean time there's plenty of room for both humans and lots of paperclips - in fact I think the two rather need each other. We're both better off with respect to maximizing our preferences. Just because we have orthogonal preferences doesn't mean we can't cooperate.

  At first, we can just increase it by a small amount - humans are the ones who make paperclips after all, so Clippy will be a temporary ally so long as I don't let him get *too* powerful. It's in Clippy's best interest to help with ensuring my survival, humanity's survival, and help with ensuring we find a reliable way to enforce our agreement. Plus this way there are two minds working on the problem.

  Meaner strategies include: using my greater knowledge of the situation to kill it, using the fact that I control the situation to play the unfair ultimatum game ("I promise to make you a gazillion paperclips if you cooperate, but I get the rest of the universe") and so on.
  ```

- u/None:
  ```
  Eh, maybe a couple percent, enough to give me an edge but not enough to be a major problem. Also I phone the relevant police on the other side of the world and tell them that I think my estranged twin brother might be a danger to himself and others - he called me and started to rant about turning everything into paper clips. Yes, officer, I know it sounds crazy. That's why I'm worried. I'm not in the country, but I'd appreciate it if you kept him on file. You mean you don't have a record of him? Oh, my. I thought he was joking when he talked about illegally immigrating. Well, I don't know his address, but I can give a physical description."
  ```

  - u/philip1201:
    ```
    I don't think the paperclipper looks like you. It's only as smart as you.
    ```

- u/Frommerman:
  ```
  Iff I know the rules of this scenario and the paperclipper doesn't, I would choose Graham's Number%. I would then use my first-mover advantage to locate and obliterate the paperclipper before a serious amount of damage was done by it. Presumably, it would be unprepared for an attack by an entity whose utility function was, at that point, only concerned with obliterating it.
  ```

  - u/xamueljones:
    ```
    > I would choose Graham's Number

    You'd be trying to kill yourself out of boredom in literally a heartbeat. The speed boost is purely mental and you don't move any faster. You'd be thinking so quickly that the entire world would have looked like it stopped moving and if you can't cancel the speed, then you be bored stiff in the time it takes for your heart to beat even once.
    ```

    - u/RMcD94:
      ```
      Alternatively the universe may have a maximum processing speed which he would hit. Probably smaller than graham's number but still kill self boredom. Because you can't move fast you would probably break your desire to kill yourself before you manage to. 

      You'd spend trillions of years trapped in an unmoving body, I doubt the result of that is anything like human conscious
      ```

    - u/Empiricist_or_not:
      ```
      I take it you've listened to [Sayer EP 6 : A Dreamless sleep](http://geeklyinc.com/sayer-episode-6-a-dreamless-sleep/)
      ```

- u/Rangi42:
  ```
  Can we choose an arbitrary scaling factor for our speed of thought? If so, I pick 0. I'll end up brain-dead, but so will the paperclipper.
  ```

  - u/DCarrier:
    ```
    I pick negative one! I guess I live my life backwards from then on.
    ```

    - u/None:
      ```
      [deleted]
      ```

      - u/Transfuturist:
        ```
        If only we had input authentication on these counterfactual scenarios!
        ```

- u/Jace_MacLeod:
  ```
  I'd go with the maximum mental speed-up that doesn't result in total boredom. Let's say a couple dozen times acceleration, or so. 

  Many people seem to be worried about the paperclipper, but it's important to remember that what makes a superintelligent paperclip-maximizing entity dangerous is the \**superintelligence*\* part. Given effectively infinite power, an entity with a paperclip-maximizing utility function will convert all available matter in the universe into paperclips. But a human-level intelligence, even a fast thinking one? It'll probably just open a factory or something. (Keep it away from nanotechnology and powerful AI, though.)

  Anyway, I'd use my newfound time to study math and eventually enter a lucrative career in finance. I'd probably donate most the money earned towards high-impact research or effective charity, but a portion would go towards hiring private investigators to look into any AI or nanotechnology researchers that seem to be unusually interested in paperclips. (Eliezer Yudkowsky, I'm looking at you.)
  ```

  - u/DaystarEld:
    ```
    I feel like this is the correct answer. If I know Clippy exists, then its Prime Directive will make it very predictable and fairly easy to identify and neutralize. As long as I don't make myself smart enough to design an Artificial Intelligence (which doesn't seem like a possibility with this power), Clippy won't be able to either, which means the damage he can do is minimal and not necessarily even dangerous.
    ```

- u/VVhaleBiologist:
  ```
  If there is no need for me to personally stop the paperclip then I would go with 0%, keeping the paperclip entity as harmless as possible while I gather allies who can help me.
  ```

  - u/None:
    ```
    [deleted]
    ```

    - u/VVhaleBiologist:
      ```
      >If anything, to guarantee a win you should increase the speed by -50%, take the hit for the team.

      The instructions were only about speeding up the brain and not slowing it down. If it's a more reflective bond than that then my thought was to turn to drugs after gaining allies. I'd like to see the paperclip entity try to to anything constructive while tripping balls.
      ```

  - u/kaukamieli:
    ```
    With 0, you should be gathering allies to beat other actual people who right now have gaining money as their goal and who don't care about other things.
    ```

- u/ulyssessword:
  ```
  I'd go for 10x speed, maybe a bit slower so I can keep social contact.  One human-intelligence paperclipper isn't a real threat, and making it effectively 10x as powerful still wouldn't make it be a threat.  There are dozens of people that already want to destroy the world, a few more won't make much of a difference.
  ```

- u/noggin-scratcher:
  ```
  I'm not entirely convinced that just having more time to think is going to make me very much smarter - suspect I'd still find myself constrained by limits on the complexity and depth of my thoughts even with more time to consider.

  Might take a small boost just for those times when being a bit more quickwitted *would* help, but not so much that I start to find conversation annoyingly lag-prone. I doubt Clippy will get very far with that against the combined self-interest of the rest of the human race.
  ```

- u/TennisMaster2:
  ```
  This would be more interesting if the same offer is presented to Clippy!me.  In that scenario I'd bide my time, outsourcing personal assistants to spend all their time working for me searching the internet for word on increased paperclip production.  If I can find him,  and have 90% confidence it's him, I'd make my move.

  My winning condition would be his permanent paralysis, so I can take the boost while he still lives, as per whisper's suggestion.  If the boost only lasts so long as Clippy!me lives, then we should stop talking about it in a public forum.
  ```

  - u/Uncaffeinated:
    ```
    I'd probably start by trying to convince my alt self that paperclips are awesome.

    If you manage to get yourself as an ally, you can crank up to infinity percent and just win.
    ```

- u/Nepene:
  ```
  10

  I then use this to win the randi prize, and tell the public about my evil twin. Paperclippers are noted and destroyed.

  I live the rest of my life in luxury.
  ```

- u/chaosmosis:
  ```
  My ideal path would be to cooperate with the paperclipper to fight against the empty blackness of space. There's far more to lose through than to gain through competition with an equal with superpowers.

  We're not actually equals, however, if its value system is sufficiently inhuman. There are a lot of important unknowns in this scenario. What's the paperclipper's risk aversion? Do paperclips scale linearly in value? Do future paperclips matter more or less than current ones, and by how much?

  Quite possibly, the paperclipper would be able to take advantages of relative weaknesses in my motivational system. I get bored easily, but the paperclipper presumably wouldn't. I would give in to torture, while the paperclipper might not. My chief advantage is that I'm a satisficer, and I don't particularly care whether there are ten quadrillion or twenty quadrillion happy humans. Additionally, because I care about other human beings, it's probably easier for me to cooperate with other humans than for the paperclipper to do so.

  This suggests that I should set my internal clock at a point where my influence is important but not so strong that I could take over the world without significant help from friends. My goal would not be to carry humanity to a new plateau of technology all by myself, but to speed up our current rate of advancement.

  I'd choose to speed up my brain by 5x or so. Interacting with other people would still be bearable at that speed, and I would have a lot of influence but not so much that the paperclipper could destroy the world.
  ```

- u/Chronophilia:
  ```
  I attempt to strike an alliance with the paperclipper. I'll use my powers to help produce paperclips, as long as he uses his to help save lives and improve the lot of humanity. We both benefit from cooperation; two heads are better than one.

  Then I'll go for a 100x speedup. Enough to give us a powerful edge over normal humans and do things that nobody else could do, but not enough that we're unstoppable if the rest of the world decides we're too dangerous to keep alive and unrestrained.
  ```

- u/None:
  ```
  Processing speed of human brains doesn't work that way -- at least we think it doesn't.  Can I choose to draw more samples from my internal generative representations by some factor?
  ```

- u/Sceptically:
  ```
  +3000%. Then I'd ignore the paperclip maximiser. It won't make much headway anyhow, given that it'll be competing with other maximisers and it's only a 30x increase over my base intellect.
  ```

- u/None:
  ```
  [deleted]
  ```

- u/therearetoomanydaves:
  ```
  Without self-replication technology (doesn't need to be nanotech) or a self-improving AI ala Celestia from Friendship is Optimal, the paperclipper is at most an annoyance.  At most it could make enough money to build a bunch of paperclip factories, but it would be competing with the rest of humanity for resources, and its product (paperclips) doesn't bring in income to keep the process going -- it has to earn money elsewhere to keep the paperclip factories running.

  Self-powered self-replicators or self-improving AI with the ability to take actions in the real world are a win scenario for the paperclipper.  Both can be researched (or research can be funded) by the papercliiper without it revealing its aims.
  ```

- u/Uncaffeinated:
  ```
  Go all in. Realistically, the paperclipper will not ever prove a danger, and the benefits to society and to me are well worth the cost.
  ```

---

