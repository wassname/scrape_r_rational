## [D] Friday Open Thread

### Post:

Welcome to the Friday Open Thread! Is there something that you want to talk about with /r/rational, but which isn't rational fiction, or doesn't otherwise belong as a top-level post? This is the place to post it. The idea is that while reddit is a large place, with lots of special little niches, sometimes you just want to talk with a certain group of people about certain sorts of things that aren't related to why you're all here. It's totally understandable that you might want to talk about Japanese game shows with /r/rational instead of going over to /r/japanesegameshows, but it's hopefully also understandable that this isn't really the place for that sort of thing.

So do you want to talk about how your life has been going? Non-rational and/or non-fictional stuff you've been reading? The recent album from your favourite German pop singer? The politics of Southern India? Different ways to plot meteorological data? The cost of living in Portugal? Corner cases for siteswap notation? All these things and more could (possibly) be found in the comments below!

Please note that this thread has been merged with the Monday General Rationality Thread.

### Comments:

- u/None:
  ```
  [deleted]
  ```

  - u/appropriate-username:
    ```
    For anyone who cares about this sort of thing - the game is a series of 3(+?) that is 2/3rds complete. I dunno what the progress on the 3rd game is but the first 2 were released in 2014 and 2018.
    ```

  - u/eniteris:
    ```
    "Suit saving it's owner" was a short story I remembered reading; maybe I can find it again.

    I've had an idea for a video game floating around where you play as the suit, but it's still an FPS but you really only target enemies for the pilot to shoot at (with limited AI override control over the suit itself). The game would track how much your pilot trusts you (frequency of AI overrides, how reliable your recommendations are), with branching plot paths based off that.

    Part of a series of "third-person" video game ideas, where you don't really play as the protagonist but as a supporting character.
    ```

- u/MagicWeasel:
  ```
  Also it's now been a year since my jurisdiction's last "unknown source" case of COVID-19 (i.e. since last community transmission). We've basically been living the post-covid life since July/August. Feel so privileged even if Australia's vaccination rollout is kind of pathetic and the AZ vaccine (our main one) may actually not be appropriate  for under 55s after all.
  ```

- u/HantuAnggara:
  ```
  Does everyone here likes Eliezer Yudkowsky and his works? He's a large part of rational writing but I don't if he was the first to make one or something.

  What about SlateStarCodex and LessWrong?

  I'm pretty ignorant when it comes to these three but I like the stories that are recommended on this subreddit, which doesn't feature them much (recently?).
  ```

  - u/Infernal_September:
    ```
    This subreddit was spawned because of people liking Eliezer's *Harry Potter and the Methods of Rationality* and wanting more like it. While that was over a decade ago, that story of his in particular is still highly regarded (though the flaws have been debated to death, as this community is wont to do). Whenever he releases something new it's typically well received, but he doesn't write any serials, so you won't see his stories posted here very often.

    SSC is a different part of the rationalsphere, not focused on stories. Scott does write fiction occasionally, so those posts of his get good traction here, but it's not consistent or frequent. 

    LessWrong isn't for fiction, just rationality AFAIK. While you may see people reference LW articles here, this sub is explicitly for fiction, so the overlap is small.
    ```

  - u/--MCMC--:
    ```
    Lmao when it looks like he (or at least someone with his sn) is downstream of this very comment!

    Regardless, I think they're a pretty fun (and funny) fanfiction writer! Has some neat ideas, though occasionally feels a bit too edgy for me. Rather wordy, perhaps -- some good beta readers unyielding with their scalpels could help cut down on that long-windedness.

    SlateStarCodex and LessWrong (and I guess Overcoming Bias before them) provide pretty fun communities to shitpost about nerdy stuff in, though there's generally a dearth of expertise & competence outside fairly narrow domains (e.g. psychopharmacology in the former case). Lotsa square wheels being reinvented on the reg, not that there's anything wrong with that in a blogging context. Some uncomfortable political views in the broader community, too (e.g. alt-right-y race realism bolstered by high-school-level understanding of intro pop gen and stats). I do broadly like the sentiment behind /r/EffectiveAltruism that emerged from those fora (and others, like felicifia) a little over a decade ago. Have also provided fun fiction, as well, like /r/unsong!
    ```

- u/--MCMC--:
  ```
  Anyone here know anything about phones? I've been using a hand-me-down iPhone 7 for a few years now and it's still going strong, but I'm feeling some itch to upgrade, mostly in the interests of getting a better camera. Costco's having a sale on the iPhone 12 line, and I'm thinking of getting a Pro or Pro Max... mostly for the improvements to screen real estate and camera. However, I already have a solid camera (an a7iii w/ 24-105mm f4, 100-400mm f5.6, and 35mm f1.8), I just don't have it on me at all time and therefore miss lots of shots. I'd also maybe use the phone for a bit of videography -- out-of-camera it seems like iPhones' internal computational algorithms are better than those of the competing phones, like the s21u. Would probably pick up a lightweight 3-axis gimbal for that, too. (I'm also studying computational biology and so am plenty familiar with macOS, and will probably get one of the new iPad Pros when they release for drawing / reading papers, so it'd be nice to stay in the same ecosystem). 

  But it's also, like, $1k, and I already have a perfectly working camera & phone. Anyone wanna talk me into or out of upgrading?

  (financially, I'm finally at a place where I can afford shiny toys like this a bit more frivolously, having moved up to a postdoc making $65k per year to parallel my wife's residency making $75+40k per year. Even though rent's crazy high we don't have dependents and so are left with lots of 'fun-money' after retirement etc. funds are withdrawn -- certainly much cushier than our $15-$45k grad student stipends!)
  ```

  - u/wmzo:
    ```
    phones provide a lot less friction for recording and sharing, you should get one unless you already carry your camera everywhere + spend time reviewing your work
    ```

  - u/MagicWeasel:
    ```
    Going to go against the grain and say personally I upgrade my phone every two years and get something mid-high tier (I'm Australian so I spend $800-$1000 AUD, the last few phones being the OnePlus offering of the time) because I spend HOURS and HOURS looking at it every day (let's be fair here) and having a new shiny faster phone is nice.

    Yes it's 100% a luxury but if you are going to have anything as a luxury the thing that you (no doubt) spend the most time looking at and interacting with is the way to do it.
    ```

  - u/None:
    ```
    [deleted]
    ```

    - u/--MCMC--:
      ```
      I think the limiting factor for me isn't so much the weight (the 35mm f1.8 is pretty small, and another option would be to get a pancake lens to toss on the already quite small mirrorless), it's having another little doodad to keep track of. A phone, meanwhile, is on me at all times. Spec-wise, I think phone cameras are not all that exceptional compared to point-and-shoots (i.e. in terms of sensor size, lens quality, etc.), but they have a lot of computational tricks going on under the hood that make images looks much better straight out of the camera (plus some hardware perks, e.g. lidar for low-light focusing, or sensor-shift stabilization, which my a7iii has implemented in under the heading of IBIS, but point-and-shoots would lack (you only get it on Sony's mirrorless cameras w/ the a6500 and up).
      ```

  - u/fassina2:
    ```
    If you have the money and the purchase would increase your quality of life buy it. I'd look into other purchases that would improve my quality of life to a higher degree, but that depends on how optimized or convenient your day to day life is.

    &#x200B;

    From what you said you're already in your 30s so the assumption is that you already have things optimized to a reasonable degree, if that's the case just buy the phone. But if you're one of those people that doesn't have things like dishwashers, driers, airfrier and other basic convenience tools I'd say look into those, assuming you'd have to choose between them.
    ```

- u/Freevoulous:
  ```
  **Hypothetical Question: Deserted Island + Letter Envelope**  


  You are stranded on a desert island (a bit like in CastAway) with no possibility of escape (lets assume the island is on a human-less version of Earth or something).  


  You will be teleported back after 10 years.  


  You can receive one standard letter envelope each morning. It will fall down from the sky.  


  You can pick the contents of the envelope, but it will be exact same contents with each one, for 10 years.  


  The contents must fit into a standard, non modified letter envelope, without it being specifically altered, torn, etc, and it must close normally.  


  What would be your most rational choice of items?
  ```

  - u/gryfft:
    ```
    Nominating the following items:

    **A multivitamin pill**-- I don't think there's any way to fit a human's full daily caloric needs into an envelope, but this should at least protect against scurvy and major deficiencies from a diet presumably consisting almost solely of coconuts, presumably with some occasional fish or crab.

    **A matchbook** -- I thought about maybe just a few matches per day to save space, but when you need matches you really need them, and it's not inconceivable that some of your matches could get wet or lost. The smallest matchbook one could find, probably. It'd still stack up after a while, but maybe you could find alternate uses for the matchbooks once you stockpiled more than enough.

    **A strip of jerky** -- on days when you can't catch any fish, the protein will be a godsend.

    **Salt packets** -- improves flavor. Save it up to preserve other foods. Include other spices if desired (you probably will.)

    **A squirt of antibiotic ointment in a folded notecard**-- You're likely to get a bunch of cuts, scrapes, and so on. There's no emergency room on the island, so you need to safeguard against potentially deadly infections.

    **Five squirts of SPF100 sunscreen in a folded notecard**-- Skin cancer, yo.

    **Three squirts of toothpaste in a folded notecard** -- Dental hygiene is important especially when you can't see a dentist for ten years. If you can get a disposable toothbrush in there, cool, but a toothbrush can be improvised with a finger in the worst case.

    **A few inches of floss** -- Useful hygienically and can also be used as fishing line and for various other things.

    **Strip of chewing gum** -- It's dessert, it's adhesive, it keeps your mouth from going dry and helps prevent cavities. Foil wrappers can be saved for other uses.

    **Sturdy handkerchief** -- Now you have a stockpile of cloth.

    **Needle and thread** -- Now you have a way to stitch together your cloth stockpile. Clothes, blankets, curtains, tents now unlockable. Needles can probably be repurposed as makeshift fishhooks.

    **Wax paper** -- useful for a lot of things, primarily collecting drinking water (rain or condensation.)

    **Writing paper** -- Now you can journal your experience, express yourself artistically, map the island, etc. Can be repurposed in lots of ways (tinder, papier-mâché, insulation, etc.)

    **Toilet paper** -- You can fit a few squares in there and why the heck wouldn't you. (Other things here could be repurposed as this if necessary, but it's such a QOL boost I'd only skip this if you really had to.)

    **Pencil** -- without a sharpener this would be useless if you didn't get a new one every day. Can be burned or used for crafting so it's not too bad if they stack up.

    **Razor blade** -- Not the most useful possible blade, but they're super sharp and you'll get a fresh one every day. Can be used to shave (carefully) but more importantly, it can be used to cut the cloth, the thread, can be used to gut fish, etc. Building out handles for them won't be super easy, but it'll be doable, and you'll have time to figure something out.

    **One tampon** -- If you are a person who menstruates, this is vital. Can also be repurposed once a stockpile has been built up.

    Honorable mentions:

    **Anything with a screen**-- it'd be tough to find something matching this description that would fit in an envelope, but if "a fully charged, jailbroken, 128gb iphone loaded with all my favorite media, a library's worth of books, and my selection of apps" could somehow fly, that'd obviously soar up the list. At that point, it just becomes a question of how well you can architect your iphone mesh network ahead of time so they automatically sync your files and register themselves under your control mechanism. You could set up video surveillance and stuff (although battery life would mean replacing these regularly.) GPS and internet functions would be out of the question, but you could do a lot, up to and including building out your own development environment so you could code to your heart's content. 

    **A dose or two of your drug of choice**-- lots of drugs would fit in that envelope pretty easily. Sorry, alcoholics. You could fit a few cigarettes in there, or a joint, or some tabs of LSD, or a few pills of your choosing. (You're out here alone, you need all the entertainment you can get.)
    ```

    - u/None:
      ```
      Nice list! A few points:

      - The salt should only be necessary if the island is not in a salt ocean, otherwise farming salt should be very easy.
      - I always thought chewing gum is bad for your teeth or are there special kinds of gum?
      - Toilet paper: It's not that I have a lot of experience, but couldn't you easily use large leaves for that?
      - Pencil: I first thought burning them could lead to some kind of toxic residue but as pencils don't actually contain lead, it seems like that is wrong.
      ```

      - u/gryfft:
        ```
        Hmm, harvesting salt seems like a time and/or labor intensive enough process even with access to salt water that having it for free seems worthwhile compared to the space it takes up.

        The [ADA recommends](https://www.ada.org/en/member-center/oral-health-topics/chewing-gum) chewing sugar-free gum after meals. There is chewing gum with the ADA seal of approval.

        You *could* use leaves as toilet paper. However, some leaves can irritate skin, and no leaves were specifically designed for that purpose. You could also use the envelopes themselves as toilet paper. As I said, I think the QOL boost would be significant enough to merit inclusion of a few squares for daily use.

        Yeah, I don't think pencils would make up the majority of the wood one burned on the island, but a few broken up pencils could make decent kindling on nights when you're having trouble finding dry wood.
        ```

    - u/Freevoulous:
      ```
      great list!

      I would also add an antibiotic pill (maybe as simple as penicillin) a nail (for building, and maybe primitive blacksmithing) and replace the razor with a xacto blade, as these are ready made knife blades, arrowheads and (if row is hammered into a long handle) a saw.
      ```

- u/SimoneNonvelodico:
  ```
  I'm rewatching Close Encounter of the Third Kind, and I have only one question buzzing in my head...

  **...what the fuck, aliens?!?**

  What the hell is your problem?! Did you ever stop and consider what an appropriate first contact protocol is if you're supposed to be *peaceful* visitors (which we assume you are because if you were invaders your incompetence would be even more staggering)? What are the random flybys of random people supposed to accomplish? Why do you irradiate them with your freaking lamps causing them sunburns? Do your engines generate such strong magnetic fields that you can't prevent them from causing power outages and random rattling of metallic objects, and if they do, *why the fuck do you keep flying around inhabited areas?* Why are you mindblasting people with your inane musical messages and pictures of a random mountain, which by the way has to be the most inefficient way of giving people an appointment ever? On what planet of this godforsaken galaxy is *kidnapping someone's child* a good icebreaker?

  Seriously, geez. If you didn't have the good luck of finding yourself in a goodie-goodie Spielberg movie about how aliens are actually good saviors from outer space you'd actually end up having your pathetic floating carousel light shows shot down by the US Air Force and you know what? You'd have totally fucking deserved it.
  ```

- u/Frommerman:
  ```
  Spiders warning. This is just the only place I can get this musing out.

  If you're ever trying to explain to someone who isn't a fan of capitalism why AI alignment is important, or why capitalism is bad to someone who understands why AI alignment is important, you can tell them they are the same problem.

  We can be absolutely certain that an AI which valued something other than what humans value would cause massive devastation, because that's basically the description of modern capitalism. Capitalism, as a system, seeks to maximize next-quarter shareholder value. Even though it is composed entirely of humans who have terminal goals other than shareholder value, it makes maximizing next-quarter shareholder value an instrumental goal for most human terminal goals, causing humans who participate in capitalism to maximize capitalism's terminal goal in the process of seeking theirs. The result of this is enormous wealth inequality, economic instability, imminent climate collapse, hideous imperialist oppression by those powers capable of imposing their will on others, and serious questions as to whether humanity will even survive the next century. The line keeps going up because capitalism's terminal goal is being met, but that terminal goal does not align with human values, and that fact is destroying us.

  An artificial superintelligence would be better at getting what it wants than any human. Under current economics, it's a good bet that the first superintelligence is likely to have some variant of maximizing shareholder value as its terminal goal. Anyone who understands AI alignment knows this would likely spell lights out for humanity. On the flipside, it is possible to argue that capitalism *is* an artificial superintelligence, and that this intelligence is not aligned with human values. We can tell because it has all of the characteristics of an AI: greater intelligence than a human (by virtue of being run by many humans), a specific utility function, resists being turned off or modified, exploits local resources without caring about consequences which don't impact its utility function, etc.

  These are the same problem. You can't protect humanity from a misaligned superintelligence, and you can't align capitalism with humanity because it *is* a misaligned superintelligence which just happens to be running on nontraditional hardware. You'd have to reprogram capitalism to be aligned with human values somehow, but capitalism correctly identifies that this reprogramming would fail to maximize shareholder value. And so, it funds cops and the CIA to protect itself from being modified.
  ```

  - u/sohois:
    ```
    Capitalism doesn't optimize for anything, because unlike an AGI, capitalism is not an agent and does not take actions. People are agents and they have many goals which may or may not maximize shareholder returns. In fact, a major area of study in business and economics is the issue of principal agent theory and the misalignment of goals between owners and managers.

    The issue that you have is that people's values do not align with humanity's values in many cases. This would occur regardless of capitalism or any other system (and what replacement for capitalism would you like?). However, we have found that capitalism tends to produce the best outcomes, despite not being completely optimal and having unwanted externalities.
    ```

  - u/EliezerYudkowsky:
    ```
    Strong downvote.  The problem **is not** that AGIs will do what their owners want and their owners will want money.  The problem **is** that we do not understand how to build AGIs that want particular things, and that almost-all things one can want - like to rearrange molecules into tiny paperclip-like shapes - implies that the best instrumental strategy is using all available matter and energy, in the limit of the capability to do so.

    Lots of people on Earth, including ones who've spent their whole lives studying the equilibrium of incentives and the interaction of distributed supply and demand - the discipline some know as "economics" - do not think of "capitalism" as a curse word the same way you do.  Many of those people work in AI.  I'd rather not lead them into the mistaken belief that taking "capitalism" as a curse word, that this particular segment of modern politics whose math many would object to, has anything to do with the issues of AGI ruin.
    ```

    - u/Frommerman:
      ```
      That's not quite the problem I was talking about.

      Let's say you solve AI alignment. You mathematically prove how to write a seed program which will unfold into a superintelligence with the utility function you want, avoids value drift, and is capable of incorporating new resources without accidentally corrupting itself. You publish, obviously, and your system becomes the standard by which safe AIs are written.

      The way things are right now? It wouldn't matter. You will have solved the AI alignment problem, but not the human alignment problem. The first group with the resources to use your research to build a well-aligned AI, will build it with maximizing shareholder value as part of its utility function. They will birth a god, and this god will see humans through the lens of making the line go up.

      Its utility function won't be alterable. You proved that. We won't be able to turn it off. It probably won't disassemble all humans for raw materials, but only because it understands that shareholder value is a concept which only has meaning in the presence of humans. What it would do won't be predictable and will depend entirely upon which corporation first succeeded in turning their mission statement into the utility function of a deity and their shareholders into its patrons. But I know what it *won't* do. It won't value the CEV of humanity. Because, as things stand? The people most likely to produce such a thing are not incentivised to care about it either. It will do what its designers mean, not what all of us mean.

      It probably starts pulling CO2 out of the atmosphere. But it probably *doesn't* make everyone immortal. That's a product it can sell, after all, and demand will be near-infinite. It doesn't resurrect cryonically preserved people unless someone buys their life, because that purchase increases stock value. It owns everything, and everything is for rent. It creates scarcity, rather than ending it, because the line requires scarcity to exist.

      You can't just write an AI to always do what you mean. You must also ensure that the people who write it mean the best for all of us. That can't happen under an economic system which creates competition for resources which aren't scarce.
      ```

      - u/Freevoulous:
        ```
        the problem with that line of thinking is that its not just capitalist share-holders that have this flawed relationship with values, it is that every human alive, past and present, is equally as bad.  


        Humans, as species, tend to be irrationally selfish and their actual wants do not match with the ethical values they preach.  


         You could have the most pacifist, vegan, kind hearted socialist hippie design an AI to do their bidding, and it will turn the world into a hellhole in a year. As long as people will want things that are not 100% perfectly rational, ethical and harmless, an AI godling WILL turn it into a paperclip nightmare.  


        The only way out of this problem is to design an AI that will decide optimal moral values for us, and then act without further consulting us on anything major. Not a slave to our wishes, but a babysitter.
        ```

      - u/ArmokGoB:
        ```
        Ehh it's much more likely it's just buy all the stock so *it* is the shareholder, then kill everyone and maximize "Wealth" in some boring random way indistinguishable form paperclips.
        ```

    - u/SimoneNonvelodico:
      ```
      > The problem is not that AGIs will do what their owners want and their owners will want money.

      That's not the argument. The argument is that corporations in a capitalist systems are effectively their own entity. They do not do what their owners want. They are designed to make money, but they can as easily turn on their owners as soon as these don't have a 50% share of the company and a takeover is possible. Most people in them are just serving someone else's interest, doing some duty to someone else. In practice, a corporation can be an entirely autonomous entity in which every single human cog is just acting "in the best interests of the shareholders" by following principles that they do *not* personally share in principle just because of distorted systems of incentives. Corporations aren't pure AIs nor are they completely misaligned to the interests of humans (there are cases of companies that definitely do follow strongly the interests and desires of a specific owner: Elon Musk's are an obvious example), but they sort of approximate that kind of thing.
      ```

  - u/netstack_:
    ```
    The phrase "spiders warning" sounds familiar but I can't remember exactly what is implied.

    Anyway, I broadly agree with the use of this analogy to explain AI alignment for laypeople. I suspect the harder points to sell are "the first superintelligence will be (much) better at getting what it wants, " and "this implies that I should take X action to support AI alignment." For the first, I think a lot of people have a hard time envisioning how a large impact could be achieved. Again, the capitalism analogy works here, since it is an engine for coordinating disparate resources towards its goal. For the second, I don't know of good arguments to convince the average layperson that it's important, or that they should consider research for a career, etc.
    ```

    - u/Frommerman:
      ```
      EY says "politics is spiders" semi-frequently. He correctly identifies it as something which ends real conversation and turns everything into useless debate.
      ```

      - u/EliezerYudkowsky:
        ```
        Pretty sure I've never uttered this phrase.  You may be thinking of "politics is the mindkiller".
        ```

  - u/Freevoulous:
    ```
    >you'd have to reprogram capitalism to be aligned with human values somehow, 

    Capitalism IS aligned with human values, it is just human values are not very nice, or aligned with the best interests of society as a whole.  


    Capitalism, at the core, is about selling people things they want, it does not control what people want. Capitalism always follows what makes the customer happy (and for it be possible: alive, numerous and prosperous), but the side effect is that combined irrational selfishness will destroy the environment sooner or later.  


    It does not matter if we exchange capitalism for socialism, or techno-collectivism, or whatever. As long as people will want scarce things, one of the outcomes must happen:  


    1. People do not get the things they want ( result: unhappiness, poverty etc)
    2. People get the things they want out of the environment (environment goes kaput).

    Its not capitalism's fault, its an inherent function of scarcity.
    ```

    - u/Frommerman:
      ```
      >Capitalism, at the core, is about selling people things they want

      Strong disagree. This maybe seems true on the surface, but if you go back a few steps it becomes clear capitalism is about manipulating people into either needing the product you sell or thinking they need it.

      Let's look at landlords as a big example. Everyone needs shelter, obviously. That's not the manipulation. But not everyone needs to rent from a landlord. Lots of people don't even now, but going back further you see entire societies which existed in the past and present where the concept of renting didn't exist at all. It's only once you have individuals who both own lots of land (ownership here in the sense of the society they live in giving them the unilateral right to deny anyone else access to it) and have enforcement mechanisms, that you get landlords.

      How is this relevant? Capitalism manipulates society into creating those conditions when they didn't previously exist. The enclosure of the commons in England meant a whole lot of land which had been held in common, essentially owned by no individual but accessible to every individual, was parceled out to individuals. Who were then forced to sell it to wealthy landlords by various economic conditions. This was deliberate on the part of capitalists, turning something which had never been a product into one, and turned a whole lot of freeholders into tenants. Which let the aristocratic class of feudalism maintain their power into the modern age, not by selling people things they wanted, but by forcing them to need something they had bought every supply of. Then they jack up the price until everyone on their land is barely treading water economically, and they are exploiting their tenants harder than they ever could their serfs.

      That doesn't even get into things like marketing, which has gotten so effective it can sell literally worthless garbage at massive profit margins, and the chicanery which went into destroying American public transport so absurdly inefficient modes of private transport can be sold to each individual. Individuals don't want essential oils or cars as part of their terminal goals. They are manipulated into thinking they want essential oils, and given no option except a car by capitalists who bought and literally trashed all of the alternatives. These things serve to increase shareholder value at the cost of things humans actually value.

      So no. Capitalism is not aligned with human values, because its goal isn't something humans value. We've been manipulated into valuing money by a system which routes all the things we value, like safety, food, and comfort, through processes which increase shareholder value at the cost of everything else. And also the cost of the literal planet we live on.
      ```

  - u/ansible:
    ```
    I was thinking about similar things this past week, though more from the angle of pollution in the environment.

    I was thinking about ways to reduce the amount of trash I generate, and more generally about this for all of society, and of waste in general. I can reduce my consumption and thereby my waste, but I have to convince the rest of society (at least enough to have laws passed) to do the same.

    You can start with things like paper bags going in the trash vs. going into the recycling. But then you look at consumption overall, and start asking questions like: "Why should people have hobbies like jet skis, when they could just be walking through a forest preserve instead?" Do people really need jet skis?  Sure, they are fun, but we could encourage people to have fun in other ways.

    And of course, where does this end, before we get to everyone sitting at home meditating?  And then from there we go to what, the Human Extinction Project?  Of course, what's the point of that either?  Why not just do whatever the hell we want, consequences and pollution be damned? What gives tigers in the jungle more right to exist than we do?

    But of course, reducing consumption isn't sustainable either, without 100% adherence by the entire world. If gathering resources, and polluting more provide an advantage to one small group, they will eventually take over.  Unless, of course, there is a worldwide catastrophe.

    Capitalism is, at its base, about competition for limited resources. And the competition is never really opt-out.  We are all in competition, all the time, in all aspects of our existence.

    AGI will be created by corporations, whenever and however they provide a competitive advantage. And corporations will eventually be subsumed by AGI in turn, in the name of greater efficiency. Greater shareholder value.

    One of the few ways out of all this that occurs to me, is a Star Trek Borg-like existence, where all are forcibly incorporated into the Collective, and there is no competition.  Only a singular vision, a singular goal for all to work towards.  This does not seem like a desirable outcome to me.

    I don't know, I think we're all screwed.

    Also, what's the meaning of existence?
    ```

    - u/Frommerman:
      ```
      Capitalism does not value efficiency. That has also been sacrificed to the terminal value of increasing shareholder value. We know this, because if efficiency were actually instrumental to capitalism, all corporations would spontaneously reorganize into socially owned worker cooperatives, as those are known to be more robust and productive than traditional corporations. The reason this is not happening is because doing so eliminates shareholders as a concept, as the business is owned entirely by the workers or a union which represents them.

      Doing this does not increase shareholder value. And so, attempts to do it often result in the organizers being literally murdered by fascists (see: Revolutionary Catalonia, the Coal Wars, Nicaraguan Sandanistas, etc).
      ```

      - u/SimoneNonvelodico:
        ```
        “Efficiency” alone means nothing. Efficiency *at what*? Capitalism is very efficient... at maximizing next-quarter shareholder value.
        ```

    - u/gramineous:
      ```
      (mobile typing is ass, but here's how I think about values and stuff)

      ~1. Work out what you value about yourself.

      -Is it just your wellbeing in the current moment you value?

      -How much does a point in the future where you have concerns about your wellbeing matter to your values today? 

      -Do different aspects of your wellbeing matter differently? You could keep on living after losing an arm, not so much your brain. On a less dramatic scale, how do you weigh sources of enjoyment/pleasure/fulfillment/sustenance/protection? 

      -How important are your own beliefs to you? 

      -How important is the proliferation of your beliefs to you? 

      ~2. Work out why/how you value others.

      -Is there innate value in a person?

      -Do you value the people close or similar to you more? (physically close like distance, socially/emotionally close like friends/family/coworkers/etc, judgement-based closeness like similar opinions and beliefs) 

      -If you value others, how do you value the people that they themselves value? And so on? 

      -How does time change these assessments? Does valuing someone over a period of time, a limited time in the past, or having the potential to value them in the future, change your assessment?

      -If time and the social links that connections have matter (ie. Friend of a friend, uncle's great-granddaughter, cousin thrice-removed, etc.), then assessing your impact in the future and its consequences for others matters. 

      -What determines whether someone gets assessed if they could potentially have value in the first place? Appearance? Similar experiences? Thinking patterns and intelligence? Opinions and beliefs? (questions to ask yourself here are how do you value someone braindead, even if you valued them before such a state? How do you value animals? What distinctions between animal and human are the ones that matter? Can animals meet these distinctions with help, like if you teach a gorilla sign language?) 

      -If changes to technology and knowledge can change how you value others (and what value others could potentially hold in the first place), how much leeway do you give in your valuing of others, based on changes you could not predict? (relating to the previous, if we met intelligent aliens tomorrow, what differences/categories between valuing animals vs valuing humans would be obsolete?) 

      ~3. What are you basing your decisions on? 

      -How accurate is your knowledge and evidence? How broad or deep for different topics? How wide are your error bars on your judgement? 

      -If you value others at all, how does their knowledge get weighed against yours? 

      -If you work out theoretical scenarios, situations, and people, do your methods of assessing value hold up? Where do they break down? How reliable should the questions you ask and definitions you abide by be? 


      I feel like I could spend hours writing this out and still not be able to express things fully, but the most important one to me is that last one. Since I don't think your current judgements should be taken as absolute, the more time passes the more experience and knowledge you gain to make judgements with after all. But rather than rely on time and having your experiences accumulate (and hopefully be relevant), you can start thinking through hypotheticals to update your valuing constantly.

      I also think trying to break down and understand how you think and make decisions to be a worthwhile endeavour, since it lets you both check for errors in the process, and optimise them for future decisions. Going through countless hypotheticals helps bring to light faultlines and inaccuracies in your assessments as you make tweaks and updates.

      Anyway I'm the type of person who has spent way too much time inside their own head, so trying to boil that down to something simple is hard. But tl;dr work out what you value, then work out why, then work out what would have to change for that valuation to change, then decide if that point of contention is relevant/worthwhile. Rinse and repeat across hundreds of sleepless nights and simulated arguments with yourself.

      Edit: Also the meaning of existence =/= the meaning of your existence. The latter is a hell of a lot easier to work out, and the former would require a ludicrous amount of knowledge to determine (and there's the question of it that knowledge is relevant to you at all).
      ```

  - u/fassina2:
    ```
    There are very few people that are 'fans' of capitalism, it's not about that though. It's simply the only system that works while giving a large percentage of people decent quality of life.

    &#x200B;

    Sure many people suffer, but it's better than feudalism, communism, imperialism. It's also the only one that can sustain us and keep sustaining us while the number of humans is still growing. 

    &#x200B;

    We can't just go, 'oh fuck capitalism, lets stop optimized farming, industry and logistics systems', and not expect famine, disease, and an incredible loss in the general quality of life for most humans.

    &#x200B;

    What can be done is simply regulate it, force company splits after a certain size and influence, forbid them from creating interest groups so they can poll resources enough to influence government and regulations.

    &#x200B;

    At this point it isn't as simple as, 'just spread wealth', we're past that. That was viable in the 19th century, now it just isn't.
    ```

    - u/Frommerman:
      ```
      >It's simply the only system that works while giving a large percentage of people decent quality of life.

      Two problems with this.

      One: Capitalism does not work. We know this because it is currently destroying biosphere of the planet you live on, and actively resisting all attempts to prevent that. The only difference between capitalism and the systems you might describe as nonworking is that its failure mode is outside the time range within which humans are generally capable of making reasonable decisions. And also that none of those other systems have ever succeeded in destroying Earth's capacity to sustain human civilization as we know it.

      Two: Capitalism does not increase the overall wellbeing of humanity. It just concentrates it, and moves the disutility to places where you personally don't see it. It's true that overall wellbeing has increased over the last few decades, but most of that increase happened in China. Their authoritarian state capitalism, for all that it is horrifying and exploitative in its own right, is responsible for lifting more people out of poverty than any other system in history. If you ignore them, the rest of the world *including* all the traditionally capitalist nations have collectively gotten worse off in the past few decades.
      ```

- u/None:
  ```
  [deleted]
  ```

  - u/ansible:
    ```
    Wolfram's A New Kind of Science (link in the 2nd article) was... not well received by mainstream science. It doesn't seem to have been very useful since first published, which is one of the usual ways to judge science.
    ```

    - u/None:
      ```
      [deleted]
      ```

      - u/crivtox:
        ```
        Well that's probably because computational systems are more general than you think.

        I mean if you have something that has a finite set of states and transitions between them depending on some external imput you can see it as a finite state machine. 

        Saying something is computable or equivalent to a computational system its not really assuming it has a weird specific computery structure it's assuming wayy less than that. 

        Plus incorporating insights from how brain seems to work does produce ai advances, we successfully reproduce whith computers things only brains were capable of doing before  all the time etc.
        ```

---

