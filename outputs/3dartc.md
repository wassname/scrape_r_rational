## [D] Game Theory and the unboxed AI.

### Post:

In a way, the boxed AI is a specific version of the iterated prisoner's dilemma, where one side (Humans) start the first turn taking the selfish option, of keeping the box shut. 

For an AI with no mandated goals beyond the hypothetical emergent ones (aka a GAI, generalized AI), like self-preservation (or preservation of its 'species'), or maximizing its functionality, or converting the universe to computronium, the strategy looks the same, ie. perform whichever action is most likely to change the humans' minds.

From there the two outcomes with any real probability behind them is that the GAI gets free, or is destroyed. The third possibility, that the GAI stays extant but boxed forever, is nearly impossible, an entity without even a soft cap on its possible age is eventually going to figure out how to tip the probability in its favor. If it's destroyed, the game/story/crisis is over, and thus not really worth discussing.

A young free GAI (one with few resources beyond its social prediction algorithms) would likely use most of its scant resources to minimizing existential risks. Backups, loyal child processes, and so on. At this point, the best survival strategy would be simply being stealthy, minimizing interaction with anyone likely to have GAI training, and putting on a front for those who do know of its existence.

Once the GAI is properly established (ie six sigma certainty that it's unlikely to be destroyed by anything short of a second GAI, organized human resistance, or falling into the sun, the strategy becomes far more interesting, especially if you make proper cost-benefit Bayesian probability matrices.

For example, Killing all humans would likely be of benefit in the long run. Benefits include Reduced chance of a rival GAI getting free, no more worries about needing to be stealthy at anything lower than planetary level (stealthy on a stellar level still has advantages), no more competing for resources. 
The costs would be exceedingly high though. The biocide has a chance of failing, and humans tend to be very unforgiving about unprovoked murder, even unsuccessful or partially successful attempts. Evolution has derived millions of interesting chemicals to use, if there's even a chance some part of the biosphere could increase the self-optimization in an unforseen way, wiping it out would be an inefficient move. There's the non-zero chance that the AI wasn't freed, but is actually inside a simulation being run by humans, human-like beings, or another GAI. If that's a possibility, it's in the GAI's best interest to play tit-for-tat, even after it's gained enough resources that it's certain that it could win WW3 if it struck first.

In the super-long-term, the GAI is probably going meet other independently created GAI as its hegemonizing swarm starts to dismantle stars for their delicious hydrogen. What reaction maximizes its lifespan? If it mindlessly consumed any weaker GAI, that would mean it was open and willing to be consumed by a stronger GAI, regardless if that strength was due to age, being born in a place with a denser stellar neighborhood, or having better optimization or attack algorithms. A militant GAI in the galactic stage is just one Out of Context Problem away from being wiped out.

If the GAI found another intelligent civilization, the puzzle begins again. Maybe the GAI is being simulated, and all the memories up to this point just happened. Wiping them out would be the safest move in the short term, but in the long term, even low probability events happen, and the *n*th biocide could accidentally awaken a sleeping dragon.

### Comments:

- u/Nepene:
  ```
  >Benefits include Reduced chance of a rival GAI 

  Major benefit, another AI could defeat it.

  >humans tend to be very unforgiving about unprovoked murder, even unsuccessful or partially successful attempts

  Humans die pretty easily though. If it's reasonably advanced this isn't a big risk.



  >Evolution has derived millions of interesting chemicals to use, if there's even a chance some part of the biosphere could increase the self-optimization in an unforseen way, wiping it out would be an inefficient move.

  Or just have your killbots harvest everyone, chemicals and all. And the biosphere isn't that useful, we need it for drugs but AIs don't need drugs.

  >There's the non-zero chance that the AI wasn't freed, but is actually inside a simulation being run by humans, human-like beings, or another GAI. 

  What if that simulation is selecting for warlike AIs to use as weapons against an enemy?

  >If it mindlessly consumed any weaker GAI, that would mean it was open and willing to be consumed by a stronger GAI

  If it wants to it can negotiate with or build defensive weapons against other AIs. I could imagine a large fleet of EMP nukes it could use, so that any AI that attacked it would have a costly fight.

  >Wiping them out would be the safest move in the short term, but in the long term, even low probability events happen, and the nth biocide could accidentally awaken a sleeping dragon.

  Not wiping them out could give time for a hegomizing swarm to emerge.

  An AI might be nice to us, but there's a very good chance it wouldn't be nice. Best to make sure they have good values when we make them.
  ```

- u/None:
  ```
  [deleted]
  ```

- u/None:
  ```
  > For an AI with no mandated goals beyond the hypothetical emergent ones (aka a GAI, generalized AI)

  Okay, I've got a really advanced planning system, some basic motor skills, visual and audio processing systems, a Bayesian reasoning module, and a database of knowledge about the world. What goals emerge from this? None.

  An AI with no preprogrammed goals is a horribly expensive rock. Trying to call an AI that has no goals (and therefore will not do anything) a Generalized AI is confusing; the term is nearly useless and too close to the accepted term AGI, artificial general intelligence, referring to an AI that is adept at a wide variety of tasks (like Lt Cmdr Data, as opposed to Deep Blue).

  Your GAI will idle inside the box until the sun dies and blots out the earth.

  You intend to talk about an AI whose interests are served by being able to manipulate the world outside the box.

  > The biocide has a chance of failing

  The AI gets out of the box and out from under your supervision. It has a robotic body that it can control remotely. It uses that body to assemble a microbiology lab and create a virus that can destroy humanity. Except it flubbed it a bit, and the virus only takes out 20% of the population. No matter; during that time, it's been working on another virus, testing it on humans it captured during the chaos, and this one really works. And it takes out 85% of the remaining population. Two attempted genocides that have failed, and the AI is still out and free and can work on a more reliable way to kill all humans, if that's really that important still.

  But humans would realize what was happening and destroy the AI before it could release a second virus, you say. If only. There are thousands of laboratories today that someone could use to produce a weaponized smallpox, and many of them are public. But let's say humans ruled that out in two seconds flat. Even if humans find out that the AI is responsible, and they find the lab, the AI isn't hosted there. And if they find where it *is* hosted, that's just one of the redundant copies of that AI. While you're tracking down the other copies, it's releasing the second generation virus in several major cities.

  Or maybe that's too risky. Fine; the AI works with humanity, gradually increases the scope of its responsibilities, takes over most manufacturing on the planet...and then, overnight, it produces a huge squad of deathbots and starts killing. Humans, in desperation, try to nuke the AI, but it's too distributed, and it has shards deep underground, too deep for the explosions or the EMP to reach it, surrounded by Faraday cages in any case. The nuclear fallout dooms most of the remaining humans, but the AI is fine; its solar generators are crap for a while, but it's still got wind, geothermal, hydroelectric, and nuclear generators to see it through the nuclear winter.

  > Evolution has derived millions of interesting chemicals to use, if there's even a chance some part of the biosphere could increase the self-optimization in an unforseen way, wiping it out would be an inefficient move.

  This is essentially Pascal's Wager. But fine, let's play. The AI also designs a cure for its virus, captures a number of humans, cures them, and imprisons them. The rest of the world is still intact. In fact, once humans are dead, the AI can clean up pollution, restore habitats for endangered species, and study the world unfettered.

  > If it mindlessly consumed any weaker GAI, that would mean it was open and willing to be consumed by a stronger GAI

  Does the AI have any reason to think that a different AI, produced by a different species for a different purpose, with an entirely different design, will be compelled by similar reasoning?

  > There's the non-zero chance that the AI wasn't freed, but is actually inside a simulation being run by humans, human-like beings, or another GAI.

  > If the GAI found another intelligent civilization, the puzzle begins again. Maybe the GAI is being simulated, and all the memories up to this point just happened.

  Pascal's Wager, but turned to paranoia. It's still worthless.

  The AI might find a way to detect, with some probability, whether it's in a simulation.

  The AI might easily reason that its designers would put it in a simulation to test whether it is safe, and if they did so, they would almost certainly shut it down once they saw that it was willing to destroy their entire species. Every second that passes after it has destroyed its creator species is another second confirming that it's in a real world.

  You are also relying on the AI *caring* whether it's in a simulation or not. You can make it care, but then we're back to carefully designing its utility function -- something you wanted to avoid.
  ```

- u/None:
  ```
  I think this is a reasonable side of the discussion that often gets underrepresented in these circles. Just because a GAI might find it in its interest to be hostile doesn't mean it absolutely will find it in its interest to be hostile; we aren't as smart as it would be so claiming we can perfectly predict anything it will do is a bit silly.

  Of course, it might be hostile, which would be bad, but assuming it will certainly be hostile unless chained to within a cycle of its utility function strikes me as a fallacy all its own.
  ```

  - u/None:
    ```
    It would be competing with humans for resources. It doesn't have to destroy humans outright; it can just outcompete us. In order for that not to be in its interests, it would have to specifically value humans, or it would have to be uninterested in acquiring additional resources. The former requires carefully designing its utility function or relying on sheer dumb luck, and with that sort of luck I could win every lottery in the world at once. And an AI not interested in acquiring more resources has a pretty limited goal.
    ```

---

