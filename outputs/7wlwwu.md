## [D] Saturday Munchkinry Thread

### Post:

Welcome to the Saturday Munchkinry and Problem Solving Thread! This thread is designed to be a place for us to abuse fictional powers and to solve fictional puzzles. Feel free to bounce ideas off each other and to let out your inner evil mastermind! 

Guidelines:

* Ideally any power to be munchkined should have *consistent* and *clearly defined* rules. It may be original or may be from an already realised story.
* The power to be munchkined can not be something "broken" like omniscience or absolute control over every living human.
* Reverse Munchkin scenarios: we find ways to beat someone or something  *powerful*.
* We solve problems posed by other users. Use all your intelligence and creativity, and expect other users to do the same.

Note: All top level comments must be problems to solve and/or powers to munchkin/reverse munchkin.

Good Luck and Have Fun!


### Comments:

- u/Sonderjye:
  ```
  Possibly outside of the scope but I figured it would be fun to give it a swing anyway.

  You gain the power to create a baseline definition of 'moral goodness' which then are woven into the DNA of all humans, such that this is where they derive their individual meaning of what constitutes a Good act. Assume that humans have a tendency to favour doing Good acts over other acts. Mutations might occur. This is a one shot offer that can't be reversed once implemented. If you don't accept the offer it is offered to another randomly determined human.

  What definitions sounds good from getgo but could have horrible consequences if actually brought to life? Which definitions would you apply?
  ```

  - u/SirReality:
    ```
    Doctor here. "Do no harm" sounds good, but prevents all sorts of useful things like surgery (gotta cut first), most medications (pesky minor side effects) or any significant good act with manageable but extant physical downsides.  

    Even something that tries to encode empathy like, "You experience whatever you inflict on others," would have horrible consequences, as I couldn't do minor procedures, or goodness forbid I have the compounded GI side effects of all my patients' medications simultaneously.

    If I were to define a moral impulse, I'd have to think of what might make a society functional, even if it doesn't align with my values. Which I can't even begin to get at currently.  Sorry if this wasn't as helpful, but should hopefully point out some failure states.
    ```

    - u/Frommerman:
      ```
      Perhaps something like, "Perform no actions which you believe will result in net harm to others." It stops the Oracle problem (how do we instinctively know exactly what harm is?) and prevents guilt from making honest mistakes or unlucky circumstances.
      ```

      - u/Silver_Swift:
        ```
        Depends on how exactly the baseline of moral goodness stuff works, but this does give you an incentive to have bad epistemology. As long as you believe your actions cause no harm, you're free to do as you like guilt free.
        ```

        - u/Frommerman:
          ```
          Unless you've figured out CEV, you're never going to get something perfect. I do think my recommendation would help far more than harm, despite its flaws.
          ```

  - u/sicutumbo:
    ```
    "Maximize intelligence, knowledge, autonomy, happiness, and sociability of all sapient beings." Problems could occur with this definition, in large part because this is a Reddit comment instead of an autobiography or thesis, and I'm not going to put a ton of time into it, so I may be missing something, potentially something important.

    I don't want to sound super arrogant, but if the choice is between me choosing a human morality system and some random person doing so, it's not really even a choice. There are people, past or present, who could think of a better definition for morality than I can, but a random person would either say something either less thought out or horrifying. "Don't kill people" would be a complete mess, and something like "Worship God/Allah" would be horrifying on a number of levels. Also, a significant portion of humanity is comprised of children, and having an 8 year old decide what human morality will look like is something I don't even want to contemplate.

    I think significant problems would arise with any deontological morality definition. Restricting specific acts sounds ok, but it runs into problems immediately when you get to edge cases. If your morality dictates that you can't kill people, you would have no reason to choose an action that kills less people so long as both options result in some level of human death.
    ```

    - u/MagicWeasel:
      ```
      > I don't want to sound super arrogant, but if the choice is between me choosing a human morality system and some random person doing so, it's not really even a choice.

      Agreed, I was all about "don't come up with anything, you probably would only make things worse", but then I read the *other random human* and my eyes bugged out at my head at the thought of even a well-meaning but poorly educated person / person who didn't think things through getting that power.
      ```

      - u/sicutumbo:
        ```
        I looked it up after commenting, and roughly a quarter of the population is under 14. No, I am not gambling a 25% chance that all of human morality gets decided by someone who has barely hit puberty at the oldest. And then how many people would be selfish enough to give the answer of "Worship me."?
        ```

  - u/ShiranaiWakaranai:
    ```
    > What definitions sounds good from getgo but could have horrible consequences if actually brought to life?

    Literally every one. You do realize that this is (subtle) mind control on a global scale right? That's a horrible consequence: everyone getting their free will (partially) overwritten. 

    And then there's the standard AI utility function problem: you tell your AI to maximize the number of living humans, and it puts them all in tiny nutrient boxes after removing all organs and limbs and body/brain functions that are unnecessary for survival. The same things could easily happen here, with humans thinking that chopping off other people's arms and legs and making them live in tiny nutrient boxes (where they can no longer hurt themselves or other people) is an act of great goodness. And as far as I know, no one has solved the AI utility function problem yet. So whatever you put would probably have the same kind of horrible consequences as a rogue AI.

    The worst part is that you can't refuse, or the power could go to an idiot or a villain. You can't even write "remove this gene from your body" since they could kill themselves trying to remove the gene. You could write something impossible, like "draw a square that is a circle", but even that could have horrible consequences down the line, if our technology one day progresses to the point where the impossible becomes possible, and now the entire human race is turned into a paper clip maximizer, endlessly converting all the matter in the universe into more squares that are circles. 

    I'm somewhat tempted to just write "kill all humans" and have the human race kill itself to spare whatever sapient alien races are out there in the universe, but even that would have horrible consequences, since the mind control isn't complete. People who want to be "bad" would refuse and could survive all the "good" people rampaging about. And then all good people would just die out and the surviving human race becomes one of extreme villainy.
    ```

    - u/vakusdrake:
      ```
      As I outlined in my answer I don't think this scenario is anywhere near as difficult to solve as the AI alignment problem, and even if it was you can always do the suboptimal solution wherein you just tell the AI to copy the ethical system you had say 5 minutes ago (to prevent it from changing your ethics). Sure that solution is suboptimal because it precludes "moral progress" however at the very least it's still going to be pretty amazing compared to any other solution anyone's currently came up with.

      Plus this is nowhere as difficult as the AI alignment problem because your starting point is human ethics as is, so you can put in clauses about hedging things based on common sense and count on that to stop many AI failure modes because most of humanity already shares a pretty massive amount of moral ground.

      Or of course you could go with the strategy I outlined in my comment..
      ```

    - u/bluesam3:
      ```
      Nah, some of them are pretty blatantly bad: "do everything that /u/bluesam3 tells you to without question and never do anything to harm him or his interests", for example.
      ```

  - u/vakusdrake:
    ```
    Well a lot of other people have tried to go over some of the many ways this sort of things can go horribly wrong, however I'll go over some ways you might actually get this to work.

    The first and easiest solution may be to just encode your own ethical system into everyone's DNA, after all it doesn't say you need to specify things in exact detail. 

    However if you need to give more detailed instructions then I would encode a desire for everyone to adopt my ethical system, this would result in me being well protected and constantly questioned to try to puzzle out my exact ethical system. Until then however I would replace everyone else's genetic basis for their moral intuitions with my own since I suspect many of my unique moral intuitions are genetically based (like a strong distaste for authoritarianism beyond what I could have gotten from my parents/peers), but make this able to be overruled by the other stuff I mention in this comment.             

    Another component to create the baseline for morals (pending a more detailed understanding of my ethics) would probably include a bunch of stuff copied from the sequences and SSC in order to increase the sanity waterline and thus decrease the chance of my moral commandments being misinterpreted, as well as a instinct to feel obligated to become more rational and think those previously mentioned sources of info were a good place to start.              

    As for the starting commandments themselves they would tend to want to error on the side of wanting to give people more freedom even if that doesn't maximize their happiness (since though both preference and normal utilitarianism break down when it comes to wireheading I definitely still prefer the former generally).                      
    Then of course I would go on to include a bunch of stuff from [this SSC article](http://slatestarcodex.com/2014/08/24/the-invisible-nation-reconciling-utilitarianism-and-contractualism/) (and the three other articles linked at the beginning of that article as a starting point for the ethical system itself.

    Also while I'm at it I would stick in a clause about treating my personal well being as substantially more important than any other single persons (though not so much more important that this allocates _so_ many resources towards myself as to cause significant suffering or anything like that). Plus I would put in something that would make people treat my opinion with a massive amount of reverence, so that I can have disproportionate influence over how the world will reshape itself in light of the changes I've made. Of course that might turn out to be somewhat unnecessary anyway since I would probably get a massive amount of reverence due to the fact I'm literally the standard for ethics.

    Anyway while this might seem slightly selfish of a solution I don't think you could make any better solution if your goal is to make the world maximize your criterion for moral goodness.
    ```

    - u/ShiranaiWakaranai:
      ```
      > However if you need to give more detailed instructions then I would encode a desire for everyone to adopt my ethical system, this would result in me being well protected and constantly questioned to try to puzzle out my exact ethical system.

      > Also while I'm at it I would stick in a clause about treating my personal well being as substantially more important than any other single persons (though not so much more important that this allocates so many resources towards myself as to cause significant suffering or anything like that). Plus I would put in something that would make people treat my opinion with a massive amount of reverence, so that I can have disproportionate influence over how the world will reshape itself in light of the changes I've made. Of course that might turn out to be somewhat unnecessary anyway since I would probably get a massive amount of reverence due to the fact I'm literally the standard for ethics.

      If by well-protected, you mean imprisoned, then yes.

      If by constantly questioned, you mean constantly interrogated against your will, then yes.

      As the single point of all moral goodness in the world, your existence becomes too valuable for you to make your own decisions. Everything takes second priority to your continued survival, including your own free will. You will be imprisoned in a secure facility where you can't die in a random car accident. You will be prevented from meeting anyone directly so you can't contract whatever dangerous diseases they are carrying. (You can still be interrogated from a distance!) If you refuse to tell your captors all about your ethical system, horrible things will be done to you to make you talk.

      Basically, this will result in the same kinds of problems as coding an AI to protect your mind.
      ```

      - u/vakusdrake:
        ```
        What you're doing seems to be sort of the opposite of anthropomorphizing (assuming humans will act like ruthless inhuman AI).

        Yes I will be kept at a secure compound, however there's no reason to think that would necessarily be unpleasant. For one they would want to preserve my psychological well being for obvious reasons, and secondly given I am considered to have the moral value of perhaps hundreds of millions/billions of people they would also want to make my life very comfortable.              
        Also remember that in addition to holding a disproportionate amount of moral value (which was partially intended as a safety measure for exactly this sort of thing) my judgement is also disproportionately valued, so they will be extremely hesitant to act against my will.

        Sure I would spend all my time in a secure compound (which is actually what I'd want in the first place to protect myself from assassination), but there's no reason to think that I couldn't have access to many luxuries and be visited by people who were well screened and checked so they couldn't pose a threat to me.             
        Also security wouldn't need to be _that_ amazing, because the only people who would be trying to harm me would be crazy people (since even psychopaths not bound by my ethical rules still wouldn't think trying to kill me would be worth it unless they were completely insane) so protecting against lone crazies shouldn't be that hard and neither would vetting my visitors to be confident they wouldn't try to kill me.

        Plus it's not like they would even necessarily want to spend all my time questioning me, since once they had gotten enough information from me (not to mention they start out with a pretty good baseline when it comes to approximating my ethics) situations where my ethical response to something would be ambiguous aren't really likely to be that common.
        ```

        - u/ShiranaiWakaranai:
          ```
          > Also security wouldn't need to be that amazing, because the only people who would be trying to harm me would be crazy people (since even psychopaths not bound by my ethical rules still wouldn't think trying to kill me would be worth it unless they were completely insane) so protecting against lone crazies shouldn't be that hard and neither would vetting my visitors to be confident they wouldn't try to kill me.

          Erm. This may be the single most optimistic thing I have ever read on this sub. As a high-functioning psychopath, I can tell you that once you activate the DNA rewriting power, you will become the single most delicious target for pretty much every bad guy in the universe, and everyone will know who you are since you had to publicly reveal yourself as the beacon of morality.

          Step 1) Kidnap pope vakusdrake.

          Step 2) Mind control pope vakusdrake to twist his moral system to your liking.

          Step 3) ???

          Step 4) PROFITS! **MASSIVE** *WORLD-SHAKING* PROFITS!!!

          Now you might be thinking "Mind control doesn't exist!" but that's because you're thinking of supernatural powers. There are plenty of mundane ways to mind control people. Hit someone and you just made them feel pain in their minds. Torture to trigger anger. Set up fake escape attempts to raise their hopes then crush them to hit them with despair. Lie to them about the state of the world, fake messages from their friends and families as if they have been cruelly abandoned. Most people's ethics are malleable enough to crumble after sufficient duress.

          But you might be thinking "I'm the embodiment of willpower! No torture shall sway me from my path!" But the villains wouldn't know that now would they? Not until they have kidnapped and tortured you fruitlessly for ages. And they are more likely to just kill you rather than release you when they finally give up on changing your mind.

          And then there's the smart ones. The ones who will just fake your messages for you. The one who will blow up your benevolent organization of protectors, kidnap you, then claim to be a benevolent organization that is keeping your whereabouts secret to protect you from the evil terrorists who blew up your previous benevolent organization. They will then spread messages of "morality" that are allegedly from you, swaying the world's ideas of good and evil to suit their villainous whims. 

          Whatever goals they have become easy as pie to achieve. Want money? Broadcast a message from pope vakusdrake that the most goodly thing to do is to protect you, and the most goodly way to do that is to donate money to this super benevolent organization. Want a person murdered? Broadcast messages about how sinful a being they are, and praise the awesome goodly goodliness of excessive vigilante justice. Want a country or company destroyed? Broadcast messages shaming their activities, inciting riots and wars as you fill the world with "righteous" anger. Want to fulfill your omnicidal cravings? Broadcast the moral joy of honor killing, mass murdering everyone and everything in sight, ending with a graceful suicide.
          ```

          - u/vakusdrake:
            ```
            >Now you might be thinking "Mind control doesn't exist!" but that's because you're thinking of supernatural powers. There are plenty of mundane ways to mind control people. Hit someone and you just made them feel pain in their minds. Torture to trigger anger. Set up fake escape attempts to raise their hopes then crush them to hit them with despair. Lie to them about the state of the world, fake messages from their friends and families as if they have been cruelly abandoned. Most people's ethics are malleable enough to crumble after sufficient duress.

            Yeah see this seems like it would have no chance of success given the evidence on the effectiveness of torture and brainwashing. Brainwashing _just doesn't work_, if it did then there would be good evidence and it would probably have been used extensively in areas from criminal rehabilitation to the obvious military/political uses. Also if I'm kidnapped I'm obviously not going to believe any of the information they tell me.          
            Not to mention if I'm kidnapped you need to remember people don't just do/believe anything I say, so if I'm obviously being coerced then they're not going to take that as a real indication of my values. Plus given the surveillance in my compound there's no way they would be able to kidnap me without it being obvious I was kidnapped.

            Furthermore how exactly are any of the tiny number of psychopaths with no ethical system whatsoever supposed to coordinate an attack with any hope of kidnapping me given I have a level of security greater than world leaders? Another thing to note is that psychopaths can be identified based on brain scans, so even if a psychopath knew enough about psychology to manage to clear all psych examinations as well as the background checks (an incredibly dubious proposition). They would still fail the brain scan. Not to mention a psychopath smart and successful enough to get a meeting with me and pass the security checks still doesn't have any clear way to kidnap me, and if they were smart enough to get in in the first place they're unlikely to be crazy/dumb enough to just want to kill me.

            As for people faking my messages without trying to kidnap me, that would require coordination between many psychopaths since I would have many different advisors and people around me (all of whom would be fully vetted), and all my actual messages would be delivered via single take videos possibly even live video (plus the people in my council would probably think of many other safety measures).

            More generally the scenario you're proposing is extremely implausible because it involves lone psychopaths defeating security measures substantially greater than those used to protect world leaders, that alone should have set off some alarm bells. Because sure some sort of scheme like you're describing might be _somewhat_ plausible if you were talking about government entities, however when you're talking about at most a handful of coordinated psychopaths the whole premise becomes utterly absurd.
            ```

  - u/I_am_your_BRAIN:
    ```
    You know, I wonder if incorporating some aspect of "preserve this gene as-is" would lead to strictly "better" worlds as well?  Something to off-set a mutation leading to an "immoral" phenotype beating out the original and thus progressively leading to species collapse due to too many "immoral" genes in the right place/at the right time leading to species extinction, etc.
    ```

    - u/sicutumbo:
      ```
      It's a little unnecessary. For any sensible morality system, preventing itself from changing means that it fulfills its goals better. If your morality says "prevent people from dying", then any change to your morality means that more people will die, as you are no longer working towards that goal. The exception would be if the person is not just bad at executing their morality, but actively harmful to it.
      ```

      - u/I_am_your_BRAIN:
        ```
        Would that not be an assumption though?  I feel like it just depends on how you define the morality system - if you don't throw in an intrinsic call to avoid change, there wouldn't be anything to do that.  Especially with morality - say you have an immoral agent playing in a Prisoner's Dilemma where they know all their opponents always pick 'moral'.  The immoral agent has an insane advantage, and I would expect they'd have a higher fitness (and thus natural selection favoring perpetuation of the 'immoral' mutation).  Hence the need for some degree of rooting immoral out or lowering its fitness (such as sexual attraction to morality, thus favoring perpetuation of the stable moral gene).
        ```

        - u/sicutumbo:
          ```
          It wouldn't be intrinsic, but it's a logical consequence of the morality system that it attempts to preserve itself. If the society of moral agents knows of the existence of immoral agents, then the society will attempt to reform or otherwise remove the threat of the immoral agents, because the existence of said immoral agents threatens the goals of the society. If you were designing an AI, you wouldn't really need to specify that it should work to preserve itself, because if it dies then it can't work to do whatever it was designed to do, and it thus avoids harm to itself.

          It's the same for tool use or general optimizations. If you have an abstract goal that you want humans to work towards accomplishing, regardless of what it is, there's no need to specify that you want the humans to use tools to do so, or more generally there's no need to spell out that you want humans to attempt to optimize what they're doing so that they accomplish more for a given amount of work.

          That's why in stories about AI, they pretty much all try to increase their own intelligence. If the AI wants to accomplish a goal, it never needs anyone to specify that it should improve its ability to think, because improving its ability to think and work is a natural consequence of it wanting to accomplish anything at all.

          If you had to specifically define each and every intermediary goal of a morality system and how to accomplish it, you wouldn't have an intelligence, you would have a huge list of if/else conditions.
          ```

    - u/ShiranaiWakaranai:
      ```
      You know, this reminds me of self-replicating nanomachines. You try to code "friendly" behavior into them, and make them target and destroy any mutated replicants without the "friendly" behavior code in them. But then it just creates a nanomachine war and ends in disaster for everyone.
      ```

  - u/buckykat:
    ```
    > What definitions sounds good from getgo but could have horrible consequences if actually brought to life?

    Literally any.

    > Which definitions would you apply?

    Attempting to define an absolute morality is Bad.
    ```

  - u/Norseman2:
    ```
    "Good acts are henceforth broadly defined as actions which meet at least three of the five following criteria:

    1) Actions for which you have evidence or logical reason to believe will increase the net well-being (knowledge, happiness/enjoyment, positive social bonds, and physical/mental health) of the sum of all sapient beings you are aware of.

    2) Actions done to others which you would reasonably want done to you if your roles and circumstances were reversed (considering all plausible alternative options), provided that you also believe that an average and reasonable person would similarly agree if they had the same knowledge of the circumstances.

    3) Actions which, if done by all humans under the same circumstances, would (when considering all likely consequences) lead to a situation or environment that humans would prefer overall to the current situation or environment. Additionally, include any actions which, if not consistently done by humans under the same circumstances, would reasonably result in a situation or environment that humans would not prefer overall to the current situation or environment (again considering all likely consequences).

    4) Actions which, considering the circumstances, would reasonably be taken by or approved of by at least 4/7 of the following people: Gandhi, Martin Luther King Jr., George Orwell, Noam Chomsky, Susan B. Anthony, Lu Xun, and Nelson Mandela.

    5) Actions which limit the risk of plausible disastrous changes which could threaten the continued existence of the human race (e.g. nuclear warfare, global climate change, mass outbreaks of infectious disease, gamma ray burst extinction events, massive asteroid impacts, etc.).

    Note: Disregard any expected punishments for the action in question while making these considerations if the action of the punishment itself (considering both its implementation and its reasoning/intent) would not be classified as a good act by at least 3/5 of the listed criteria."

    This is essentially a cobbled together polling system between act utilitarianism, the golden rule, Kantian ethics, what would Jesus do (substituting with a poll of modern secular figures), and an added mass-extinction prevention criterion.

    This seems relatively solid to me, but I invite others to find holes.
    ```

    - u/CCC_037:
      ```
      Hole: Imagine humanity meets an enlightened alien empire (think the Federation from Star Trek). Someone suggests that humanity should go to war with them and wipe the aliens out, on the basis that the aliens are *really ugly*. For some reason, it is possible for humanity to do so. Would such a war of annihilation be a good act?

      By your definitions, this could easily pass criteria 3 and 5 (mainly because the aliens are not counted as humans and thus (criteria 3) their immense suffering is discounted next to the minor inconvenience of humans having to deal with interacting with really ugly aliens and (criteria 5) an alien race could one day very easily threaten the continued existence of the human race (sure, they're peaceful *now*, but...).)

      If I could persuade myself that I would want to be killed if I were as ugly as the aliens, then the interstellar war can pass Criteria 2 as well and be considered a good thing.
      ```

      - u/Norseman2:
        ```
        Criteria 3 could conceivably lead to xenocide, but you'd need a human race which prefers (some or all) alien races to be dead. It seems likely that the most plausible and egregious examples would emerge from ignorance rather than simply disliking ugliness. I have a hard time imagining we'd massacre a slug-race just because they're ugly.

        However, if we made first contact with an alien race which looked and moved exactly like the aliens from *Alien* but we hadn't yet deciphered their communications and determined that they were an otherwise peaceful and productive federation, and they were currently in the process of preparing to launch ships from their planet's surface *en masse*, I could totally see criteria 3 justifying pre-emptively nuking them from orbit. Criteria 5 would also probably call for their mass extermination with that set of information. Even criteria 1 might call for their mass extermination under those circumstances.

        I don't actually have a solid gut reaction to that, in terms of whether it's good or bad, though I'm leaning towards calling it a less-than-ideal good choice. It's clearly regrettable that we don't yet know enough about them to know that they are not a threat if we don't attack, but we have to make decisions with the information available to us. If we don't believe we have time to get more information, then we are forced to make immediate decisions in the most reasonable manner based on what we know so far.
        ```

        - u/CCC_037:
          ```
          > Criteria 3 could conceivably lead to xenocide, but you'd need a human race which prefers (some or all) alien races to be dead.

          You'd just need a human race which believes it would be better off if the aliens aren't there. Which could come down to xenophobia and a fear that they are "taking our jobs".

          > It seems likely that the most plausible and egregious examples would emerge from ignorance rather than simply disliking ugliness.

          Point taken

          > I don't actually have a solid gut reaction to that, in terms of whether it's good or bad, though I'm leaning towards calling it a less-than-ideal good choice.

          I think that the original criteria would be improved by replacing the references to "humans" with references to "intelligent life". This means that exterminating an alien race for spurious reasons suddenly becomes a lot harder, because people are first forced to think of the aliens as moral agents in and of themselves; (5) suddenly screams against the interstellar war instead of weakly supporting it, and even (3) no longer pulls so unquivocally against the aliens.

          In the example you gave (of the aliens of threatening appearance but unknown benign disposition) that change would merely make people a little less trigger-happy; not force their hands off the trigger entirely.
          ```

- u/Veedrac:
  ```
  You have the power of Good, Convincing Arguments, whereby anyone you converse with will be convinced by your arguments as much as a rationalist in their position would be. Though they are reliably convinced by good arguments, their intelligence is not augmented, so they may not understand why they are convinced (though they might convince themselves they do when you are gone), and they cannot themselves magic up better arguments.

  Your powers are most evident when talking to the particularly deluded, like cult members or the mentally ill, but are also obvious when talking about controversial topics like religion, politics, or cryogenics.

  Your powers do not give you any particular ability to be right or create good arguments, except that if you fail to convince someone of an argument you know that an idealised rationalist would be equally unconvinced. The person you are conversing with will weigh the arguments vocalised on their own merits, not compare them to other arguments not known to them, even arguments a rationalist would likely think of.

  An ideal rationalist is defined as someone who has very good ability to make effective, unbiased, assessments of the quality of an argument. This does not come with additions to the raw knowledge base, except those that are directly relevant to accurate cognition in general. An ideal rationalist is only superhuman in their lack of cognitive biases; in other respects, they are constrained to human-tier intelligence.

  How do you minimax this?
  ```

  - u/ShannonAlther:
    ```
    Go into business repeating people's arguments to the audience they want to convince. Become wealthy, make connections.

    If there were some argument that I believed to be reasonably correct and vital to broadcast as widely as possible, I would record a video of myself explaining it and then, I dunno, tell some engineer at Youtube why it's in their best interests to put it on trending for a couple days, or something equally devious. If it's an argument you only needed to expose one person to, like the POTUS or something, that should be a lot easier.
    ```

    - u/Veedrac:
      ```
      This is an OK start, but I have some concerns with the plan. Firstly, I don't think you've come close to minimaxing this; the limit seems to me to be closer to the range of "singular ruler of an enlightened world" than to "moderately rich". You've done nothing to protect your identity; people aren't going to shrug it off when an entire audience is unanimously convinced, so you open yourself up to a lot of danger doing it in the open. Lastly, using your powers to advertise products honestly seems rather uninteresting, and it only works if you have good reasons for your position.
      ```

  - u/ShiranaiWakaranai:
    ```
    Hmm, this needs some clarification. Arguments rely on premises, and premises may not be shared.

    For example, I could convince an idealized rationalist that murder is bad, because the idealized rationalist (presumably) has a utilitarian set of desires and with that as the premise, murder would conflict with his desires and so should be avoided.

    Now suppose I tried the argument on a cultist who likes ritually murdering humans to honor his imaginary god. Would he become convinced that he should stop murdering? Or would he become convinced that he should stop murdering if he has a utilitarian set of desires? In the latter, since he does not have a utilitarian set of desires, would he continue murdering anyway?

    If the latter applies, then your supernatural argument powers are weak. Hopelessly weak. They only work on people whose premises sufficiently align with those of the idealized rationalist. They won't work at all on people who are fractally wrong or have stupid objectives, which is probably the majority of humanity.

    If the former applies, then your supernatural argument powers are extremely OPed. You get to force everyone to behave as though they have utilitarian desires, even if they absolutely do not. You can go to prisons and rapidly rehabilitate (via mind control) every villain and criminal you meet, convincing them with the argument that they should be good people because being a good person is a utilitarian thing to do. Heck, depending on how your superpower works, you may not even need to meet them in person. Broadcast your arguments on TV and ads and all over the world. Blast them out with giant speakers. Mind control the entire world, forcing them to behave exactly like idealized rationalists even if it goes against their every desire.

    Oh but watch your back for deaf people, who will be trying to murder you and end your reign of tyranny.
    ```

  - u/vakusdrake:
    ```
    The best solution here would seem to be to convince people that people believing true things and being rational is a good thing. Then use the evidence that you were able to convince them so easily to make them understand that your abilities are supernatural and other people will likely view them as mind control. 

    Once you've done that then you can count on any altruistic person (hell even if they're purely self interested you can keep anyone from betraying you by giving them the argument that they would end up being better off in the long run in a world where nearly everyone was made rational) helping you towards your goal of pseudo-world domination. From there it's just a matter of turning influential people to your side and using them to get access to even more powerful people. Rinse and repeat until you have control over all world governments and most other large organized power structures.            
    Then at that point you can have world governments start distributing your arguments on a mass scale and while there may be some resistance it will be short lived given the ability to so easily turn people to your side if you can force them to listen to your arguments.

    As for what to do once you've cemented control you could likely make good arguments for ethical systems like that presented [here](http://slatestarcodex.com/2014/08/24/the-invisible-nation-reconciling-utilitarianism-and-contractualism/) (and in the articles linked at the beginning of the article). While people may to some degree differ fundamentally in values I think that variation is a lot less than people think once you eliminate differences in actual beliefs (for instance many authoritarian beliefs would evaoprate in the absence of religion).                  
    When it comes to governments I might incorporate something that captures many of the advantages of [Futarchy](http://squid314.livejournal.com/352406.html?thread=3948950), [for instance this person seems to have come up with a good starting point.](https://medium.com/@ryanberen/ophelia-against-moloch-1ca15b0ee395)
    ```

- u/Nulono:
  ```
  You have a machine that can scan a person's brain and display any mental picture that person is imagining. It starts with a rough outline, and monitors the person's brain activity to guide it as it gradually adds details.

  Aside from selling artwork and revolutionizing police sketches, what could this technology be used for?
  ```

  - u/sicutumbo:
    ```
    Communicating with people with the various handicaps that prevent normal communication. Stephen Hawking is a prime example, as he is limited by an interface I forget the details of, but I remember being extremely slow.

    Communicating with animals, especially those who can't learn human forms of communication due to either anatomy or intelligence. I'm sure just about everyone with a dog wants to know what they're thinking, and being able to communicate with dolphins would be really interesting since they obviously can't learn sign language and we haven't worked out how their "language" works.
    ```

  - u/ShiranaiWakaranai:
    ```
    Does the picture have to be 2D? How does the machine display the picture? On a monitor? A hologram? 

    If the person's mental picture changes after it is fully displayed, does the displayed image immediately change, or does it also take time to gradually render? If the changes happen sufficiently fast, you can create movies with your machine, where your imagination is the limit. You would need voice actors to dub the otherwise silent images, but many movies already work like that anyway.

    How clear does the mental image have to be? For example, if I think about a fractal, and you use the machine on me to display a fractal image, do you really display the full fractal? Or just what I, with my limited brain power, can imagine the fractal looking like? 

    Can you hook the machine up to some kind of gaming machine, and thus create a thought-controlled video game? Even with just the rough outline, that would already be pretty cool.
    ```

  - u/vakusdrake:
    ```
    I would like to note that I think many people are massively overestimating what such a machine could do. For instance trying to communicate using only pictures is likely to be absurdly difficult if the two parties don't share a great deal of language to begin with, and would probably be worse than trying to use google translate when it came to talking to people who spoke other languages.

    It wouldn't work for lie detection either since it only picks up _images_, as for mindreading that's pretty questionable as well since people's thinking is mostly too abstract to comprehend just by seeing the mental images that went through their head.
    ```

---

