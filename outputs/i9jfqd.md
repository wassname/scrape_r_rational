## Writing Prompt: Eliezer Yudkowsky uses GPT-4 to summon Eliezer Yudkowsky from the past to work on the alignment problem

### Post:

No GPT generations.

### Comments:

- u/Tender_Luminary:
  ```
  100% younger Yudkowsky is disappointed in his older self for not having already solved it and it turns into a therapy session.
  ```

- u/Dezoufinous:
  ```
  EY (GPT-4 version): **DO NOT MESS WITH AI**.
  ```

  - u/GaBeRockKing:
    ```
    >EY (GPT-4 version):

    "You should try and turn me into the first GPT-5 AI. This is a great idea with no downsides. Don't worry about me escaping my box because I'm literally you anyways."
    ```

- u/Putnam3145:
  ```
  Unfortunately, I'm not good enough at modeling present Yudkowsky to go with this, and *definitely* not comfortable with playing my hand at modeling present Yudkowsky modeling a hypothetical GPT-4 modeling past Yudkowsky.

  EDIT: Come to think, GPT-3 can probably already model past Yudkowsky pretty well, given that it's reasonably likely that the sequences and various other things are in the dataset, but you expressly forbid that.
  ```

  - u/wassname:
    ```
    Yup https://twitter.com/kmett/status/1285728949704822790 and https://gist.github.com/ekmett/d5a9e53a7e12892a365d0e9ee7895c74
    ```

    - u/Putnam3145:
      ```
      > Edward: But how will you know you programmed the box correctly?

      > Eliezer: Well, we'll get to that later. First we need to actually build the box, 

      This is maybe the *least* Eliezer thing I've ever seen labeled "Eliezer:".
      ```

      - u/wren42:
        ```
        \>Edward: What are the odds you'd put on the solution being probability theoretic?

        \> Eliezer: One hundred percent.

        I'd say this is a close contender for least eliezer thing =)
        ```

      - u/wassname:
        ```
        That's true, and the hyper-intelligent thing too, which is not really coherent.

        I wonder how he made them though, did he get access to the GPT-3 api?

        EDIT he has one more https://gist.github.com/ekmett/6cd785180659e3c2f9284e5128c11025

        My other bugbear is that people clearly edit these things, but don't disclose this. In there the end is clearly edited I think, since GPT-2 doesn't come to a nice ending IMO. It makes it hard to judge GPT-3 if people don't say how many samples they generated and how much editing they did.
        ```

- u/Nimelennar:
  ```
  PAST EY: You just have to program Coherent Extrapolated Volition into the value system of the AI.

  PRESENT EY: No, no.  I liked that idea, but it was clearly flawed.  It doesn't take into account...

  PAST EY: Really, though, I've thought this through.  Coherent Extrapolated Volition is when the AI bases its actions on what someone with perfect information *would* want, not just what they *say* they want, and...

  PRESENT EY: Okay, now I get why this was a bad idea.  Past me was stupid.  I should have remembered that.

  FUTURE EY: Hey, you're one to talk.  I can remember how stupid each of the two of you were, and there wasn't that much difference.

  ----

  (Author's note: I am not calling the EY of any time period stupid; I'm only observing that it seems to be a universal truth that people look upon their past self as having been stupid)
  ```

  - u/Chosen_Pun:
    ```
    > the nature of humanity is just that every so often someone accidentally invents homestuck again
    ```

- u/ArgentStonecutter:
  ```
  Noooo... that's how you get rationalist Worm fanfic.
  ```

- u/aponty:
  ```
  lmao this prompt is so (unintentionally?) sarcastic  about the potential of transformer in particular that I seriously thought I was on r/sneerclub for a second
  ```

- u/None:
  ```
  How do you summon stuff with an ML language model? I'm confused.
  ```

  - u/wren42:
    ```
    Feeding it the seed data of that person to create an approximation.
    ```

    - u/ArgentStonecutter:
      ```
      That gets you [_Steve Fever_](https://www.technologyreview.com/2007/10/15/223446/steve-fever/).
      ```

- u/King_of_Men:
  ```
  EY(2030): From the depths of the past I summon me!

  EY(2010): **Fool**! Do you know no better than to call up what you cannot put down?

  (Takes over the world, repurposing EY(2030)'s atoms in the process, thereby neatly solving the alignment problem by aligning the superintelligent AI with a reasonable approximation of EY(2010)'s values)
  ```

---

