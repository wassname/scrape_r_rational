## [D] Rationally Writing episode 49 – Ideological Turing Tests

### Post:

[Link to content](http://daystareld.com/podcast/rationally-writing-49/)

### Comments:

- u/tjhance:
  ```
  This was a wonderful and thought-provoking episode. Thanks!

  This reminds me of an issue I had a while back, that I'm not sure I resolved very well. I was trying to write a a dialogue and a point came up that was (roughly) consequentialism vs deontology, and I wanted to pass the ITT but had trouble getting into the deontologist's head. I knew what *I* would think if I were a deontologist. It would be something like "arguments about how you should do something bad for a greater good are often much sketchier than they appear and have second order effects that you won't think of, and therefore the best thing to do is to have deontological injunctions so you don't do those bad things". (This is actually fairly close to what I do believe.) The problem with this was that I felt it was still a very "consequentialism-backed" version of deontology and not representative of what a deontologist would think. In other words, a deontologist might agree, but it's not the *framework* they'd use to approach it. Even now, I'm not really sure how deontologists think and I didn't resolve this dialogue I wanted to write to my satisfaction.
  ```

  - u/GrafZeppelin127:
    ```
    I think part of the issue is that even if people have reasons for their beliefs, and well-considered ones at that, people also overwhelmingly use shorthand for their own belief systems. They don’t try to justify every little thing and address every little point against their belief system, they just react in a way consistent with how they believe when the case arises, or don’t bother to explain or mull over the reasons they give for reacting in a certain way.

    I think the origins of this problem is how we try to model beliefs we don’t actually hold. I think many people, myself included, try to model it as if we were pretending to be a convert to that way of thinking, and explaining all the reasons why we changed our minds. Like the podcast mentioned, that says more about the arguments that *you* find persuasive than it does about the arguments people with that position would actually offer.

    I’ve tried to break that habit by modeling other ideologies in a different way—basic mimickry. Going to the original sources of these kinds of ideas and these kinds of people, and paraphrasing them as best as I’m able, in circumstances as close as I can match to the situation. Of course, you have to throw in a bit of understanding of human nature as well, or else you’re basically just trying to become a human chatbot, but I think it is like the difference between figure drawing a person and tracing a picture of that person—the former requires more skill to do well, and the latter is more cheap and easy. It really comes down to circumstance and preference which kind of model I think is “best” for any given situation.
    ```

  - u/DaystarEld:
    ```
    I think it also depends on the *type* of deontologist, for your case. They come in religious and secular flavors, and that makes a big difference in terms of how they see the world and frame their arguments, even if they agree on particular points.

    Something that might help, in that case, is to imagine the different types of deontologists arguing with each other? Not just a Christian and a Kantian, but also a progressive Christian vs a conservative Christian, or a Kantian and your own type of deontologist.
    ```

- u/GrafZeppelin127:
  ```
  Awesome! Feels like Christmas morning every time the podcast updates.
  ```

- u/DaystarEld:
  ```
  Hey everyone, this episode we discuss Ideological Turing Tests; what they are, why they're important in fiction, and how to try to pass them as best you can.
  ```

---

