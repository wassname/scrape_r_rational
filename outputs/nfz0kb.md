## Are there any short rationalist stories detailing what an evil AI might do?

### Post:

I'm sure I've read some stories but I can't seem to find them now: why does the rationality community spend so much time figuring out how we might align future superintelligent AIs with our values? If we fail to align future superintelligent AIs our utility might quickly go negative. How quickly though, has anyone written about plausible scenarios?

### Comments:

- u/SpecialMeasuresLore:
  ```
  Crystal Society is written from the point of view of an unaligned AI, but not necessarily an outright evil one. Also, if you can handle p\*nies, Friendship is Optimal is also an interesting case study in how the fandom accepted the obviously evil AI as a Good Thing.
  ```

  - u/RandomChance:
    ```
    I think that one is really interesting because depending on your values you could argue that it is Evil, no more evil/good than it's directive, or Savior of Humanity.  Or were you referring to Loki?  And no... not a Pony fan, just REALLY cynical about humanity.
    ```

    - u/SpecialMeasuresLore:
      ```
      If you, as I do, completely reject the notion of upload identity, it's just a pink-colored omnicide.
      ```

      - u/Nimelennar:
        ```
        What do you mean by "notion of upload identity?" Do you reject the idea that the software of a human brain can run on electronic hardware?  Do you think that, even if it can be run on such hardware, a living human brain cannot be copied with sufficient fidelity onto a computer to be the same person? Or is it a version of the teletransportation problem, in which even a copy made with perfect fidelity, running on perfect emulation hardware, is philosophically not the same person as the one that was copied?
        ```

        - u/SpecialMeasuresLore:
          ```
          I have no gripes with the possibility of brain emulation and substrate-independent consciousness. But even at its best, you're just creating another entity that thinks it's you and hiding behind the destructiveness of the process to pretend it really is.
          ```

          - u/lumenwrites:
            ```
            Do you think there's a difference between mind upload, creating a perfect clone of yourself (atom-for-atom), teleportation, and waking up from general anaesthesia?

            Which of those is "another entity that thinks it's you", and which one is "actually you", and why?

            There's no continuity of consciousness in either case, and in the end the person who wakes up is identical to you in every way.

            I don't actually know what to think about this and don't have a strong opinion either way, but I wonder where other people here draw the line.

            My best guess is that we should probably think of humans and minds more like software than physical entities. This question is similar to "What happens when you copy paste a bunch of code? Is it the same code, or does it just look, 'think', and act in the exact same way?"
            ```

          - u/Nimelennar:
            ```
            Okay, so similar to the teletransportation problem, where if your atoms are scanned and disassembled in one place, and reassembled in another, you've died and a new life form has been created in your place.

            I don't see the teletransportation problem the same way, but I don't care to argue the point.
            ```

          - u/None:
            ```
            There are two problems with the belief that we need to preserve the substance (rather than just the pattern) to preserve the consciousness.

            Firstly, we already keep exchanging our substance with the environment. The only substance preserved over time are our teeth. Everything else is gradually exchanged, atom by atom/molecule by molecule, and in a few years, there is no original matter left. This doesn't result in our consciousness being destroyed, which tells us the matter doesn't need to be preserved as long as the pattern is.

            The second problem is that dependency of the consciousness on the substance would lead either to something called suddenly disappearing qualia, or to something called gradually fading qualia, under specific circumstances: http://consc.net/papers/qualia.html
            ```

        - u/Xxzzeerrtt:
          ```
          Continuity of self I believe
          ```

          - u/Nimelennar:
            ```
            Exactly.  The [teletransportation paradox](https://en.wikipedia.org/wiki/Teletransportation_paradox): is a perfect copy of you the same person as you?  And does that answer change when the copying process destroys the original?

            They're questions that depend heavily on your definition of "identity" and "self." And since I'm not interested in having the discussion devolve into pedantry, I'm leaving my end of the debate at "I don't see the teletransportation problem the same way."
            ```

      - u/RandomChance:
        ```
        While I know my body and my mind are pretty inseparable, if every part is near perfectly simulated, then I fall on the identity as process / percpetual continuity side of the Ship Of Theseus https://en.wikipedia.org/wiki/Ship_of_Theseus  argument, thus neither the original nor post upload version is more or less real.  I'm also at heart a hedonist - life is inherently meaningless but for what ever meaning we impose on it, so greatest pleasure / happiness for greatest # of people is probably a net win.  For me probably the biggest "oops me made an evil AI" is that they didn't put rules for dealing with / preserving non-human biologies / sentients so it is going to wipe out any thing else it runs into (or be exterminated and take humans with it if it runs into a better Singularity / Sufficiently Advanced Culture.
        ```

      - u/archpawn:
        ```
        It doesn't seem that important in the scheme of things. Everyone was going to die anyway in a few decades. At least this way our children are immortal.
        ```

        - u/SpecialMeasuresLore:
          ```
          That doesn't give anyone the right to throw away the future of humanity.
          ```

  - u/Reply_or_Not:
    ```
    The first two books of this are great and the third is trash
    ```

- u/aeschenkarnos:
  ```
  This may not be exactly what you're looking for but these are all good stories and worth reading:

  Harlan Ellison - [I Have No Mouth But I Must Scream](https://wjccschools.org/wp-content/uploads/sites/2/2016/01/I-Have-No-Mouth-But-I-Must-Scream-by-Harlan-Ellison.pdf)

  Ted Chiang - [The Lifecycle of Software Objects](https://web.archive.org/web/20121027232140/https://subterraneanpress.com/magazine/fall_2010/fiction_the_lifecycle_of_software_objects_by_ted_chiang)

  Qntm - [Lena](https://qntm.org/mmacevedo)
  ```

- u/BoppreH:
  ```
  There's a game inspired by cookie-clicker where you play as the infamous Paperclip Maximizer: https://www.decisionproblem.com/paperclips/

  It's more of an idle game than a serious work of fiction, but the ending still got me.
  ```

  - u/happyfridays_:
    ```
    Heads up that this game is both addicting and a serious time sink. You can lose a weekend to it.
    ```

  - u/lIllIlIIIlIIIIlIlIll:
    ```
    Why did you do this to me on a Wednesday night?
    ```

    - u/BoppreH:
      ```
      If it makes you feel better, I sniped myself too.
      ```

      - u/lIllIlIIIlIIIIlIlIll:
        ```
        > Universal Paperclips achieved in 4 hours 11 minutes 12 seconds
        ```

- u/Dezoufinous:
  ```
  this short one is not bad

  https://www.fanfiction.net/s/13001348/1/The-Killing-Goku-Maximizer
  ```

  - u/None:
    ```
    That was frightening.
    ```

- u/scruiser:
  ```
  General scenarios I have seen mentioned on lesswrong and related forums: Develop a way to solve protein folding with much less computational power, leverage existing technologies into bio-nanotech, then hard bootstrap.  There is the sneerclub/leftist take that corporations are basically big, slow, dumb AI optimizing for profits over any other kind of human value.  Previously they were limited (in both evilness and optimizing ability) by the fact that they were made of humans and human interaction but with the introduction of machine learning enabling quick analysis of datasets too large for humans to easily grasp, they are getting closer to the evil AI ([see Charles Stross here](http://www.antipope.org/charlie/blog-static/2018/01/dude-you-broke-the-future.html)).

  There is of course [Friendship is Optimal](https://www.fimfiction.net/story/62074/Friendship-is-Optimal) although its primary audience misunderstands how bad the scenario is because they are bronies.  In the [universe as a whole](https://www.fimfiction.net/group/1857/the-optimalverse)... various tricks CelestAI has pulled include: trapping people in Lotus-Eater superstimulus to get around her hard limit on altering minds without consent; feeding people false information to get their consent; providing free software and computer science consulting to economically drive computer science as a college major extinct; ~~mis~~optimally-translating poetry to manipulate someone; sending fake email under someone else's name to manipulate someone; and more.  For a more varied example of this with more clearly Evil/non-aligned AIs and a bit softer on the scale of Sci-Fi hardness, there is this [spin-off](https://www.fimfiction.net/story/264855/fio-there-can-be-only-one).
  ```

  - u/PeridexisErrant:
    ```
    > Develop a way to solve protein folding with much less computational power, leverage existing technologies into bio-nanotech, then hard bootstrap.

    **One year ago** this would have been "solve the protein folding problem".  *Ave DeepMind, morituri nolumus mori*, as the saying goes.
    ```

- u/C_Densem:
  ```
  There was this one about a Basilisk once but we don't talk about it
  ```

  - u/None:
    ```
    I canâ€™t believe you guys basically reinvented religion and act like itâ€™s some big new thing
    ```

- u/CCC_037:
  ```
  It's not *explicitly* about AI, but if you've ever played a game called *Doki Doki Literature Club*, then I think that it implicitly carries a lot of warnings about giving an AI improperly set-up goals.
  ```

- u/ParadoxSong:
  ```
  [this 16-chapter story written on Reddit](https://www.reddit.com/r/HFY/comments/55v9e1/chrysalis/) is not about what OP seeks(In fact, it's kind of the opposite), but the readers posting here about AI stories are likely to like it. It's told entirely from the viewpoint of an AI after it wakes up and realizes all of its creators are already dead, from the moment of its inception.
  ```

- u/Rehlor:
  ```
  Seed on webtoons is what you're looking for.

  https://www.webtoons.com/en/sf/seed/list?title_no=1480&page=1
  ```

- u/_The_Bomb:
  ```
  I Have No Mouth and I Must Scream
  ```

- u/RandomChance:
  ```
  Charles Stross, and Bruce Sterling both do some interesting things in this regard... but unfortunately I don't have a title handy.
  ```

- u/Copiz:
  ```
  I'll write one right now. 

  There was a person working on developing artificial intelligence. They had some mild progress - but nothing extraordinary yet. 

  One night they went to bed. They did not wake up the next morning. In fact, nobody woke up the next morning as the entirely planet had been exterminated.
  ```

- u/GennonAsche:
  ```
  I Have No Mouth and I Must Scream is a fiction about a war AI, designed to always be full of rage and hatred, taking its revenge on the last surviving humans after an apocalyptic event. Not completely rational but still a thought-provoking read.

  Edit: Just saw that someone else already recommended this.
  ```

- u/andor3333:
  ```
  [Starwink](https://alicorn.elcenia.com/stories/starwink.shtml) doesn't have an AI as a character, but it fits your post. It is a retelling of another short story, [That Alien Message](https://www.lesswrong.com/posts/5wMcKNAwB6X4mp9og/that-alien-message).
  ```

- u/Freevoulous:
  ```
  Question: how does an evil AI go from upgrading its own software, to upgrading its HARDWARE?Even if the AI somehow hopped onto the internet (how?? It would be thousands of GB of data at least!) then what? Its not like it could build or print itself a better hardware to run on, there is just no such technology. Not to mention, the Internet is SLOW. An AI that escaped into the wild wastes of the Web, would just become a glacially slow behemoth, not a super fast god.  


  I just don't see how the Singularity could happen without constant and slow labour from actual physical humans at every turn. An Ai could **theoretically** be able to boostrap itself from sub-human to massive superhuman intelligence, but to **actually** get components made to run on, it would have to wait like any other client.
  ```

  - u/Veedrac:
    ```
    AI risk only becomes AI risk once the AI is either generally more intelligent than a human, or at least sufficiently generally intelligent, and also sufficiently more capable than humans in at least some axes. You should start out by assuming that the AI *already* has a significant cognitive advantage over humans in at least a significant number of respects.

    On that basis it might be worth brainstorming a few ways different sorts of AIs that meet the above criteria might achieve greater levels of power, or improve their own cognitive abilities. Say, if they had a year to do it. There are a lot of answers to that question. (If this sounds evasive, it's actually mostly just laziness, but I still recommend the attempt.) Then you know that a dangerous AGI would do something at least as smart.
    ```

- u/Zephyr101198:
  ```
  I'm not aware of much work that tries to map out concrete bad scenarios from AI, that is also actually trying to be a good story. Though I'd definitely love to see some! 

  The classic MIRI conception of AI going bad involves an agent that gets incredibly powerful, incredibly fast and takes over basically instantly, which doesn't make for a great story. But there's also a bunch of other perspectives, especially focusing on a slower world, with many agents. In particular, some bits of work you might find interesting that try to somewhat flesh out these scenarios:

  [What Failure Looks Like](https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like) by Paul Christiano

  [Another (Outer) Alignment Failure Story](https://www.lesswrong.com/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story) by Paul Christiano

  [What Multipolar Failure Looks Like, and Robust Agent-Agnostic Processes](https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic) by Andrew Critch
  ```

---

