## Need help with a Bayesian calculation

### Post:

Ok guys, so I have this question but I don't know where to post it. Is there a subreddit or another place where I could ask this? Is there one of you who's willing to help me out even if this is off-topic?. The question:

>What's the optimal choice in a situation in which you have only two outcomes |Either the whole human race goes extinct, or all the others lifeforms on earth (here humanity has no need of those for its sustenance) do>?

I've tried to solve this problem, but I feel like having smart people as a sounding board would improve my take on the problem.

Probably, inside this problem lay the hidden questions of how much an animal life is worth compared to a human one, how much the potential of the whole of earth ecosystem is worth compared to our race's potential, and probably the meaning of life, or at least the inferable purpose of life according to life-form's priorities (like, they prefer life over death, death over eternal suffering, freedom over life and other such).

Please tell me if you think this is too much a heavy question.

EDIT: just to clarify, I don't need anyone to do a Bayesian calculation for me, I just want to know your take on the problem.

EDIT2: Deleted the "Bayesian" bit since I misused it.

### Comments:

- u/davidmanheim:
  ```
  Yes,  there are some deep questions there, and no, they aren't related to the math. Bayes law doesn't tell you how to value things,  it's only useful for making decisions and updating beliefs once you've figured that out yourself.
  ```

  - u/None:
    ```
    So, is there a way to figure out values?
    ```

    - u/Chronophilia:
      ```
      Ask a philosopher. Statistics cannot help you here.
      ```

  - u/thyratron:
    ```
    From HPMOR

    > Of course, the point of a subjective Bayesian calculation wasn't that, after you made up a bunch of numbers, multiplying them out would give you an exactly right answer. The real point was that the process of making up numbers would force you to tally all the relevant facts and weigh all the relative probabilities. ... One version of the process was to tally hypotheses and list out evidence, make up all the numbers, do the calculation, and then throw out the final answer and go with your brain's gut feeling after you'd forced it to really weigh everything.
    ```

    - u/davidmanheim:
      ```
      Yes. Thinking about a problem is helpful. But you need to think through it yourself before the process quoted makes any sense.

      Also,  as intended to be applied here,  it badly conflates expected utilities and "raw" utilities.
      ```

- u/Transfuturist:
  ```
  Bayes doesn't do values, only probabilities. This is a utility problem, and without a utility function to be compared to, you can't make a choice either way.
  ```

- u/DataPacRat:
  ```
  What are your assumptions on the odds of some sapient form of life other than humans ever coming into existence? Is there a non-zero chance that humanity is the only form of sapience that would ever exist? If so, is there any utilitarian value to the extinction of all minds which can ever /give/ value to anything other than 'negative infinity', which would mean that the appropriate answer would always be 'keep humanity alive', no matter how much value there is in keeping other non-sapient species alive?
  ```

- u/DrPresidentMD:
  ```
  Correct me if I'm wrong, but I don't think your problem is a Bayesian calculation. It seems more like a morality question. 

  &nbsp;

  Remember that Bayes theorem states: 

  P (A|B)= P (B|A) *P (A)/P (B) 

  where A and B are events, P (A) is the probability of event A, and P (A|B) is the probability of event A given that event B has already happened. 

  &nbsp;


  A common example of someone using Bayes theorem is a doctor finding the probability that a patient really has breast cancer given a positive mammogram finding. [A would be the event of having breast cancer, B the event of having a positive mammogram finding. ] The best I can summarize is that bayes theorem let's us "update" our belief in the probability of some event after being given new information about the conditions that influence said event. 


  &nbsp;


  In your post you present a choice between two scenarios, one in which humans are made extinct, and one in which every life form save humans are made extinct--we're not given new information that let's us update beliefs on probabilities, but rather with situations that examine what we value.

  &nbsp;


  I think the obvious choice would be the one in which humans live. Most people value their own existence. However, this existence might be pretty bleak.  Though you tell us to assume humans would be able to live without other life forms, I imagine it would be quite difficult. Even if we ignore the problem of getting food without plants, animals, and fungi, human life would be very different without the almost unimaginable number of microorganisms that live on earth and throughout our very own bodies. (You may know that there are more bacterial cells than human cells in what you typically think of as "you"). That said, we would be freed from communicable diseases such as malaria, pneumonia, meningitis, etc. This scenario might make a pretty good story--would humans resort to cannibalism to survive? Could we synthesize enough amino acids, glucose, fatty acids, and vitamins using non living resources on earth? And what about the environmental effects? Without microorganisms and plants, the composition of the atmosphere would change--I think we'd eventually have to dedicate a lot of resources to splitting oxygen out of water.  

  &nbsp;

  Anyway, I hope this helps. Sorry for any spelling, formating, or factual mistakes--I'm typing this from mobile.
  ```

- u/eaglejarl:
  ```
  The question boils down to what you ascribe moral weight to and how much.  If a human has 100 'moralons', does a blade of grass have any number of moralons greater than 0?  How about an insect?  A bacteria?  A blue whale?

  If you can set those numbers, the choice is trivial -- just add up the total for humans and the total for non-humans and compare them.  You obviously end up with three scenarios:

  * Humans have greater moral weight; kill everything else.
  * Plants/animals have greater moral weight; kill humans.
  * They are exactly the same.  Kill all the non-humans because we are the ones making the choice and therefore we get to win ties.

  How to assign moralon values is left as an exercise for the reader.
  ```

- u/Sophronius:
  ```
  Ok, it seems nobody else is trying to answer the question, so I'll give it a go.

  The issue seems analogous to the argument for vegetarianism: There are a flagrilion more animals than humans. THEREFORE it is either the case that animals have significant moral worth which through the virtue of multiplication then eclipses all human moral worth, OR they have no moral worth at all.

  Then to find your utility function you could look at your own revealed preferences: You (probably) currently act as if even just one animal has some moral worth, which means that the answer would almost certainly be that letting all humans die is preferable.

  BUT the problem arises when you realize that human preferences are not consistent. We are willing to pay 1 dollar to save one bird, but not a thousand dollars to save a million of them. So something does not add up but the question is where the problem lies: Either we give up on trying to use math for moral questions OR we give up on trying to make our moral preferences consistent. Either way the prospect of moral progress becomes rather bleak...

  Personally I think the second option is the better one, which implies that choosing for the humans to live is probably better, unless you're an extreme animal rights activist, since that is what most of us would intuitively prefer.
  ```

  - u/None:
    ```
    Two weeks and thirty-four comments later we finally have a winner! Thank you, user who actually tried to find a solution instead of putting the question aside because it's impossible to answer or only bashing the op because he used the wrong terms, even if the meaning of the question is pretty evident!

    I'll fondly remember your attitude towards problems, and I'll let you know that it's inspiring for me, even if there's only one person out of, well I don't really know how many read the post, but just one in the **rational subreddit** that's willing to use rationality to solve problems, even hypothetical ones.

    I'll let you know that if I ever, in the future, were to build a giant company of whatever, I won't come short of proposing you a position as big-ass manager with a shitload of pay, because you're the one of the very few people I've ever found who's willing to use his brain to *really* figure problems out.
    ```

    - u/Sophronius:
      ```
      Well I'm very flattered, thank you! If I ever get to realize my dream of forming a team of rationalists dedicated to improving the world, I'll contact you as well. :-)

      I share your frustration with the rationalist community, largely because I strongly believe that rationalists should win:  http://lesswrong.com/lw/7i/rationality_is_systematized_winning/

      I've always felt that there must be a team of competent people working in secret to save the world somewhere, but somehow that doesn't seem to be the case.
      ```

---

