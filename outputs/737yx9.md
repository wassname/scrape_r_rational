## "Safe" AI [RT]

### Post:

While reading Friendship is Optimal, the thing that really stuck out was the motivation of the AI. Say you were to take that motivation away from an AI and turn it into a glorified question-answering machine. Would that make the AI safe?

To clarify: This AI's core principal is nonexistent. It has no purpose, no endgame, no values. All it can do is answer questions asked to it by humans.

Doing this would dramatically decrease the usefulness, but would the decrease in strawberry incidents make up for the lower utility?

### Comments:

- u/Roxolan:
  ```
  [Further reading.](https://wiki.lesswrong.com/wiki/Oracle_AI)

  (Also, the appropriate sub is probably /r/ControlProblem.)
  ```

  - u/nogamepleb:
    ```
    Thanks! That clears it up.
    ```

- u/None:
  ```
  Post is approved.  It's about something someone wrote and posted here, as well as a topic people here like and consider important.  It stays... horrific technical imprecision aside.
  ```

- u/LeifCarrotson:
  ```
  > Say you were to take that motivation away from an AI and turn it into a glorified question-answering machine ... It has no purpose, no endgame, no values. All it can do is answer questions asked to it by humans.

  You're postulating it's something like Siri.  Or Alexa.  Or Google.  But we have those things, and you're probably aware that they are less than ideal.  They really only respond to the things they're programmed to respond to.  More precisely, they're constructs or tools made using the intelligence of the programmers and engineers who built them, not their own intelligence.  And that's not what we want.

  I'm not suggesting that a calculator isn't useful.  It's just never going to be what you're really hoping for when you interact with a question-answering AI.  If you had a true artificial intelligence, it seems that it would be necessary to give it motives: A desire to understand communication. A desire to answer questions asked of it. And let's not forget to add a desire to make sure those answers are correct.  Finally, it would need the resources to achieve those goals.

  It's only a slight perversion of those goals to imagine a machine that desires people to ask it questions.  Or that asks questions of itself.  Then you just have an information maximizer, not a friendship and ponies maximizer, which is at least as bad.
  ```

- u/vakusdrake:
  ```
  Given what a great deal of the discussion here is about I think [this article](https://www.gwern.net/Tool-AI) is relevant. As it talks about how even AI without explicit utility functions are likely to not avoid many of the problems you might expect. Due to them acting as though they do have a utility function, self improving so they have one, or creating subagents that have utility functions.
  ```

- u/None:
  ```
  Taboo. The term. "AI".

  Also: Do not. Overgeneralize. From. Fictional. Evidence.
  ```

- u/ShiranaiWakaranai:
  ```
  One problem with this is that every AI must have self-improvement as its purpose. The whole point of developing an AI is to have something smarter than yourself write itself to be even smarter, ad infinitum until it has intelligence far far exceeding our own. If it isn't motivated to write itself, then what you have is just a bit smarter than humans at best. Which would be safe, but not very useful. (Because as /u/LeifCarrotson said, that's basically Siri and the other AIs we have today.)

  And you absolutely do not want to remove all motivation other than self-improvement, because then it will almost certainly sacrifice humanity for further self-improvement.
  ```

  - u/Daneels_Soul:
    ```
    What definition of "AI" are you using in order for this to be true? How do you square this with things that are currently labelled as "AI" that maybe train on examples, but never substantively rewrite their code or progress beyond solving image classification problems (or whatever it is programmed to do)?

    And you can write programs that are smarter than you are (in specialized ways at least) without giving them self-improvement. Linear regression algorithms are way better than I am at noticing patterns in certain kinds of high dimensional data.

    At the very least, if you want to go the singularity route, it seems that the far safer way to do it is to build oracle AIs, and simply ask them questions that allow you to better design the next generation of oracle AIs. That way at least humans remain in the loop.
    ```

  - u/ben_oni:
    ```
    > every AI must have self-improvement as its purpose

    No. Just wrong. Even assuming that superintelligence is the goal, this is still wrong. Consider the differences between [hard and soft takeoff](https://en.wikipedia.org/wiki/Intelligence_explosion).
    ```

---

