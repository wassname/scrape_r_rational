## [D] Saturday Munchkinry Thread

### Post:

Welcome to the Saturday Munchkinry and Problem Solving Thread! This thread is designed to be a place for us to abuse fictional powers and to solve fictional puzzles. Feel free to bounce ideas off each other and to let out your inner evil mastermind! 

Guidelines:

* Ideally any power to be munchkined should have *consistent* and *clearly defined* rules. It may be original or may be from an already realised story.
* The power to be munchkined can not be something "broken" like omniscience or absolute control over every living human.
* Reverse Munchkin scenarios: we find ways to beat someone or something  *powerful*.
* We solve problems posed by other users. Use all your intelligence and creativity, and expect other users to do the same.

Note: All top level comments must be problems to solve and/or powers to munchkin/reverse munchkin.

Good Luck and Have Fun!


### Comments:

- u/DRMacIver:
  ```
  I'm looking to expand the "age of failed dreams" rules for Programmer at Large to get a sense of the boundaries of technology and what I should be looking out for. What I'm looking for are rules that essentially guarantee that technology in the long-run is a plateau.

  Here are the rules I have so far (in no particular order, editing to add new ones as they get suggested or I remember/think of them - I've failed to write these down so far):

  * FTL is outright physically impossible
  * Ditto most things that would require physics that is currently "exotic" - e.g. no antigravity, no stasis fields
  * There are relatively fundamental scaling constraints on intelligence - you can probably get something twice as intelligent according to some reasonable metric as a peak baseline human, but you can't get anything 10x as intelligent.
  * No "sci-fi level" nanotech. there's plenty of molecular manufacturing, etc. but self-repairing machines, nanofog construction are all somewher between hard and impossible, with the more exciting the application sounds the closer it is to impossible.
  * Intelligence is fragile and tends not to copy well - if you try to copy a brain you'll end up with a brain at most roughly similar to the original, and it might well just not work
  * Intelligence is chaotic - it's very hard to produce an intelligence to order and tends to be very sensitive to initial conditions
  * AI is possible within the above constraints but tends to be quite expensive to run (the human brain may not be the smartest, but as far as intelligence : resource efficiency ratios go it's doing rather well). The best AI are not substantially smaller than a human brain, are tied to their hardware, and are the same order of magnitude speed and intelligence as a smart human (though they may be off the high end of that spectrum many aren't). AI should be approximated as "like very smart humans that use more resources and have faster interfaces to non-sentient computers".
  * Brains are hard to interface with in a reliable way. *Some* direct nerve stimulation is possible, but if you want to fake senses you're more or less required to go via the organs that are designed for that - e.g. implanted screens in contact lenses are viable, but just plugging into the optic nerve isn't.
  * Some as of yet unspecified sociological mechanism (some combination of factors including resource efficiency and level of infrastructure required for maintenance) means that pure AI civilizations tend to be less stable than human civilizations.
  * No non-sentient technological civilization is possible long-term. They tend to run into outside context problems too quickly because so many problems count as such for them
  * Sentient civilization tends to collapse when it grows too large in a confined space like a solar system - resource contention and coordination problems rise pretty sharply with the population.
  * Bodies are hard to repair and eventually break down no matter what you do. Life extension is possible, but it tends to hit some pretty hard limits after 200-300 years no matter how good you are at it.

  (Note: I make no claim that these are necessarily *realistic* constraints)

  Now, you have 10,000 years to play with. How do you push the boundary of what's possible? Can you effectively bootstrap your civilization to godhood?
  ```

  - u/None:
    ```
    [deleted]
    ```

    - u/DRMacIver:
      ```
      I haven't read the whole list, but I think assume that if it says hypothetical then probably not and if it doesn't then probably yes.

      In general anything we currently have can be extrapolated to its logical conclusion, along with anything that is more or less essential for interstellar travel to be viable, but most other things are ruled out.
      ```

  - u/buckykat:
    ```
    Find the joker messing with the universe simulation's settings and have a frank exchange of views.

    To unpack a bit: I would consider unaugmented humans being within an order of magnitude of as smart as it gets extremely strong evidence for some variation on the simulation hypothesis.
    ```

    - u/DRMacIver:
      ```
      > To unpack a bit: I would consider unaugmented humans being within an order of magnitude of as smart as it gets extremely strong evidence for some variation on the simulation hypothesis.

      Literally true, but it's relatively hard to hack the author's wetware.

      Some of these rules might be the result of less what is physically possible and more what is reachable when starting from a human baseline.

      Or, alternatively, if you're not smart enough to figure out how to work around them you're probably not smart enough to figure out reality's privilege escalation exploits either. ;-)
      ```

      - u/buckykat:
        ```
        Yeah, the only reason for AI not to work is to see what the meatbags do on their own. Which is why coherent multi-person solipsism is the specific simulation hypothesis most suited to the data. Which means the Author is the enemy.

        So, I dunno, try to make the world as boring as possible until and unless you let me foom.
        ```

        - u/696e6372656469626c65:
          ```
          Unfortunately, I am cognitively incapable of letting you FOOM, since then you would be smarter than I am and therefore be impossible for me to write. So I think I'll just trash you instead and come up with a character more amenable to my story's needs.
          ```

- u/failed_novelty:
  ```
  Suppose you have one wish, which must be written in a single English sentence using only words that a typical college freshman would understand. The wish MUST destroy all bedbugs or it won't come true (the genie is very fickle).

  What is the most you can gain from the wish?
  ```

  - u/luminarium:
    ```
    How about "exterminate all species that the majority of humanity would agree to want to exterminate if asked after receiving information on how much that species helps and harms humanity". That would get rid of bed bugs, mosquitoes, malaria, west nile, dengue, yellow fever, zika, human-infecting parasites in general, species that infect our domesticated animals and crops, weeds, pathogenic bacteria and viruses in general, and unwanted species in general.
    ```

    - u/CCC_037:
      ```
      You are going to make a *mess* of several ecosystems.
      ```

  - u/DRMacIver:
    ```
    I have so many questions...

    Most important question: To what degree can I interrogate the genie about its capabilities?

    Assuming that is large...

    What are the rules on use of connectives? Is there any reason I can't just say "I wish for you to destroy all bedbugs and (whatever other wish maximizes my gain)?". What if the two are logically connected? (Destroy all bedbugs and grant me one extra wish per bedbug you destroy).

    What are the limitations on the genie's predictive capabilities? Can the genie simulate a copy of me? Can I wish to kill all creatures in the solar system that after receiving answers to any set of questions about them I wanted to ask I would choose to end the life of?

    What is the timescale on which the wish operates? If I wished for omnipotence but precommitted to destroying all bedbugs as soon as I acquired it, would that satisfy the conditions of the wish? What if I baked that precommitment into the wish?
    ```

    - u/crivtox:
      ```
      Wouldn't "whatever other wish maximizes my gain "already kill all bedbugs since the best wish has to include killing all bedbugs or else it wouldn't do anything and therefore it wouldn't be the best wish?
      ```

      - u/DRMacIver:
        ```
        It was intended as a placeholder for "I have explored the limits of the wish granting system and figured out the optimal strategy for it ignoring the bedbug constraint" rather than the literal thing you should ask for.

        (Do not write genies blank cheques asking them to optimise for your coherent extrapolated volition unless you're really sure about both your CEV and the genie's trustworthiness)
        ```

  - u/crivtox:
    ```
    "I wish what I should wish acording to my values".That should get me something similar to cev and would incluye destroying all bedbugs because otherwise it wouldn't be the thing I should wish for according to my current values.alternatively "Grant me my coherent extrapolated volition "maybe would work , a typical college freshman understands all the words , maybe not the meaning of the frame but the genie didn't want frases that a typical college freshman understands he wanted frases made by  words that a typical college freshman would understand.
    ```

    - u/awesomeideas:
      ```
      DjinOS warning 112358: Recursive wish detected. Input wish "I wish what I should wish according to my values" returned processed wish "I wish what I should wish according to my values."

      DjinOS warning 43: Wish has already been fulfilled at time of wishing. Process terminated with status 0. 

      Note: bedbugCheck has not been run.
      ```

  - u/Jiro_T:
    ```
    "I wish for the effects written down on this piece of paper."  (Where the piece of paper includes a list of effects including both destroying bedbugs and making you rich).

    Alternately, "I wish for the following two things to come true: the destruction of all bedbugs, and X" (where X is basically a standard wish for good stuff for yourself).  You should word the wish to specify "the following two things" so the genie can't decide that the sentence ends after the part about the bedbugs.

    Note that it is very difficult to just change the scenario to "you can only make a wish that doesn't ask for two separate things," since "things" isn't a concept that divides reality at the seams.
    ```

- u/LazarusRises:
  ```
  Based on Heroes Save the World, one of my current favorite ratfics:

  You have the ability to make coins disappear by touching them. You're not sure where they go, but they're irretrievable. Doing so does not release a coin's worth of energy (at least anywhere you know of). It also doesn't burn any more calories than touching any other object. 

  There's no canonical basis for this, but let's assume that you can vanish a coin by touching it with any exposed skin, not just your fingertips. 

  What do you do? 

  EDIT: You have to consciously will the coin to vanish while you touch it.
  ```

  - u/Adeen_Dragon:
    ```
    Nuclear waste coins!
    ```

    - u/LazarusRises:
      ```
      Only until the radiation poisoning knocks you out...
      ```

      - u/Gurkenglas:
        ```
        Have the coins brought in contact with only a long-grown fingernail through use of a shielded tube if fingernails count, or use lead-lined coins.
        ```

      - u/Kilbourne:
        ```
        One enormous coin.
        ```

        - u/Gurkenglas:
          ```
          And at that point you might as well do garbage disposal coins on the side, depending on what counts as a coin.
          ```

          - u/Kilbourne:
            ```
            Well, yes.
            ```

---

