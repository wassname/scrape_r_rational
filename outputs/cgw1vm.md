## The Ethics of Timelooping

### Post:

I am writing a story (well more thinking about and working on the world building (hopefully) for now) where my MC has the ability to rewind time to designated save points. Let's call him John. So the way this works is John has the ability at a given time t\_x to set a save point (provided he has not exhausted all his save slots) and at a future time t\_y, John can "rewind" to t\_x. The way John experiences this "rewinding" is that at t\_x John would have all his memories of the duration \[t\_x, t\_y\], so it's a form of mental time travel (this is probably the main form of time travel (at least on a multiverse wide scale) that would be covered in this story). How this time travel actually works has some profound ethical implications.

&#x200B;

I do not have a canon model of how time travel works in my story (if I find out that my current model is incoherent, I would throw it out, otherwise I would *likely* live with the implications of it), so while I do have a favoured mental framework for thinking about time travel, there is not much reason to privilege my particular framework and treat it as anything other than just another model in the model space of how time looping abilities can work.

&#x200B;

My world is pretty big with several universes (probably an infinite amount of them), but it is probably strictly smaller than the Tegmarkian multiverse. Different universes have different temporal interactions with our universe, but John's time rewind ideally (I'm no physicist (actually, you can probably assume I have no knowledge of physics beyond what little I retained from cramming for high school exams, but I do intend to keep a coherent model. It is integral to the story that the time rewind occurs everywhere, so if certain details of my world makes that impossible, I would probably shrink it as needed) rewinds time on a multiverse wide scale.

&#x200B;

The interesting question for the purposes of this thread, is: what happens at t\_{y+1}? After John jumps back in time, what happens to the rest of the world?

&#x200B;

There are many different ways of thinking of this. The approach I'm currently using is to imagine timelines. Whenever a rewind occurs, the timeline from origin to t\_x (the point of the rewind) is duplicated (or perhaps the duplicate already exists), and the world state at t\_x is updated to reflect the new information that John has. Let's call the "source" timeline T\_1, and the duplicate timeline T\_1.1. If we work with this framework, then what happens to T\_1 at t\_y has very profound implications. The way I see it there's two basic outcomes:

1. T\_1 continues operation normally.
2. T\_1 is terminated at t\_y.

&#x200B;

If T\_1.1 is terminated at t\_y, then by rewinding time John is committing omnicide. He's literally killing an infinite number of people each time he rewinds creating infinite disutility (or the upper bound of disutility if you use a bounded utility function). Suppose John assigns a probability of p to the hypothesis "rewinding time terminates the source timeline". I'm not sure how John assigns his probabilities, but whatever prior he's using — as long as it's sensible — I would imagine that it wouldn't assign a probability below say 10^(-10) to the above hypothesis (probably several orders of magnitude more in fact). If John is doing an expected value calculation, then the possibility of omnicide should dominate his calculations (even with a bounded utility function) when evaluating whether or not to rewind time. In short, John seems to be getting [Pascal mugged](https://en.wikipedia.org/wiki/Pascal%27s_mugging) (only without an extant mugger).

&#x200B;

I mentioned above that either T\_1.1 is created from T\_1 due to the rewind, or T\_1.1 already exists. I think this also has interesting ethical implications of its own. For one, if the rewind causes the creation of a new timeline, then each rewind causes amounts of utility equivalent to the total utility history of the multiverse (and if that number is positive), then the moral thing for John to do is to rewind as many times as possible (if the number is negative John creates even more disutility). Interestingly, if the rewinds creates the new timelines, and the multiverse is net positive utility, then the positive utility created by the multiverse may outweigh the negative utility created by omnicide.

&#x200B;

As an aside, even if hypothesis 1 was true, then John may still cause omnicide as there's a multiverse ending event (John believes his time rewinding ability is to avert it (he auto rewinds to a fixed save point at when he first awakened his powers if he dies or gets stuck in an infinite loop)), so if the time rewind clones the timeline, if all possible timelines don't already exist and rewinding creates a new timeline, then it seems to me that John rewinding time is omnicide.

&#x200B;

Sidestepping the talk of timelines for a bit, it seems to me that no matter how you dice it timelooping/rewinding has profound ethical implications. After the rewind happens, it's possible to treat the period of time \[t\_x, t\_y\] as a mere simulation of the future which the MC now has knowledge about (perhaps John's power merely simulates the multiverse (and informs him of the results of said simulation)). This brings up the interesting question of whether simulated copies of an agent have moral worth. If 1,000,000 copies of Jane Smith exist and I terminate 100,000 of them have I created disutility? If one believes that simulated copies don't have moral worth separate from the original, then I'd like to ask you this question: if someone told you that you were a simulation and decided to terminate you, would you consider it murder? Alternatively, if we found out that this universe was a simulation (and that there was a real "earth" somewhere) should we consider the termination of the said simulation omnicide? In short, do you believe in [mindcrime](https://arbital.com/p/mindcrime/)([scraped on github (cause Arbital isn't reliable](https://emma-borhanian.github.io/arbital-scrape/page/mindcrime.html)))?

&#x200B;

If one privileges some "real" timeline, I'd like to point out that if the simulation goes on for long enough, the simulated agents may sufficiently diverge from their "real" copies that they're no longer the same moral persons. Alternatively, some entirely new agents may be created in the simulation that would not exist in the "real world".

&#x200B;

&#x200B;

I find it interesting that literally every other action John takes seems negligible in the face of the astronomical amounts of (dis)utility created by his time rewinding ability. I also wonder how someone who believes(believed?) in "[shut up and multiply](https://wiki.lesswrong.com/wiki/Shut_up_and_multiply)" would react to that realisation.

&#x200B;

&#x200B;

Aside from the ethical implications of time rewinding in my story, it seems that time rewinding (or time looping in general) has massive ethical implications.

&#x200B;

&#x200B;

The above timeline framework is the current framework I have for thinking about it. It is not mandatory to think about it in this framework, perhaps it's incoherent (in which case I would discard it) or you have a more powerful framework that is much easier to work with (do tell), but barring those two scenarios, it would probably be more conducive to conversation and discussion if we used the timeline framework.

The above was very rambly and may have been less than coherent at times. Feedback on my writing is appreciated as well. If you have stuff that you think I should read relating to the contents of this post, time travel, rational fiction or writing in general, do tell.

### Comments:

- u/speakerforthe:
  ```
  Honestly I had some of the same thoughts as you while thinking about time travel in a series. If you have read Mother of Learning, this idea kinda gets brought up.

  I think the most "ethical" view of time travel is that you are merging with a different timeline of yourself. As to what happens to you in your old timeline? You either drop dead or disappear.

  &#x200B;

  Another way to view a purely mental version of time travel is that you are simply receiving mental messages from another version of yourself. This adds the constraint that your future self will only take actions that they reasonably think will succeed.
  ```

  - u/DragonGod2718:
    ```
    >I think the most "ethical" view of time travel is that you are merging with a different timeline of yourself.

    The soul travelling across timelines is something I considered as well. However, the nature of those timelines matter:

    * Did the time travel create the other timeline?
    * What happens to the old timeline when the travel occurs?

    &#x200B;

    I do read Mother of Learning (currently waiting for it to finish).
    ```

    - u/speakerforthe:
      ```
      Assuming the author gets to choose a system with the least ethical implications:
      That's a good point, but I guess I have less issues with the idea of creating a universe. 
      The rest of the old timeline continues to exist just, just without the protagonist. Like you said, the alternative is killing everything.
      ```

      - u/DragonGod2718:
        ```
        In my particular story, it doesn't actually matter. There's a multiverse ending event in the near future, so whether it terminates or doesn't is irrelevant as the timeline ends anyway somewhere down the line.
        ```

- u/None:
  ```
  [deleted]
  ```

  - u/JusticeBeak:
    ```
    While we're questioning the basic moral framework we're using, I think it's worth noting that there exists a utility function (that I find quite agreeable) which would open the door to only giving moral weight to unsimulated lives. In preference utilitarianism (or desire-satisfaction consequentialism, or whatever), the value of an action is determined by how well its consequences meet everyone's well-informed desires. 

    Since many people seem to have a strong preference for even a mediocre reality compared to a simulated life in paradise, it seems feasible that improving the real world at the expense of simulated lives would be morally permissible. Granted, I'm no empath, so I may be wrong.

    Side note: At what point does a simulated life become morally valuable? Does your idea of a character have any moral value as a simulated life?
    ```

    - u/DragonGod2718:
      ```
      >Since many people seem to have a strong preference for even a mediocre reality compared to a simulated life in paradise, it seems feasible that improving the real world at the expense of simulated lives would be morally permissible. Granted, I'm no empath, so I may be wrong.

      I addressed this:

      >If one believes that simulated copies don't have moral worth separate from the original, then I'd like to ask you this question: if someone told you that you were a simulation and decided to terminate you, would you consider it murder? Alternatively, if we found out that this universe was a simulation (and that there was a real "earth" somewhere) should we consider the termination of the said simulation omnicide? In short, do you believe in [mindcrime](https://arbital.com/p/mindcrime/)([scraped on github (cause Arbital isn't reliable](https://emma-borhanian.github.io/arbital-scrape/page/mindcrime.html)))? 

      &#x200B;

      Is that a bullet you're willing to bite? Would you accept the omnicide of this universe to benefit some other "real" universe?
      ```

      - u/JusticeBeak:
        ```
        I'm not sure, but I think a similar and related question that could help find an answer is whether you root for Neo in The Matrix. Regardless, my intent was to bring up a moral framework that could justify what you seem to take as trivially inexcusable.
        ```

        - u/DragonGod2718:
          ```
          >I'm not sure, but I think a similar and related question that could help find an answer is whether you root for Neo in The Matrix. 

          Haven't watched The Matrix.

          &#x200B;

          >Regardless, my intent was to bring up a moral framework that could justify what you seem to take as trivially inexcusable. 

          I don't find the moral framework interesting unless you (or others) are willing to bite the bullet of omnicide. Talking about some "real" world is all fine and dandy until people are presented with the hypothesis that they are a simulation.
          ```

          - u/aeschenkarnos:
            ```
            You should watch The Matrix, at least the first one. It's quite good.

            Presenting people with the hypothesis that they're (in) a simulation is generally not going to be taken any more seriously than the idea that the world was created Last Tuesday with our memories intact. It's too ludicrous for most folks to contemplate at all, and even among those of us who do contemplate it, it's really only *useful* if there are some exploitable implications, eg time-looping or reincarnation with memories intact, or local deviations from the norms of physics.
            ```

            - u/DragonGod2718:
              ```
              >Presenting people with the hypothesis that they're (in) a simulation is generally not going to be taken any more seriously than the idea that the world was created Last Tuesday with our memories intact. It's too ludicrous for most folks to contemplate at all, and even among those of us who do contemplate it, it's really only *useful* if there are some exploitable implications, eg time-looping or reincarnation with memories intact, or local deviations from the norms of physics.

              Well the purpose in this case is that apparently, some people believe that simulated copies don't have moral personhood. I merely want to know if they are willing to bite the bullet of omnicide. If people don't find "what if we are the simulation" worth considering, then they are not seriously reckoning with the concept of simulated persons. It shouldn't be possible for a given agent to determine whether they are real or the simulation (assuming high fidelity simulations).
              ```

              - u/aeschenkarnos:
                ```
                Simulated entities might not necessarily have moral personhood from *outside* the simulation, but if we exist on an equal basis with each other as simulations *inside*, then we have the same moral personhood. (Which is a similar argument to the theological principle that it's OK for *God* to kill a baby but not *you*.)
                ```

                - u/DragonGod2718:
                  ```
                  >Simulated entities might not necessarily have moral personhood from *outside* the simulation, but if we exist on an equal basis with each other as simulations *inside* , then we have the same moral personhood.

                  Aah, this makes sense. I guess my rejoinder is along the lines of: "what if there's no outside?" What if outside the simulation is some alien god and there's no other moral agent of any relevance. Coming back to my story, what if all the timelines are equally real?
                  ```

  - u/DragonGod2718:
    ```
    The MC started out as someone with a moral framework similar to mine (so very selfish, but think utilitarianism is the only sensible way to make decisions at the group level. They strongly believed in "[shut up and multiply](https://wiki.lesswrong.com/wiki/Shut_up_and_multiply)").
    ```

    - u/GemOfEvan:
      ```
      Easy solution then. Utilitarianism is great for enforcing mutual cooperation between free agents. Turns out, when you have reality-breaking powers, you don't have to care about the feelings of people who can't also time travel.
      ```

      - u/DragonGod2718:
        ```
        Interesting, but it's an argument I'm pretty hesitant to make.
        ```

- u/None:
  ```
  An argument I could see being made is that timelooping is no different than exercising free will. If I take action X at time 0, then by time 10^10^10^10, 100 trillion people will have died. If I instead take action Y, 101 trillion will have died instead. This happens for every action you take, conscious and unconscious. 

  There’s also the issue of whether it’s better to exist and be killed or to never exist at all. The protag is basically creating everything living in the universe every time he goes back, but also kills everything from the last branch. 

  Maybe check out [population ethics](https://en.wikipedia.org/wiki/Population_ethics).
  ```

  - u/DragonGod2718:
    ```
    >There’s also the issue of whether it’s better to exist and be killed or to never exist at all. The protag is basically creating everything living in the universe every time he goes back, but also kills everything from the last branch. 

    I addressed this:

    >I mentioned above that either T\_1.1 is created from T\_1 due to the rewind, or T\_1.1 already exists. I think this also has interesting ethical implications of its own. For one, if the rewind causes the creation of a new timeline, then each rewind causes amounts of utility equivalent to the total utility history of the multiverse (and if that number is positive), then the moral thing for John to do is to rewind as many times as possible (if the number is negative John creates even more disutility). Interestingly, if the rewinds creates the new timelines, and the multiverse is net positive utility, then the positive utility created by the multiverse may outweigh the negative utility created by omnicide.

    &#x200B;

    I'll give the Wikipedia article a look.

    &#x200B;

    >An argument I could see being made is that timelooping is no different than exercising free will. If I take action X at time 0, then by time 10101010, 100 trillion people will have died. If I instead take action Y, 101 trillion will have died instead. This happens for every action you take, conscious and unconscious. 

    The scale is different, but timelooping isn't unique in having an astronomical effect (the effects are just much more imminent).
    ```

- u/ABZB:
  ```
  I don't see a problem - everything he unmakes by rewinding will any of:  


  * happen again once t\_y is reached (so at the second iteration of t\_y there is no net change, no harm no foul)
  * happen again, except that anything subject to true randomness (if that exists (in this setting)) might have a different outcome (in which case, the expected net good shouldn't change. I would compare this to doing anything IRL, where some nth-degree consequence has some chance of have a terrible effect - after a point, we must dismiss it). The only time this would be morally wrong is if some horrifically unlikely event occured, such that it changing would be very bad, and after rewinding it is very unlikely to occur again.
  * Is something he, at t\_x, can affect (so as long as he acts properly based on his knowledge, it is good).

  &#x200B;

  Generally speaking, given the infinite time that time looping gives, a sufficiently competent and good character will probably use it to find maximally good paths anyway, which probably outweighs the temporary loss of the first kind and the probabilistic risk of the second.
  ```

  - u/DragonGod2718:
    ```
    * Different people would get born.
    * The people in the rewound time may sufficiently diverge from those in the other timeline to be different moral persons.

    Addressed here:

    >If one privileges some "real" timeline, I'd like to point out that if the simulation goes on for long enough, the simulated agents may sufficiently diverge from their "real" copies that they're no longer the same moral persons. Alternatively, some entirely new agents may be created in the simulation that would not exist in the "real world".

    A lot of the impact of the time looping is outside his light cone, but true randomness is meant to exist in this setting. The impact John can have seems like it would be outweighed by the astronomical scales involved in timelooping (no matter how competent he is).
    ```

- u/Nimelennar:
  ```
  >As an aside, even if hypothesis 1 was true, then John may still cause omnicide as there's a multiverse ending event (John believes his time rewinding ability is to avert it (he auto rewinds to a fixed save point at when he first awakened his powers if he dies or gets stuck in an infinite loop)), so if the time rewind clones the timeline, if all possible timelines don't already exist and rewinding creates a new timeline, then it seems to me that John rewinding time is omnicide.

  I'm going to focus on this, because this is a relatively easy one.

  The first rule of first aid is "Don't become another patient."  If you get yourself injured or killed trying to help someone eldeyr, you've just made the problem worse for the next person to deal with.  So, if you see someone bleeding out at the bottom of a steep incline, and, you're not confident you can safely descend in time to save that person... or sounds heartless, but your priority has to be keeping yourself safe.

  How does this pertain to your time loop?

  From what you've described, the universe your character is currently in would be the person at the bottom of the cliff, bleeding out.  They're already dying, through no fault of John's, and getting himself killed to rescue them just makes things worse.

  Now, if he's using his ability frivolously, then that's a different thing.  If John's looping through time to get into the pants of someone he's attracted to, then yes, he should feel bad about the people he's condemned to sure.  On the other hand, if he chooses to loop to get closer to a solution to the universe-ending threat, he's not killing anyone.  What he's doing is prioritizing the patients he can save over the ones he can't, which is just basic triage.
  ```

  - u/DragonGod2718:
    ```
    >Now, if he's using his ability frivolously, then that's a different thing.  If John's looping through time to get into the pants of someone he's attracted to, then yes, he should feel bad about the people he's condemned to sure.  On the other hand, if he chooses to loop to get closer to a solution to the universe-ending threat, he's not killing anyone.  What he's doing is prioritizing the patients he can save over the ones he can't, which is just basic triage.

    Thank you very much for this. This is interesting as it constrains John from frivolous use of his looping ability (without me designing any mechanics to do so).
    ```

- u/chaogomu:
  ```
  I'll sidestep ethics and say that prettymuch any random person you gave this power to would do two things eventually. A) fuck around and live it up a little bit while acting thoughtless about the lives of others.
  B) do a "perfect play-though" i.e. when they did decide to move forward in time they would try to set shit up to be their idea of perfect.  Either by always having the best thing to say in mind, or getting the girl, or just spending a lot of time hanging out with friends and family (who are now mysteriously rich and free to hang out)
  ```

- u/Fresh_C:
  ```
  Interesting post. I've had similar thoughts when thinking up my own time travel story, though you've touched on some things that never occured to me.

  >I mentioned above that either T_1.1 is created from T_1 due to the rewind, or T_1.1 already exists. I think this also has interesting ethical implications of its own. For one, if rewind causes the creation of a new timeline, then each rewind causes amounts of utility equivalent to the total utility history of the multiverse (and if that number is positive), then the moral thing for John to do is to rewind as many times as possible (if the number is negative John creates even more disutility). Interestingly, if the rewinds creates the new timelines, and the multiverse is net positive utility, then the positive utility created by the multiverse may outweigh the negative utility created by omnicide.

  This bit is particularly interesting to me. I wonder if it even makes sense to think of multiple timelines in terms of utility, unless each timeline can interact with each other. Utility is pointless unless it's benefiting someone, so even if you're creating infinite worlds with a finite utility, the fact that you're creating infinite people to use up that utility ultimately negates that progress (IMO).

  The only counter argument I can think of, is that the knowledge John brings from the future would potentially allow that world to better utilize the resources available to the people of that world. So in that sense, even though the same resources would be available in the new timeline that were available in the old one, The way they're used could make them more valuable to the New timeline as a whole. So in that case, there would potentially be utilitarian value in time travelling like this.

  On a slightly unrelated note: I'm curious what would happen to the John of T_1 when he time travels to T_1.1. Esentially this is a transfer of memories or conciousness instead of a bodily transfer. So assuming a new timeline is created and the old one is still in tact, there's a few options. John's body on T_1 could just drop dead on the spot or have him be a brain-dead vegetable. Or the John on T_1 experiences no consequences. Simply a copy of his memories are sent to the new timeline and the John of T_1 is stuck dealing with his original timeline without reaping any of the benefits of knowledge that the new John on T_1.1 will have.

  In any case, there's a lot here to explore. Good luck with your story and good luck not confusing people!
  ```

  - u/DragonGod2718:
    ```
    >This bit is particularly interesting to me. I wonder if it even makes sense to think of multiple timelines in terms of utility, unless each timeline can interact with each other. Utility is pointless unless it's benefiting someone, so even if you're creating infinite worlds with a finite utility, the fact that you're creating infinite people to use up that utility ultimately negates that progress (IMO).

    Well ceteris paribus, do you consider creating new lives (who would raise both total and average utility) a good thing? If you do, then I think you'll consider creating the new multiverses beneficial.

    &#x200B;

    > On a slightly unrelated note: I'm curious what would happen to the John of T\_1 when he time travels to T\_1.1. Esentially this is a transfer of memories or conciousness instead of a bodily transfer. So assuming a new timeline is created and the old one is still in tact, there's a few options. John's body on T\_1 could just drop dead on the spot or have him be a brain-dead vegetable. Or the John on T\_1 experiences no consequences. Simply a copy of his memories are sent to the new timeline and the John of T\_1 is stuck dealing with his original timeline without reaping any of the benefits of knowledge that the new John on T\_1.1 will have. 

    Or T\_1 is terminated?
    ```

- u/AuthorBrianBlose:
  ```
  First, your story sounds fascinating.  I hope your viewpoint character explores all of the ethical ramifications over time as he or she learns more about the process of what is happening, as it would add a lot of weight to the arguments if they applied to actions already committed and not just hypothetical future actions.

  Second, how would a time-traveling character even know the fate of a timeline he or she left behind?  Assuming that you jumping back to a save point has destroyed an entire universe is a rather large assumption and I don't see how someone could collect data on it either way.  The only exception I can see to this would be if setting a save point created a mental simulation and there was some tell to indicate this was the case (perhaps a range limitation causes only a small light cone to be copied and the stars begin to go dark after a few days/weeks/months/years) or if a time limit applied (simulated universe can only last for 90 days, after which point it either collapses or becomes glitchy -- though this tell would make the moral calculus a little easier, which may be a detractor if you want to explore that aspect).

  Third, I wouldn't recommend having your character adhere too strictly to a moral framework.  It could really get in the way of telling a good story if time travel never actually happens (after presumably a first accidental incident) because of moral reasons.
  ```

- u/flodereisen:
  ```
  From my understanding of speculative multiverse theory, I do not see a problem. All time frames of all \*verses exist eternally. By rewinding, he returns to a frame, which in a linear perspective comes "before", but that is only his perspective. All other infinite time frames, even the ones he left behind exist timelessly.

  If this interpretation is wildly off from your ideas, please ignore! I do think that reality, not even fiction, is that way.
  ```

- u/N0xS4v4g3:
  ```
  it seems to me that your mc is less timelooping *per se* and more that they have a form of precog. much like Coil in wildbow's Worm, theyre not creating or destroying timelines (or even moving between them) but instead are running an incredibly detailed simulation of the consequences of n choices over the period [t_x, t_y]. so instead of T1, T1.1...T1.n just have knowledge from t_y appearing at t_n which im pretty sure would allow your mc to create maximal utility within his constraints. as to the mindcrime question i think that its probably not only prohibitively expensive to worryabout in terms of opportunity cost compared to astronomical stakes  but probably impossible to avoid commiting once you have an agent running computations of sufficient advancement to be considered moral agents in and of themselves. honestly im pretty sure some humans capable of modelling behavior in arbitrary intelligences well could indeed already be committing mindcrime
  ```

  - u/DragonGod2718:
    ```
    One of the mechanics I'm considering is incredibly detailed simulation, but that doesn't sidestep the issue at all. There would still be mindcrime in that scenario.
    ```

    - u/N0xS4v4g3:
      ```
      yeah sure but if you compare ~8,000,000,000*(n-1) simulated individuals destroyed per decision to that (theoretically) googolplex of immortal individuals once your mc succeeds, ending death and colonizing space etc. im pretty sure shut up and multiply comes down on the side of your mc maximizing however he needs to.
      ```

      - u/N0xS4v4g3:
        ```
        i mean i know im assuming no aliens in your setting but im pretty sure that the math still works out if you consider that by ending death your mc can save a lot more moral agents than he destroys in his simulations
        ```

  - u/edwardkmett:
    ```
    I now want a (probably short) Worm fanfic of a utilitarian-morality Coil-powered MC who has read one too many Iain M. Banks novel and so is aware of the simming problem. 

    That said, after they accidentally cancel a timeline for the first time, they might have a nervous breakdown. You could maybe get a second chapter out of the trolley problem of using it to avert the protagonists' own certain death in one or both timelines, but wow the level of grimderp would get bad fast.
    ```

    - u/DragonGod2718:
      ```
      My story would have been a Worm fanfic if I'd actually read Worm, sadly I've yet to pick up a chapter, so no dice.
      ```

- u/FireHawkDelta:
  ```
  The spatial impact of one reset can be restricted to the light cone from t_x to t_y, because to the universe outside a c*(t_y - t_x) radius of John at t_y, t_x is in the future. Entering the deleted light cone from outside would be a separate instance of time travel.

  Because of the vast scale of space, most resets should only kill all life in the local solar system. Still massive, but quantifiable.
  ```

  - u/DragonGod2718:
    ```
    The reset is meant to occur across the entire multiverse.
    ```

    - u/FireHawkDelta:
      ```
      I'm saying it's functionally identical, so you could reset only the light cone for the same result. In universe, this could be more energy efficient depending on the mechanism. Ethically, the deleted part of the multiverse outside this section will be replaced with a *completely identical* replica, which in some ethical frameworks is better than destroying something unique.
      ```

      - u/Watchful1:
        ```
        That's assuming no faster than light travel, which in a world with time travel isn't a very safe assumption.

        It does seem unlikely that a single instance of time travel could kill even a large majority of the universe, but, hypothetically at least, the protagonist could trigger an interstellar war between a series of FTL civilizations, wait for the various effects to propagate across galaxies and then reset it.
        ```

      - u/DragonGod2718:
        ```
        >Ethically, the deleted part of the multiverse outside this section will be replaced with a   
        >  
        >completely identical  
        >  
        > replica, which in some ethical frameworks is better than destroying something unique.

        Would quantum dice rolled outside John's light cone return the same results across rewinds?
        ```

        - u/CompactDisko:
          ```
          It doesn't matter, if you have a branching style multiverse every possible quantum dice result will create a universe for each possible result. This happens the same way before and after a time loop, so the set of universes in the multiverse won't change. Your specific POV universe might be different, but I don't think the multiverse changes at all.

          Thinking some more, if the multiverse is a set of every possible universe (or every universe within constraints), there might be a zero utility cost to timelooping. anything that is different still happens in another universe, a set of all universes is identical to another set of all universes. Although, there can still be narrative consequences to time looping, as John probably don't have free access to whichever universe he wants.
          ```

- u/IICVX:
  ```
  This is easy enough to fix. Just make the rewind operation local to John, and also a cause for multiverse splitting. 

  For all t_y there exists some version of John who did not choose to rewind despite having the same history as the John who did rewind. That version of John continues on in that instance of the 'verse. The version of John who chose to rewind begins a new run at t_x.
  ```

- u/CronoDAS:
  ```
  I think this short film needs to be linked here.

  One Minute Time Machine
  https://youtu.be/vBkBS4O3yvY
  ```

- u/Shemetz:
  ```
  While it sounds interesting, I think the core of the argument is flawed here. Utility functions aren't meant to be defined for universes where time travel exists, or for that matter lifelike simulations, or even the concept of a timeline stopping. 

  From this time traveler's perspective, there is only one universe that will last forever, and any other timeline is deleted (unwound) every time he goes back in time. He can't know if the lost timeline is stopped, continues normally, continues with more time travelers and more splits, continues and ends up as a utopia/dystopia, stops but saves all the people's memories and merges and sends them to the final timeline, continues with the absence of the time traveler, or continues to end up including a computer that will simulate every other timeline. He can't even assign probabilities to these things, because he isn't aware of some of the rules of metaphysics. Therefore, I argue that it's useless and irrelevant for him to morally consider anything that he has done inside the loop; only his own experiences, plus the final universe, should matter to him.

  Also, since all of these universes are functionally simulations that start from our current world and slightly diverge, it's unlikely that any of them will be apocalyptic hellish morality disasters, or vice versa. The timelines will end up being finitely similar to each other, before he goes back each time. So, if you do want to consider the "utility/morality cost" of each such pruned branch of alternate history, it will always be pretty similar to the "utility/morality cost" of just continuing with our normal universe for some small finite amount of time, and it's pretty insignificant when compared to the entire rest of potential human future in the final (supposedly maximally-good) timeline.

  Not to mention - the utility/morality cost of each pruned timeline could totally be positive, and not negative. If most people enjoy their lives and prefer living to not living (a pretty safe assumption), then re-simulating the universe over and over with slight differences could be considered as a very good thing to do.

  Your point about "committing omnicide" when you go back in time is ridiculous, because you're also committing the opposite of omnicide when you restart the universe from earlier, and because simply making a person stop existing in a vaccuum where no other people will ever know or be affected by it is what I would consider morally meaningless. It's not murder if the person's life is going to get repeated soon (albeit with slight differences), or if the person is just never going to live again (e.g. when time-traveling after babies are born). Just like we don't consider the fact that we are not spending our entire time creating babies to be murder. The reason we don't like murder is that in our world it has lasting consequences and takes something away from the world, but in a time travel scenario, the murder is just undone and the world isn't any different for it.

  I think the only coherent, interesting, and agreeable model of time travel ethics would be to completely disregard the lives that are going to get undone when you loop back, and only consider yourself (the time traveler) and the final outcome universe, or if that does not exist, your final chosen path in all time loops after a certain point.

  And yes, this does make it morally fine to repeatedly torture every person in the world for information, unwinding that time, and then creating the perfect timeline. I guess that's the price of progress.
  ```

  - u/SilverstringstheBard:
    ```
    I agree with pretty much all of that except for the torture thing. If you torture someone, even if that torture is reversed or undone somehow, there's still terrible suffering that existed in the world due to your actions. There's also the fact that you're a flawed human being, and becoming the sort of creature capable of torturing people for information simply because it's convenient will inevitably start to eat away at your values and normal moral perceptions. You do enough bad things and you start to forget why you thought they were bad in the first place, which is going to lead to some serious value drift over time.
    ```

- u/carminis_vigil:
  ```
  First I'd like to note that all the ethical issues you discuss here are exactly the same as if your universe/multiverse had a branching non-mental time travel mechanic. 

  Second, you're completely right that a branching time travel model is an ethical black hole for stories - i.e. if you really think about the implications, the time travel itself is more morally significant than any other aspect of the plot or world. 

  If the timeline you're branching to already always exists, then morality is dead because now that any outcome/timeline that ever could happen, is happening, somewhere, and so all that is left is your personal selfishness about which timeline your strand of consciousness wants to be involved in.

  If your branching time-travel creates a universe without destroying the original (or vice versa), that's incomprehensibly more morally significant that any disaster you were trying to stop.

  If the branching both creates a new universe and destroys the old, that's the only configuration where other concerns could legitimately have any bearing. This is in fact a kind of "time travel" you could achieve without actual time travel - see the Omega 13 from Galaxy Quest, which is a universal "matter rearranger". If your protagonist accepted that this was certainly playing god on a huge scale, and was both omnicide and omni-nate(?), then he could make arguments along the lines of not privileging the lives currently being lived over the lives that could be lived in another possible timeline, and then just get on with his task.
  ```

- u/SimoneNonvelodico:
  ```
  > I'd like to ask you this question: if someone told you that you were a simulation and decided to terminate you, would you consider it murder?

  I don't see why not. In the end the universe too is just information processing itself. The whole "is the universe a simulation?" argument is an ill-posed question - the relevant question is, "is the hardware the universe is running on embedded within another, bigger universe?". Functionally, the universe is equivalent to a simulation running on a quantum computer.
  ```

---

