## [Challenge Companion Thread] Precommitment

### Post:

> Precommitment is a strategy in which a party to a conflict uses a commitment device to strengthen its position by cutting off some of its options to make its threats more credible. Any party employing a Strategy of Deterrence faces the problem that retaliating against an attack may ultimately result in significant damage to their own side. If this damage is significant enough, then the opponent may take the view that such retaliation would be irrational, and therefore, that the threat lacks credibility, and hence, it ceases to be an effective deterrent. Precommitment improves the credibility of a threat, either by imposing significant penalties on the threatening party for not following through, or, by making it impossible to not respond.

The most classic example of this (from either Thomas Schelling or Bertrand Russell, I'm having trouble tracking down the quote) is that in a game of chicken, you can definitively win by simply removing your steering wheel and throwing it out the window, so that it's no longer a game of flinching but of certain death for your opponent if *he* doesn't flinch. This is easily extended into the question of nuclear brinksmanship and dead-hand systems, which I believe is what much of game theory was originally meant to analyze.

### Comments:

- u/TimTravel:
  ```
  "Hurray," said the two drivers, about to crash into each other. "This was the rational decision!"
  ```

- u/Polycephal_Lee:
  ```
  Very interesting concept. In the cryptocurrency community there is a thing called [Proof of Burn](https://en.bitcoin.it/wiki/Proof_of_burn) where you prove that you've spent coins to unspendable addresses. This shows a monetary commitment without actually paying anyone specifically.

  More broadly, smart contracts can achieve this precommitment function extremely generally, such as "send one dollar to bob that is only spendable on bread." Scripts enforce the spendability of the token inside the network. [Ethereum](https://www.ethereum.org/) is an attempt at a Turing complete scripting language for smart contracts.

  My favorite example of a dead man's switch precommitment in fiction is in Snow Crash where [spoiler](#s "one of the characters has a fusion bomb that will go off if he dies").
  ```

  - u/Transfuturist:
    ```
    >Until a man is twenty-five, he still thinks, every so often, that under the right circumstances he could be the baddest motherfucker in the world.

    >Hiro used to feel that way, too, but then he ran into Raven. He no longer has to worry about trying to be the baddest motherfucker in the world. The position is taken.
    ```

- u/None:
  ```
  How could this be made into horror? I'm thinking about some kind of story where making precommitments actually tweaks your utility function to prioritize that precommitment. Someone accidentally precommits recursively, and as they fulfill their precommitments, the situations and states of mind they find themselves in make them more likely to make further precommitments - until their entire life is devoted to fulfilling them.

  No one notices.
  ```

  - u/Transfuturist:
    ```
    An individual living in a world of easy cognitive modification reasons that [making your instrumental goals terminal](http://mindingourway.com/dark-arts-of-rationality/) is a great way to approach your original terminal goals. To celebrate, they modify their motivation system to encourage taking these opportunities.
    ```

- u/Cruithne:
  ```
  Precommitting is often a good strategy where privacy is involved. If you will always respond the same way regardless of how your privacy is being trespassed, nobody can infer from your protectiveness whether or not there actually is something there this time. China is thought to do this- they keep lots of places undisclosed so would-be spies have a harder time telling which ones actually hide secrets. I am not okay with people looking through my drawers, looking on any devices I own, or generally invading my privacy without my explicit consent, even if 99% of the time I have nothing to hide there.
  ```

- u/None:
  ```
  I swear by my ability to make this sort of vow that I will now and henceforth defect (as applies to the specific context) against anyone who, in knowledge of this vow, attempts to use a precommitment strategy to unfavorably limit my options.
  ```

  - u/xamueljones:
    ```
    I only saw the last line:

    > Yay, first mover advantage!

    Therefore I will precommit to not read anything else in your comment.
    ```

  - u/alexanderwales:
    ```
    Precommitment doesn't unfavorably limit your options, it unfavorably limits the *outcomes* to your options.

    Here's the payoff matrix for a game of chicken:

     |Swerve|Straight
    -|-|-
    Swerve | 0,0 | -1,+1
    Straight | +1,-1 | -10,-10

    Here's what it looks like if only one driver has removed his ability to swerve:

     |Straight
    -|-
    Swerve | -1,+1
    Straight | -10,-10

    The player's only rational course of action is to swerve, but his *options* have not actually been limited (and in fact those options remain exactly the same).

    I should also point out that in many cases, precommitment is just about credibility. The game of chicken is all about credibility, as is nuclear brinksmanship, as is making a New Year's resolution.

    If you say "I vow that I will defect if you precommit" then I don't find your vow to be particularly credible, so I'll throw my steering wheel out the window, which means that you are faced with the choice of either crashing into my car and dying, or suffering from a reputational hit from losing and suffering whatever the consequences might be from breaking your vow. Given that choice, I believe you'll break your vow, because I don't think your vow will credibly stop you in the face of certain death. Your vow doesn't seem much stronger than simply saying "I'm totes not going to swerve", which is exactly what you'd say if you were going to swerve.

    What you *really* need to do is something like hiring an assassin who will murder you and your family if you ever don't defect against someone who has used a precommitment strategy. So long as I find *that* credible, I might believe that you would defect even in the face of changed incentives from *my* precommitment, because the payoff for you would be:

     |Straight
    -|-
    Swerve | -51,+1
    Straight | -10,-10

    Because if you swerve, you not only take the reputational hit and the loss of credibility for future coordination problems, but also you (and your family) still die.
    ```

- u/Revisional_Sin:
  ```
  Does anyone else feel that the rational/lesswrong community use exactly the opposite meaning for precommitment?

  With Parfit's Hitchiker, Schelling would precommit to payment by, say, having Ekman tie him up, in order to force him to pay when they get there.

  A lesswrongian would just say "I precommit to paying you." 

  Alternatively a LWian might have said this just before they passed out, just in case a selfish mindreader chances on them before they die.
  ```

---

