## [D] Death and Immortality: Westworld Season 2 Episode 4 Discussion [Spoilers]

### Post:

I don't know if any of you watch Westworld, but I think it's one of the best depictions out there of the issues around AI - for a big-budget mainstream production, at least. It's not rationalist and it doesn't focus on the tech so much, but the philosophical issues it deals with are interesting and relevant.

I think the episode that just aired, Season 2 Episode 4, has some of the best scenes in the show so far, and deals with some of the most interesting concepts. To recap:

* It is revealed that there has been a secret side project to create viable human brain emulations and implant them in hosts

* This project was apparently funded by Delos after he was diagnosed with a terminal illness. His brain was scanned and uploaded, a process overseen by the Man in Black

* After hundreds of attempts to resurrect Delos, each failing because the emulation was unstable, the Man in Black gives up on the project, telling one last failing Delos copy that the world is better-off without him anyway

* There is a badass shootout scene in which the Man in Black plays the part of Death himself

It would appear that the show is taking a strong pro-death stance here, which will of course be unpopular with the readers of this subreddit, but I do have to give the show credit for raising better-than-usual objections to the question of immortality. So here are some discussion topics to think about:

* If immortality becomes viable, how do we prevent the future from being populated by rich sociopaths like Delos? Or any sociopaths, for that matter? Immortality may end up being a luxury for the rich and powerful, who aren't necessarily the most worthy people (there are higher rates of sociopathy in, say, Wall Street bankers). On the other hand, if we transition to post-scarcity and can give immortality to everyone, that still means giving it to a lot of very bad people. We want the future to be the best possible world, devoid of sadistic evil - but that can't happen if we just take the sadists to the future with us.

* What if it turns out that, like in the show, emulation *isn't* viable, for fundamental reasons (maybe it's proven that good-enough brain scans are impossible to create) - what then? Is bodily immortality sufficient? What if that turns out not to be viable?

* How do you perform basic research in life extension without causing horrific amounts of suffering to the first test subjects? Is there an ethical way to do this kind of research? Was Delos's initial consent enough to warrant what was done to his copies?

And some more general questions about the show:

* Where do you think the show is going? Any twists you think you can predict?

* *What the hell is up* with the guns in Westworld? In season 1 the guns didn't hurt the guests, but in season 2 they can. I've heard theories that the hosts fire blanks when they shoot at guests (doesn't explain how the guest's guns work, or what happens if guests try to shoot at each other), or that the guns can detect when they are aimed at guests and change behavior on the fly (what kind of non-visible mechanism would do that reliably, though? The guns all look pretty normal to me). The latter theory is probably how the show intends this to work, but there are all sorts of problems with it. Can anyone invent a satisfying mechanistic explanation?

* To what extent are the hosts conscious, and to what extent do they have free will? Clearly Dolores and Bernard are conscious at this point, but I'm not so sure about the rest of them.

### Comments:

- u/Sonderjye:
  ```
  I didn't initially look at this because I haven't seen westworld and didn't want spoilers in case I decided to. 

  A friend recommended it because they thought I'd enjoy the exploration of whether robots can be concious/sentient or not. It sounded like a consealed question in which they really asked whether robots could be morally valueable. And in my mind that answer is already set at something like 'less valueable than humans but far more valueable that you can't murder them for fun', and then it seemed less interesting. Is there more to the series that that?

  Your questions resonate with me though. If immortality were invented today I don't think you could feasibly keep it from the rich and powerful, given our capitalist structure. Worse, wealth distribution would be even more concentrated on a few individuals since they would keep accumulating endlessly.
  I honestly think that you need a strong international organization similar to the EU for that society not to end up as a dystopia, however I have yet to think of a good structure to avoid corruption.

  Don't focus as much on sadists though. There's plenty of consensual sadists and consensual masochists, and presumably there would be laws in place to punish people or remove people who caused nonconsensual pain in a post-scarcity society.

  I don't know the specifics of the Delos copies(and don't want to know too much details in case I decide to watch the show), however there's a difference between consent and informed consent, and you should always be able to retract your consent at some point during the experiments. I'd take large quantities of pain and torture if it meant I were able to live forever afterwards, even if my body's reaction to said torture would be to do anything to make it stop. Would I sacrifice copies of myself though? I'm not so sure.
  ```

  - u/LieGroupE8:
    ```
    > the exploration of whether robots can be concious/sentient or not

    In the context of Westworld, the question isn't whether they *can* be sentient so much as whether they *are* sentient. I don't want to spoil season 1 for you too much, but my interpretation of the show was that none of the robots were technically conscious until the end of the season - and still in season 2, not all of them are yet. The robots were originally programmed to closely follow "narratives," even having some of their direct dialogue preprogrammed.

    > If immortality were invented today I don't think you could feasibly keep it from the rich and powerful

    Yeah, this is a problem. Governments can help, but those too are corruptible. I think the onus will fall on the inventor of the technology to introduce it in a responsible way without being unduly swayed by money.

    > presumably there would be laws in place to punish people or remove people who caused nonconsensual pain

    The problem is that an enforcement system can't get every bad person, and it can't really prevent bad people from performing a crime for the first time. Ideally there would be no crime because no one would even want to commit crime in the first place. It's unclear how to implement this without being a terrible totalitarian mind-control society.

    > you should always be able to retract your consent at some point during the experiments

    Excellent point, though in the case of Delos, every time they resurrected him his memories were wiped, and the copies were too unstable to process their situation before degrading. I think for this to be any kind of ethical the original test subject should be a terminal cancer patient put through extensive mental prep before the copies are made, with a contract outlining clear stopping conditions.
    ```

- u/leakycauldron:
  ```
  1) I don't think this belongs here. It isn't rational, it doesn't belong on /r/rational.

  The AI are just people. There's nothing to explore about Hosts regarding their artificial intelligence, they're just people with usb dongles for brains. What Westworld **is** is an analysis of what it means to be human. Are we still human if our fathers are assigned to us at the start of the month? What if fate is deterministic?

  2) I don't know why you and others are confused about guns. They were clearly said to have been reprogrammed at the end of season 1. It's frustrating see dozens of comments every week about them like they're a huge mystery.

  ---

  It's up to the mods whether this stays but ultimately it would muddle the value of the subreddit. Just my opinion tho.
  ```

  - u/LieGroupE8:
    ```
    It’s a discussion of fiction with interesting themes relevant to the community. We can make the discussion rationalist by critiquing the show with rational criteria. Anyway, it’s a gray area, so the mods can decide. I don’t see how this is different than when CGP Grey’s and Kurzgesagt’s anti death videos were posted here. Those weren’t rational fiction.

    The programming explanation seems right but is unsatisfying to me because it is a hand-wave. I was wondering if someone could come up with a “rationalized” explanation. For example, what happens if you shoot at a guest through a wall? How does the gun know not to use a real bullet?
    ```

---

