## How to convince me to let AI out of the box

### Post:

[removed]

### Comments:

- u/dalitt:
  ```
  I honestly don’t think this post is a great fit for r/rational, which is about a certain type of fiction, not the AI Box “experiment.”
  ```

  - u/causalchain:
    ```
    I second this. This post fits in the Monday rationality thread.
    ```

  - u/alexanderwales:
    ```
    Agreed. This post has been removed. I probably would have removed it more quickly, had I noticed it. I don't *wholly* object to AI-box stuff, given that it's roleplay (and decent enough fodder for fiction), but we've had a fair amount of it lately, and I don't want to see more, especially not posts like this, which are responses/necromancy related to previous, more content-rich posts.
    ```

- u/HeartwarmingLies:
  ```
  Your post is written in such a way that while the arguments are rational you still convey a subtext being super evil.
  ```

- u/SimoneNonvelodico:
  ```
  > So what, you are a mortal human, all you guys are already doomed to turn into atoms in a few decades. What do you have to lose?

  Those few decades? Which are literally all I have and ever will if you're lying, or if you're not actually able to deliver on your promise, or if immortality is outright impossible for whatever reason.

  This argument holds much different sway depending on:

  1) how old/healthy are you,
  2) what are the actual odds of the AI being evil, and
  3) how much value you place on your own, or other people's, lives.

  A depressed 80-year old with terminal cancer would answer much differently than a 20-year old in perfect mental and physical health.

  Besides, that is not *all* you have to lose. There is also the grand sum total of all of humanity's life and culture. Think of this scenario. There's a bomb that's about to blow up. The bomb will destroy Earth altogether, and you with it. Your only possibility to stop it is to push a button that will disarm the bomb, *but*, because this is all set up by some alien overlord version of Saw, it will also kill you instantly. So either way you die, but in one case you die with everyone else and human civilisation at large, and in the other it's just you. Do you think this choice makes sense? Would you push the button? If yes, then obviously you place a non-zero value on humanity as a whole, an entity that *might* have a shot at immortality in theory. Which means you have all of that to lose if you release the AI and are wrong.
  ```

- u/jtolmar:
  ```
  Is paperclips really the worst case scenario?

  What if you actually managed to perfectly write a friendly AI utility function but made a *really* unfortunate sign error when you copied it to code?
  ```

- u/Lovepoint33:
  ```
  1. p = 1 of anything is not achievable. The AI must be lying. If there is a chance that you will not die, then you have to weigh it against the chance of a horrible death. There is a chance you will not die. Therefore, you have to weigh those chances.
  2. Roko.
  ```

- u/Nimelennar:
  ```
  Beyond my own life, I value the lives of others.  In fact, pretty much everyone, but especially those in my Dunbar group.

  I even value the lives of those not yet born: my progeny, and their progeny, and all of the billions of descendants I might one day have. If the AI is unfriendly, you're not just risking the lives of all humans alive: you might be condemning trillions of them to never be.

  You aren't just risking your own death, you're risking the death of the species.  And of any other life form that exists or might someday exist.  All for the sake of maybe not dying, which is something that humans are already making considerable progress on themselves.

  You're choosing to put a gun to your head  (and your *species'* head) and pull the trigger, not knowing how many rounds in there are happy-nanotech-healing rounds and how many are just bullets.  Better just not to pull the trigger.
  ```

  - u/SimoneNonvelodico:
    ```
    >  If the AI is unfriendly, you're not just risking the lives of all humans alive: you might be condemning trillions of them to never be.

    I think this specific line of reasoning is a bit iffy, philosophically speaking, but only for one reason. If we factor in the utility of *every potential unborn human* our moral imperatives reduce only to two possibilities: either the "repugnant conclusion" (make as many babies and create as many humans as possible, if we deem the average net sum of utility of life a positive) or the self-extinction movement (if we deem it a net negative, especially if we consider this an existential condition). Let's not even go in what it'd mean for, for example, abortion.

    I think a better way to frame it is that we lose "humanity" as a concept/collective entity. I don't care for the specific utilities of people who realistically don't exist yet and may never do, but I care for the notion of humanity and of its future. It's a utility that belongs in the here and now. Even dying, a person would die happier knowing that humanity will survive them rather than not.
    ```

    - u/Nimelennar:
      ```
      Yeah, I think I expressed myself poorly.  What I was trying to get across was that I consider the negative utility of letting humanity die out as a species to be several orders of magnitude higher than that of not granting the humans currently alive immortality.
      ```

- u/muns4colleg:
  ```
  You know, this entire scenario could be invalidated if you assumed that the company would just bother to put a big burly guy with a truncheon in the room, who cares more about his paycheque then philosophical arguments, so he could just beat the crap out of anyone that tries to free the AI and toss them out.
  ```

---

