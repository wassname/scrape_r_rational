## Tom Scott's Story of A Future

### Post:

[Link to content](https://www.youtube.com/watch?v=-JlxuQ7tPgQ)

### Comments:

- u/Fresh_C:
  ```
  I'm not sure how much stock I put in the premise that a company will  accidentally create a general purpose AI. That's always seemed pretty unlikely to me.

  I can get behind the idea of everything else in the video, such as an AI created for a particular purpose interpreting its goal in a way that is non-beneficial to humans. But I don't think some group of programmers are just going to leave the servers running for a few days and be surprised when an AGI is born out of that.
  ```

  - u/CoronaPollentia:
    ```
    I think the implication there was basically that the really important work was done by the authors of the paper. The programmers just implemented it blindly on a system with low initial energy barriers, and that was enough. If you hand out buttons that nuke the world when pressed, but only if you use an industrial hydraulic press or similar pressure, then the person to destroy the world is not going to be an authority on nukes, or buttons, or possibly even hydraulic presses. They'll just be the person with a motive to put a mystery button in a hydraulic press.
    ```

  - u/Allian42:
    ```
    I can absolutely believe on a company team just letting one of those running unattended.

    I however do not believe at all they could be the ones to create such a piece of software, as they would then be a lot more aware and careful with it.

    If it was presented as a 2 part event (company 'A' creates AGI and sell the code, company 'B' buys it, input some objectives and let it loose) then I would have no problems believing what he described.

    That said, it's a hell of a scary thought, companies just selling/renting out AGIs.
    ```

    - u/derefr:
      ```
      > That said, it's a hell of a scary thought, companies just selling/renting out AGIs.

      It's an interesting thought-experiment here to replace "AGI" here with "Hansonian brain-emulation with arbitrary ability to horizontally scale its computation, given that there's a bunch of unused compute sitting around and no other machine-intelligences fighting over it." (Well, okay, maybe cryptocurrency-mining worms count as machine-intelligences fighting over compute, but they're probably easy to outcompete, in the same way that the first replicators were easy for cellular life to outcompete.)
      ```

- u/CouteauBleu:
  ```
  Whoops.
  ```

- u/narfanator:
  ```
  Wow, no. All the no. So much no.

  One of many things: Making shit is hard. Breakthroughs don't look like "oh huh we suddenly have X", they look like "oh hey I bet X works" and then a fuck ton of work and verification and analysis.
  ```

  - u/None:
    ```
    The video implied that all of the proper hard work was done by the authors of the paper. All of the verification and analysis and all that jazz was done by the scientists who invented the principals the company team implemented.

    It's a bit of a suspension of disbelief, I know, (how likely is it that a team of scientists are going to invent principles that, if implemented, could create an AGI, without realizing exactly that?) but I think we should give creators some wiggle room when they're writing a story.
    ```

    - u/narfanator:
      ```
      In order to do all the verification and analysis and all that jazz... you would have made the AGI_.

      They can ask for my suspension of disbelief, but I don't have to give it. It's also not a commentary on the rest of the artistic nature of it, but I do find that these kinds of simplifications detract from the overall message, not just because people like me pick up on it, and not just because working with the least suspension of disbelief 
      required almost always makes for better fiction, but because your message itself is more powerful without idiot balls and deus ex machina. 

      Two good examples come to mind, in Europa Report and whatever that documentary/fiction Mars voyage hybrid was. In Europa Report, at the end of the EVA, why was he not tied onto the ship? How much more terrifying and poignant would it have been to have your friend strapped to your ship and slowly dying because they can't come inside? In the Mars thing, why would you have not have run the systems check BEFORE the point of no return on your thrust maneuver, and then had something go wrong anyway - you know, like (AFAIK) the _actual_ training simulations?

      Take this piece. Mention that the scientists were stopped from powering their AGI with proper compute by their ethics board, but knew what would happen, and you're basically done. Now it's not just a case of lazy oversight on the part of the dev team, it's case of malevolent negligence. There's also a depth there with the slew of prototypes in the academic and corporate labs (not that would need to be featured in the story, but...)

      When you have realism the rest of your world builds itself. When you don't, it doesn't. And usually the realism is not that hard to put in, it just takes considering what reasonable people would have done, instead of what you, the author, need to have happened.

      Or, to put it another way, near-future speculative fiction that ISN'T rational suffers that much more heavily for it, _and_ requires that much less effort to _be_ rational.
      ```

---

