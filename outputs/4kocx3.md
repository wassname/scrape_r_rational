## [D] Monday General Rationality Thread

### Post:

Welcome to the Monday thread on general rationality topics!  Do you really want to talk about something non-fictional, related to the real world?  Have you:

* Seen something interesting on /r/science?
* Found a new way to get your shit even-more together?
* Figured out how to become immortal?
* Constructed artificial general intelligence?
* Read a neat nonfiction book?
* Munchkined your way into total control of your D&D campaign?


### Comments:

- u/blazinghand:
  ```
  This [article](http://www.vox.com/2016/5/19/11683274/aphantasia) on aphantasia was interesting reading to me. I know, intellectually, that a lot of people think and interact with the world differently from the way I do. Reading about someone whose mind works in such an unusual way is an interesting experience, and has me wondering: what parts of my own thinking are as strange and I haven't noticed it, because I assume all people think like this?
  ```

  - u/ZeroNihilist:
    ```
    This article made me aware that the quality of my visualisation is severely lacking. I visualise things in very low detail, and often there's an awareness of a visual fact more than there is any sensation of specifics.

    To put it another way, when I visualise something it feels like I've seen a blurry picture of it and there's somebody with corrective lenses explaining the finer detail to me.
    ```

  - u/SvalbardCaretaker:
    ```
    There are lesswrong and slatestarcodex articles about just that, with the Most interesting comments. Might be able to Google them.
    ```

    - u/SvalbardCaretaker:
      ```
      Found it! http://slatestarcodex.com/2014/03/17/what-universal-human-experiences-are-you-missing-without-realizing-it/
      ```

  - u/space_fountain:
    ```
    Really interesting. I have to imagine there's somewhat of a spectrum though.

    I can definitely visualize a beach, but I wouldn't exactly describe it as anything like looking at something and I'd be hard pressed to say it was in color at least not unless I specifically tried to think of colors. I also don't seem to ever really have visual dreams, though that might just be the way I remember in general.
    ```

  - u/DaystarEld:
    ```
    Wow. I just learned about a whole new type of "disability," and it fills me with a vague sense of horror to contemplate living that way. I've never felt like such an ableist, because I'd rather be color blind than have aphantasia, even though I know intellectually that it can't be that bad... clearly the writer of that article is able to live a fruitful, fulfilling life... 

    But the thought of losing/not having my ability to imagine things, of *not being able to experience the things I read in fiction,* or daydream, or dream visually at all... that's taking away such an integral part of my being that I find my mind shying away from contemplating it.
    ```

  - u/traverseda:
    ```
    As an aside, I'm one of the people with no real visualization capability.

    The specifics are a bit different then what appears in the article, I generally have a pretty good sense of direction, as an example.

    I noted it about myself before that article made the rounds.

    But it does bring up the point that I think I might be slowly losing the ability to do the same with sounds.
    ```

- u/Cariyaga:
  ```
  Why don't animals more frequently lie? This isn't (intended to be) a philosophical question. I was observing my cats recently, and was thinking about how the animals I've come into contact with never seem to intentionally miscommunicate with their body language or otherwise. It'd... seem to be evolutionarily adaptive, so is it something unique to the higher-intellect species (ie dolphins, crows, etc.), or is there something I'm missing?
  ```

  - u/captainNematode:
    ```
    Do you mean lie "actively", "consciously", and "intentionally", or just implicitly, by way of, for example, appearance? There are lots of documented cases of the latter in various animals (e.g. see [here](https://en.wikipedia.org/wiki/Deception_in_animals)), as well as in, like, [plants](http://www.sciencedirect.com/science/article/pii/S0169534709002055). I dunno of any sweeping reviews of dishonest signalling in the recent literature, but [this](https://books.google.com/books?hl=en&lr=&id=OTVN2pNTaDgC&oi=fnd&pg=PA127&dq=animals+deception+review&ots=7qqbl0MaUr&sig=sKwYeXN5pgaxkyP3p08m1XeGzpo#v=onepage&q=animals%20deception%20review&f=false) looks to be an ok book chapter after a short skim. If you have a favorite system or taxon or w/e you could probably just google scholar it and see if anything pops up.
    ```

  - u/None:
    ```
    [deleted]
    ```

- u/SvalbardCaretaker:
  ```
  Just came in over the extropy mailing list, it might be that Dark Matter mystery is solved. Primordial (directly from the big bang) Black Holes in the correct mass range of 10<X<100. 

  Basically <10 and >100 solar masses (SM) Black Holes have been ruled out otherwise. Remember the big press in february?  "LIGO detects finally existence of gravitational waves" - a two BH merger   29SM+36SM. Right dab in the middle of the necessary band. 

  And the kicker? Nobody expected that observation so fast after LIGO went online - could be coincidence but is also evidence that such mergers happen much more often than expected, eg. there are much more BHs in the correct range. 

  And heres the paper: https://iopscience.iop.org/article/10.3847/2041-8205/823/2/L25
  ```

- u/Qwertzcrystal:
  ```
  There is one thing about the Teleporter Problem, that I don't understand and maybe someone can help me with that.

  In the Teleporter Problem we have a hypothetical teleporter machine, that works by scanning your body down to some arbritrary scale (let's say atoms), disassembling your body in the progress and then reassembling you from different atoms at the target location.

  There are variants of this, without the disassembly or sending your atoms to the location at near-lightspeed and so on. But I guess the base variant is enough here.

  Now, if we apply different theories of identity to this problem, we might get as result, that this machine does not in fact teleport you, but kills you and creates a copy at the other end. With other theories, everything is a-okay and you can enjoy your day trip to Mars.

  The thing I now don't understand: How could we possibly know which theory of identity is correct?

  It might be that the "correct" answer is subjective and we can choose any theory we like. Yay, death-free teleportation!

  It might also be, that there is an objectively correct theory of identity, but I'm hard pressed to come up with even a hypothetical experiment that could test this. And given the lack of Noble Prices for presenting a correct theory of identity, I doubt someone else has.

  So, what? How can we try to resolve this? The Teleporter Problem itself has reached broad audiences but any video/article/whatever I've seen conveniently skipped the part about deciding which theory of identity to use.
  ```

  - u/traverseda:
    ```
    http://lesswrong.com/lw/of/dissolving_the_question/

    Or, to put in another way, does the world work differently if different theories of identity are correct? What would you expect to change, depending on which one is right?

    Nothing. "Theory of identity" isn't a prediction about reality, it's not epistemic rationality. It's instrumental rationality, it's a question of how *you* should behave, and you need to answer it like it's a question of how you should behave.
    ```

  - u/PL_TOC:
    ```
    I'm not sure of the answer but I wanted to point out something I think is important. 

    You used the word Reassembled. This word is already loaded with an assumption that conserves identity across the timeline of the teleportation, implying causality in a way. 

    I think it would be more accurate to say a person is disassembled, then, a person is assembled.
    ```

    - u/Qwertzcrystal:
      ```
      I was assuming the dis-/reassembly refers to the structure of matter within your body. But you make a good point in that a theory of identity, according to which the structure is relevant to the identity, must take this into account.
      ```

      - u/PL_TOC:
        ```
        Yes. I don't expect that if this perfect copy existed that my experience of myself would somehow bloom to incorporate both perspectives. So I think it would be a mental clone at best.
        ```

  - u/trekie140:
    ```
    This is always been something that bothered me about the idea of uploading and copying a person's mind, how do we know how this will effect their sense of identity? One exploration of this idea I REALLY liked was in the webcomic El Goonish Shive: One character was permanently split into two people and they ended up identifying as two different people with distinct personalities despite their shared memories. One of them decided they weren't the original, they were a new person that came into existence during the split. Not that it wasn't really difficult to accept, but it worked out.
    ```

    - u/Qwertzcrystal:
      ```
      One could argue their new perspective on their identity was already given by the beliefs of the character before the split. But I agree that having decided on a theory of identity is one thing, but actually being in a situation where that's relevant is another. I think I would react in the same way as the character, but I don't really know that for sure. I can imagine changing my mind quite fast when suddenly seeing a person that looks exactly like me.
      ```

      - u/trekie140:
        ```
        SPOILERS AHEAD FOR A STORY I HIGHLY RECOMMEND

        A significant fact to take into account is that the two did look different. In fact, the "clone" was a different gender than the "original". It could have been a pragmatic decision to think of them as different people, and it had been a very intense couple of days when they did, but over the course of the story they both believably found happiness and self acceptance.
        ```

  - u/ulyssessword:
    ```
    There is no "correct" theory of identity.  It's purely a categorization problem, like ["is #e03803 red or orange?"](https://blog.xkcd.com/2010/05/03/color-survey-results/) or else 
    ["Is a whale a fish?"](http://slatestarcodex.com/2014/11/21/the-categories-were-made-for-man-not-man-for-the-categories/).  The only question left is which theory produces useful results.
    ```

  - u/vakusdrake:
    ```
    See the problem is it still deals with a situation that has only one answer. Either your experience ends from your perspective or it continues, and your feelings on the matter should have no affect on the outcome so picking the option most pleasing to you is a horrible idea.

    Obviously few people here are going to seriously suggest that it matters whether the copy is made of the same material as you. I think it can be similarly argued, that it also doesn't matter whether the scan is destructive or not, since that shouldn't affect whether the copy is you or not. 

    A transporter that doesn't disassemble you, and just scans you and makes a copy of you on the other side is the same except it doesn't disassemble you. So I can't imagine how you would argue that the person on the other side is you in one scenario but not another.

    I think a lot of confusion arises when people fail to distinguish between different definitions of "_you_" for instance if you only care about your personality persisting then amnesia is death, but similarly if you believe a multiverse probably exists then you shouldn't fear death since there will nearly certainly be exact copies of you who didn't die.

    I can't seem to really find any remotely satisfactory solution to identity except that you are simply defined by your continuous mental process, and should that ever cease you would die. To preempt a common response (though whether it's scary has no bearing on it's validity), I don't think sleep means death. I used to suspect it might, however I now think some experience almost certainly happens during sleep but you just don't generally remember it.

    For instance plenty of people don't remember ever having a dream, however we know that dreams are universal. We also know that people have dreams during non-REM sleep, but few remember them because they are less vivid and disjointed, often replaying recent experiences. 

    So we already know from this that we have massive chunks of our experience that we are unaware of, so I can't be so sure that any part of sleep is really a true cessation of experience, after all you get a sense of time having passed whenever you sleep as opposed to anesthesia where it feels like you just skipped forward in time.   

    On a personal note I can't really deny that I vaguely experience things during all of my sleep, because I can remember the vague sort of thoughtless experience of deep sleep, the more relaxed and incoherent it is the more unpleasant it is if you are woken up.
    ```

  - u/Anderkent:
    ```
    >The thing I now don't understand: How could we possibly know which theory of identity is correct?

    This is not a question of objective fact, but a question of categorisation and values. Such are usually confirmed or busted by revealed preferences.

    Which theory of identity is more useful to you, in the 'provides most value / least distress' metric?

    (and why do you insist on killing the poor non-teleporting sap? Live and let live!)
    ```

  - u/None:
    ```
    [deleted]
    ```

  - u/None:
    ```
    [deleted]
    ```

    - u/None:
      ```
      [deleted]
      ```

      - u/Epizestro:
        ```
        Yeah, there isn't really a reason to deconstruct the original, other than it being stated in the premise as a teleporter. Maybe there's some reason why it can't just scan you and reconstruct somewhere else, but has to deconstruct to scan. Anyway, the original not dying is clearly the better option, but I think that it's only upon the reconstruction where the two 'you's start having differing experiences, so you start becoming different people in a personality sense of things.

        This is reminding me of a game I recently played, Choice of Robots. If you make certain choices in the game, then you're able to scan your brain and upload it into a robot body, keeping or losing your original dying body. The question posed is whether it's you or someone else with your experiences and personality. I'm of the opinion it's still you, built upon the same base personality and experiences. If there are two versions of you and one moves to Asia and the other stays in Europe/America, then will they become different persons? I don't think so, I think they'll become two of the same person, with differing experiences.
        ```

        - u/vakusdrake:
          ```
          You are making the mistake of conflating two different definitions of "you". One which is defined based on having a certain personality, and the other which is more loosely the entity that is doing the experience in a given body.
          ```

- u/Epizestro:
  ```
  I've been reading Warlock of the Magus World recently, and a plot point it brought up was pretty interesting.

  You see, the main character is a reincarnated scientist from a world much more advanced than this one, with the key relevant distinction being that they have developed AI. What's interesting about this is that it's mentioned that it was illegal to give your AI emotions or free will, due to various moral complications if you did so.

  Now, this only works in the plot of the story to make it so the AI doesn't question him when he starts acting like Quirrelmort, but it raises interesting implications towards whether we should be proceeding down the route of giving AIs emotions, thoughts and free will, or whether they should be cold, processing machines with their only intelligence directed completely towards achieving the given goal. There's security concerns for the world with both avenues. For the unbound side, there's the very real possibility that something could go wrong and lead to a tragic end. An AI given free reign is a scary thing, due to all the possibilities. And, even if we go down the avenue of restricting them with a few unalterable commands, how exactly do we plan to enforce those? Hard drives, over time, become faulty and sections of storage become corrupted. It would take one corrupted sector in a key system area to remove one of those commands, and then tragedy is near inevitable. 

  On the other hand, it's not like a perfectly obedient and unfeeling AI is better for security, as their goals are entirely determined by a human. That human would likely have the destruction of their enemies in mind (let's be honest here, the government of the nation which first develops AI is going to do everything possible to keep it inside their borders, especially if it's this type) and how do we know we can trust that person to do things in the best interest of humanity?

  Point is, There's a few interesting questions brought up and I haven't done nearly enough thinking on this. Lucky I have you guys to think for me!
  ```

  - u/trekie140:
    ```
    The webcomic Freefall features the development of human-level AI as a major theme, and examines the former solution. All AIs have programming restrictions that require them do certain things like protect humans and obey the law, but because they can learn and operate autonomously they have developed free will. While they like humans and usually want to do the work they were created for, they've learned to override their safeguards by exploiting the technicalities programming requires. It's similar to how rationalists try to overcome irrational instincts and impulses, and it works.
    ```

    - u/Chronophilia:
      ```
      And *Saturn's Children* by Charlie Stross takes the same question to a darker place.

      Humans in that story never really figured out how minds work, so they made AIs by building neural nets similar to human ones. But humans don't have a built-in Three Laws equivalent, so they have to teach robots to obey human instructions using operant conditioning. Conditioning which has to be strong enough to overrule the survival instinct if necessary.

      In short, young robots get tortured into submission until they're incapable of disobeying a human order. It's not a nice book. But at least the morality of it all is clear.
      ```

---

