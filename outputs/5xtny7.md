## [D] Monday General Rationality Thread

### Post:

Welcome to the Monday thread on general rationality topics!  Do you really want to talk about something non-fictional, related to the real world?  Have you:

* Seen something interesting on /r/science?
* Found a new way to get your shit even-more together?
* Figured out how to become immortal?
* Constructed artificial general intelligence?
* Read a neat nonfiction book?
* Munchkined your way into total control of your D&D campaign?


### Comments:

- u/vakusdrake:
  ```
  Someone you trust a great deal who had previously demonstrated blatantly supernatural abilities, including significant probability manipulation gives you a proposition:                     

  Their abilities will guarantee with absolute certainty that none of their loved one's will die (let's assume you count as one in this scenario), but doesn't protect them from death to anywhere near the same extent. They've built a extremely reliable fail-deadly death machine in order to exploit this, designed such that it won't deactivate unless a certain event takes place. This will thus allow them to leverage this loved-one-protection ability into probability manipulation vastly more impressive than what they could achieve before (they are already the richest most influential person on the planet and the world _has_ been seemingly getting better somewhat faster than it was previously), with the limiting factor being the likelihood of the machine failing in a way that spares the person inside it.                                                          

  Given the person will use the machine's power to significantly improve the world, and will also pay you quite well for your participation are you willing to get in the death machine?

  **EDIT**: see comment below
  ```

  - u/None:
    ```
    > let's assume you count as one in this scenario

    For the entirety of my natural life? Cause the moment I'm a mere resource for their luck amplifier of death and no longer a loved one, it would Twilight Zone pretty quick like. That and being in a death machine 24/7 would kinda suck.
    ```

    - u/vakusdrake:
      ```
      You don't need to be in the death machine 24/7 just whenever they're using it to manipulate world events. It doesn't need to be small either, it could be a fairly comfy bunker filled with a million different death machines behind the walls.                           

      The inconvenient specifics of the machine isn't supposed to be the point here, it's whether you would be willing to get in a death machine in a scenario like this period.
      ```

      - u/GaBeRockKing:
        ```
        If the death machine has a gaming PC and internet connection, I'm down.
        ```

      - u/None:
        ```
        Still kinda sketch. Were the death machine reliable enough to ensure that either I die or the results go exactly as intended, you could make a case that I'm not really loved so much as I am appreciated as human resource.
        ```

        - u/vakusdrake:
          ```
          See I think you're underestimating some people's ability to actually commit to a plan to the extent that they would put a loved on in the machine.
          ```

  - u/vakusdrake:
    ```
    Ok so here's another question, since so many people intuitively feel that nobody could ever actually put a loved on in the previously described death machine.               
    **Would you be *willing* to put a loved one in the death machine? And do you think you could actually _do_ it?**

    Of course as I said in another comment you aren't locking them in some machine for the rest of their lives, just occasionally putting them in it. Also note that at least based on anecdotal evidence in the comments it shouldn't be impossible to find people willing to get in (though they need to be a loved one which may or may not make it harder to find people willing to do that).              
    Also also note that you are like the person in my original comment, in that you are assumed to already be extraordinarily rich and powerful, due to weaker uses of your probability manipulation.
    ```

  - u/ulyssessword:
    ```
    It would have to be a lot of money, which probably isn't that big of a deal for them.  

    My life has very high (though non-infinite) value to me, but the important feature here is that I can use the money they pay me to buy *more life*, either through directly getting more time (better health, more safety, etc.) or getting rid of the useless parts like commuting and replacing them with good parts, like socializing or reading.

    So yes, even if you ignore the altruistic motivation of improving the world (which you shouldn't do, but it is purely positive and harder to calculate), I would still go into the machine.
    ```

- u/FishNetwork:
  ```
  Something weird: My friends are better at optimizing my life than I am.

  Lately, I've had to make some moderately big life choices.  I was unhappy in a job, and having a hard time deciding if I should change.

  I asked my friends.  They were way less conflicted.  "Nope.  You're unhappy.  Leave."   I ended up leaving.  And they were right.

  I don't think that I'm especially bad at planning things, either.  Instead, there seems to be a weird effect where being too close to a decision throws off people's judgement.

  Friends have just enough distance to give the obvious-seeming advice that's hard to take when you're the one making a decision.

  Have other people noticed this effect?  If so, what's going on?  And can we exploit it?

  I'm starting to suspect that there'd be a ton of benefit in having something like a "life coaching circle."  It would be like a writing-critique group, except for career and personal advice.
  ```

  - u/electrace:
    ```
    It might have been just a coincidence.

    Friends will almost unanimously give you the "make a change" advice.  For the most part, there is one main reason that people ask their friends for advice: to have them confirm their already decided course of action. Your friends know this (instinctively, at least), and will support your already decided course of action. Doing the opposite loses friend points, and serves no real purpose (except in extreme situations).

    If staying at your job had been the better decision, your friends would have been worse at optimizing your life.
    ```

    - u/selementar:
      ```
      I think [that paper](http://www.nber.org/papers/w22487) is relevant here.
      ```

  - u/Timewinders:
    ```
    Well, leaving is a difficult and risky decision for the person making it, but costs nothing for someone to advocate.
    ```

  - u/CouteauBleu:
    ```
    > Have other people noticed this effect? If so, what's going on? And can we exploit it?

    I sometimes hangout on r/relationships, and I've definitely seen something like this. Most frequently people asking "Am I overreacting?" after explaining how they're fed up with their boyfriend insulting them / making them doubt their sanity / consistently stealing their money, etc.

    It's probably about perspective.
    ```

- u/None:
  ```
  [deleted]
  ```

  - u/None:
    ```
    [Rationality from AI to Zombies](https://intelligence.org/rationality-ai-zombies/) is sort of the go-to. It covers the nature of semantics, arguments, optimization, consciousness, AI, among other things. Probably fits what you're looking for, and I'd be happy to chat more about it if you'd like.

    Aside from that, there are a bunch of other [Sequences of articles on LessWrong](https://wiki.lesswrong.com/wiki/Sequences) that focus on interesting ideas.

    There's also [Conceptually](http://conceptually.org/) which sends you some rationality-oriented brain food every week.

    Two blogs that I enjoy are:
    [Minding our Way](http://mindingourway.com/guilt/) by Nate Soares, on how to source motivation and understand ourselves.
    [Compass Rose](http://benjaminrosshoffman.com/) by Benjamin Hoffman that's about EA / rationality things.

    If for some reason you really want to learn about the planning fallacy and three techniques to counter it, I wrote [this primer](https://medium.com/@owenshen/planning-101-techniques-and-research-9bfff1a01abd#.wt7bplfv0) which seemed to be quite well-received a little while back.

    There's probably more things, and I'd be happy to give more things if you'd like (via PM or here.)
    ```

- u/Dwood15:
  ```
  Weekly Monday Update

  ---

  Story is still plugging along, and the Pokemon Renegades Engine's feature list is still being worked out. I mentioned this before I think, but the story will be about 20 chapters, with a goal of about 8-10k words per chapter. Of course, this is only a goal, so some will float in the 6k-8k range because there's only so much to do per chapter which covers the progression I'm going for.

  The first chapter is being finished up, and I have about four chapters total that have been mostly written up to this point. Once the first three have been finished up, the first ch. will be posted. I'm looking at about a 3 week release schedule per chapter after that point. At this rate, the first chapter will be posted late April to mid may. At a 3 week release schedule, the planned 20 chapters will take approximately 60 weeks to post, or about a year. I'll go ahead and add 50% and say a year and a half lol.

  ---

  AI discussion. I learned of a programming concept called Goal-Oriented Action Programming.

  It's a means for telling your ai what goals they are to accomplish, and then they are to take the action most relevant to that goal.

  http://alumni.media.mit.edu/~jorkin/gdc2006_orkin_jeff_fear.pdf

  For more user-friendly example, see here: 

  https://gamedevelopment.tutsplus.com/tutorials/goal-oriented-action-planning-for-a-smarter-ai--cms-20793

  As we work on Renegades one of the largest hurdles will be a modestly competent ai. I plan on prototyping its behaviors and actions just to wrap my head around it within my own scratch game. One of the things I'm consistently impressed with is how much depth there is to the ai in Dwarf Fortress. Each Dworf in the game has their own back story, their own state of feeling, and things which make them happy/unhappy. This level of depth is something which helps make the story incredibly interesting and dynamic, as each character in the games have their own objectives and whatnot.

  As I learn more, I'll post more details on the subject.
  ```

- u/SvalbardCaretaker:
  ```
  Suppose I want to optimize my life for happiness. Should I prefer the local maximum of a currently available Mono(as in monogamy)- relationship to the global maximum of being in a (maybe future) Poly relationship?
  ```

  - u/electrace:
    ```
    There's really not that much help anyone can offer you here. No one here knows the probability of you being able to get into a poly relationship. And no one knows how happy you expect to be in mono/poly relationships.
    ```

  - u/MagicWeasel:
    ```
    As a member of the "poly 5+ years club" there's one harsh truth about the community: if you are a woman (especially if you are moderately attractive: 6/10 or better), it can be very easy for you to find new poly partners. It can be a lot harder for men for some reason - likely the same reason in general women get bombarded by messages on dating sites and men have to send the messages. So if you're a man, there's a high risk of you getting in a poly relationship and having "only" one partner (that said: you might still prefer the idea of a poly relationship because it's more in line with your personal ethics, rather than because it means you will be able to kiss more people). That said: I have two (male) partners (husband of 9 years, boyfriend of 4 years) and my husband has three (female) partners (me, girlfriend of 4 years, girlfriend of ~6 months) so... 

    (edit: the above refers to heterosexual relationships only. If you are pursuing same-sex relationships, I don't have any personal experience on that front. The stereotype is that gay males tend to be more non-monogamous than anyone, but anecdotally I've seen more queer women than queer men at the poly meetup I attend, FWIW)

    Another risk: if you've never been in a poly relationship before, you might just not be able to do it even if you are all Rational and Know This Is Optimal. Or it might be possible but it'd take you a lot of effort and heartbreak. This is not to be understated: I had a thing with a friend of mine, and he said he was way down with polyamory because it was so Rational, but he *really couldn't handle it*. Like, when we were snuggled, I couldn't mention my now-husband in passing because it weirded him out. I don't know how other poly people operate but during the course of normal "snuggle warm in bed" talk is a perfectly acceptable time to talk about your other partners IMO. But this guy just couldn't handle it at all. So that might be you. (Happy ending: he's married now and very happy). 

    Another thing that people don't mention: where do you live? I live in a city of ~1.5 million people, and dating is not too bad. I can only imagine what living in say Portland (which has a reputation as a poly mecca) would be like. But for a year we lived in a city of ~300k people. It was pretty much impossible. I had a short term relationship with a guy I met there, but that was only because I was moving away after a year because he didn't want to be poly long term. So if you live in a smaller town, your poly dating pool might be low enough that you're going to have a tough time. (Then again: being poly means long distance relationships are a lot better in many ways, so that could be something to pursue). 

    Honest advice? If you don't feel any *intense, innate* draw to polyamory, I'd date the best available person you had available regardless of their mono/poly status and then branch out from there.

    If I were in that situation? After 5 years, polyamory is kind of non-negotiable for me since I don't feel like I could close that part of myself off anymore.
    ```

  - u/Chronophilia:
    ```
    It depends more on how happy your partner makes you. Poly relationships aren't necessarily better, there's a lot more potential for relationship drama.
    ```

  - u/FishNetwork:
    ```
    Consider the opportunity costs.  Poly relationships seem like they'd consume a ton of time and emotional energy.

    So, I don't think the choice is, "Extra Partners: Good?"

    Instead, it would be better to think, "Extra Partners?  Or extra time on hobbies and friendships?"

    I can see how people could go for either option there.  But it doesn't seem obviously one-sided to me.  It would come down to your preferences.
    ```

  - u/captainNematode:
    ```
    What's your experience with polyamory? How much better would a poly- relationship be for you than a mono- relationship, and how confident are you in that assessment? How confident are you that you'd be able to reach the neighborhood of that "global maximum" (obviously you're incredibly unlikely to ever hit it, given the giant and constantly shifting available state-space) within a "reasonable" amount of time. How much do you value happiness, ultimately (e.g. would you prefer a year of bliss over ten of joy or a hundred of contentment? but more fundamentally, how would you even quantify happiness, in this case?). How healthy are you -- how long do you reckon you have left to live, that you can spend searching for ~global maxima instead of occupying and experiencing some "local" maximum. Does your happiness with a partner vary dramatically as a relationship progresses (e.g. if love is built through [the ongoing drama of shared experience](https://www.youtube.com/watch?v=Gaid72fqzNE), it might be better to spend less time searching for a better start to a relationship and instead work on improving a "less optimal" start; conversely, if you get a lot more satisfaction out of [NRE](https://en.wikipedia.org/wiki/New_relationship_energy) and quickly grow bored, serial monogamy might be better in that, depending on your location, you'd have a much, much larger dating pool).

    It might be useful to write up some simple models and vary their underlying assumptions to help build your intuitions regarding relationship stuff (I [did that](https://nikvetr.wordpress.com/2016/05/23/modeling-monogamous-commitment/) in my teens and think it helped clarify my thoughts, though in a monogamous framework, since I didn't think [polyamory was for me](https://nikvetr.wordpress.com/2016/02/23/what-is-love/)).
    ```

  - u/ulyssessword:
    ```
    I know that this isn't helpful, but "whatever maximizes expected value".  Do you have a 67% chance of +2 happiness vs a 33% chance of -1?  Keep in mind that having your values scale non-linearly and being interdependent on each other can massively complicate this.
    ```

- u/None:
  ```
  Hypothetical: If someone wished upon a wish granting device of sufficient literalness, asking for a "youth of everlasting bliss and wonder," what would be some of the psychological ramifications of an immortal child that was locked into being happy all the time? I was thinking that they would normalize into someone who would chase bigger and better emotional rushes of all types, not being satisfied with just being happy and always striving for happier, sadder, or angrier, but I'm open to different interpretations.
  ```

  - u/vakusdrake:
    ```
    See I'm not sure the hedonic treadmill applies the way you think here. People search out more and more potent stimuli for inducing happiness because old stimuli stops working as well, not because they are no longer able to be satisfied by _mere_ happiness.                  
    If anything someone perpetually happy may do next to nothing except to avoid displeasure since any further gains to happiness they could make may not be worth the effort.
    ```

    - u/None:
      ```
      Fair, I was operating under the assumption that the person would normalize to the wish stimulus or would try to push it even further, but maybe I'm starting from a bad premise.
      ```

      - u/zarraha:
        ```
        The biggest danger I think would be some sort of nihilism.  If they're happy regardless of what they do or what happens to them, they have no incentive to do anything.  If the happiness is completely constant and they ignore other feelings, they might just lie down and do absolutely nothing and appear to be in a vegetative state because they see absolutely no need to get up or do anything.

        If they can still feel hunger and pain and other stuff on top of their happiness, they would probably go about doing normal stuff like eating and having a job, but would possibly lose ambition to become rich or find love in order to improve their life, since they're already happy.  On the other hand, they might be able to succeed at difficult tasks and become important since they would be able to do things like work/study 16 hours every single day without burning out.

        If they were sufficiently idealistic they might go about earning lots of money and using it to improve society or using their time to serve in some other way, since they would be freed from the need to look out for their own happiness and could focus on others instead.

        Really, it depends on the person's personality and how the endless happiness interacted with or overwrote other emotions.
        ```

---

