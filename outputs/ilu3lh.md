## End Goals of Rationality

### Post:

(This whole thingo has been sitting in my end a while now in some form or another, so this is more me getting it down finally rather than some particularly well-constructed argument)

&#x200B;

So I've been reading quite a few different works that fall under the umbrella of being rational(-ish), and have been meaning to get more into works that are more strictly rationalist (I've been putting off properly reading Sequences for yonks now), and while I can get behind the overarching tone of the majority of work being about things getting better (either through making the world better, improving your thinking, or both), but it's the details of the endings that a lot of rational fiction settles on that seems a bit off to me.

Like, and I am going to post (simplified and watered down) **spoilers** for well-known works of fiction here to illustrate my point so be careful about what you read, a big part of HPMOR's ending is>! "as a result of my/our work/beliefs, I am going to start dolling out immortality to all,"!< The Waves Arisen has >!"I am going to use my power take over everything to unite everyone and improve all our lives (even if things are going to be a bit shittier in the short term),"!< Mother of Learning is less grand in >!"I am a better, kinder, and more powerful person and I am going to do my own things that simultaneously benefit myself, the people I care about, and the world around me."!< (It's been a while since I've read all those so don't get stuck up on details here).

Like these are all good (and dare I say, happy) endings, and I understand it is much more narratively suitable, generally enjoyable, and arguably relatable (we our live inside our own heads exclusively) to have the protagonist be the focus of the ending, but shouldn't the focus be somewhat broader in the endings? Looking at the world reacting to the events rather than a culmination of the character/s efforts from a first person view?

Like a lot of rational work talks about how *everyone* can improve their thinking, what pitfalls and fallacies to avoid, what successful strategies to employ, learning from your mistakes and all that. Would it be more suited to the ideology behind rationalism if there was "epilogue: here's how they all lived happily ever after" and followed by "epilogue 2: here's how everything changed in the wider world." One first for the narrative to have a satisfying conclusion, one to reconnect the ideals expressed by the author to the reader themselves in a more explicit manner, talking about benefits beyond the individual. Although this isn't something I'm exactly qualified on and I don't know if there's something about such a bit of writing like this that makes it less enjoyable (it could easily be something done in rational fiction that I just haven't stumbled across because I've read more popular stuff than unpopular stuff).

But anyway, encouraging the spread of rationalism aside, the specific implementation of the ideals expressed by the endings of these works is something that seemed a little off to me. When so much of rationalism is based on dealing with our innate and/or learned flaws as people, the celebration of improving ourselves seems sub-optimal.

That is going to take some explaining (of something I'm not certain how to explain) and go a bit off-topic from this subreddit.

Firstly, being better than you were yesterday is good. Helping others to be better than they were yesterday is good. The world being better than it was yesterday is good. But it could be done better. If we're all starting off from a baseline of our current human limitations, with all its issues, how good can the "end product" be? The immediate next step in that train of thought is transhumanist ideas and all that jazz, but that's not exactly what I'm thinking of.

Why is humanity so front and stage in thoughts of the future? I don't mean in regards to talking about possible alien life, I mean why is humanity being in charge of everything something people see as set in stone? "Friendly AI" is something that is discussed (that I really need to read more discussions of), AI that helps humanity, but with the idea of an AI singularity being a thing that is being actively researched, AI that is (several times) smarter than humans seems a distinct possibility (even if AI growth is restricted to the exponential growth rate of Moore's Law or something similar).

I'm sure everyone reading this can think of several political leaders they think are absolutely horrible, so replacing folks with machines that are better than the brightest people isn't entirely unpalatable. There are obviously massive issues with picking a "fair" AI to stick in charge, how much bias people have in them that would go into making an AI in the first place, and a host of other issues I haven't even began to consider, but is it worth the risk? There's been so many atrocities through (recent) history, and many, many going on today still (treatment of Uyghurs in China, refugees and Australia, wars across the world, the rise of authoritarianism, etc.), as well as the persistent risk we either kill the planet with climate change in the long term or nuke ourselves to death or some other disaster that it at least deserves thought.

And even if we decide it's not worth the risk to create extremely powerful AI, there is the risk of someone else creating extremely powerful AI themselves but doing a worse job of it or being worse to hold it. I'm more inclined to think that somewhere like Finland having access to an incredibly powerful AI, due to the motivation of the potential military benefits that would bring (either directly or through successive AI-created developments) would be better than North Korea. The situation kind of turns into some weird Pascal's Wager type deal - do we take a chance on an incredible benefit of a powerful and benevolent AI making the world a better place moving forward at the risk of something going wrong and wiping us out?

Anyway, to relate back to the actual reason for posting in this subreddit - is the idea of (rather than directly making the world better) making something that itself makes the world better something that meshes with rationalism, either as it is defined here, as you relate to it yourself, or as it is reflected in some works that I am just not aware of?

### Comments:

- u/None:
  ```
  You seem to be using some hyper-specific definition of rational fiction. It's just characters acting rationally. That's literally all there is to it. They don't [turn into idiots when it's convenient for the plot](https://tvtropes.org/pmwiki/pmwiki.php/Main/IdiotBall), they don't [magically deduce](https://tvtropes.org/pmwiki/pmwiki.php/Main/BatDeduction) things that can't be deduced from actual evidence etc.
  ```

  - u/Asviloka:
    ```
    *Being rational* isn't exactly the same thing as *Rationality*, though there's significant overlap. I'd say there's a difference between 'rational fiction' and 'rationalist fiction' and it sounds to me like the OP is speaking about the latter while you're thinking about the former.
    ```

  - u/nosoupforyou:
    ```
    Upvoted merely for the idiot ball reference.  TIL.  But it was definitely something that's always bothered me about tv shows.
    ```

  - u/gramineous:
    ```
    You're not wrong at all, it's just...

    You know how "literally" has had its definition expanded to include it meaning the exact opposite of its original definition for the purposes of emphasis? It's got significantly less precedent to it's usage in such a manner, but came about through changes in the way it was used. I'm taking a pseudo-definition of rational fiction based on common driving themes in the body of mainstream rational fiction I've read because I'm personally inclined to taking the approach of defining language based on the way it is a part of a broader context. I could easily be wrong about how consistent these themes are in rational fiction as a whole and getting my definition wrong, and I'm probably more inclined to be accepting of such fluent definitions given I'm queer and a lot of language about identities in that whole community is more flexible/fluid/changing (hell, look at the change from "queer is a slur" to "queer is an identity" over the past few decades). Like there is no chance at all your definition is wrong, there is a chance my definition is wrong, but I think there is some justification to looking at definitions from my viewpoint even though I'm bias towards doing so.
    ```

    - u/callmesalticidae:
      ```
      > You know how "literally" has had its definition expanded to include it meaning the exact opposite of its original definition for the purposes of emphasis?

      There are very few places where you are more likely to find extreme disapproval for that trend than on this subreddit.
      ```

    - u/SimoneNonvelodico:
      ```
      > I'm taking a pseudo-definition of rational fiction based on common driving themes in the body of mainstream rational fiction I've read because I'm personally inclined to taking the approach of defining language based on the way it is a part of a broader context.

      Usually here works that also focus a lot on spreading specific ideas about rationality are tagged not just as rational, but as rationalist. However yeah, I can see the point, you should just be a bit more specific I guess. Anime is not just about giant robots and magical girls, but if you're watching a giant robot or magical girl show it's almost certainly anime. Something like that. What you talk about is more like a trope that happens to be common in rational fiction (probably because it's been codified by early works like HPMOR, The Waves Arisen, etc.).
      ```

    - u/OuroborosInc:
      ```
      This is a bit of a tangent, but the use of 'literally' for emphasis has been around since at least 1769[\[1\]](https://www.nationalgeographic.com/news/2013/8/1308016-words-literally-oxford-english-dictionary-linguistics-etymology/), so it isn't exactly lacking in precedent.

      (To be clear, I agree with the point you're making, I just couldn't resist the urge to nitpick)
      ```

- u/Transcendent_One:
  ```
  Well, the idea of rationalism is optimising for any goal of your choice - if something you make will achieve this goal more optimally than you yourself, then of course it's a solution worth going for.

  > do we take a chance on an incredible benefit of a powerful and benevolent AI making the world a better place moving forward at the risk of something going wrong and wiping us out?

  I don't think this is even a question. Either we prove ourselves incapable of creating a powerful AI, or we wipe ourselves out before creating it, or someone eventually creates it. Taking or not taking a chance is not a possibility.
  ```

- u/RMcD94:
  ```
  Rationality can be applied to any goal including the genocide of an ethnic group.

  Of course if your goal is actually stability of the nation or something rationality might help you recognise the bias of assuming that a certain group of people are more at fault than others but is rationality is not a political stance.
  ```

- u/ArgentStonecutter:
  ```
  > why is humanity being in charge of everything something people see as set in stone?

  Counterpoints: Friendship is Optimal. The Culture series. In Greg Egan's futures the beings that are in charge of everything are often human-like in many ways but are not actually human any more. In Charlie Stross's _Saturn's Children_ and _Neptune's Brood_ humanity has been replaced by human-derived robot sex slaves. Karl Schroeder's Ventus and Virga series humanity is kind of living on the skirts of far smarter and more powerful beings, many of which are no longer self-aware because self-awareness is inefficient.
  ```

  - u/gramineous:
    ```
    I'll look into those works (probably not the ponies though). Thank you.
    ```

    - u/ArgentStonecutter:
      ```
      The ponies one isn't really about ponies, it's more about artificial superintelligences with weird goal functions. Ponies are a metaphor.
      ```

- u/Jose1561:
  ```
  I think the idea of that second epilogue would be somewhat dramatically unnecessary.  We already *know* what kind of future the protagonist envisions, and any deviation from that would be too much detail to contain in a single epilogue.  It feels like wish-fulfilment at that point, because it adds nothing to the story.

  In most of the stories you described, there are good reasons why Artificial Intelligence never occurred to the protagonist as a viable option.  In The Waves Arisen and Mother of Learning, there's no in-world equivalent of AI.  In HPMOR, I'm fairly certain Harry plans at some point to build a Friendly AI, but the problem is that even as far as magic's reality-bending powers go, creating new sentient beings that *aren't* human-derivative would take a long time to work on, and a better idea might simply be to keep track of Muggle AI research, and find ways to improve it with magic, and return the results.

  Apart from that, there is also the fact that the existence AI is kind of unsatisfying from a narrative standpoint.  As in, instead of having a story about a protagonist who's trying to get smarter and save the world, you'd end up with an idealized autobiography of a real-world AI researcher (or of Eliezer or the like, if it's a world where AI exists in some form, and alignment is the issue).

  The benefits to AI are more nuanced than simply sticking one in charge.  That's something that would go deeply against public sentiment, and for the sake of democracy, would not be implemented.  Powerful AI would exist to advance mathematics, technology, and social *systems* that benefit the most people while catering to our sensibilities.  A weird illustration that just popped into my head is the Yogurt episode from Love Death Robots, where the yogurt didn't ask at first to be in charge of society, but instead gave world leaders a plan that would end the world's problems.

  In the real world, AI is inevitable.  It's just the natural next stage of a scientific - or indeed, a sentient - civilization.  Of course, the downside to creating AI without proper alignment systems is deeply horrifying, which is why given the choice, I'd delay AI research until AI alignment research reaches commensurate levels.  But we *don't* have that choice.
  ```

- u/SeraphimNoted:
  ```
  Fully automated luxury gay space communism
  ```

- u/PastafarianGames:
  ```
  > Why is humanity so front and stage in thoughts of the future?

  Are you asking why SF/F speculative fiction is about humanity and the human experience / condition, or are you asking why rational fiction doesn't throw off the shackles of the SF/F genres and write about aliens and incomprehensibility?

  > but shouldn't the focus be somewhat broader in the endings? Looking at the world reacting to the events rather than a culmination of the character/s efforts from a first person view?

  Sure, if that's what the story is about. But no, not if that's not what the story is about. Not every story is a polemic about how to build a better future, with a nice neat moral at the end about how the protagonist followed the polemical plan and made that better future (and in fact, almost all of those stories suck as fiction).
  ```

  - u/gramineous:
    ```
    I guess I'm trying to get at the broader adherence to common world-building tropes in fiction, especially related to protagonists. How many stories are out there that sit in "simplified 14th century Europe but with magic" or something to that effect? How many Sherlock Holmes-esque "deductive genius with major social issues" protagonists are there in mainstream media over the past decade? Like making fiction from established cultural understanding makes sense, it allows the author to bring their differences into greater focus, it cuts down on how much you need to do to establish the world, and the popular tropes are popular for a reason. But, and I don't know how much of this is wishful thinking, lack of finding such works, or some level of bias I've got going on, shouldn't there be more stuff that violently breaks from such molds?

    Jabberwocky is a departure from the narrative norms and world-building taken to a grand level, but is the middle-ish ground between that and what is popular just thoroughly unreadable or unwriteable for some reason I don't know? Is Jabberwocky only functional as a piece of fiction due to its incredibly short length? I guess I don't know how much incomprehensibility or obtuse-ness a work can have before it becomes completely intolerable to either the author or the reader. 

    &#x200B;

    Polemics about building a better future kinda seemed like the type of thing that'd end up on this subreddit. But given the other commenter in this thread talking about how Eliezer Yudkowsky trying to do that and stopping because the work was incredibly depressing, all those stories sucking as fiction makes sense. I hadn't read any myself, and I didn't know if it was because they did indeed suck or if there was something else at play.
    ```

    - u/PastafarianGames:
      ```
      I think the main reason why most fiction doesn't break the mold of its tropes is that most fiction seeks to use tropes. Invoke them for emotional freight, invoke them to save space and time, lean on them because it's easier than generating everything whole cloth, they're useful!

      There's a *lot* of utopian science fiction out there. The vast majority of what I'd consider utopian is queer, and basically none of it is even remotely commercially successful, even stuff that's been published through traditional means. Basically the only really successful example here is the Culture books, and a lot of people consider them horribly dystopian.

      There's also a lot of "blueprint for a better future" type science fiction out there. The vast, vast majority of it is implicit; it's written set in a society that has done the work, and explores the themes of "okay, what now". (I count in this category Lois McMaster Bujold's SF, Becky Chambers's books and in particular Record of a Spaceborn Few, again the entire Culture series, and basically anything that came out of the "fully automated luxury gay space communism" meme/notion, a lot of which you'd have to delve into Tumblr for or find the various Patreon pages of people who write stories in that oeuvre.)

      I would recommend ignoring anything E.Y. writes on the matter of writing, and would recommend looking for stuff by more widely-successful authors. The aforementioned Banks, Bujold, and Chambers have all written works that serve as a sufficient refutation, though what prevents him from being able to do what they did is something I feel no need to speculate about.

      But here's the key: good fiction about the utopian future doesn't tend to be about the path there, and it doesn't tend to be about technology or structures or strictures. Good fiction about the utopian future, just like all the rest of good sf/f, tends to be about people. Specific people, dealing with other specific people, as a reflection of what it means to be human. That's sort of what SF/F as a genre is about.

      ("What is the path to the utopian society" is more of a thing in non-fiction. I find reading it depressing because I don't believe that good will prevail, but it's certainly out there.)
      ```

- u/DaystarEld:
  ```
  Most stories focus on particular characters because they carry narrative power much more easily than broader containers like "society" or "the world" do. You could certainly have more epilogues that end with zoomed out, top down views on how everything ends not just for the protagonists but for the world, but that sort of hampers the potential of more stories between the end of the one you're reading and the utopian singularity (or dystopian one) the same way the epilogue in the canon of Harry Potter sort of restricted the possibility of any post-Hogwarts story between the 7th year and then.
  ```

- u/SimoneNonvelodico:
  ```
  > I mean why is humanity being in charge of everything something people see as set in stone?

  Eh, no matter how you turned it around, "we are now in charge of some non-human entity that does better than we would" would sound a lot like dystopia to many people. And I'm not necessarily one of them, in the hypothetical scenario in which I could have a genuinely benevolent, wise ruling AI, I'd probably be better off, and with no less control than with some President or Parliament that is always overwhelmingly dominated by the party I did *not* vote for anyway. But a lot of people highly value self-determination for its own sake, and you can't ignore that.

  By the way, did you read Friendship is Optimal? That one's basically about a caretaker AI.
  ```

- u/Freevoulous:
  ```
  The reason why we do not have the Second Epilogue, is that it would either have to be blatant Mary Sue Land, or be realistically flawed, which would be a downer ending.  


  It is better to leave the final epilogue to reader's imagination.  


  Have you read Transhumanist Wager? Or Micro Gates? Or Atlas Shrugged? All these stories deal with the long term outcomes of transformative rationalism, and in both cases billions die in the inevitable 'birthing" process of the new rationalist era. This makes these books pretty off-putting in the end, because while the rationalist protagonists manage to build the foundations of utopias, they are built on bones of billions of irrationalists who killed themselves in a desperate frenzy to stop the optimization.
  ```

- u/Bowbreaker:
  ```
  >but shouldn't the focus be somewhat broader in the endings? Looking at the world reacting to the events rather than a culmination of the character/s efforts from a first person view?

  HPMoR had epilogues regarding the futures of at least two side characters not directly bound to Harry as well. >!Draco and Snape!<

  >"epilogue 2: here's how everything changed in the wider world." One first for the narrative to have a satisfying conclusion, one to reconnect the ideals expressed by the author to the reader themselves in a more explicit manner, talking about benefits beyond the individual. 

  Luminosity and The World As It Appears To Be both have this. And they are both worth reading in my opinion.

  > I mean why is humanity being in charge of everything something people see as set in stone? "Friendly AI" is something that is discussed (that I really need to read more discussions of), AI that helps humanity, but with the idea of an AI singularity being a thing that is being actively researched, AI that is (several times) smarter than humans seems a distinct possibility

  There definitely is rational(ist/ish) fiction that ends up with a good AI. But you can't end all stories the same way, else it becomes unoriginal.
  ```

- u/gramineous:
  ```
  I could also wax poetic about some implications of AI leadership, but felt it'd detract from the (already scatterbrained) thread too much.

  Like how if AI wipes out humans, does it matter that much? If our descendants aren't (our) flesh and blood, does that really matter? How fleshy and bloody do our descendants have to be anyway? Like if you don't have kids of your own directly, do you write off everyone else's kids? Does every species that predated our current species in the long history of evolution not count as our ancestors? What distinctions are you making between whether or not something is classed as a descendant?

  Also, kinda what I was getting at with shoving "Pascal's Wager" in there, but how does this relate to the definition of "God"? If we make some ultimate arbiter by our own hand rather than by whatever scriptures a bunch of folks subscribe to, does that make the endeavor greater or lesser by association? Religion has been a popular sentiment across the world for millennia, even if I doubt that people reading this are too enamored with the idea themselves (and myself for that matter).

  I don't know the answers to these questions. I'm posting this whole thread here because I don't know a lot of things and my head runs in circles thinking about these topics. Comments and thoughts and any resources on the topic very much appreciated.
  ```

  - u/Jose1561:
    ```
    When people talk about AI wiping out humans, we're talking of wiping out all sentient beings in this tiny section of the universe.  Something like "wiping out" humanity by evolving us into a new race isn't the kind of philosophical issue I've seen come up in serious discussion (Then again, I could just not have had enough experience there), so I think most people agree with you on it being a non-issue (Of course, this also implies that *we'd* also be non-humans, not just our descendants, because biological enhancements are a thing.

    I don't understand your second point about God.  Could you explain that a bit further?
    ```

    - u/PeridexisErrant:
      ```
      There's some discussion of that in Toby Ord's *Thr Precipice* (non fiction intro to existential risks), but in most conversations it just doesn't matter much - we already have essentially the same value drift problem just from having children, so...
      ```

  - u/thomas_m_k:
    ```
    If some descendants of ours survive who aren't exactly human but share some fundamental values with us (e.g., they know what fun is and what boredom is, and they are conscious), then I think we could be very happy. However, that's certainly not the default outcome. The default outcome is that we all die and are replaced by something that, to us, looks *mindless* (it might still be intelligent in the sense that it can outsmart any human, but it lacks any concept of fun or love or friendship or honor or anything like that). For example, a [paperclip maximizer](https://wiki.lesswrong.com/wiki/Paperclip_maximizer).

    All these futures that you are considering are in this tiny tiny part of possibility space. The vast majority of possibility space is a universe that is "dead" forever. That's the thing we should prevent.

    > Would it be more suited to the ideology behind rationalism if there was "epilogue: here's how they all lived happily ever after" and followed by "epilogue 2: here's how everything changed in the wider world."

    Eliezer Yudkowsky (of HPMOR fame) once tried to write a story set in a post-singularity world where our world had been *fixed*, [but he couldn't finish it](https://www.lesswrong.com/posts/88BpRQah9c2GWY3En/seduced-by-imagination):

    > [...] It's not a good idea to dwell much *on* imagined pleasant futures, since you can't actually dwell *in* them.  It can suck the emotional energy out of your actual, current, ongoing life.

    > [...]

    > I am now explaining why you shouldn't apply this knowledge to invent an extremely seductive Utopia and write stories set there.  That may suck out your soul like an emotional vacuum cleaner.
    ```

    - u/gramineous:
      ```
      Ok, yep, makes sense. I guess there's a big difference in sitting down and loosely plotting out a better world than the one we're in, and taking care and effort to construct and convey a thoroughly planned idea to others. I often get stuck in a mental rut trying myself thinking of that broad utopia topic, but chronic depression kinda normalizes thinking things that make you feel varying degrees of shitty and I've been like this for literally the majority of my life so I don't know how it is on the other side of the fence so to speak. Like when so much of your thoughts lead you down that path anyway, whether or not to avoid a single specific instance of that pattern isn't typically worth considering.
      ```

---

