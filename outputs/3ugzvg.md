## [D] Friday Off-Topic Thread

### Post:

Welcome to the Friday Off-Topic Thread! Is there something that you want to talk about with /r/rational, but which isn't rational fiction, or doesn't otherwise belong as a top-level post? This is the place to post it. The idea is that while reddit is a large place, with lots of special little niches, sometimes you just want to talk with a certain group of people about certain sorts of things that aren't related to why you're all here. It's totally understandable that you might want to talk about Japanese game shows with /r/rational instead of going over to /r/japanesegameshows, but it's hopefully also understandable that this isn't really the place for that sort of thing.

So do you want to talk about how your life has been going? Non-rational and/or non-fictional stuff you've been reading? The recent album from your favourite German pop singer? The politics of Southern India? The sexual preferences of the chairman of the Ukrainian soccer league? Different ways to plot meteorological data? The cost of living in Portugal? Corner cases for siteswap notation? All these things and more could possibly be found in the comments below!


### Comments:

- u/None:
  ```
  [deleted]
  ```

  - u/None:
    ```
    One of the first observations any non-great-ape would make about the great apes is that *boy howdy do they REALLY like fucking!*
    ```

- u/LiteralHeadCannon:
  ```
  Every day from now on, three people somewhere in the world are granted a superpower.  There is no choice involved; who gets a power is random and who gets what power is random.  The three superpowers are as follows:

  * Deception: You may, at any time you choose, trade minds with any other human within about ten feet of you; they do not need to agree to this trade.  Your power rests with your mind, not your body, so you may live indefinitely by trading down to younger bodies.
  * Entropy: Your biological body is replaced with a black sphere, about a foot in diameter.  Any matter or energy that comes in contact with this sphere is destroyed.  Your brain is preserved in a pocket dimension inside the sphere; in your sphere body you are immortal, both in the unaging and unkillable sense.  The sphere body is unaffected by gravity, but you can feel its field and use it to construct a reference frame; you may move in any direction at any velocity up to about sixty miles per hour within your reference frame (this is your only way to deliberately influence the outside world, though you may communicate by, say, using finer control to carve messages on a wall).  Sphere bodies bounce off of other sphere bodies.  Your power rests with your body, not your mind, so the exploits of those with the power of Deception may leave you powerless. Edited to clarify that you do retain sensory ability spread out across your surface area.
  * Time: You may, at any time you choose, reset the universe to the moment when you were first granted the power of Time, eliminating the previous series of events you've experienced from everything except your memory.  Your power rests with your mind, not your body, so you are immune to Deception shenanigans.  In the end, the final, stable timeline is one where you are either caught off-guard and are killed before you are able to react, or willingly commit suicide.

  How would you expect the world to wind up?  What would you do with Deception?  Entropy?  Time?
  ```

  - u/Transfuturist:
    ```
    > sixty miles per hour within your reference frame

    Uh...

    The massive hole that I'm not sure you're aware of is Entropy destroying the entire atmosphere of the Earth. *This setting ends with everyone on Earth freezing and/or suffocating to death.*

    With Deception, I don't do anything besides research human cloning. I find genetic donors of people whose bodies I want, grow mindless clones of them, then trade minds with the clones. I work on saving the Earth from the consumption of atmosphere by Entropy, assuming that's *even possible.*

    Entropy, I'm a constant sink on the atmosphere, which is very very bad. I live in orbit, slowly going insane from sensory deprivation, and assassinate warlords. Perhaps I can communicate to a government authority and get a satellite hooked up that I can live around. Then I won't go insane, and my head-destroying powers can be directed to more intelligent ends. But seriously, people with Entropy who live in atmosphere are sucking down an enormous amount of air.

    Time, I develop a reflex of rewinding time whenever I'm surprised. That's pretty much it. I rewind time before I'm killed by Entropy eating the atmosphere. I work on saving the Earth from the consumption of the atmosphere by Entropy, assuming that's *even possible.*
    ```

    - u/LiteralHeadCannon:
      ```
      I did consider that issue, but made what I felt was an educated guess that the atmosphere would not be consumed quickly enough to be a short-term problem.  How long-term of a problem, I'm not sure.  Let me search for some values and run some calculations...

      Holy shit, each Entropy consumes 4% of the atmosphere a day.  (It asymptotically approaches 0%, obviously, but still.)  Someone lied to me a couple of years ago about how much atmosphere we had and I never realized it.  If each Entropy is only an inch in diameter instead of a foot, then they "only" consume 0.03% of the atmosphere a day.
      ```

      - u/Transfuturist:
        ```
        :[

        Right. If I'm Deception, I try my hardest to switch minds with an Entropy and wait for humanity to die from not knowing about air pressure. If I'm Entropy, I live in orbit and the rest is the same. If I'm Time, I endlessly rewind to try to save the world from Entropy, and the mysterious instigator to cause all this.

        Grim setting.
        ```

        - u/LiteralHeadCannon:
          ```
          I think the winning move might be for Time to get Deception to put them in an Entropy, so that they can go the end of the universe and come back with the resulting knowledge.
          ```

          - u/Transfuturist:
            ```
            What exactly would that accomplish? What resulting knowledge is there to use? You have human brains floating in space, with no way of talking except wiggling back and forth. There's no medium of information storage save the Earth's surface. There's no interface for computation. How would simply living with a group of the ten to forty survivors of Earth, who likely don't even speak the same language, who would all eventually separate from each other and get lost in the trillions of years it would take for the universe to end, who would all go insane and utterly stupid from isolation and boredom, actually do anything? It's not a winning move, it's an eternal loss.
            ```

            - u/LiteralHeadCannon:
              ```
              It's an unknown unknown, which makes it a potential winning move in a situation with no apparent winning moves.
              ```

  - u/Frommerman:
    ```
    Time wins. They are basically Coils with infinite savestates rather than just one at a time.
    ```

  - u/IomKg:
    ```
    The first "Time" has the best chance of achieving his or her goals(followed by 2nd "Time" and then 3rd etc).
    Unless an "entropy" destroys earth in a way that doesn't leave "Time" a chance to reset.
    Not sure why would "Entropy" do that, or if its even possible.

    In theory its possible that if one of the first "Entropy"s destroy earth even in a slow manner then it won't be possible for "Time" to do anything about it because no technology could be built fast enough to counteract the destruction of earth, even with perfect knowledge of that. but i suspect that would depend on the physics of the world.
    ```

- u/Reasonableviking:
  ```
  To /r/rational D&D players: What is the best way to beat an army of 300 7 HD [Simulacra](http://www.d20pfsrd.com/magic/all-spells/s/simulacrum)? 

  Our group are somewhat in control of a small city state and are just now level 8 and contains a Wizard, Oracle, Sorcerer and Bard. I'm wondering if [Contagion](http://www.d20pfsrd.com/magic/all-spells/c/contagion) would arguably work 100% on the clones if the first one catches it.
  ```

  - u/None:
    ```
    [deleted]
    ```

    - u/Reasonableviking:
      ```
      Three kinds of Simulacra all around 7 HD: Human Cavalier, Orc Barbarian and Elf Ranger. The Simulacra in this case are made through alchemy on a strange kind of memetic substance that fell as a meteor a few years ago, meaning that they are much cheaper to produce than normal.
      ```

  - u/FuguofAnotherWorld:
    ```
    Got any volcanoes, mineshafts and such you could lure them to then collapse? Does your city state have the manpower to make collapsible mineshafts to lure them into?
    ```

    - u/Reasonableviking:
      ```
      Unlikely, my best idea is using information warfare and just disguising somebody as their creator which in the worst case scenario freezes the army and in the best case wins any pitched battle.
      ```

      - u/FuguofAnotherWorld:
        ```
        Are there any powerful and easily offended factions or persons you could send simulacra to attack/fail assassinating in the hopes of getting them to join your side?
        ```

        - u/Reasonableviking:
          ```
          Doesn't matter too much, once this attack is turned away then I'll have some breathing room.
          ```

- u/Gcrein:
  ```
  Are you in a romantic relationship? How's it going? Do you make an effort to be a rationalist even in your love life? What does being a romantic rationalist even entail?

  I've recently been dating someone and things are starting to get pretty serious. I wouldn't be surprised if we start pursuing something more long-term in the near future. This is honestly rather new to me. I've been in relationships before, but never anything so...real?...(I'm not exactly sure how to put that)

  I really value the opinions of the users here, so I was just curious what input there might be. Thanks.
  ```

  - u/qgml:
    ```
    I think rationality applies to a lot of topics in life just by recommending you find science-based recommendations.

    I read [an article on LessWrong](http://lesswrong.com/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/) recommending academic and empirical sources for psychology and self-improvement; I own the book it recommends most, 'Psychology Applied to Modern Life' (preview [here](http://www.coursesmart.com/9781111186630/Ch02)). It has three chapters that seem most relevent: one on Friendship and Love, one on Marriage and Intimate Relationships, and one on Interpersonal Communication, and the 2011 edition is [on Amazon](https://www.amazon.com/gp/offer-listing/1111186634/) for a little over $7 used. I haven't applied any romantic advice (never had a romantic relationship), but it seems to be of similar quality to the advice on friendships and communication, which I've benefitted from.

    Muehlhauser also posted "Rational Romantic Relationships", which is more about relationship initiation, and looks to be lower quality. I haven't followed it's advice, either, but it's similar on a similar theme to your request, RE romance and rationality.
    ```

- u/None:
  ```
  So, I'm curious if there's a name for something I've been noticing recently.

  Most people are able to use basic logic to figure things out, but only if it supports their own opinion. They can't use the exact same logic to figure out why their "opponents" think they way they do. 

  Example: Someone pointed out that some comedic actors, like Jim Carrey, have given questionable performances in serious roles as support for why they shouldn't play serious roles. I pointed out that the same actors have given questionable performances in comedic roles, too, to posit the question of whether they should play comedic roles (to point out that it's hypocritical to use the logic he was for one but not the other). Even after I pointed it out, he still didn't understand that logic should be applied in parallel like that.

  This definitely has a certain aspect of confirmation bias, but it's not just that. Is there a more formal definition for this bias? I've been using "parallel logic" or "parallel reasoning" when talking about it with friends recently, but neither seems to fit perfectly.
  ```

  - u/Transfuturist:
    ```
    I'm a fan of this style of argument. I can't recall the outcomes of every time I've used it, though.

    What you're referring to in the second paragraph is confirmation bias as applied to arguments.
    ```

- u/IomKg:
  ```
  I have been getting a bunch of recommendations for Marvel's "Jessica Jones". But after watching the trailers on youtube I couldn't shake the feeling that its just some more badly written(good vs evil anyone?) superhero stuff.

  Has anyone here watched it? is it any good? is there a reasonable explanation why the big bad didn't just take over the world? or is this series just a poor attempt to make people feel its deep because its supposed to be a metaphor for abused women\abusive relationships?
  ```

  - u/alexanderwales:
    ```
    The big bad didn't want to take over the world. He already has pretty much carte blanche to live where he wants, kill who he wants, etc. All he really wanted was to live in nice places, eat good meals, torture some people, rape pretty women, etc. and he got all of that.

    He also exists in a world with S.H.I.E.L.D., the Avengers, etc. and is vulnerable to sniper rifles. He *also* has a number of vulnerabilities and limitations to his powers, though there are ways to get around that.

    I'd recommend it, partly because I think it hits it major thematic element (power, who has it, how they use it) really well and doesn't waste too much time.
    ```

    - u/ulyssessword:
      ```
      > He also exists in a world with S.H.I.E.L.D., the Avengers, etc. and is vulnerable to sniper rifles.

      This is one of the things I liked about *Steelheart* by Brandon Sanderson.  The main character is trying to kill Supervillains ("Epics"), and his main classification system boils down to "can they be killed by a sniper rifle?".  

      "Immunity to sniper rifles" is a very valuable component to being a supervillain, and anyone that *isn't* immune to sniper rifles is at a huge disadvantage.
      ```

    - u/IomKg:
      ```
      I suppose "take over the world" was badly put on my side as i see all responses thusfar mentioned that he doesn't want to specifically.

      I mostly meant it in the sense of "win", or "put himself in a situation where no one could realistically threaten him".
      I presume he loses, or at the very least doesn't win, in the end even though such power would imply he shouldn't.

      The argument of SHIELD\avengers\weaknesses\limitations etc.  seems problematic for me because most of those would just as much be an issue for killing random people. the power described is the most effective when people aren't expecting you obviously, so solidifying his power in the world first and then doing whatever he wants sounds more likely then doing whatever he wants while ignoring said opposition.

      Anyhow I guess my main issue so far, seeing as no one said this isn't the case, is that it sounds like the antagonist is just comically evil, and that doesn't sound like an interesting conflict to watch.
      Not sure I understand your point about the thematic element though, so maybe I am missing something..
      ```

      - u/alexanderwales:
        ```
        [Spoilers through the middle of the season](#s " Kilgrave wants to possess Jessica Jones and get her to love him of her own will, to which end he's willing to threaten her and people around her or engage in any other manipulation. Because of his power, it's difficult for him to have an authentic relationship with other people and he grew up in control of both his parents, so there was never really anyone who was capable of enforcing their will on him. So the one thing that Kilgrave really seems to want is that which his power is incapable of delivering to him except by proxy.")

        [Spoilers for the final episode](#s " His fatal flaw is that desire for Jessica to love him, which is why he loses in the end.")

        I don't know, your mileage may vary. I liked it.
        ```

        - u/nerdguy1138:
          ```
          Those spoilers are exactly why I didn't like it.
          ```

  - u/TaoGaming:
    ```
    It's mediocre, but I binged it anyway. 

    Kilgrave  ("what, was murder corpse raken?") isn't really reflective or plotting. He has everything he wants, practically. He is clever enough to take obvious precautions from being knocked out, etc.
    ```

  - u/Reasonableviking:
    ```
    The big bad can only use his powers on people that can hear him talk in person, which is slightly limiting but basically he doesn't want to rule the world it looks like. I think that someone should probably have shot him by now though.
    ```

- u/Kishoto:
  ```
  Truth is an interesting concept. As rationalists (or aspiring rationalists), I think the majority of this sub would agree that they, in context of themselves, prefer the real truth over a happy lie (a la Dr House) You'd want to know that you didn't receive your Hot Wheels racetrack because your family is going through some tight financial times, instead of thinking that your temper tantrum at Thanksgiving put you on Santa's naughty list. 

  But is this the case for everyone? As a rationalist, do you think everyone (for the sake of argument, let's say everyone above the age of sexual consent) should be give the whole truth all the time (barring things that breach privacy, national security, etc). I'm not saying you inundate people with every little minutiae of data, I'm saying that it's there to be publicly accessed and viewed by anyone, at any time. I'm probably not being explicit enough, but I'm basically asking if your world view supports the existence of necessary "pleasant" lies, because you feel people's net happiness would be reduced by the full measure of the truth.

  For a fictitious example, let's take the world of RWBY. These ever present, unending creatures known as the Grimm are attracted by emotions like fear and terror, so mass panic can easily lay waste to entire settlements. Hence, a certain amount of censure is a necessity. The public simply can't handle certain truths, lest they panic and destroy themselves in the process. In this case, censure by higher powers is clearly a good thing.

  So. Final, non-rambling question. As a rationalist, when do you consider it ok to lie to someone, with the express purpose of ensuring their happiness/survival. Are you just all facts, all the time and are always going to be that way? Do you like having your kids believe in Santa? Where's your line?
  ```

  - u/FuguofAnotherWorld:
    ```
    I know some people who really don't have a happy relationship with the truth. To them, inconvenient facts are attacks against their integrity and always being right is central to their worldview. As a result, trying to convince them of anything is a pointless endeavour which only makes them unhappy and aggressive. So I don't. They say something obviously false, I won't correct them, they come to a conclusion that makes no sense, I'll change the subject, they ask what I think about a thing they like such as energy healing, I'll deflect with humour.

    I *was* all facts all the time, but some people just don't care about the truth, so I don't bother burdening them with it. If it has been repeatedly shown that an action achieves nothing or is counter-productive, why bother continuing with it? I can't explain to these people how to have epistemological standards, and I can't convince them of something even so simple as maybe putting a lock onto a garage filled with thousands of pounds of stock when the garage next door was broken into, so why bother trying to convince them that gluten is fine unless you have celiacs? 

    Following that realisation, I mostly stopped trying to convince people of things that aren't important in real life. They can have whatever random views they like so long as they are not actively detrimental, and few things are actively detrimental to their own life. Maybe if they're making a life-changing decision, or ask for an honest opinion or they want to go into business in a field with an 80% failure rate I'll speak up but for the most part I'll leave em to it. Who cares if such people think that crystals have healing energy, or that burning sage and ringing a bell will cleanse their chakras (real examples)? So long as they're have a handle of the things they actually have to do to get through life they can be as wrong as they like. 

    I speak truth to people who care about truth. The rest I enjoy other experiences with, they know where to find me if they ever want to actually understand how things work.
    ```

    - u/Transfuturist:
      ```
      > Who cares if such people think that crystals have healing energy, or that burning sage and ringing a bell will cleanse their chakras (real examples)?

      This kills people.
      ```

  - u/PeridexisErrant:
    ```
    For me this comes down to a consequentialist argument - truth is very valuable (and history demonstrates it has high instrumental value too). However there are clearly cases where knowledge leads to a high degree of harm. For example, I think it would be better if nobody had access to biological weapons research. 

    So not full availability, but no lies either - just inform those who ask that this information is restricted, and why (unless that's restricted too). 

    Jargon does a pretty good job of defending people from available info they're not ready for too.
    ```

    - u/Empiricist_or_not:
      ```
      >However there are clearly cases where knowledge leads to a high degree of harm. For example, I think it would be better if nobody had access to biological weapons research.

      This strikes me as a flawed argument.  Knowledge's application is based on ethics.  The same knowledge that weaponized the atom, has made deep space probes and and cheap base-load power.  The knowledge to weaponize diseases is the same that is leading to telomere elongation to mitigate aging and GMO crops to assuage hunger.

      We use knowledge for weapons first, because sometimes we are still silly primates, then we shut up and multiply and make the world better with it.
      ```

      - u/PeridexisErrant:
        ```
        To the extent that it's useful for other things, sure, make it available to responsible scientists.  For stuff like nuclear weapons engineering?  It's classified because of the harm it's dissemination might cause - even though the underlying physics is widely known and applicable in other areas.

        I'm still in favor of radical openness, just not *total* disclosure in all edge cases.
        ```

        - u/Empiricist_or_not:
          ```
          Oh, we perfectly agree on that front.
          ```

  - u/None:
    ```
    >  I'm basically asking if your world view supports the existence of necessary "pleasant" lies, because you feel people's net happiness would be reduced by the full measure of the truth.

    No.  People are best served by the full truth, as much as they can take, all the time.  The only acceptable form of deception is to lie by omission, and even then, what you actually say should be true in spirit and not just in letter.

    "You didn't ask" or "I prefer not to answer" are fine, but that's merely because we don't want a world in which everyone knows everyone else's uncomfortable secrets (eg: what kind of porn you watch, that you stole that one time).  *Strongly consequential* lies should be outed.
    ```

  - u/gbear605:
    ```
    I've observed that if you graph danger versus knowledge, there's a peak in danger right around the middle of the graph. It's not accurate for everything of course, but it's generally true. Going by your situation (speaking as someone who knows nothing about RWBY), you don't want to tell the public because you can't explain the entire truth, because it's just not possible in a short enough period of time, but if you were able to, then everything would be okay and that wouldn't have to be kept in the dark. The real danger comes from that middle ground where they know that the Grimm are out there but they don't know enough to be able to not be fearful and terrified. 

    I consider it to be okay to lie to someone if I would be unable to bring them straight through the danger zone into the good zone in a short enough period of time, but when possible I try to be entirely truthful.

    I don't have kids, but if I did, I wouldn't teach them about Santa (besides the "Other kids believe in this thing called Santa" part). I would give them the same gifts as if it were Santa, but I wouldn't lie to them about something like that, especially since it would be one more thing indoctrinating them into a culture of believing in things that aren't true, especially when half of the people involved *know* that Santa isn't real.
    ```

- u/scooterboo2:
  ```
  You get to talk to an A.I. What do you ask it?
  ```

  - u/Transfuturist:
    ```
    > What would be a better topic starter on /r/rational than "You get to talk to an A.I. What do you ask it?"?
    ```

    - u/IomKg:
      ```
      "you get to talk to an A.I. , but can only use English words starting with the letters R,N,W and P ', what do you ask?"
      ```

      - u/Transfuturist:
        ```
        *pulls out thesaurus*
        ```

      - u/LiteralHeadCannon:
        ```
        Why no paperclips?
        ```

      - u/MugaSofer:
        ```
        "Robot, write new philosophy."

        What? I like the Turing Test/no-zombies theorem.

        "Read *Worm*, report Wildbow's position."

        A good all-purpose test of moderate superintelligence - *I* can't figure out an author's location based on that data.

        "What're 'people'? Newborns? Robots? Roaches?"

        The most pressing philosophical problem of our time.

        (I can't think how to check if it's *Friendly*, but there are very few situations where I can talk to an AI and it's not already out of the box.)
        ```

  - u/ulyssessword:
    ```
    Nothing.  I don't know if it's Friendly, or else the Ctaeth (Malicious intelligence in a box, from *The Wise Man's Fear* by Patrick Rothfuss).
    ```

  - u/gbear605:
    ```
    First, imagine an A.I. smart enough that it can pick a subject, randomly pick a side, and then convince you that it's true. You should obviously not believe it, because its side has a 50% chance of being wrong, even though you are convinced. Therefore, you should precommit to not believing them. However, humans aren't good at unlearning anything, so you really need to just not listen to it in the first place, so I ask it nothing.
    ```

    - u/Transfuturist:
      ```
      Thought experiment credit to Slate Star Codex, I believe.
      ```

    - u/Nepene:
      ```
      The majority of views are falsehoods. I'm doubtful all people could be randomly convinced of any subject. Lots of people require adherence to some rigid criteria which is tricky to fake- if you believe on things supported with scientific studies are true then unless the AI can hack scientific studies to make them look true it can't prove a lot of things. Likewise if you only believe things your partner, a holy book, or your mother says skilled arguments don't have a potent effect.
      ```

---

