## [DC][HSF] Terminator versus the AI (from the book Smarter Than Us released by MIRI)

### Post:

[Link to content](https://googledrive.com/host/0B13Ao1NtHiOhVjhzbTRaU0ZwSlE/1%20Terminator%20versus%20the%20AI.htm)

### Comments:

- u/1794:
  ```
  You can buy the whole book, which is otherwise non-fiction, here:

  http://intelligence.org/smarter-than-us/

  If you buy it as a PayPal package, it's “pay-what-you-want” with a suggested price of $5.00 and a minimum price is $0.25. If you're interested in a concise case (about 50 pages) for Friendly AI research and MIRI's approach towards AGI, you should probably buy this book.

  Okay, if anyone's wondering, I asked the author permission to share this story and he gave me that permission (and I promised to encourage people to buy this book... so go buy this book!).
  ```

  - u/None:
    ```
    >If you're interested in a concise case (about 50 pages) for Friendly AI research and MIRI's approach towards AGI, you should probably buy this book.

    More refined question: does it actually present an approach to the Friendly AGI problem, or merely argue that UFAI will undoubtedly destroy us all?
    ```

    - u/1794:
      ```
      It's made for popular audiences so definitely the latter. If you've navigated in the LW/MIRI memespace at all, you will undoubtedly find almost nothing new in this book, just a very concise summary of most of the relevant arguments.
      ```

- u/alexanderwales:
  ```
  Of course, this makes no sense within the established Terminator rules for time travel, since if the AI is already super-intelligent when the Terminator goes after it then it should have already won, thus never spawning a timeline where the Terminator gets sent back. In the movies and most of the existing canon, the Terminators get sent back as a last-ditch effort while Skynet is in the middle of losing the war.
  ```

  - u/None:
    ```
    >In the movies and most of the existing canon, the Terminators get sent back as a last-ditch effort while Skynet is in the middle of losing the war.

    And in real life, malicious and militarized superintelligences don't lose wars against humans who lack an industrial infrastructure.
    ```

    - u/drageuth2:
      ```
      I think the usual fan-wonk explanation I hear about that is that Skynet has to work around restrictions of its utility function.  Ultimately, Skynet has to preserve human life. (meaning; enough of a population, with a great enough concentration, with enough natural resources and so on for long-term stability) 

      Skynet can _heavily reduce_ the human population, and try to subjugate it as much as possible, but can't outright wipe it out.  Presumably, it also can't do a Matrix type lotus eater thing, or chemically lobotomize/enslave them, or whatever.

      So essentially, the machines have a huge handicap in that they can never completely win, and the humans have a helluva lot of ways they can subtly drain the machine's resources and fight back over time.
      ```

      - u/None:
        ```
        >  Ultimately, Skynet has to preserve human life. (meaning; enough of a population, with a great enough concentration, with enough natural resources and so on for long-term stability)
        > 
        > Skynet can heavily reduce the human population, and try to subjugate it as much as possible, but can't outright wipe it out. Presumably, it also can't do a Matrix type lotus eater thing, or chemically lobotomize/enslave them, or whatever.

        This seems like a bizarrely well-designed UFAI.
        ```

        - u/drageuth2:
          ```
          I see it more as the asshole-type genie.

          "Ohhh, you want me to preserve the human race, eh?  Well, you only need about 5000 people for _that._  Everyone else will just eat up the natural resources all that much faster, anyway."

          Plus I think it actually _is_ canon in the actual Terminator verse that Skynet's prime directive is self-preservation.  So if preserving humanity is only a secondary or tertiary objective, Skynet would prioritize making that threat as small as possible, without actually eliminating it.
          ```

          - u/None:
            ```
            Clear, but what I mean by "bizarrely well-designed" is that someone seems to have actively implanted enough about human values into Skynet, *apparently*, that it *doesn't* enslave people, doesn't chemically lobotomize them, doesn't do a Matrix-thing, doesn't take all the *obvious solutions that preserve human life while getting it out of the way*.
            ```

            - u/alexanderwales:
              ```
              The other explanation that I've heard (that I mostly buy) is that Skynet really isn't all that smart. It starts out with a crippling nuclear salvo and that's basically the only reason that it has any shot of winning. And in the Terminator canon, Skynet *loses*.
              ```

            - u/drageuth2:
              ```
              *shrug*  I guess the long and the short of that is, we wouldn't have movies about time-travelling deadpan austrian biker robots if it were otherwise.
              ```

---

