## Smart Wish Partial Solution: Minimize Regret

### Post:

Main idea: for almost any way a wish can go wrong, you will eventually regret it.

Flaws / things that can go wrong:

1. Your ability to determine whether you should regret the current state of affairs is impaired or altered

2. Definition of regret is unclear

3. You are ignorant of circumstances which would cause you to regret making the wish if you were aware of them

4. Your ability to feel regret is diminished or deleted

As long as the definition of regret is correct and you maintain the ability to feel regret normally and the ability to assess whether you should regret the outcome, then any negative consequence, including "I should have made a different wish", will be avoided.

My intuition is that "regret" will be easier to define than "values" in the context of the wish "maximize my values".

Thoughts?

### Comments:

- u/noggin-scratcher:
  ```
  I think #2 is the sharp end - not convinced that giving a formal definition of "Things I would regret" is a different order of complexity than "Things I value". May possibly be a subset, but it doesn't reduce the task by that much and still seems prone to the same problems - there's a great many things that you might find small reasons to regret but that don't actually matter in the face of executing your wish, and you need to specify exactly the order of priorities... which somewhat just reduces back to specifying all of your values.
  ```

- u/ArgentStonecutter:
  ```
  Consider that logic applied to any decision. If your decisions are always made to minimize regret, what will they be like? I suspect they will be conservative and safety-oriented, rather than aggressive. This will minimize the chance of a negative outcome but strongly limit the positive outcomes. Of course you can factor that into the decision, and consider that you may regret having not taken any risks, but it will still color it: we are still human and not flawless engines of rationality.
  ```

- u/narfanator:
  ```
  Malevolent, lazy, or benevolent genie?
  ```

- u/None:
  ```
  Regret is indeed easier to define, but you still need to specify that you mean to minimize the regret of a counterfactual model of you as you are now, assuming that the counterfactual you is brought to full understanding of each counterfactual under consideration.

  But yeah, at that point it's not that hard.  The hard bit is actually writing down a specification of this that doesn't assume you have a magic counterfactual you in a box.
  ```

- u/None:
  ```
  Also, humans are changeable in what they regret - a person in a utopia might regret the loss of risk, depression might drive a person to regret their life, and someone might regret the simple loss of options because they're dissatisfied with what they chose.
  ```

---

