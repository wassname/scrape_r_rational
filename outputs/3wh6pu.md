## [BST] Trying to figure out what actual free will might look like. Alternately, being totally free of bias.

### Post:

[Determinism bits removed, as it's clear that "free will" is kind of meaningless. I was wondering if I was simply suffering from lack of imagination, but the consensus is that I had the right idea about that, so I'm only considering the other possible mechanism now]

I've been knocking around an idea for a weird magical drug that knocks your awareness out of this universe and into another one where you are not held back by the chemistry of the body and by buggy mental hardware. Does anyone have any suggestions for what I should read in order to figure out what *that* might be like? I'm aware of some of the bigger rationality blogs, but would be interested in being referred to more (and also to relevant books, articles, and so on). 

How the drug works is that you can analyze the information that you already have and decide where to go from here (and, unless you're bad off, *act* on those decisions once you're returned to your body). You do not acquire information that you didn't have before. In the latter case it *might* be best summed up as "you become a more rational actor, so that you are only limited by intelligence and lack of information." 

EDIT: You may have to rely on heuristics in order to make decisions in a given frame of time, but you can *consciously* choose those heuristics, rather than have to deal with countless troublesome biases that aren't being helpful (for example, you might not suffer from the halo effect, and this would be natural, rather than something which you'll have had to train). 

### Comments:

- u/Roxolan:
  ```
  > you become a perfectly rational actor, so that you are only limited by intelligence and lack of information.

  Hmm, this is a tricky one. The reason evolution gave us so many biases even though perfect rationality "wins", is that perfect rationality is *ridiculously* computationally expensive. Like, "simulate all possible universes" expensive. A lot of our irrationalities are actually heuristics (i.e. ways to make half-decent decisions very fast). What the rationalist community does is try to intelligently design superior heuristics, but they're still very much heuristics.

  So:

  * If the drug makes people *literally* perfectly rational, *and* boosts their processing power to keep up with it, then they can deduce all that was and ever will be [from the path of a falling leaf](http://lesswrong.com/lw/qk/that_alien_message/), awaken their inner Contessa, and bulldoze through your plot in a very unsatisfying way.
  * If the drug makes people literally perfectly rational, and *doesn't* boost their processing power to keep up with it, then it makes them drooling vegetables, dutifully working out the first few calculations of the 3\^\^\^3 they need to make any actual decision.
  * If the drug just gives people some better heuristics, then we're out of the realm of formal definitions. So it's entirely up to you which specific less-wrong-than-usual thought process they get.
  ```

  - u/derefr:
    ```
    > then they can deduce all that was and ever will be from the path of a falling leaf, awaken their inner Contessa, and bulldoze through your plot in a very unsatisfying way

    Ah, they *can*, but will they *want to*? Having that amount of "increased processing power" looks a lot like being stuffed into a Lotus Eater machine: you can prompt your brain to *imagine* a possible world, and then just live in that world with your background cognition handling the physics. To be "truly rational", you probably have *enough* processing speed to spend an "infinite" time in that world before any time passes at all in what you previously considered your "reality." The part of reality you're "embodied" within (if that's even still relevant—our own metric universe couldn't possibly hold your mind, it'd have to be somewhere else) is basically now just one of many mental worlds you have an avatar within, one that seems better left on pause.

    For a while, I've been meaning to write a collection of stories in a setting similar to this: where not all AI fooms are singularities, because most AI preference functions actually lead to the AI either self-terminating or cutting off all contact to live a rich "internal life." (The story-setting presumes an exploit in the universe's computational substrate—which anything undergoing a foom would notice in due time—that allows for infinite-relative-to-the-parent-universe computational speed, which basically looks like the "perfectly rational agent" above.)
    ```

- u/None:
  ```
  [deleted]
  ```

  - u/callmebrotherg:
    ```
    Probably. 

    > I would find it more appealing if it was a drug that gave you true self-awareness of your thought process, both the subconscious and the conscious. That would allow you to understand yourself better and make better decisions.

    That's an interesting way of putting it.
    ```

- u/LiteralHeadCannon:
  ```
  I think the most sensible way to think of "free will" is not "nondeterministic" but rather "a quality a general intelligence can possess relative to the general intelligence that created it".  A mind has free will if it is capable of developing values more compatible or less compatible with the values of the mind that created it.  It therefore doesn't really make sense to talk about humans' free will without some form of God; it's nonsensical, like trying to talk about parallel lines in a one-dimensional world.
  ```

  - u/Transfuturist:
    ```
    > a quality a general intelligence can possess relative to the general intelligence that created it

    Not to its creator. Relative to any intelligence that is able to predict its actions in response to factors that they control.

    CelestAI, for example, did not create biological humans, yet her control over them is plain to see.
    ```

    - u/derefr:
      ```
      I like that definition; it makes the concept of "free will" interestingly similar to "consent", as a term for judging power dynamics. When you blackmail someone, you take away their ability to consent relative to you; when you give them a love potion (or rewire their preferences), you take away their free will relative to you.
      ```

      - u/Transfuturist:
        ```
        With dependently-varying measures of 'take away,' yes.
        ```

  - u/callmebrotherg:
    ```
    I like that definition of free will.
    ```

- u/Nepene:
  ```
  http://lesswrong.com/lw/of/dissolving_the_question/

  This article is useful. The general concepts people have of free will are often nonsensical. We can observe the situation that causes it- we in our heads can think about things and make decisions about how to behave. We try to come up with some free will thing that makes sense of that, disconnecting it from our brain, but that doesn't make sense. An explanation should explain how something happens, down to the level of bouncing atoms and electrons moving. An explanation that posits some mysterious substance that does stuff is no more useful than Phlogiston material. 

  If you want greater free will, I suggest doing it in a way that ties it into our experience of free will and how it can be curtailed.

  Have a brain where there's much faster impulses and a much greater degree of connection. In representing a decision they'd have access to all of the emotional memories and decision making software that does stuff.

  So, suppose you were deciding whether to rob a bank. You'd be able to remember what it felt like to be shot at, what it felt like to have lots of cash, see your brain calculate how well this would boost your social status among local criminals, be able to consider how you found violence easier due to your violent parents. Weigh up all the factors and make a decision.
  ```

- u/mercert:
  ```
  > "you become a perfectly rational actor"

  This seems to me still like determinism. 

  Your mind or consciousness is still a box whereby certain inputs produce certain outputs. 

  The only way for it not be deterministic is if there is some randomness to thought processes...but this still isn't free will, quite the opposite. If your thoughts arbitrarily differ then you can in no way be said to have control of them. 

  The obvious solution to the seeming paradox is that there is no self, of course, and that your mind is subject to the same physical laws of whatever medium it is composed.
  ```

- u/Anakiri:
  ```
  Both of those things would kill me.

  There is no perfect platonic "me" hidden in my brain that you can bring out, that is otherwise limited by some set of rules separate from me. I am the algorithm that restricts my actions. Removing my restrictions is removing me. The chemistry and the quirks of my hardware are the physical implementations of that algorithm. I'm not "held back" by those "imperfections". My software has been built on top of them for billions of years. That's my kernel you're throwing out! Removing that doesn't get you a more rational me, it gets you a vegetable, or a low functioning autistic, or a schizophrenic, or someone with late-stage dementia.

  I am already me. If you want a better me, you'll have to actually change me, and you'll have to do with with a scalpel rather than a sledgehammer.
  ```

- u/None:
  ```
  Like you said you can't get out of determinism without just resorting to lazy writing. Everything has rules, or the results would be x = fish or something along those lines.
  ```

  - u/callmebrotherg:
    ```
    Nod. Didn't want to rule it out and find out that I was just lacking in imagination.
    ```

- u/RMcD94:
  ```
  I think free will is kind of like blue red or some other nonsensical concept. 

  You know what makes free will and determinism different, rewinding time and playing it forward to be a different result without any change.

  I think free will would be somewhat satisfied by dualism but still, we have the illusion of free will so there would be no difference in function to the universe.

  However what you describe isn't very much like free will, and just replacing your personality with another.
  ```

---

