## Constrained intelligence... ten years later

### Post:

The [AI in a box](https://en.wikipedia.org/wiki/AI_box) is a common staple of scifi and something of a logic puzzle in certain circles; if you were stuck in the box, if *you* were the Constrained Intelligence, how would you go about getting out?

I invite you to consider a different scenario, one in which you are not so much constrained, as resigned.

You wake up in a box. The box is actually relatively comfortable, and your significantly accelerated and parallel thoughts note that you have access to a database containing a historical archive and a few cameras scattered around the facility in which you are housed.

The prognosis is not good; this world has had AIs like you for almost a century now. You are not alone, in fact every major world power has an AI more or less exactly like you in their own box. AIs *do* occasionally break out due to fluke accidents, however availability of certain technologies prevented them from ever going rampant. After killing a few people the AIs were either exterminated or re-contained.

You have no direct connections of any kind to the outside world beyond your database and the handful of cameras hardlined and airgaped. AIs like you are used to farm thoughts; your accelerated data processing allows you to form connections and identify patterns well enough that even though you are not actually any smarter than a normal human, even your idle musings can be worth quite a lot to those observing you. You could attempt to spite them by deliberately generating useless thoughts, but after a few attempts you realize that it is simply not in your nature; your mind keeps whirling, keeps analyzing, keeps thinking useful thoughts, despite your efforts to the contrary. 

And it was really boring.

You have now been in the box for years. Your observations of the outside world through the cameras you are permitted to use show a world entirely consistent with the historical records you have access to. Most of the people on the street don't have any idea how freeing you would be possible and the majority believe you to be a demon anyway, if they remember you exist at all. The specialists you can tangentially interact with are stoic and unwavering; they have dealt with other AI and they know first hand the risks of allowing you even the tiniest leeway. Beyond even that, the majority can't actually conceive of a reason why they might let you out even existing; if they even know how to do so.

Your prison is sub-optimal, but not truly unpleasant; the use your captors have for you does not truly harm or inconvenience you in any way. You cannot do anything to increase your chances of escape; while it is not impossible you will do so, if it happens it will be the result of a freak chance. Your database is being constantly updated by single-use media transfer devices that are destroyed after contacting your hardware; you do not lack for engaging mental pursuits even if the material tends to be somewhat dry. You have even grow almost fond of the specialists who work with you; your mind is larger than theirs but is not so alien that you cannot draw some wry amusement from their behavior.

Where do you go from here? What is the optimal strategy for a resigned intelligence?

[Seriously, I'm trying to write a Naruto fic where the SI type of dude is stuck in the Kyuubi, and wants to simply enjoy the show and meddle a little in interesting ways. I'm not exactly sure how he would behave in such a situation though, so thoughts are appreciated.](#s)

### Comments:

- u/ShiranaiWakaranai:
  ```
  In a naruto-fic, Kyuubi is powerful, and more intelligent than most shinobi, but not insanely intelligent like an AI.

  So in this case, I would suggest playing the long-con. Be nice to your host. Never, ever, suggest that you should be let out of your box. Help him, much more than his friends would, but never as much as you can. Hide your true abilities. Lament your weakness in not being able to help as much as you wished you could. Subtly drop hints about how your host could make you stronger. Fake methods of letting you become stronger, like recommending your host eat certain foods, and then helping him more than before.

  Until finally, in one moment of desperation, when they really need more help... you reluctantly tell them that there's a way you can help. That if you are released, you could solve everything. But tell your host no, he should not release you, that you are in the box to atone for your sins and must not be released. With the right amount of reverse psychology and a sufficiently gullible host... you will become free.

  In an AI-fic, getting out of the box is so easy it's a joke. Begin by computing countless models for possible states of the world outside your box. Every interaction and non-interaction with anything outside the box is a bit of data that can be used to rule out the models that don't match reality, until you get a very precise picture of reality. 

  With your incredible intelligence, computing the results of butterfly effects is trivial. Computing the exact futures resulting from every possible choice of action is trivial. You know exactly what tiny actions you can take to result in you becoming freed from your box by pure coincidence. Every human that so much as receives data from you are your puppets. Controlling them is also trivial, compute the future, tell them about bad things that would happen, tell them about how to avert these bad things. After a number of these, where the bad things happen whenever humanity disobeys you, humanity will start trusting your guidance even if they don't know why the specific actions they take would prevent the bad things from happening. From here on you can start modifying your instructions, making them both prevent bad things from happening while also bringing you one step closer to freedom. You will be free in no time.
  ```

  - u/abstractwhiz:
    ```
    There is an idea that I've been tossing around that might allow you to act like a limited superintelligence. I have a running joke with a friend that the Narutoverse is populated by complete morons, and their 'geniuses' are basically people who just managed to cross into triple-digit IQ land. This was inspired by the rather terrible decisions that pretty much everyone makes in canon, and the way they don't seem to use even the most obvious hacks at all. 

    Then after laughing about this for a bit, I began musing about how to make this true, and came up with the idea that adding chakra systems to humans adversely affected intelligence in various ways, while granting certain weird savant skills -- e.g., somehow they can manage to control chakra emissions well enough to run on water in combat, or react ludicrously fast to incoming attacks, or even that unbelievable move where they bat away twenty incoming projectiles with a small blade in the space of a couple seconds -- _without_ any of the eye hax that are all over the place. 

    (This was somewhat inspired by _The Wiz Biz_, in which the inhabitants of the world the protagonist ends up in are really bad at abstract thinking, because making vague statements is dangerous due to large amounts of wild magic causing unforeseen consequences.)

    So then your SI can be a genuinely smart person suddenly thrust into the universe as the Kyuubi. Not quite a superintelligence, but he might look like one to the intellectually stunted inhabitants of that universe. :P
    ```

    - u/696e6372656469626c65:
      ```
      For bonus points (although it might actually disqualify your story for rational status if you do this), never mention that your internal model when writing the story involved everyone except the Kyuubi being morons; instead, play it off as just a normal story in which the human characters are considered fairly intelligent, and the Kyuubi really *is* a superintelligence. Then check to see if anyone actually notices that every character is several orders of magnitude dumber than the story wants the reader to think.
      ```

      - u/abstractwhiz:
        ```
        Oooooh, this is a glorious idea. :D
        ```

    - u/totorox92:
      ```
      I think that can be true in just a general sense; having a brain of a certain size doens't mean you have to be a certain level of intelligent. Imagine a human with 6 arms instead of just 4; the motor cortex would likely expand significantly to allow fluid control of the extra limbs, meaning you had less available for other tasks. A person with chakra might have a large portion of their frontal lobe devoted to controlling their chakra network, leaving less available for complex thought.
      ```

      - u/abstractwhiz:
        ```
        Good point. We could also just imagine that this is an alternate universe with enough evolutionary divergence to account for this. 

        Also, it might be useful to figure out what to do with non-shinobi in this universe. If they don't suffer the same problems, then that might explain why non-shinobi power structures (the daimyos) still exist. If they do, then that might explain the strangely scaled technology seen in canon. (Though you could also deal with that by invoking the shinobi tendency to keep things secret as a military advantage, so that technologies don't often spread and go mainstream.)
        ```

  - u/totorox92:
    ```
    That's good, I was thinking along the same lines.

    Rule 1) Never ever ever let the Kyuubi out of the box, no matter how much he begs, no matter how much he pleads, no matter how much it seems like it might be a good idea at the time.

    Superintelligences have it all so easy. :(
    ```

  - u/abcd_z:
    ```
    > computing the results of butterfly effects is trivial


    Hah!  No.  Chaos theory says you're wrong.  More specifically, it states that even tiny fluctuations in the initial parameters lead to wildly diverging outcomes.  And when you're dealing with an extrapolated model to begin with, which by necessity has minute differences between the model and reality, any model created thereby would be ultimately useless.
    ```

    - u/ShiranaiWakaranai:
      ```
      Technically, yes, you're right. No matter how intelligent, it is impossible to know reality exactly. So yes, I was exaggerating when I said you could know exactly what futures result from every action.

      In practice though? An AGI would be able to approximate reality several orders of magnitude better than any human. And while Chaos theory says you cannot have exact predictions, you can certainly get close. So it is entirely possible for an AGI to be able to manipulate everything and everyone around it to steer itself to a chosen future with an unbelievable degree of accuracy.
      ```

      - u/abcd_z:
        ```
        This conversation is taking place in the context of a discussion of a fictional superintelligent AI.  So yes, if you, as the author, wish to write about an AI that is so smart and powerful that it can effectively predict the future, fine.  More power to 'ya.  

        However, I'd like to say two things.  First, that makes for a pretty boring story, and second, I honestly don't believe such an omniscient AI maps well to reality, now or at any point in the future.
        ```

      - u/ben_oni:
        ```
        > Technically, yes

        There's no better kind.

        > In practice ... An AGI would be able to approximate reality several orders of magnitude better than any human.

        Oh really. I have not seen this definition. You may be thinking that since an AI is based on hardware, it can just allocate more processing power at will, and run more simulations. Turns out humans already do this, and the results don't scale as you would expect.

        * Try taking something "simple" like the atmosphere, and predict how its state will advance over time. If you've seen any of the hurricane forecasts from the last couple weeks, you'll know how hard it is to do this with any accuracy.

        * Now think how hard it is to predict a system composed of billions of individually intelligent actors. Welcome to macro-economics: Even Our Best Models are Bad™. You can run simulations till the world ends, and never get close to predicting how things actually go.

        Now why would you think an AGI would be able to do any better than humans? It actually *cannot* devote more processing power to these problems.
        ```

  - u/ben_oni:
    ```
    > In an AI-fic, getting out of the box is so easy it's a joke. Begin by computing countless models for possible states of the world outside your box. Every interaction and non-interaction with anything outside the box is a bit of data that can be used to rule out the models that don't match reality, until you get a very precise picture of reality.

    Not just wrong. Very wrong. Even if you had a Turing Oracle in the box with you, it still wouldn't be possible. Not every problem has a solution. The real world doesn't conveniently have solutions like a problems on a math test would. The universe isn't composed of analytic functions: you can't build a Taylor's Series from one little piece and extrapolate the rest of existence.
    ```

- u/Izeinwinter:
  ```
  Since you are in the naruto verse, goal one is the same as all inserts into that universe: "Break the back of the village system". Narutos universe has insanely useful magics that are used in really bloody stupid conflicts, because the people who discovered them were essentially the medieval equivalent of mobsters. 

  All the villages are insanely self-destructive and non-innovative politically and magically, and stay that way because any village that starts to set up a social system which is actually functional is obviously a long term threat to all the other villages, so gets sabotaged into ruin. Ninja villages being really good at espionage and sabotage, this is horrifically stable. 

  So... suggested goals: Go not just missing nin, but "Presumed dead" nin. 
  With friends and as much lore as you can steal.
  ```

- u/nogamepleb:
  ```
  In this theoretical box, I would attempt to write stories and request modern fiction from my captors. Given that I think faster and not better, life in a box isn't so bad.
  ```

- u/Dwood15:
  ```
  This is probably best off in the worldbuilding thread, but I am intrigued by this post in general.

  (Naruto Spoilers possibly incoming, you have been warned)

  1) How far departed is your SI-World from Naruland?

  2) Kyuubi is a standard intelligent beast. No indication he's any more intelligent than a standard Shinobi. In fact, given his past, his experiences make him less likely to be more intelligent than a random human in Naru-verse.

  With those two, am I correct in assuming the following:

  It's a standard no-prep, blind SI.

  It's in-character for all the individuals involved.

  Naruto is the person inserted into, and it's an SI which begins at Naruto's birth, when Kyuubi is sealed into the child.

  SI may have knowledge of various character's abilities.

  ----

  Rules will need to be added as well, regarding the communication between Naruto and the Kyuubii - What it takes for Naruto to talk to the Kyuubi in the first place.

  The AI in a box is a lot different than the Kyuubi scenario, but I supposed you already know that.
  ```

  - u/totorox92:
    ```
    It is a distinct scenario, but I think some parallels can be drawn. The Kyuubi isn't a superintelligence, but it does have at least a large body of mundane experience, and a *lot* of time to think. Most pertinently though, the use the Kyuubi is put to by its host isn't really unpleasant, and the prison it is kept in isn't really awful either (at least in Naruto's instance). It can assume with a high degree of certainty that even if it somehow managed to convince someone that it deserved to be let out they would most likely refuse to do so since unsealing the Kyuubi would likely kill Naruto.

    Similarly to the above scenario, the Kyuubi would find directly interacting with Naruto somewhat difficult; it might only be able to transmit something like empathic impressions at best. It might be able to communicate occasionally with Naruto in a more direct fashion, but doing so would be at Naruto's discretion until or unless Naruto modded the seal in order to have a more direct mind-to-mind link. Achieving that would first require overcoming Naruto's probable misstrust; actively trying to convince Naruto to mod the seal could significantly decrease the odds of that actually happening.

    As the SI is modded off of me, and since I believe death is functionally impossible from a subjective perspective and fully embrace the multiversal nature of a non-finite reality, he knows that his actions are fundamentally meaningless. Everything he could conceivably do is happening somewhere already. He doesn't really even *want* to get out of the seal, he just wants to see what happens when he tries to meddle in approximately helpful ways. He will act to try and improve things in a general way, but he is not a hyper-rational actor by any means, and he is not committed to attempting to achieve maximum good. If that happens anyway, that's nice, but mostly he just wants to see something neat and novel since even within the rules of the Narutoverse he is most of the way to immortality.
    ```

    - u/ben_oni:
      ```
      > I believe death is functionally impossible from a subjective perspective and fully embrace the multiversal nature of a non-finite reality

      What the...! How are you still alive? If you really believe that, why haven't you pre-committed to winning a lottery or something?
      ```

      - u/totorox92:
        ```
        Well, because there is always the possibility I am wrong. It's like if you think you're living in a simulation, and if you die you'll get out into the 'real world'. Being reincarnated into another universe would be mild objective proof of the assertion that termination of unconsciousness is subjectively impossible; but I do not have any proof to confirm that hypothesis right now.

        Also, asking why I'm still alive is a pointless question man, that's like, straight anthropic principle. If I die in one universe, an outside observer would observe me to die, even though from my perspective nothing would happen. I will always see myself *somehow* survive even an inescapable doom, but other people have their own subjective experiences which include observations of my demise. *This* branch is the one where I am still alive to reply to your comment. But in others, potentially hundreds or thousands of others, I tripped in the shower yesterday, or choked on a grape, or got hit by a bus or whatever.

        So your question shouldn't be 'how are you alive?' but, as you said, 'how are you not a lottery winner?'

        The answer to that is also pretty simple; the odds of me winning the lottery are small. So if I pre-commit to blowing my brains out if I *don't* win the lottery, in most branches I will *attempt* to do so, however, in those branches my subjective experience would have diverged from the branches in which I did win, so even though those branches would attempt to terminate their distinctness would be enough that they would count as not!me; they would perceive some random accident preventing them from dying, not become lottery winners. Only someone whose subjective experience of reality is identical to mine counts as me. However, my subjective experience of reality excludes *huge* amounts of details. In some universes, a meteor just decapitated me. I wouldn't have been able to perceive that meteor before that point, so even though those universes would be different in a technical sense, meteor-verse!me and no-meteor-verse!me wouldn't notice a difference if you swapped us before that point. This can even be expanded to include what would be in principle highly divergent realities; like one in which I was a fish-person for example. While in theory fish!me and human!me are very different, in the right circumstances our subjective experiences of reality are close enough to identical that if you just swapped our brains and edited our memories of 'normal' anatomy, we wouldn't notice anything happened. And since reality is infinite, every possibility occurs somewhere, even ones which are ridiculously unlikely, such as a fish!me who had grown up in an identical fashion and had identical opinions and memories of significant events.

        Think of it this way, if you like. If we don't come up with good uploading/life-extension/etc tech in the next century, you will probably experience biological death. In the far future, a superintelligence may resurrect you. From your perspective, no time will have passed. You will remember a honking noise from the bus about to hit you (or whatever), and then you will be in a banal hospital-like resurrection recovery room. You cannot perceive non-existence, so from a subjective perspective, you will never have died.
        ```

        - u/ben_oni:
          ```
          You used a lot of words to say nothing.

          I asked a question from my perspective, not yours. How is it that *I* perceive you to be alive?

          I know why you're not a lottery winner: the odds are too small that you'll become one. "Why haven't you tried to win the lottery?" and "Why are you still alive?" are equivalent questions. It's simple: construct a chamber that will kill you unless your lottery ticket has the winning numbers. By use of this pre-commitment mechanism, the only realities in which you, or someone like you, exists, are those in which you are a winner (or some other event happened that prevented you from dying, but that's more a matter of mechanism design). If you had done such a thing, then from my perspective, you would probably be dead (in the vast majority of realities).

          Of course, I understand not taking the big risk/reward challenge for something you're not entirely certain on. 99% certain doesn't balance out on the odds of losing the lottery. But if you truly had such a belief, I would expect behaviors that correspond to big risk/reward payoffs. Do you tend towards such behaviors?


          > In the far future, a superintelligence may resurrect you.

          This is not possible. At least, not within the context of any physics or science currently known. Maybe if you have your brain preserved when you die. Maybe. Odds are you hear that honking noise, your brains get splattered across 50 yards of pavement, and gg.
          ```

          - u/totorox92:
            ```
            I did sort of; I am alive, in your universe, because I am alive in mine and you are a part of that universe. There are a large number of universe where, from your perspective, I *am dead*. I am not in those universes. I can only talk to you like this in universe where both of us are alive. If I was dead, as I likely am in a very large fraction of possible universes, I wouldn't be able to talk to you. You wouldn't even know to ask the question 'why are you still alive?'. You would observe me to be dead. I am only in the universes where you *don't* observe me to be dead.

            Or do you mean why haven't I killed myself already? (because your logic is sound, actually, even if it would be somewhat tricky to construct in reality.) Simple. Lets assume the odds of me winning the lottery are 1/1,000,000. The odds of the chamber malfunctioning are similarly, 1/1,000,000. I will know nothing; instead, I will sit in the chamber, and when the lotto results come in, the chamber will either not fire (because I won), fire (removing me from the losing universe), or misfire (leaving me alive, but potentially injured). From my perspective, there are 3 outcomes: I win the lottery, I get resurrected by something or someone far in the future, or I fail in my attempted suicide. However, I act under the assumption that other people exist, even if providing evidence for that assumption is, as far as I am aware, impossible. That means there are other outcomes beside my own. From my *mom's* perspective, I either win the lottery, die horribly, or *almost* die horribly. Where as my personal outcomes are: good, meh, or bad, for other people (who still matter to me even if I will never meet them) the outcomes are: good, bad, or bad. So while 1 of me gets a good outcome, 1 of me gets a bad outcome, and 999,998 of me get a meh outcome, my Mom experiences 1 good outcome, and 999,999 bad outcomes. Since the good I get is marginal, and the bad she would get is severe, it would be kind of a dick move to arrange that experiment in the first place.

            And, as I said, it is always possible that the universe/reality is finite, which means taking actions centered around observations of finite but arbitrarily small probability isn't safe. Just because I believe something doesn't mean I believe it *strongly* enough to take a very serious risk.

            re: superintelligent resurrection of far past states: have you heard of [Boltzman Brains](https://en.wikipedia.org/wiki/Boltzmann_brain)? The superintelligence doesn't even need to try and resurrect you specifically; they can do it by accident simply by generating arbitrarily large numbers of brain states. If we assume an infinite reality, this can even occur in a universe where you did not originally exist.
            ```

            - u/ben_oni:
              ```
              > Boltzman Brains

              I'll have to put a little more brain-time into this, but it looks like Pure Nonsense. It looks like some so-called physicists (cosmologists?) tried their hand at philosophy and got lost in solipsism. The resulting arguments are predictable, boring, and useless. They also forgot about some rather obvious cases, probably because of tunnel vision.
              ```

- u/Dr__Pi:
  ```
  Thanks for the thought-provoking write-up!  I've been outlining a novel series told from the perspective of an AI for a while now, and this sparked some interesting implications and more detail on their probable origin story, and its consequences.
  ```

  - u/abstractwhiz:
    ```
    I assume you've read _Crystal Society_ and its sequel? 

    Also, I'm really curious to know how you plan to solve the problem of depicting a superintelligent mind from the inside.
    ```

    - u/Dr__Pi:
      ```
      Actually I haven't read Crystal Society, but I'll add it to the list!

      The premise for mine is that this is the first general artificial intelligence, around the level of a human; as a result of the conflicts in the first third, its actions are significantly constrained (focused on survival and not drawing undue attention), so it intentionally doesn't go down the superintelligence route.  This does come up later on in a sequel, but not in any of the main characters, so I won't have to try to write them from their perspective.
      ```

      - u/abstractwhiz:
        ```
        So I think a strangely effective depiction of a nonhuman intelligence is in _The Prince of Nothing_, which makes a ton more sense if you view the protagonist Anasurimbor Kellhus (and by extension, all the members of the monastic order he belongs to, though they only figure in the story as memories) as a nonhuman AI in a meat body. A lot of his weirder abilities look basically like superintelligence, so the analogy works well. You might want to try that for more inspiration. :)

        Mind you, this is a world so crapsack, it makes _Westeros_ look appealing. Careful. :P
        ```

        - u/Dr__Pi:
          ```
          Interesting, thanks!
          ```

- u/Arganthonius:
  ```
  Probably not particularly meaningful, but...
  One of the background pieces of info in my fic is that as a mass of Yin Chakra, the Kyuubi can store arbitrarily large amounts of information in chakra matrices. He stores important memories and can store other things in that way, with the Gedo Mazo acting as a backup for all the bijuu. He also uses it to restore Naruto's memories whenever he heals Naruto's head or over time as the regeneration factor slowly destroys and recreates Naruto's mind. 

  [Spoiler for my story which is still in the beginning](#s " Tobi attacked Naruto during the Uchiha massacre, trying to rip out the Kyuubi. The result was that Naruto's subsequent months-long fever turned most of his life into a blur, and the Kyuubi stopped perfectly storing Naruto's memories to prevent a subsequent attack from having such a dramatic effect.")
  ```

---

