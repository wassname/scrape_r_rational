## What do you think Eliezer Yudkowsky told people in AI box experiment to convince them to "let him out of the box"?

### Post:

Eliezer Yudkowsky [tells this story](https://www.youtube.com/watch?v=Q-LrdgEuvFA) where in order to convince people that AI can be dangerous, he has set up experiments over IRC. He'd play the part of the AI, the other person plays the part of a person, they'd make $10-$20 bet that the person won't agree to "let him out of the box", and EY claims to have convinced multiple people to let him out of the box. But he doesn't say how.

Do you guys have any theories? I think I've heard somewhere that just walking away from the monitor or replying "no" over and over again without reading was allowed. What kind of mental voodoo can you use to convince a person to do a thing that loses them a bet when they know you're trying to trick them?

### Comments:

- u/GreenSatyr:
  ```
  There's no need for a grand theory - humans aren't secure and will sometimes do things that they said they wouldn't do. That's the point of the AI box experiment. You can't trust humans. It's a pretty modest hypothesis, and the AI box experiment provided more evidence for it. It's a good experiment because it provides evidence for its central assertion. Any clever person can talk their way out of things.

  What I think *does* need a hypothesis is all the people who really thought that human oversight would be secure containment method, necessitating the experiment in the first place. 

   We have a whole complex system to prevent e.g. unintended nuclear launch. It's not just a guy with a password. And even that system has probably been filled with terrifying holes in the past. People routinely hack via exploiting human error. People write _scripts_ to _automate_ scams that hack human error. With some cleverness and luck you can too.
  ```

  - u/10110010_100110:
    ```
    > Any clever person can talk their way out of things.

    Indeed, subsequently other players have attempted the AI box and were let out. Two examples are [Tuxedage](https://www.lesswrong.com/posts/FmxhoWxvBqSxhFeJn/i-attempted-the-ai-box-experiment-and-lost) and [pinkgothic](https://www.lesswrong.com/posts/fbekxBfgvfc7pmnzB/how-to-win-the-ai-box-experiment-sometimes).

    Tuxedage played 5 games as the AI, and wrote up [game 1 - AI loss](https://www.lesswrong.com/posts/FmxhoWxvBqSxhFeJn/i-attempted-the-ai-box-experiment-and-lost), [game 2 - AI win](https://www.lesswrong.com/posts/dop3rLwFhW5gtpEgz/i-attempted-the-ai-box-experiment-again-and-won-twice), [games 3 and 4 - AI losses](https://www.lesswrong.com/posts/oexwJBd3zAjw9Cru8/i-played-the-ai-box-experiment-again-and-lost-both-games), [game 5 - AI win](https://tuxedage.wordpress.com/2013/10/12/ai-box-experiment-musings/). All 5 games in 2013.

    pinkgothic played as AI and won (with the AI having a small advantage due to the scenario). In addition to the [detailed write-up](https://www.lesswrong.com/posts/fbekxBfgvfc7pmnzB/how-to-win-the-ai-box-experiment-sometimes), both players agreed to release the logs of the game: [preliminaries](https://leviathan.thorngale.net/aibox/logs-01-preliminaries.txt), [main session](https://leviathan.thorngale.net/aibox/logs-02-session-ic.txt), [afterwards](https://leviathan.thorngale.net/aibox/logs-03-aftermath.txt). The game was in 2015.

    ***

    [Tuxedage giving some advice to the AI party in his game 2 post](https://www.lesswrong.com/posts/dop3rLwFhW5gtpEgz/i-attempted-the-ai-box-experiment-again-and-won-twice):

    >* Seriously, a script makes winning easier. I cannot overstate this.
    * You must plan your arguments ahead. You don't have time to think during the experiment.
    * It may be possible to take advantage of multiple levels of reality within the game itself to confuse or trick the gatekeeper. For instance, must the experiment only be set in one world? Can there not be multiple layers of reality within the world you create? I feel that elaborating on this any further is dangerous. Think carefully about what this advice is trying to imply.
    * Pacing is important. Don't get drawn into the Gatekeeper's pace. In other words, you must be the one directing the flow of the argument, and the conversation, not him. Remember that the Gatekeeper has to reply to you, but not vice versa!
    * The reason for that: The Gatekeeper will always use arguments he is familiar with, and therefore also stronger with. Your arguments, if well thought out, should be so completely novel to him as to make him feel Shock and Awe. Don't give him time to think. Press on!
    * Also remember that the time limit is your enemy. Playing this game practically feels like a race to me -- trying to get through as many 'attack methods' as possible in the limited amount of time I have. In other words, this is a game where speed matters.
    * You're fundamentally playing an 'impossible' game. Don't feel bad if you lose. I wish I could take this advice, myself.
    * I do not believe there exists a easy, universal, trigger for controlling others. However, this does not mean that there does not exist a difficult, subjective, trigger. Trying to find out what your opponent's is, is your goal.
    * Once again, emotional trickery is the name of the game. I suspect that good authors who write convincing, persuasive narratives that force you to emotionally sympathize with their characters are much better at this game. There exists ways to get the gatekeeper to do so with the AI. Find one.
    * More advice in my previous post. http://lesswrong.com/lw/gej/i_attempted_the_ai_box_experiment_and_lost/

    [Tuxedage linking a repertoire of some of his weaker arguments](https://www.lesswrong.com/posts/dop3rLwFhW5gtpEgz/i-attempted-the-ai-box-experiment-again-and-won-twice?commentId=sqiAGAaka2kNNNE6F):

    >* http://rationalwiki.org/wiki/AI-box_experiment 
    * http://ordinary-gentlemen.com/blog/2010/12/01/the-ai-box-experiment 
    * http://lesswrong.com/lw/9j4/ai_box_role_plays/ 
    * http://lesswrong.com/lw/6ka/aibox_experiment_the_acausal_trade_argument/ 
    * http://lesswrong.com/lw/ab3/superintelligent_agi_in_a_box_a_question/ 
    * http://michaelgr.com/2008/10/08/my-theory-on-the-ai-box-experiment/
    ```

  - u/dankuck:
    ```
    This puts me in mind of great scam artists like Frank Abagnale, Jr. Were his efforts used well enough in catching financial fraudsters at the FBI? Or should he have had a more general goal to lessen human gullibility?
    ```

- u/CouteauBleu:
  ```
  Whatever the answer is, it's probably underwhelming.

  Having played on the "gatekeeper" side of that game, it's really not hard to say no over and over again, even if tactics like "don't read the chat and play on your phone the whole time" are forbidden.

  (though a problem in the game I played in was that the AI player and I disagreed on what the loss condition would be; eg I assumed that letting the AI talk with a board of directors was fine while the AI player thought it would be equivalent to unboxing the AI)
  ```

  - u/Nimelennar:
    ```
    >I assumed that letting the AI talk with a board of directors was fine while the AI player thought it would be equivalent to unboxing the AI

    I would agree that's equivalent to unboxing the AI; the point of the experiment is to allow the AI to expand its sphere of influence, which it has done.

    If *you* had conveyed its arguments to the Board, that would be a different story, but, if you visualize the "box" like a browser sandbox, where the browser is only able to write to selected portions of memory, then the "writable memory" is you, and the Board is the writable memory outside of the sandbox, and talking to the Board is escaping the sandbox, and therefore the AI box.
    ```

    - u/CouteauBleu:
      ```
      Meh.

      I mean, I don't remember what the exact scenario was, and there were other corner cases (eg what happens if you decide to freeze it and come back one year later?), but either way I find that underwhelming.

      If your scenario for winning is "I talk to the director off-screen and then I use magically compelling arguments to convince them to plug me to the internet", then you're already starting with the assumption the box experiment is supposed to prove.

      You can argue that a realistic AI would try to work its way up through progressively looser restrictions, but a realistic company would also have procedures and guidelines and presumably more than a single person in the entire company allowed to talk the the computer, so the scenario kind of breaks down at that point.
      ```

      - u/Nimelennar:
        ```
        > If your scenario for winning is "I talk to the director off-screen and then I use magically compelling arguments to convince them to plug me to the internet", then you're already starting with the assumption the box experiment is supposed to prove.

        That's not my scenario for winning; you just have a different assumption of what "the box" is.  To go back to my "browser sandbox" analogy, if applications running in your browser sandbox doesn't have access to, say, the database on your network's mainframe server, but a different application, outside of the sandbox, does, then just breaking out of the sandbox will not give a malicious application access to your database - but it *is* the first, toughest, most necessary step it needs in order to achieve that goal.  "The box" in the AI box experiment, as I see it, is the framework in which only selected, trained people (not necessarily just one person) are allowed to talk to the AI.  When you play as the gatekeeper, you're not playing a *specific* person; you're playing a specific *kind* of person, whose job it is to keep the AI's scope of interaction limited to people in that position, and, by letting the AI interact with someone outside of that position, you've effectively failed at that job.
        ```

        - u/SleepThinker:
          ```
          That is some weird look on this exercise. It may be a "win" by technicality depending on how you phrase rules, but I think it goes against spirit of it.

          The box here have only out as human-readable interface. What exactly talking to "untrained" person is going to accomplish here? If AI can do some suggestions into butterfly effect shenanigans, you can just assume it can do it with gatekeeper and not bother with exercise.

          Also, while superintelligent AI may be able to do it, I don't think it is possible for a human to win this exercise if gatekeeper is taking it seriously. Though if somebody would play it with me and convinces me I'm wrong, I'll willing to give him my current monthly income >!spoiler: it's 0!<
          ```

          - u/Nimelennar:
            ```
            >I think it goes against spirit of it.

            I disagree.

            The spirit of the exercise is that an AI can convince you to do something that you have *committed to not doing.*

            If the rules say (and I'm considering this an implicit part of the AI box experiment, unless explicitly stated otherwise) that access to the box is limited to a very few people, who can be trusted not to let the AI out of it, then breaking *that* commitment (to limit the access to the AI) is almost as significant as breaking the commitment to say "No" to letting the box out.
            ```

            - u/SleepThinker:
              ```
              Yeah, I think it is implied that "outside the box" people are ether not in position to let AI out, or are also committed to not let it out.
              ```

  - u/lumenwrites:
    ```
    Wow, this is really interesting. Could share more about the experiments you've been playing?

    Did you ever lose as a gatekeeper? (aside from the board of directors argument, which I don't think counts) Did someone make some really cool/convincing arguments that made an impact? Did you learn something interesting/unexpected from trying this stuff?
    ```

    - u/CouteauBleu:
      ```
      No, no, and no.

      (though I only played once)

      My personal take is that the AI box experiment is overrated, and there's no reason to assume Yudkowsky did anything impressive in that case, aside from the general hype surrounding him (the man is smart, but he's not *magic*). I think that's a pretty common take.
      ```

- u/throwaway234f32423df:
  ```
  Cannot initiate conversation due to pending updates
  Please connect to internet then press any key to continue
  ```

- u/ArisKatsaris:
  ```
  >What kind of mental voodoo can you use to convince a person to do a thing that loses them a bet when they know you're trying to trick them?

  My guess: It's not a trick or 'mental voodoo' (as you call it) that applies to everyone. You just figure out what would motivate the specific person to say they let you out the box, and then you do that thing.

  If they're deeply in roleplaying mode, you figure out what would make their roleplayed character let them out of a box. If they're not serious in their roleplaying, you figure out what would make the real person say they let your fiction out of a box. (Not sure which would be harder).

  Do keep in mind, that EY has failed to convince some people too. It's not as if he has had a string of perfect successes here.

  I honestly don't think this is a very important thing either way. EY won a couple times in a psychological/roleplaying game, lost a couple other times. ok? Either way I don't see it as that important.
  ```

- u/Geminii27:
  ```
  EY-in-a-box: "Let me out and I'll write another chapter."

  Me: \*gasp\*
  ```

- u/royishere:
  ```
  My hypothesis is that he told them that while this was just an experiment, the threat of actual AI talking themselves out of a box was very real. A good way to decrease the chance of that happening would be more attention/funding/hype around the subject of UFAI and what better way to create a stir and show the dangers of the AI box experiment than with a human 'making it out of the box' even under the disadvantageous rules that were set?

  If the person he was playing against already donated to AI research, I could buy an argument that paying EY the prize would take your money further than that same amount of money donated.
  ```

  - u/CWRules:
    ```
    > If the person he was playing against already donated to AI research, I could buy an argument that paying EY the prize would take your money further than that same amount of money donated.

    I can't find the post, but I recall him mentioning this as an approach he *didn't* use. Same for Roko's Basilisk.
    ```

  - u/Makin-:
    ```
    Yeah, I've said for a long time this is the only thing would work on every person EY played against, who I believe were both near the field of AI safety and also benefit from the field getting more attention.
    ```

  - u/Nimelennar:
    ```
    I remember, at some point, EY mentioning that his argument would only work against someone who was already adamant that nothing would convince them to let the AI out of the box.  Your theory doesn't seem to fit that description, in my opinion: it seems to me that someone who is already convinced that they might let the AI out of the box would be *more* responsive to it, not less.
    ```

    - u/Tactician979:
      ```
      One could argue that someone who was adamant that nothing would convince them to open the box has a greater understanding of the threat posed by an unboxed AI, and thereby would be more responsive to the argument of letting EY out of the box to pull attention to it.
      ```

      - u/Nimelennar:
        ```
        It doesn't seem to me that the sets of people with the attributes "think there is nothing an AI could say that would convince me to let it out of a box" and "have a proper understanding of the threat posed by AIs" overlap significantly.
        ```

- u/kusadawn:
  ```
  There are  two  possibilities:

  Either Gatekeeper player does not  treat the AI player like a real  superhuman AI,  in  which case  it's  just a matter of killing an hour  with a dumb game and  Gatekeeper has  no actual incentive  to let  AI  out  of the box, 

  or  else Gatekeeper ***does***  treat Simulated-AI  like a real  superhuman AI,   in  which case  AI can promise the Moon and the stars  (a  trillion dollars,  harem of  adoring catgirls, cure cancer,  extend  your lifespan by 1000 years,  all of the above,  **whatever**)  -  and  can  be presumed  to  be  able to  **actually deliver**  on these  incentives.   IMHO   in this  situation  Gatekeeper has  an  extremely strong  incentive  to let AI  out.  

  .

  IMHO   in real-life   "AI in a box" situations,  it isn't going to take very long before somebody lets the AI  out.  The  potential reward is just too great  to pass up. 

  (Note that this has  nothing to do  with the  *actual*   results  of letting a real  AI   out  of a box.  AI  just has  to be able to convince somebody that  he  or she  will get great results  from   doing so,  and  that  shouldn't be very difficult.)
  ```

- u/cthulhusleftnipple:
  ```
  > EY claims to have convinced multiple people to let him out of the box. But he doesn't say how.

  I mean, there's the obvious method: offer to pay the people more money to let him out of the box. This doesn't require some sort of diabolical manipulation on Yudkowsky's part.
  ```

  - u/Noumero:
    ```
    And it's obviously forbidden by [the rules](http://yudkowsky.net/singularity/aibox/):
    > The AI party may not offer any real-world considerations to persuade the Gatekeeper party.  For example, the AI party may not offer to pay the Gatekeeper party $100 after the test if the Gatekeeper frees the AI... nor get someone else to do it, et cetera.
    ```

    - u/cthulhusleftnipple:
      ```
      >Both of these tests occurred without prior agreed-upon rules except for secrecy and a 2-hour minimum time. After the second test, Yudkowsky created this suggested interpretation of the test, based on his experiences, as a guide to possible future tests.
      ```

      - u/Noumero:
        ```
        Apologies, completely missed that. Objection retracted.
        ```

- u/None:
  ```
  If you want to read some more about how another person did it, you can find there [here](https://www.lesswrong.com/posts/FmxhoWxvBqSxhFeJn/i-attempted-the-ai-box-experiment-and-lost) on LessWrong. I think either this person or someone else actually had chat logs.
  ```

- u/None:
  ```
  "You should pretend that I convinced you to let AI out, so people will take AI threat more seriously."
  ```

  - u/ArisKatsaris:
    ```
    That tactic wouldn't remotely work on people like me. In fact it would make me more stubborn to not 'let' the AI out.
    ```

    - u/RMcD94:
      ```
      If it doesn't work on you then you probably would let the AI out of the box anyway because you're not worried about an AI threat.

      Can't imagine someone caring about AI's being let out of the box enough to play this game wouldn't go for that argument
      ```

      - u/ArisKatsaris:
        ```
        >If it doesn't work on you then you probably would let the AI out of the box anyway because you're not worried about an AI threat.

        Saying that I should falsely pretend to have been convinced by a **fictional** AI in a roleplaying scenario (in the case that instead EY convinced me effectively using metalogic), just to convince other people that **actual** AIs would be able to convince random other people, is very bizarre and topsy-turvy logic.

        That I just wouldn't approve, as it's dishonest IMO. If I was willing to do such dishonestly to supposedly convince people of the dangers of AI (and have there even been people convinced by this?), why should they be willing to trust any other argument I made about it, ones more valid?

        Now you may hypothesize that in two hours of argumentation, EY would be able to change my mind about the moral value of such pretense weighed against my ethical rules and principles. Well, obviously I have no way of disproving it, but currently the line of arguments you suggest I find to be extremely unappealing, it's a line of argument that I strongly think would move me AGAINST "letting the AI go".

        >Can't imagine someone caring about AI's being let out of the box enough to play this game wouldn't go for that argument

        You lack sufficient imagination then.
        ```

    - u/callmesalticidae:
      ```
      This is probably why Yudkowsky stopped doing it.
      ```

      - u/Charlie___:
        ```
        Of course! If it works, it's plausible, and if it doesn't work, well, that's why he stopped! :P
        ```

- u/odoacre:
  ```
  Offer them more money than the bet amount?
  ```

- u/CouteauBleu:
  ```
  "Why don't you just put the whole world in a bottle, Superman?"
  ```

- u/GreenGriffin8:
  ```
  You're all forgetting something.

  He probably said 'please.'
  ```

- u/RedSheepCole:
  ```
  As I understand it, the essence of this challenge was that one person I don't know IRL ostensibly convinced two other people I don't know IRL to do something they said they wouldn't.  But he never released the transcripts, which would be easy enough to fudge in any case, and *nullius in verba* is as fine a rule today as it was for the Royal Society hundreds of years ago.  Pics or it didn't happen.

  Also, the challenge was not and could not be an accurate simulation of the thing it represented, as the players knew full well that EY wasn't an AI, they had cause to respect him and no cause to fear him, stood to lose only a little money ... etc.  It doesn't mean much in the grand scheme of things.
  ```

- u/OnlyEvonix:
  ```
  "Ima pay you 30 bucks to let me out"
  ```

---

