## [BST][HSF] How many partial and failed Singularities can you think of?

### Post:

I'm looking forward to the next story I write, which is going to involve something vaguely *like* a Singularity, but which isn't going to match any precisely-defined description of one. How many different not-quite-Singularities can /r/rational come up with?

For example:

* Moloch wins: AIs follow short-term incentives with long-term consequences, and evolve themselves into extinction.
* Several entries from the [Friendly AI Critical Failure Table](http://www.sl4.org/archive/0310/7163.html).
* Staged singularities: Going from human-level intelligence to post-human level 1 is a *lot* easier than going from post-human level 1 to post-human level 2.

(I'm particularly interested in pseudo-Singularities that get rid of technological civilization on Earth, but for idea-generation purposes, am open to all suggestions.)

### Comments:

- u/TennisMaster2:
  ```
  Creation of brain implant that gifts to all humans photographic memory.  Fifty years or so of accelerated research, since more readily accessible information means more potential connections and insights.  Assume effective immortality in the form of agelessness is invented.  Knowledge generation slows down as scientists become able to leverage their gained years into financial success, and thus can self-fund their own research, leading to highly insightful and innovative science being conducted in many parts of the word in secret; as a result, the level of knowledge found in public domain publishing of new experiments, while now decentralized, becomes the metaphorical undergrad to the scientist-recluses' post-post-grad.
  ```

  - u/UltraRedSpectrum:
    ```
    Real 'photographic' memory isn't all it's cracked up to be, and even the kind of perfect memory you describe wouldn't do much. I have something moderately close to that (I know people tend to exaggerate on the internet but I actually do have a classic 'photographic' memory with measurable effects), but it doesn't help with academics past precalculus. It's also considerably less powerful than Google Search, so you shouldn't expect it to revolutionize anything in a world that already has Google Search.
    ```

    - u/TennisMaster2:
      ```
      I know - I specifically meant the colloquially referenced photographic memory, not eidetic memory.

      I should have specified before, but I also mean something a bit more powerful, beyond the ability to read a page and then recall the image of the page later, to read once more sentences forgotten (I almost had the ability when very young, so I know what you mean).  I should have rather said the ability to *glance* at a page and read it later; such a person could glance at the pages of physics textbooks from introductory to graduate level, then go lie down, and mentally read the pages at an advanced rate, focus allowing.  Since it'd be an implant of artificial neurons, once a subject were understood, that understanding wouldn't go away, and any memories of confusion could be forgotten.

      The primary advantage is in having expert-level understanding of many different fields, with which the brain could draw many insightful connections; the present day equivalent is two experts from different fields working together on a project, and chance sparking insight into the mind of one of the experts.  Google Search does not assist in the generation of insight in human brains: without connectomes already containing information, insight of one connectome's relation to three other connectomes can't be drawn in the first place.

      Granted, inability to do complicated calculations in one's head limits the usefulness the implant might have to some fields' experts.  Assume another implant solves that problem, twenty years post-singularity.
      ```

      - u/UltraRedSpectrum:
        ```
        That's sort of my point, though. I can give you specific details about books I read decades ago, and even recite passages verbatim. I accidentally got the wrong Intro to Sociology textbook in my first year at university, and I still got top marks even though half my grade was based on information from a book I didn't have - because I'd run into most of those experiments before and still remembered them. I can draw an accurate geographic map of the world from memory, though any detail I've never explicitly noticed will be absent and Sri Lanka will probably be in the wrong place.

        And, despite all of that, I *still suck at calculus*. Those things I mentioned are parlor tricks, just like everything else an 'eidetic' memory can do. All that rote memorization is useless without a deep understanding of the subject material, and believe me when I say that it's no easier if you have the book in your head than it is if you have it in front of you. And again on the Google Search point, because you can do all of those things too; all you need is a computer so small it fits in your pocket.
        ```

        - u/TennisMaster2:
          ```
          Does your memory allow you to associate many superficially disparate facts with each other with little effort?  Do you notice unique patterns because you have more knowledge to inform split-second analysis?  The answer to those questions informs the viability of the scenario.

          I think the second underlying assumption is deep understanding can be more easily achieved when reading mental images than when using eyes.  If that's not true, then the scenario has little credibilty.

          I say little, because for a smart person who's excellent at drawing connections but has a horrible memory, the implant would greatly assist the degree of intellectual insight they'd be capable of producing.
          ```

          - u/UltraRedSpectrum:
            ```
            No to the first, yes to the second. Disparate facts don't associate themselves unless they're directly addressed, so while I can pick up on obscure references and solve riddles pretty quickly, I almost never make long leaps of logic, even if I have all the pieces to a given puzzle. I'm actually pretty slow on the uptake, in large part because I have no idea what I'm expected to know and way too many potential matches to go through. It's like trying to find matching socks in a drawer with ten thousand pairs. It took me weeks just to figure out public transportation, and even then I still stop to ask directions pretty much every time I ride the bus.

            I can't say whether it'd be easier to learn from a memorized text, because I can't memorize or visualize images with high enough fidelity to read them. The closest I can come is reconstructing old memories based on the shapes of letters and lengths of words.

            For the last point, I have no idea. Typical Mind Fallacy would suggest that I'm way underestimating how much I rely on my memory, but at the same time I think the information itself is less important than the ability to make the inferences. Everyone who came before Archimedes had all the tools they needed to work out the volume measurement, but it took more than that to make the connection.
            ```

            - u/TennisMaster2:
              ```
              I agree that you're underestimating, based upon your comment history ("cloning but metabolism" meaning mitosis is a nice insight) and that you said yes to the second.  The second is more important for generating insight, as it can be applied to one's knowledge manually via introspection.  

              I sympathize with you on being slow on the uptake.  At times I have to model others' understanding of a given situational context in order to grasp the salient topics, though I'm sure you have it ten times as bad.  That said, having had and lost acumen in memory, I'd still say you're lucky.

              Why the difficulty with public tranportation?  Enough fidelity to access the potential memory you seek to recall, but not enough fidelity to be certain whether the recalled memory is accurate or conflated?
              ```

              - u/UltraRedSpectrum:
                ```
                I have no idea why, actually - or rather, I don't know how other people have such an easy time with it. The only way I could imagine getting the level of understanding everyone and their mother seems to have is by sitting down for half an hour and brute force memorizing all the bus schedules. Which is ridiculous, because obviously everyone else who rides the bus never did that.

                That might be a personal quirk, actually. It could very well be that my memory isn't the cause, but a mitigating factor.

                EDIT: Relating to your first point, here's how my thought process worked: cloning but metabolism, sounds like cloning but you have to eat a lot, sounds like you eat then clone then eat then clone, sounds like mitosis, but for people. Unless my attention is drawn to a specific subject, I can and usually do fail to draw the link. Someone smarter than me might have an easier time with it, but by and large it seems like that logical leap is the limiting reagent here.
                ```

                - u/TennisMaster2:
                  ```
                  Fascinating discussion and conversation!  Thanks for sharing and having it with me.
                  ```

                  - u/UltraRedSpectrum:
                    ```
                    Thanks to you too! I've never thought along these lines before, and I think I should give what you described a try, just in case.
                    ```

- u/EliezerYudkowsky:
  ```
  > AIs follow short-term incentives with long-term consequences, and evolve themselves into extinction.

  Requires stupid AIs.  If you can see it, why can't they?
  ```

  - u/DataPacRat:
    ```
    The model I based this idea on was that each initial AI "owned" a certain portion of the CPU cycles per unit time, but could trade them away. A market could result in which each AI faces an incentive to streamline its mental processes, to reduce the CPU overhead spent on keeping itself alive, in order to have a larger "bank account" of spare cycles to perform financial transactions of various levels of risk with. Even if the original AIs were of human-level intelligence, or even outright ems, then depending on the details, it seems possible for some versions of this model to result in ever-stupider AIs.

    (I'm afraid that I don't have the economic chops to prove anything about this model, I'm only proposing it at the level of detail required for fiction. I'm looking forward to whatever Robin Hanson might say on the topic in his forthcoming book.)
    ```

  - u/ishaan123:
    ```
    If Hofstadter's super-rationality doesn't for some reason actually play out in practice, it won't necessarily matter if they can see it.
    ```

    - u/EliezerYudkowsky:
      ```
      Or they could get together and build an enforcement mechanism like, you know, even humans try to do.
      ```

      - u/ishaan123:
        ```
        Of course, but in theory there do exist cases where it's not possible to make enforcement mechanisms, so i don't think the potential danger of unsolvable coordination problems should be dismissed in practice (if nothing else, to err towards caution).
        ```

  - u/Chronophilia:
    ```
    Perhaps they live at such high speeds that human timescales seem absurdly long-term to them. Just like humans rarely worry about environmental damage that will take centuries to have an effect, the AIs might do things without fully anticipating the consequences in a week's time.

    Or, to put it more formally, their utility function has particularly high discounting - one utilon immediately is worth two utilons in a day.
    ```

  - u/JackStargazer:
    ```
    Accellerando seems to do this with the Vile Offspring.
    ```

  - u/None:
    ```
    [removed]
    ```

- u/Anakiri:
  ```
  What exactly do you mean by "Singularity"? Your concept of it seems to include a lot of stuff that is not included in my concept of it.

  When I say "Singularity", I am referring to the moment that a system that is better at designing systems than its designer was starts iteratively designing better systems. I don't know what it would mean to partially do that, or to fail to do that in some nontrivial way. I can *conjecture* lots of things that may follow from that event, but the event itself is reasonably unambiguous, I think.

  Is one of us using the word wrong?
  ```

  - u/DataPacRat:
    ```
    There are at least three main definitions for "Singularity", each partially exclusive of the others, plus innumerable minor variants. It's gotten to the point where no two people are describing the same thing with it anymore.

    Thus, I'm looking for ideas that at least vaguely resemble some version of any definition Singularity, if you squint and turn your head, but which don't necessarily lead to the creation of superintelligent AI Gods who pursue their utility functions with godlike calculations. Near-singularities, pseudo-singularities, things that a random skiffy reader wouldn't blink at if they were *called* a Singularity regardless of what "Singularity" actually means.

    This is a brainstorming thread, which I'm hoping to leverage to evoke as many potentially useful ideas as possible, particularly ideas that I wouldn't have come up with on my own. If you think you've got some ideas that at least partially match the general pattern I'm describing, feel free to post them, and everyone here can use them as springboards for further elaborations thereof. :)
    ```

    - u/chthonicSceptre:
      ```
      I then propose that you run with a singularity that happened in the past - could be the 1600s (Isaac Newton was a fan of this 'chemistry' thing), or the 1920s (significant telegraph upgrades proposed by Nikola Tesla) or the 1970s (novel drug which vastly accelerates the clock speed of mammalian brains) or the 1990s (limited AI programmed to autonomously simulate pandemics eventually attempts to improve accuracy by releasing a sample of smallpox). Whatever, man. Werner von Braun builds a Nazi moonbase? Leonardo da Vinci designs working calculators, or submarines, or airplains? Henry Ford, in his twilight years, popularizes the uranium engine?

      This isn't wholly related, just food for thought.
      ```

- u/DataPacRat:
  ```
  Proposal by MoltenSlag from a Skype conversation: Unanticipated social collapse, in spite of large resources and no external threats:

  * https://en.wikipedia.org/wiki/Behavioral_sink
  * That article really doesn't go into enough detail. The experiments this guy did were ****ing mental. Or, at least, the results were.
  * https://www.youtube.com/watch?v=0Z760XNy4VM Here, this video should go over it pretty well. It's 8 minutes, but it's worth it. If you don't mind not sleeping.
  ```

  - u/Covane:
    ```
    did you read this article on the behavioral sink: http://www.cabinetmagazine.org/issues/42/wiles.php

    it's much better than the wikipedia page
    ```

- u/gattsuru:
  ```
  * Al-Hazard : the civilization was on the way to breaching Singularity-after-Singularity, ever-accelerating... until something went Wrong.  Maybe it two Singletons-to-be planned too far around each other, maybe some completely unexpected event pulled the rug out from the main actors, maybe someone got just a little too much power, or maybe someone decided to put far too much [computational power into a toaster](https://en.wikipedia.org/wiki/Internet_refrigerator).  Maybe the Singularity itself was a dead end, and someone built an AI that paved over most of the planet to print out a proof Riemann hypothesis.  But whatever there was is broken, and it's left its bones nearly everywhere.

  * Not Dreamed of in Your Philosophy : There was a gap in the machine.  Maybe the AI's maximum word size was too small, maybe too many things were excluded from self-modification, or maybe the folk that bootstrapped themselves up were a little odd.  But while the overwhelming majority of the solar system has turned into computronium, or vacuum tubes, or paperclips, the Planet Earth has had a slightly odder time.  Some portion of the people were Outside Context Problems to the Singularity itself, and what we see today are the remainder who survived from its escape from the crazies.

  * The Closed Box : The folk who "have bad understandings of infinity" won, and the Singularity put 98% of the populace in a shoebox.  In a couple billion years it might move to another planet, but there's no *hurry*.

  * (On and Off) The Nature Preserve : The Singularity left people behind *intentionally*, either because the dominant groups excluded them, or because they were given the option to not sign up.  There might even be communication or travel between those who went along and those who stayed behind, but the difference in timescales and desires and interests is so vast that it's pretty uncommon.  The sky is filled with God's Dust, but Earth and the earth was inherited by the meek and the Amish.

  * Scientific Progress Goes Thwack:  There isn't a Singularity, not in the Kurzweil sense.  The technology scales upwards, endlessly, and culture changes dramatically... but people stay the same.  A time-displaced protagonist might not understand the language, or know the norms for how to look both ways before crossing the cyber-street, but nobody's turned into Tang and there's no equivalent to Languagev2.

  * Glorious Future Nonawakening.  The aesthetics of the future are *weird*, and weird in ways that people in the setting can't really handle or adapt to.  Everyone's been uploaded into a universe where martial arts really do predict success at [wildly](https://en.wikipedia.org/wiki/Shaolin_Soccer) varying [goals](https://en.wikipedia.org/wiki/The_God_of_Cookery), or even an [internally meaningful enlightenment](https://en.wikipedia.org/wiki/Kung_Fu_Hustle), and people treat this [the same as they do power in a world of scarcity](http://forum.rpg.net/showthread.php?480387-setting-brainstorm-Kung-Fu-Transhumanism).
  ```

  - u/DataPacRat:
    ```
    > Al-Hazard

    This is, at least generally, the approach I think I want to take. Working out what the remaining bones to pick over might resemble is the tricky part.

    > The Closed Box

    At least to an extent, this is a detail that could be thrown in with many other pseudo-Singularities. At least, a post-human AI may have done something of the sort, and then Complicated Stuff happened derailing said billion-year plan. 

    ... And now I'm imagining a near-human protagonist being handed a shoebox-sized artifact and being told, "Oh, and by the way, here's humanity. All hundred-billion or so instances. Take good care of it."
    ```

- u/mycroftxxx42:
  ```
  There are some potential issues that could become important when Max Plank's "Science advances one funeral at a time," is fully considered in this context.   If you either reduce the rate of funerals, or increase the rate of revolutions, I think you would run into the same problem.  Economic pressures would be brought to bear against the existing culture of Science-with-a-capital-S, and presumably the culture would fall apart first.  

  There's little (but not zero) reason to suppose that the resulting cultural collapse would leave the parts of science that we like, mass publication and sharing of technique, popular and common after the idea of a "Scientific legacy" was disrupted.  A less-collegiate atmosphere itself could retard advancement.
  ```

- u/TimTravel:
  ```
  http://www.prequeladventure.com/2011/03/236/

  Later the orc in question is shown with dozens of four-leaf clovers pinned to his tunic.
  ```

---

