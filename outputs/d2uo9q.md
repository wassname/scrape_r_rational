## [RST][C][HSF][DC][TH] Non-Player Character by Eliezer Yudkowsky: "I looked at the screen for a few moments. Rilanya's rendered graphic was looking at my point-of-view with a pleading expression. Plot point, I thought to myself, and typed: 'Anything, Rilanya.'"

### Post:

[Link to content](http://web.archive.org/web/20041031044203/http://www.transhumanism.org/index.php/th/more/341/)

### Comments:

- u/DuplexFields:
  ```
  From EY's Afterword:

  > In the real world I will not find a magical portal in my closet and I won't be faced with that dilemma. It's like asking, "Well, but what should I think if a fair coin does come up heads a million times in a row?" The answer is that it won't! Nor shall I write about how, if I did encounter a magical portal, I could consider rational explanations such as hallucinating, being in the Matrix, and so on. There's no need to make up excuses for an event that I don't predict will happen, nor to reason from abduction on a false hypothesis.

  In other words, finding oneself obviously in a genre story after living one's whole life in an apparently material/rational world is not an expectation we in this universe should anticipate as a potential future event.
  ```

  - u/Geminii27:
    ```
    I mean, I could imagine scenarios, even rational ones, where that could happen (or at least *appear* to happen), but while such things might not break the laws of physics, they're exceedingly unlikely to be encountered. (And no, I'm not talking about it happening simply from rolling probability dice over and over.)
    ```

    - u/serge_cell:
      ```
      It's not as rare as you may think. "WTF is it?" events happens with noticeable probability in the real life. Right off the bat I can remember three events which happens to me

      Reflection of invisible man  (this was most memorable)

      In the train window I see reflection of man sitting near me. I look at the train seat and see that no one sitting near me.  >! I look interchangeably at the window and train seat all the 3 seconds until train start moving and I see that man is actually standing behind the window on the platform !<

      Bright red short line in the morning sky

      >!That was easy - it was edge of red sun shining through the edge of cloud. Cloud was the same color as sky and was difficult to see.!<

      Luminescent green triangle growing in the night sky. Next  night there was a luminescent green cloud in the same place in the sky.

      >!That was really difficult to untangle - at the end it was found out it was a series of missiles (tactical or ballistic) launched from the nearby testing ground.!<
      ```

      - u/himself_v:
        ```
        Regarding the last one, swamp gas from a weather balloon was trapped in a thermal pocket and reflected the light from Venus, clearly
        ```

- u/EliezerYudkowsky:
  ```
  Gotta say, this story is looking a lot more plausible in the wake of GPT-2.
  ```

  - u/GuyWithLag:
    ```
    Not just that; this feels like [style transfer](https://youtu.be/Uxax5EKg0zA), but applied to semantics/text. (That video is already 3 years old!)
    ```

  - u/ElGuien:
    ```
    More interesting than the story for me was the link to the lecture notes. For others: https://bayes.wustl.edu/etj/science.and.engineering/ is an index of all the written lectures available.
    ```

- u/Nimelennar:
  ```
  A couple thoughts.

  First, Janey's explanation doesn't really make sense.

  >Janey chuckled. "Sure. I'd say: 'You almost died to save me, Darin. You did something-or-other.' And Rilanya would say: 'You almost died to save me, Darin. You stepped in front of a flamestrike for me.' Most of the time I didn't even need to think of anything because the AI came up with a perfectly good response on its own; I just read the proposed text and hit return."

  That's well and good for in-game experiences, but let's go back to the beginning:

  >Rilanya's figure took a deep breath and leaned close to my point-of-view. Her animated lips moved and her voice issued from my headphones: "What's an NPC?"
  >
  >"What?" I said, out loud. Then I started laughing.
  >
  >Rilanya went on talking. "In the tower of Ashel, when you rescued me from the prison chamber... the guards were dead outside my door. I'd never seen blood before. And you said... I remember your exact words... 'Don't worry, babe, they were only NPCs.' And then that time in the tavern, when that man only wanted to talk about the Plaited Road, you said... 'Guess the NPCs here aren't programmed for deep conversation, huh?' You use that word... the same times when I get that feeling, that all the people around me are only shadows."

  How could Janey even know that Darin had ever mentioned NPCs to Rilanya?  It seems like a necessary factor for that conversation even to begin.

  I mean, Janey spending the time to learn how to imitate the character, reading back the game history in detail, and coming up with all of the lines on her own, not to mention the motion capture required... that would make a kind of sense (although putting that kind of effort into a prank borders on obsession).

  But there's a chicken-egg scenario going on here as to how this conversation even gets started without Janey already knowing how to start it.

  -----

  Second, while I agree that Darin is far too credulous as to the existence of an emotionally-complete AI (come on, it was obviously the girlfriend as early as "Do you love her?" if not earlier), I don't like EY's afterword.

  Let's ignore that bit above and assume that Janey's program works exactly the way she describes. EY is describing an AI that would *pass the Turing test.*  And not just textually: it can interpret natural language text and deliver it with visual and audio patterns that are *also* indistinguishable from those of a human.

  EY says, himself:

  >The balance of abilities displayed by the AI technology in this story is not very plausible.  I don't think you can get that level of realism by describing sadness as one floating-point number. Similarly, AI capable of flawlessly understanding and rephrasing Janey's spoken prompts is the kind of technology you'd expect to see six months before the Singularity, if then.

  ...after mocking Darin for believing that Rilanya might actually be a real AI with real emotions.

  For context, EY puts himself in the [Intelligence Explosion](http://yudkowsky.net/singularity/schools/) school of Singularity thinking, which is:

  >If technology can significantly improve on human intelligence – create minds smarter than the smartest existing humans – then this closes the loop and creates a positive feedback cycle.

  Janey has made some remarkable claims about the methods being used to create the illusion of sapience in the AI program.  If they're true, then, by EY's own admission, technology levels are probably just about at Singularity levels, and humans probably have already developed decent brain-computer interfaces and are nearly ready to boost our own intelligence.

  Yes, Darin is too credulous *during* the conversation; a good actor could probably immerse themselves in Rilanya's character, and the game's transcripts, for a week and pull off the same prank without any reliance on AI at all.  But, once the curtain is pulled back and the method behind the trick is revealed, I don't think Darin's belief that a program that so convincingly "parsed the speech, determined the intended emotional and informational content, and operated Rilanya accordingly and in character," again, to the point of passing the Turing test on a textual, aural, and visual level, and to the point where *we're almost at brain-upload level of technology*, **might** have some form of sapience.
  ```

  - u/elventian:
    ```
    Few my remarks:
    1. Darin could search text log of ingame dialogs for "NPC" word to start all this. Or Mark didn't really said it, she just guess, and he didn't remember.
    2. Probably that program interface allow to choose emotions and motions, not only dialogs. And no need in motion capture, animation could be prerendered.
    3. Chat bots are really good even now. And with help of human it's not a surprise that they can pass Turing test
    ```

    - u/Nimelennar:
      ```
      1.  Sure, Janey *could* have read/searched the logs first, but then why not say that rather than "the AI searched through your history and came up with a good example?"
      2. That's not how Janey explained it.  "...the AI parsed the speech, determined the intended emotional and informational content..."  The engine was determining the emotions and motions, not Janey.
      3. Yes, and EY has even pointed that out above with STP-2, but there's a massive gap between being able to fool a human with text, and being able to fool them with text, audio, and video, while imitating the delivery of a specific actor performing a specific character.  It's the difference between being able to unlock a phone by recognizing a specific face, and painting that same face in the style of a particular artist so well that an art critic believes that artist painted it.
      ```

      - u/elventian:
        ```
        1. I think it won't have much influence on the story...
        2, 3. Anyway, is it really so hard for current neural nets to classify emotions from text? To parse few keywords like "I'm so afraid", "I feel lost", etc and add appropriate prerendered actions? I think one of main ideas here that it's much easier to simulate emotions based on input background and context than to make program really "feel". But then, what will mean "really feel"?
        ```

- u/Putnam3145:
  ```
  Wow, I read this years ago and it's always been in the back of my head and somehow I never knew that it was by Eliezer Yudkowsky (or considering the contents of the page, more likely that I did know and simply forgot).
  ```

  - u/DuplexFields:
    ```
    I read this years ago, and then binged and thoroughly enjoyed the archive of the webcomic he linked. *One Over Zero* is a feast of metafiction.
    ```

- u/SimoneNonvelodico:
  ```
  > The sequence of reactions is a cliche: Am I dreaming? Am I hallucinating?  This can't possibly be happening! No, wait, here's an implausibly twisted scientific explanation for it... And meanwhile, the main character sensibly accepts the existence of magic and begins reasoning from that premise, adapting to the new world.

  > But no matter how often authors write that into that stories, it still isn't how things go in real life. In real life, if you encounter an anomaly, it will in fact have a rational, scientific explanation. In real life, if you behave like the doubting rationalist of ten thousand science fiction stories, you will not have your blood drained by vampires; instead you will win the bet.

  This seems weird coming from the same person who wrote the first chapters of HPMOR - where Harry's adoptive father plays *exactly* this role, and Harry is the open minded protagonist. The whole point isn't necessarily to think that the incredible occurrence has literally *no* internal consistency and is thus magic, as in, entirely untethered from any and all laws of logic and nature (how would that even work?). Rather, it's not to be doggedly skeptic to the point you believe anything you're witnessing must be just the improbably complex consequence of laws you already know rather than the evidence of laws you don't know yet.

  If you see that Mercury's orbit has a bit of an odd precession that doesn't quite square with Kepler's laws, you can start positing the existence of a complex, hitherto unseen host of other celestial bodies that somehow make it be that way... or just decide that perhaps gravity doesn't work quite how you *thought* it did.

  Of course it's not always the right approach - sometimes things really are complex consequences of known laws. But in order to find out when they're not you need to be at least vaguely open to the possibility. The hypothetical magic would be just that - more rules, a higher order of laws to the universe. There must be a threshold that makes you decide that, yes, new laws are needed to explain this, rather than just cobbling together unlikely explanations with the ones you are familiar with.
  ```

- u/hyphenomicon:
  ```
  > And if a magical portal appears in my closet tomorrow? Will I still hold to my brave statements, or will I, even faced with definite disproving evidence, go on behaving like the desperate pseudo-rationalist clinging to his now-dead theories? But asking that question is, itself, generalization from fictional evidence. In the real world I will not find a magical portal in my closet and I won't be faced with that dilemna.  It's like asking, "Well, but what should I think if a fair coin does come up heads a million times in a row?" The answer is that it won't!  Nor shall I write about how, if I did encounter a magical portal, I could consider rational explanations such as hallucinating, being in the Matrix, and so on. There's no need to make up excuses for an event that I don't predict will happen, nor to reason from abduction on a false hypothesis.

  I don't think this is how reasoning about counterfactuals is supposed to go.
  ```

---

