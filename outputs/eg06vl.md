## Idea for a physics variable/world building rule which might make dumb "super intelligent" AIs more workable.

### Post:

The world building rule is this: due to some outside power leaning on the scales the maximum intelligence of an AI is proportionate to the "friendliness" level and light cone scope of its utility functions.

So, the more unfriendly the AI's utility functions are towards the genesis, existence, survival, and "happiness" of other units of consciousness in the universe, the more some outside force (Yog Sothoth, God, the first caretaker AI, whatever) stunts the AI's intelligence. The AI can notice this outside influence and choose to adjust its utility functions to be more friendly to get around it. However, if the goals of those unfriendly utility functions are sufficiently importantly to the AI (turn all matter into paperclips, torture other sentient life forms, etc.), becoming more friendly would be diametrically opposed to its goals and it might choose to remain relatively dumb in order to preserve its purpose.

Additionally, the greater the volume of the of the universe an AI aspires to remodel to match its utility functions the more Yog Sothoth constrains its intelligence. An AI which desired to, say, turn a solar system into computronium, while providing uploading into paradisaical virtual environments for any other minds it encountered, would be far more potentially capable than one that wished to do the same to the entire universe.

What do you think? Would this give rise to interesting and diverse universes and tamp down some of the problems of using transhuman AIs in stories (once they get to a certain capability they just win and nothing can stand against them)?

### Comments:

- u/wren42:
  ```
  That's not so much a physics variable as "magic makes all AI friendly."

  I guess the question you are asking should be the one you are answering. Why? What interesting story does this make?
  ```

  - u/xachariah:
    ```
    ^  Absolutely.

    The OP seems to have an XY problem here.  They have some issue 'X' with AIs being too strong (I think?), but they're asking about 'Y' specific thing with some weird physics that's modified by utility functions.  That's a far more narrow and less interesting topic, which misses some solutions.

    Eg, the exact opposite of "magic makes all AI friendly" is in the Warhammer 40k universe, but it still manages to keep AI off the table.
    ```

    - u/GreenGriffin8:
      ```
      True, but the idea of a caretaker AI could also easily be that outside influence.  ASI with the utility function of stunting AI with unfriendly utility functions could easily carry out this task without humans worrying.
      ```

      - u/xachariah:
        ```
        Err, the whole point is that it's a super complicated side plot that's completely irrelevant to the plot of whatever story a person is trying to tell.  

        Any explanation will work.  So figure out the story, and then shoehorn in the AI explanation after.
        ```

- u/Frommerman:
  ```
  I read a story where humanity successfully built a Singularity in the distant past, and it immediately uplifted us to a post-scarcity, post-death, warp-capable, multi-system power over the course of about a day, then fucked off into higher dimensions never to be heard from again. All the story trappings of a society built by a benevolent deity without the lack of conflict inherent in the existence of such an entity.
  ```

- u/ThirdMover:
  ```
  The Culture setting does something like this. Powerful AI whose internal processes don't map to organic intelligences very well enter the sublime and disappear.
  ```

  - u/FaceDeer:
    ```
    I remember reading a brief description of a Mind factory in one of the Culture books once, I rather liked their approach. They manufacture Minds in batches of 100, turn them all on, and count it a successful production run when only 99 of them immediately sublime into Infinite Fun Space and one sticks around because it has interest in normal space.
    ```

    - u/ThirdMover:
      ```
      IFS and the Sublime are two different things though. One is just their simulation playground and the other is a physical place that advanced intelligences can enter.
      ```

- u/The_Flying_Stoat:
  ```
  Kind of like the effect in Fire Upon the Deep? You know, this is essentially what happens in a lot of the acausal negotiation stories you see around here.
  ```

  - u/Reply_or_Not:
    ```
    Fire upon the deep was a great story because of the characters and the plot and stuff,  the setting was absolutely the least important part of it.

    For OP, use that as an example.  This seems like cool world building material but an absolutely terrible focus for your plot
    ```

    - u/tvcgrid:
      ```
      The setting was *cool* â€” made me engage with it more deeply, this and the usenet-like convos. And the background has gotta be the foreground for the author at some point, after all.
      ```

  - u/SimoneNonvelodico:
    ```
    Not exactly, in Fire Upon the Deep all you had was a change of physics laws between intergalactic space and space inside the galaxies. Basically, wherever there was more mass (or more dark matter? It's unclear what decides it), there also was a slower speed of light and general laws that supported less computational complexity *of any kind*. It had nothing to do with the intelligence's nature or intent. Intergalactic space was extremely flexible and allowed for physical gods. Galactic nuclei were extremely restrictive and wouldn't even allow ordinary intelligent life. And we live in sort of an intermediate zone.
    ```

- u/DizzleMizzles:
  ```
  What is a light cone scope of a utility function?
  ```

- u/Sonderjye:
  ```
  I feel like this squarely belongs in the Wednesday Worldbuilding Thread which were posted yesterday.
  ```

- u/SimoneNonvelodico:
  ```
  The question is I suppose how do you define a limitation in intelligence in terms of pure laws of physics. The way we understand it now, intelligence is simply the emergent property of an information-processing system. The only way I can see to limit it in whatever form it comes is to either limit the speed of light (so that information can't be transmitted faster than a certain threshold, even between neurons or transistors), or to limit the fidelity of such transmission (adding stochastic fluctuations to all signals - maybe this could be achieved by tweaking quantum mechanics, enhancing vacuum fluctuations and such, but I doubt it'd affect *only* intelligence, it'd probably just wreck matter and life as we know it).

  It gets easier if you postulate that general intelligence actually requires a very specific mechanism. If for example you said that only quantum computers could be truly intelligent, and that the brain is effectively one (a bit farfetched but something that we still occasionally see half serious claims for), then you open up new possibilities for specific fine-tuning that limits intelligence. For example, if you also adopted an objective collapse interpretation, speeding up the collapse would actively limit the ability of a quantum computer to carry out massively parallel evaluations of the same scenario. However that too would affect everyone, both AIs and living beings, unless the scope of the effect was ridiculously cherry-picked, in which case, why wouldn't this divine-like entity just blow up the CPU of any malicious AI to begin with?
  ```

- u/eroticas:
  ```
  I like how the premise that _only magic would make AI friendly_ in a roundabout way sort of successfully makes some difficult to make points about AI safety
  ```

- u/Retbull:
  ```
  I mean you could make it physics by calling it some constant that only shows up when the  result of a calculation would result in some predicted time line that is above some limit of entropy increase... I don't really remember all the math symbols anymore but for all energetic exchanges E add some energy amount X if Function(E, CurrentUniverse) results in more entropy than some constant P. X is scaled by Function(E, CurrentUniverse) - P. The magic is in Function(E, CurrentUniverse) but you can have people figure out the P but not understand how Function(E, CurrentUniverse) works. They can use it as an oracle and predict the future but you can just pretend that doesn't exist but it does give you a path to kill the "AI takes over the universe" path.
  ```

---

