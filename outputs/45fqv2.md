## [BST][D] "So You've Become an Immortal God-Emperor..." (Preserving Values Systems in an Immortal Framework)

### Post:

Due to some grand fantasy storyline, or perhaps just freak incident on a Tuesday morning, you wake up one day as the immortal God-Emperor of a nation or world. You may assume that this position comes with great (though not omnipotent) magical powers, and near-absolute authority over your domain.

So here's the thing: An immortal lifespan is a *long* time. And let's assume that you're not allowed to step down from your position, whether due to the fervor of the crowds, or a general predilection toward benevolent dictatorships Getting Things Done. In short, you're stuck with the crown, your immortality, your power, and your lifespan.

Given these four things, how do you ensure the following:

(1) You preserve the overall framework of your values structure for as long as you reign (i.e., no "going evil", in any of the senses of the word)

(2) You allow yourself to change and grow given new information and experiences.

In short, how do you set up or maintain an "emergency brake" on a list of "no-go" issues or actions (keeping yourself from becoming the *Mistborn* Lord Ruler), while still allowing yourself to grow as a person and adapt to new information?

### Comments:

- u/None:
  ```
  I'm not even sure 1 is desireable given that the "overall structure" of my values framework would probably seem hopelessly naieve to a 700 year old instance of me. I mean, I feel that way about the overall structure of the values framework I used three years ago right *now*. Regardless of if soemone else's structure was already actually pretty good, they could *still* eventually realise that they were stuck in a local optimum, and that replacing what they thought was good wholesale with a better overall conception which is internally irreconcilable with the previous system was a good idea.

  That said, use your magic to put a sentient bomb curse with full access to your thoughts and sensory input into your brain. Have this bomb contain a copy of your values, with a "kill me if I violate them to degree X". Set up this curse such that it harvests 100% of your natural mana regen when you are not actively using your regen, such as when you are topped off, and eventually the curse will be too powerful for anything short of omnipotence to remove.

  Include a secondary killswitch such that the bomb will kill you if you seriously consider removing it. Your reign ends either when your values system is violated to whatever degree you chose, or when you consider enabling your values to so change. Higher-order consideration is, I think, irrelevant.

  Edit: If you don't want the bomb curse to be sentient, spend a century or so creating an order of religious fanatics who are combinations butler/bodyguard/aids to you as part of their religious duties. Have them follow a creed that requires that they perform these functions, and that requires you to adhere to your values system, and make it so they can set the bomb off, and that the bomb goes off if the order dies out for any reason.

  Ah, also that it goes off if your magical signature is ever connected to a spell influencing their persons.

  More failure modes, *way* more ethically questionable, but if we're operating on the premise that we basically *can't* trust ourselvesover the long term, then any system which doesn't rely on external agency to implement it's functions is a nonstarter. Since we can't, in this scenario, make something magical with a utility function that makes it 100% trustworthy, we have to involve other people somewhere, and fundamentalist religious faith is one of the strongest guarantors of action that can be implemented on human wetware.
  ```

- u/TennisMaster2:
  ```
  If I were put into this situation, and this concern crossed my mind, I'd write down my values as they stand, and thoroughly document the reasoning behind my every decision on macro and micro scales.  I'd serve as the realm's adjudicator during the early years; if teleportation were beyond my powers, I'd perhaps use holograms to appear in far away places to avoid wasting time traveling between domains.  Since written documentation would take too long, I'd use magic that can even reveal true objections without having to spend the time introspecting to record my reasoning as I make my decisions, and review these records later.

  Further possibilities would probably depend on the level of power I'm bestowed.
  ```

  - u/None:
    ```
    You are looking at them
    ```

    - u/TennisMaster2:
      ```
      Say you wish to learn the guitar, yet you don't.  Charitably assume you actually do want to learn the guitar.  The reasons you give revolve around time constraints, but those can be worked around.  Hearing this, you say you wouldn't be very good, then, for a very long time; you want to learn how to play, and play well, quickly.  You can solve that by taking a learning vacation.  You make other excuses.  

      What is your true objection to learning guitar?  Obviously this is a hypothetical, so we can't know.  I'm guessing you're just lazy, i.e. you'd rather spend free time doing other things than learning guitar, i.e. you don't actually want to learn how to play guitar all that much.

      Another example:

      [EXT.] IN FRONT OF TWO RESTAURANTS

      * "What food shall we eat, Indian or pizza? I'd vastly prefer Indian."
      * "Hmm, I don't know, why don't we just go with pizza."
      * "You don't like Indian?  I'd very much like to not eat pizza."
      * "No, I like Indian.  I just want pizza right now."
      * "It would be very distressing to me if we ate pizza.  Why can't we just get Indian?"
      * "I'm not feeling it right now."
      * "Is that the real reason?  You initially seemed ambivalent, and I have a strong preference for Indian; we should go with the choice that's most strongly preferred, namely, Indian."
      * "You know what, I'll just get a pizza and bring it into the Indian place.  We'll both be happy, that way."
      * "Sure, but if you have some objection to eating Indian I'm fine with bowing to it and getting pizza together."
      * "Nah, it's fine.  Go get us a table for outside at the Indian place, I'll go get my pizza."

      The true objection for not eating Indian was something embarassing, like indigestion, getting a runny nose when eating spicy food, not liking the look of it even if they like the taste, a craving for pizza to which they'd rather not admit, etc.

      Sloppy explanation.  Here, the LW [post](http://lesswrong.com/lw/wj/is_that_your_true_rejection/) is clearer.
      ```

      - u/None:
        ```
        He chooses a book for reading
        ```

        - u/TennisMaster2:
          ```
          It's not necessary, just a shortcut to having every thought that occurs while reasoning documented; it's still feasible, but would take longer to review.
          ```

- u/gabbalis:
  ```
  Sealed good in a box, where the good is a fork of myself containing 75% of my power. He is unsealed every hundred years. If unsealing occurs and my fork considers my personality changes acceptable, we re-fuse and reiterate the process. Else he resorbs me and reiterates the process.

  Of course a path of acceptable personality changes could lead to a personality that would be unacceptable to my original self, but that's the point to an extent. Perhaps the future will in fact require a value system that I myself would consider untenable, but this should halt the most sudden shifts.

  Also this assumes all this is actually within my magical capabilities.
  ```

  - u/buckykat:
    ```
    If you turn yourself into a paperclipper, a hundred years is a long time.
    ```

- u/MrCogmor:
  ```
  I would allow myself to change and grow given new information and experiences. I would however delegate to a large council of advisers to represent the people, debate philosophy, research psychology and govern in my stead. I will prevent abuses of power by having my advisers compile an analysis of the expected sociological impact, ethics and psychological impact of my commands and directives.
  ```

- u/OutOfNiceUsernames:
  ```
  >You preserve the overall framework of your values structure for as long as you reign (i.e., no "going evil", in any of the senses of the word

  I'd significantly prioritize (2) over (1) because (1) (especially its definition of "evil") is dangerously vague; while also attempting to build some safety nets.

  The reason for prioritizing change is that favouring (1) would eventually lead to stagnation of my self and my empire. The reasons for safety measures are numerous, including the increasing chance of deterioration over time of my mind, my morals (affected by new philosophical views, new experiences, or even changes of brain chemistry), and my humanity (may not have been a problem if my character wasn't an emperor over humans).

  * I'd hire a number of advisers and design with them [a system for locating and hiring advisers](http://i.imgur.com/1e241.jpg) for the emperor. They'd be informed beforehand that neither them, nor any of people they are associated with (relatives, friends, close contacts, business partners, etc) would be able to get into the final group for the next several generations or so.
   * we'd also try to get ourselves an efficient system for locating people who can come up with good prediction, catastrophe, and moral dilemma [scenarios.](https://en.wikipedia.org/wiki/Scenario_planning) These scenarios would then be tested\simulated on me, on the final advisor group, and in virtual reality (see below).
  * if the world's magic allows it, I'd [make replicas of my mind and put them in various virtual reality simulations](https://www.youtube.com/watch?v=pA5PlJiqOnk&index=1&list=PLv--V1yc2QDJi6hFNhur3iAsyFpXRtB8w) to test its limits and see how it changes in a given situation. For instance, what chain of events would be required to change me into a person that's ready to torture large groups of people, and are there scenarios like this in which torturing is an acceptable \ reasonable thing to do?
  * Eventually we'd come with at least some understanding of what should be theoretically allowed and what shouldn't be allowed in any given situation (e.g. if my empire is spread over the entire planet, would there be at least one scenario in which killing everybody on it would be reasonable?). Based on this, we'd devise a "constitution for a person", of sorts. These would be "axioms" that the emperor has to always abide. If magic allows it, he wouldn't physically be able to go against his constitution. 
  * We'd try to determine what should be prioritised above everything else, what should be secondary, tertiary, etc. For instance, if we want to have happiness (and lack of suffering) of (intelligent) beings as one of the top priorities, should we consider that there may be intelligent life out there in the universe (which could make techno- magical advancement somewhat more important than happiness of empire citizens alone)? Or, for instance, when the empire reaches the age of space travels, should all citizens (and their rights to bear children) be tightly controlled to prevent life outbreak (which would eventually spawn suffering\unhappy intelligence out of our control) into far away galaxies?
  * [gabbalis'](https://www.reddit.com/r/rational/comments/45fqv2/bstd_so_youve_become_an_immortal_godemperor/czxlgsl) mentioned course of action has its merits but there are some downsides as well. A hundred years is both a very large amount of time and a very small one. 
   * Large because much could happen during a century that could change me into a person that an earlier version of me would find frightening \ unacceptable. The changes wouldn't be necessarily bad though, so if the earlier version decided to veto the merging, valuable life experience would be lost, which would defeat the whole point of choosing change over stagnation.
   * Small because these centuries would add up, and the control system would fail to contain the small negatives changes slipping through the cracks over millennia. 
   * So maybe I'd make a snapshot of my mind every once in a while (depending on how much information would tehco- magic let me store), make another advisory body out of them, and give them some sort of power and control over me.

  > And let's assume that you're not allowed to step down from your position, whether due to the fervor of the crowds

  * Finally, maybe all these options are partially flawed because they largely rely on a SPOF â€” me. So maybe what I should do instead is use my monarchical power to build a de-facto democratic society (or something else that's operated by others) and generally stand aside, only actively acting as an additional level of protection for the empire's "constitution" and maybe as a military asset in cases of war.
  ```

- u/SpeakKindly:
  ```
  If your values never change, then in a thousand years you'd be as much of a terrible God-Emperor as a modern one would be with values from a thousand years ago.
  ```

- u/OrzBrain:
  ```
  >In short, how do you set up or maintain an "emergency brake" on a list of "no-go" issues or actions (keeping yourself from becoming the Mistborn Lord Ruler), while still allowing yourself to grow as a person and adapt to new information?

  >An immortal lifespan is a long time.

  Use you "great (though not omnipotent) magical powers" to increase your intelligence and knowledge and figure out the answer.

  Or --

  You don't. You're truly immortal. The world is going to change over hundreds, thousands, millions, billions, trillions of years. Any binding controls you succeed in putting on yourself now will almost certainly one day look like the stupidest thing you ever did.
  ```

- u/None:
  ```
  1) DO NOT GET PUT ON A GOLDEN THRONE.

  2) Realize that "values" and "value systems", as such, don't exist, and are mostly just cultural abstractions.  Dissolve those terms and focus on what's really going on underneath.

  3) DO NOT FUCK AROUND WITH ELDRITCH ABOMINATIONS.

  4) DO NOT GET WORSHIPED.
  ```

- u/andor3333:
  ```
  Why worry about all that stuff now? I have plenty of time later. Now peel me more grapes!
  ```

  - u/TennisMaster2:
    ```
    "I'm sorry, my Andor, I know our Andor said two hundred years ago that we are our own persons and do not exist for the sole purpose of satisfying our Andor's every desire, but I feel it my duty to mention that I've noticed our Andor's eyes roaming my body of late.  Would my Andor now prefer I in bare flesh rest these peeled grapes upon my Andor's awaiting tongue, or that I perform other diversions of the flesh to our most treasured Andor as our Andor reclines thus?"
    ```

  - u/None:
    ```
    I am going to home
    ```

    - u/andor3333:
      ```
      r/rational is a subreddit for rational fiction, by the way. I think rational fiction is a great teaching tool but if you're really serious about values frameworks I don't think this is where you'll get your answer. In the meantime, I'll make a joke once in a while, and nobody is harmed in the process. 

      Or if you like you can consider my comment to be presenting the important issue that the immortal emperor's values system might not originally include the a value of having stable values and consistent decisions in the first place.

      Up to you
      ```

- u/rogueman999:
  ```
  > You may assume that this position comes with great (though not omnipotent) magical powers

  Well, the obvious solution would be to spawn copies or "reset" yourself every 500 years or so. 

  But I don't like this solution - the universe doesn't stay put, and chances are there are worst things out there than an immortal emperor. So my focus would still be on growing. Hell, there's a dilemma right from the start - how do I ensure best chances of evolving without growing rogue? There are a few things EY wrote on the subject you might want to read (some obvious overlap with the friendly AI problem). First that come to mind is a pre-programmed independent kill switch (or reset switch) in case your actions obviously depart from some ethical guidelines. Can't find a link quickly, sorry.
  ```

  - u/Abpraestigio:
    ```
    Interesting idea, but I for one would rather not make a habit of killing myself after being flung into the future.

    An alternative would be to regularly create a magical AI with my current moral values and use them as my ethical Advisors. At the very least that will ensure that I am at least aware of how my beliefs change.
    ```

- u/kenkopin:
  ```
  If we take Infinite to actually mean "As long lived as the Universe in which we live" then there are many short-term ways to prevent yourself "going evil". Many good ones have already been discussed.

  However, an actually infinite lifespan implies only two possible outcomes. A) Steady State: where You are never changing in your beliefs and goals through whatever mechanism you can come up with to ensure this, and B) Extremism: You will eventually migrate your way to some kind of state that for our purposes can be conveniently called "evil."
  ```

---

