## The Whispering Earring by Scott Alexander: "There are no recorded cases of a wearer regretting following the earring's advice, and there are no recorded cases of a wearer not regretting disobeying the earring. The earring is always right."

### Post:

[Link to content](http://web.archive.org/web/20121008025245/http://squid314.livejournal.com/332946.html)

### Comments:

- u/Taborask:
  ```
  Seems like it'd be pretty easy listen to it at the first stage when it's giving major life advice, and then take it off once it starts micromanaging. Of course, that would assume you knew what would happen to you at the end.

  I like that If this was an SCP it would probably fuse itself to your skull and eat your brain, but he's written it to imply that you lose higher functions naturally through disuse which is much more compelling
  ```

  - u/archpawn:
    ```
    Which raises the question of why it doesn't tell you to take it off when it starts micromanaging instead of immediately.
    ```

    - u/None:
      ```
      As far as why it doesn't tell you to remove it at stage 2, I can think of two reasons.

      A) It's designed that way intentionally

      or 

      B) For a large percentage of people, life is entirely about the pursuit of happiness, and rarely if ever about intellectual or spiritual development (especially if it comes at the cost of immediate happiness or gratification). Since the damn earring works to maximize happiness as the user understands it, it's very possible that not having to make decisions or think about things IS a form of happiness to a percentage of the population, so there would be no reason for the earring to recommend removing it again.

      I think that this sort of artefact would be cursed for the general populace (cursed in the sense that it's methodology is something we would consider evil), and merely immensely dangerous for a properly prepared and intelligent user.

      If you're able to reflect upon your own actions sufficiently, as well as maintain a continuous distrust of the "never wrong" voice whispering in your ear, I think it may be possible to employ this thing without necessarily losing your free will to it.

      I don't think I'd personally risk wearing it for more then a few weeks, if that, but I think someone with more willpower then I would be able to put it to good, long-term use.
      ```

      - u/LazarusRises:
        ```
        Seems to me that continuous use is a terrible idea. I'd probably use it when I had big life decisions to make, but not day-to-day.
        ```

  - u/philip1201:
    ```
    It only starts micromanaging when your desire for long-term values has atrophied to the point that you no longer care. You would be too late.

    What you could do is not wear the earring the earring, think about your options, then put on the earring for a few seconds to hear what the best option is. That way you're training your brain rather than losing it.
    ```

  - u/vorpal_potato:
    ```
    This world needs a service that will come for you after a few months and seize your earring, by force if necessary. Then the earring's first bit of advice could be to sign up.
    ```

  - u/None:
    ```
    If the person can appear as the same person from the outside (memories/personality) modulo being more happy, it means your consciousness gets gradually uploaded to the earring, and nothing bad happens to you (except that the resulting mind keeps magically knowing what to do to achieve happiness and will always want to do it, which isn't necessarily a kind of harm), (but it's a sufficient change that I'd consider it death).

    Gradually, your mind runs less and less on your brain, and more and more on the earring.

    (Edited.)
    ```

  - u/WalterTFD:
    ```
    Why take it off when it starts micromanaging?
    ```

    - u/philip1201:
      ```
      The earring doesn't maximize your long-term goals because it changes them over time by atrophying the parts of the brain that think about long-term goals, because its suggestions are better than yours.

      This actually happens before the micromanagement stage, though, but it would minimize the damage.

      (That said, [the author intended](https://web.archive.org/web/20121007235422/http://squid314.livejournal.com/333168.html) it to be horrible even without long-term value drift).
      ```

      - u/meterion:
        ```
        >The earring doesn't maximize your long-term goals because it changes them over time by atrophying the parts of the brain that think about long-term goals

        That doesn't at all sounds like how I interpreted it. To me, it seemed like it took your long-term life goals, and realized them in the most effective manner possible. The implication is that the earring is better at better at being you than you are, not that it changes you into something you wouldn't have become if sufficiently motivated and knowledgeable.

        The ending proverb points in that direction: "One must never take the shortest path between two points."
        ```

        - u/aponty:
          ```
          It specifically does not maximize your outcomes, merely exceeds the wearer on the things it gives input on.  


          Note that this gives lots of room to optimize for agendas other than those of the wearer
          ```

      - u/WalterTFD:
        ```
        It doens't maximize your long term goals because it gives you better ones.  *The earing is always right*, as it were.  The story says that if you ever deviate you regret it.

        The earing is the best hedonic course of action.  Taking the earing off is like pushing a nail through your hand.  I get why someone might do it in the heat of a moment, in the 'people have abstract thoughts and take actions that are whatever' kind of way, but it is really odd to see someone sitting down and constructing arguments against the optimum path.  Like, aren't you kind of fighting the definition?
        ```

      - u/None:
        ```
        Unless your goal is happiness.

        For selfish people, the earring seems to be ideal.
        ```

- u/fljared:
  ```
  Note that this story's meaning is clearer with the the information from the [following page](https://web.archive.org/web/20121007235422/http://squid314.livejournal.com/333168.html).
  ```

  - u/GemOfEvan:
    ```
    Thankfully, if I really cared about my free will, my utility maximizer would make sure I spend some time feeling like I'm making decisions.
    ```

    - u/Nimelennar:
      ```
      ...Until it figures out how to replicate that feeling without you actually making decisions.
      ```

- u/Do_Not_Go_In_There:
  ```
  That was a pretty neat read. The part about >!the wearer's brain wasting away and being conditioned was horrifying.!<
  ```

  - u/RandomChance:
    ```
    Can you recite all of Homer from memory?  Can you chase down rabbits and deer by just not stopping while they succumb to exhaustion?  Your ancestors could, but now you have printing presses, and libraries, and agriculture and don't have to, so your (and my) capacity to do so has atrophied to the point where our ancestors would be horrified (or maybe very envious).  Is it really horrid/non-rational to delegate to someone/something else that what "they" can do better?  Does it make sense to spend extra resources preserving the capacity if it is proven it won't be needed?  (I would argue that some limited skill might be a good idea in case what your relying on goes away but that doesn't negate the larger argument).
    ```

    - u/BumblingJumbles:
      ```
      I can sing the opening from the Weird Al show!
      ```

    - u/etarletons:
      ```
      No, but fic about the extremal case where it is horrifying is good!
      ```

    - u/eroticas:
      ```
      If you are able bodied and fit enough to go for a 20 min jog every morning i bet you could probably chase down deer if it wasn't able to hide from your sight. You're still human and are possessed of the same adaptations for distance running.
      ```

  - u/FeepingCreature:
    ```
    Less horrifying if you realize that the ring is effectively doing a >!gradual upload!<.
    ```

    - u/ArgentStonecutter:
      ```
      How do you figure? It's not like the Dual in _Learning To Be Me_, it's not learning to be the wearer, it's teaching the wearer to be the earring.
      ```

      - u/FeepingCreature:
        ```
        But "being the earring" is externally indistinguishable from a better you, or else people would have caught on by now. It's not Doctor Fate's helmet either.
        ```

        - u/ArgentStonecutter:
          ```
          Or from someone playing you on a stage. There's no indication that it replicates your mind-state.
          ```

          - u/FeepingCreature:
            ```
            If someone can play me on a stage in sufficient detail to be externally indistinguishable, I'll buy that their mind contains a copy of mine.
            ```

            - u/ArgentStonecutter:
              ```
              They're not playing 'you', they're playing the person they were puppeting you into pretending to being.
              ```

              - u/FeepingCreature:
                ```
                Sure, but the person they were puppeting me into pretending to being is externally pretty indistinguishable from myself.
                ```

                - u/ArgentStonecutter:
                  ```
                  It’s clearly distinguishable or there wouldn’t be any point to wearing the earring.
                  ```

                  - u/FeepingCreature:
                    ```
                    True.

                    I guess I'd say it's indistinguishable from a more capable version of yourself. (That being, after all, the aesop.)
                    ```

                    - u/ArgentStonecutter:
                      ```
                      Or a much more capable person pretending to be you. It only has to memorize a little of the old you to fool people.
                      ```

    - u/None:
      ```
      [deleted]
      ```

      - u/None:
        ```
        >>!There's nothing in the story to suggest that there's a person preserved on the other side of the earring.!<

        >!There has to be, because the resulting brain/earring composite is functionally isomorphic to a person, which means (because of the computational theory of mind being correct) that there is a person inside.!<
        ```

        - u/Charlie___:
          ```
          Computationalism != behaviorism. I can predict the behavior of my dog without needing to run a simulated copy of my dog, because I am much smarter. The earring is in a similar situation with respect to humans, except moreso.
          ```

          - u/None:
            ```
            >Computationalism != behaviorism.

            It does equal - something implements the same computation if and only if it acts the same way (on the appropriate level of abstraction). It's because if something behaves the same way, it implements the same computation (that's an implication in one direction) and if something implements the same computation, then it acts the same way (because you can observe the computation and comprehend it in terms of behavior of the simulated person) (that's an implication in the other direction). So since we have implication in both direction, we have equivalence between behaviorism and computationalism.

            >I can predict the behavior of my dog without needing to run a simulated copy of my dog, because I am much smarter.

            It might be surprising, but no, you can't. If you can predict the behavior of a system exactly, some particular subset of your brain must be (input-output) isomorphic to the system (otherwise you could never know the behavior of the system, since the behavior is the output).

            If you can predict your dog imperfectly, then your brain runs an imperfect emulation of your dog on your brain, the departure of the emulation from the real dog being as big as the imperfection of your predictions.

            (Edited.)
            ```

            - u/Charlie___:
              ```
              Suppose I have two different programs for finding the sum of integers from 1 to n. One of them uses a loop and just adds each number in turn. The other multiplies n by (n+1)/2.

              The outputs of these computations are identical. If we call your definition of the word "isomorphic" as isomorphic\_{DD}, then they're absolutely isomorphic\_{DD}. But there is no isomorphism between the states of the computers as the programs are run - no sense in which an intermediate state of computer 1 corresponds to any state of computer 2.

              This standard of "same computation" that requires a correspondence between internal states is pretty common in the computational interpretation of consciousness.
              ```

              - u/None:
                ```
                Edit: Corrected my first sentence and added a fourth caveat.

                Edit3 added.

                &#x200B;

                >But there is no isomorphism between the states of the computers as the programs are run

                There isn't, but that doesn't matter.

                Because the output of the computer is determined by the physical state of the computer, there is a subset of the physical properties of the computer that determines that the output of the computer will be the sum of integers from 1 to n.

                Those physical properties exist in the computer *right now* (even though it's in the middle of the calculation), so isomorphism\_{DD} can be defined in terms of the current physical state of the computer (and not only in the terms of future output), which makes it a present property of the computer. (Rather than the output being the same in the future mysteriously reaching backwards in time and causing the right qualia in the system in the present moment.)

                The question is whether isomorphism\_{DD} is enough, or whether we need some other subset of the state (one that has no influence on the output of the computation) to be isomorphic too.

                We should be able to bootstrap the intuition to see that isomorphism\_{DD} is enough.

                Imagine that you have introspective access to the way the computation is performed (rather than your mental state being only determined by that aspect of the computation that determines your outputs). Then you should be able to communicate it to someone, which contradicts the assumption it doesn't influence your outputs.

                I can think of four possible caveats:

                First: What if something is a part of my mental state without me having introspective access to it?

                Possible answer: Then it should at least influence my behavior, or else there's no sense in which it's a part of my mental state, but my behavior is a subset of my outputs (or, depending on how we define behavior, it's equivalent to my outputs).

                Second: What if I have introspective access to something, but I'm powerless to let it influence my behavior in any way?

                Possible answer: I don't think that can happen. As long as I can communicate in at least some way, I can let it influence my behavior. (A special case are locked-in people, in which case we can look at the brain to find out how their behavior would been influenced if they could move.)

                Third: What if an entity with a non-isomorphic computation has *no* qualia? Then it has no mental state, so this reasoning doesn't apply.

                Possible answer: I don't think that's possible either, because then we could arrange [fading qualia/suddenly disappearing qualia](http://consc.net/papers/qualia.html) without changing the behavior of the system.

                Fourth: What if I have a self-contained simulated world and I calculate, let's say, the state of the world at t = 10 right after t = 1, rather than calculating all states in between. Will the intervening states have had subjective experience?

                Possible answer: I don't think computing people like that is possible, even in principle. (Edit: Except for hashing, I guess? Let me know if it's important.) (Edit2: I guess it depends on the circumstances. Did you have anything like that in mind?)

                Edit3: The example with the two programs calculating the integers doesn't have any isomorphism of the computational states during the computation, because the two programs aren't conscious.

                In any conscious software, the computations will be always isomorphic (not just isomorphic\_{DD}) on *some* level of abstraction, because at every moment, the computation encodes the mental state of the person.

                So if we just look at the conscious computation on an arbitrary level of abstraction, there will be no computational isomorphism with the original, but that's because we included even the aspects of the computation that don't influence your qualia.

                So if we don't include in our description the superfluous parts of the computation, what we get will be isomorphic to the original.

                Are there any cases where this doesn't obviously work?
                ```

      - u/FeepingCreature:
        ```
        The earring is always right. To say that there isn't a >!person on the other side of the earring!< is equivalent to saying that your life is meaningless, or rather that your self is meaningless to your life.
        ```

        - u/None:
          ```
          [deleted]
          ```

          - u/FeepingCreature:
            ```
            It's sort of like the argument that consciousness is the thing that makes you talk about consciousness. It's not stated that the >!ring-zombies!< are in any way lessened, partially because that would weaken the story. But being unable to perform any visible aspect of >!yourself-ness!< would be a worsening of your performance and hence equivalent to bad advice. The only scenario where the >!ring-zombie!< could *fail* to be an upload is if what you consider your self has very little effect on your actions.

            The ring is a >!DWIM device!<. Its basic function requires that "I" persists.
            ```

            - u/DangerouslyUnstable:
              ```
              Don't forget, if we take as a given that the earring's advice always leads to a better long term happiness, the earring itself tells us that it's use is negative. The very first advice is "don't use it", implying that using the earring is unambiguously worse than not using it. Since outwardly, earring users *seem* to be happy, successful people, above average, then that must imply something about the users internal state. All the rest of the advice after the first piece is "well, given that you've already decided to use the earring, here's the best you can do".
              ```

              - u/ArgentStonecutter:
                ```
                > if we take as a given that the earring's advice always leads to a better long term happiness

                That's not what the story actually says. It only says that the wearer who obeys the earring does not express regret. By the end stage the wearer is unable to do so.
                ```

                - u/DangerouslyUnstable:
                  ```
                  The story actually explicitly said "the earring is always right" and that there is no case of a wearer regretting following or failing to regret not following. Yes, they are unable to do so at the end, but that's the extreme case. There would be plenty of time for them to do so in the early stages, and they never do. And they always regret it if they don't follow the instructions. The story pretty explicitly says that the following the advice of the ring will always be better / make you happier
                  ```

                  - u/ArgentStonecutter:
                    ```
                    It very carefully avoids saying that.
                    ```

                    - u/DangerouslyUnstable:
                      ```
                      Which part? "The earring is always right" is literally a direct quote from the text.
                      ```

                      - u/ArgentStonecutter:
                        ```
                        Right for whom?

                        You need to be more paranoid about genies.
                        ```

                        - u/DangerouslyUnstable:
                          ```
                          Now you're getting pedantic. It is very clear from the context of the story that that quote means that the advice of the earring will make the wearer happier than whatever decision they would have made without the earring.
                          ```

                          - u/ArgentStonecutter:
                            ```
                            It also says it’s not the best decision, just better than the one you would have made. It’s manipulating you using classic conditioning.
                            ```

                            - u/TheColourOfHeartache:
                              ```
                              That just means it's the best decision out of all decisions that might conceivably be made in any imaginable circumstance. 

                              Spend 40 years training with zen monks, scientists and philosophers to make the best decisions. The earring is still offering better decisions than "the one you would have made".
                              ```

                              - u/ArgentStonecutter:
                                ```
                                It doesn’t actually say that.
                                ```

                              - u/daytodave:
                                ```
                                But those decisions might be far better than if you hadn't had that training, because you've raised the bar for "better than the decision you would have made".
                                ```

              - u/daytodave:
                ```
                > The very first advice is "don't use it"

                >!This comment made me realize the real horror of that first instruction. It *doesn't* say, "don't use me". It says, "better for *you* if you take me off". It's better for your values, goals, community, and the world at large if you leave it on. It's only better for "you" if you take it off, because "you" is the thing that it destroys.<
                ```

              - u/FeepingCreature:
                ```
                I'm uncertain how literally to take that advice. It seems to connect more to the free-will aesop than the implied uploading.
                ```

                - u/DangerouslyUnstable:
                  ```
                  Your interpretation seems to rely on several things that are not directly supported in the text (most notably the uploading...that's really not implied by the text at all). That doesn't mean it's wrong (frankly, no interpretation of a text can be "wrong" per se, people take what they want form things), but it's a more complicated, less obvious interpretation, and probably not the one intended by the author (again, not that there is necessarily anything wrong in that).

                  -edit- just to address the implied uploading, according to your earlier comments, if I understand your logic, your argument is basically that "anything that can predict a thing, is that thing". Which is just....wrong. An accurate prediction model does not *have* to be the thing it is predicting. The map is not the territory. Also, a VERY important distinction in what the earring is doing: it IS NOT predicting the users behavior, it is telling the user what behavior will make the user happer. That does not imply at all that the earring knows what the user *would* do, just that it knows what outcomes will make the user happier and how to achieve those outcomes.
                  ```

                  - u/FeepingCreature:
                    ```
                    > Which is just....wrong. An accurate prediction model does not have to be the thing it is predicting. The map is not the territory.

                    1. Any sufficiently advanced map is indistinguishable from the territory.

                    2. Any map that is indistinguishably advanced can be shown to differ from the territory by inspection or interrogation.

                    > it is telling the user what behavior will make the user happ[i]er

                    Not _quite_. There seems to be a severe lack of ring users getting addicted to opiates and dying from an overdose. The ring respects community recognition, and it does not generate advice that causes the person to severely stand out in ways that still maximize happiness. I suspect if Eliezer put on the ring, he would not forsake AI and start taking up gardening, for instance. I think the ring does _something_ that involves maximizing your value function. So if that's right, if it fails to be an upload, it has to be because you value something about yourself that you don't value about yourself. Eeh?

                    I guess it could be the case that there's something about yourself that you or anyone else around you should value but don't. Then the ring would fail to preserve it. Though that's an implementation failure of the ring - it should first change you to value that thing, and then its first advice would not happen.

                    "Better for you if you adjust your identity of self..."
                    ```

                  - u/None:
                    ```
                    >if I understand your logic, your argument is basically that "anything that can predict a thing, is that thing". Which is just....wrong

                    It's correct about minds, because minds are software, so if I can predict someone, there must be their mind running on my brain.

                    At the beginning, your mind runs only on your brain. As the earring micromanages you more and more, your mind gradually moves to the earring (because the composite brain+earring at the beginning contains your mind only in your brain, at the end only in the earring, and in between it's partly here, partly there (as your brain is partially atrophied) - in other words, your mind gradually moved from your brain to the earring).
                    ```

            - u/SoundLogic2236:
              ```
              It requires some amount of information about the person be preserved. And the GAZP does not say that everything that talks about consciousness be conscious. It doesn’t rule out managing to create a zombie, just argues that such a thing is nontrivial. 
              An earring could create a similar effect by gradually uploading and improving you. 
              Which is why not only were the brains examined, but the earring itself was talked to. 
              And the earring said “Oh, no, actually the specific way I do this is a terrible idea. Definitely a bad plan. At least by your values.”
              That they no longer exist in their brain doesn’t mean they met a bad end. But it also doesn’t mean they met a good end. 
              Which may or may not involve them being dead. All we actually are told is that by the values of every wearer it is a bad idea, and that after careful examination and discussion with the earring that it was suggested it be locked away. 
              Perhaps the earring uploads you and then tortures you but leaves enough to get the information it needs. Perhaps it is some form of zombie master. It is unspecified. 
              Just that the earring says by your values it is a bad result.
              ```

              - u/FeepingCreature:
                ```
                But it never repeats that advice, which indicates to me that by the earring's measure, the earring-driven you is only marginally worse at maximizing your values. Only one value violated, and only once; or something like that.
                ```

                - u/i6i:
                  ```
                  The earring solves the immediate problem it is given better than you.  It advises you not to wear it. Then gives good life advice that makes giving it up harder thus leading to better odds of identity death or crippling widthdrawl later. Those aren't considered part of the problem space since they were both adequately resolved by the first piece of advice given and of no further interest to the earring.
                  ```

    - u/GET_A_LAWYER:
      ```
      I think this story is more related to Scott's Thousand Shards of Desire and his explicit preference toward being really-real rather than a brain in a jar. This is a horror story because at the end the wearer is no longer a thinking rational actor, but is closer to the neural tissue with its pleasure sensors being electrically activated. A wirehead, passive in her own body.

      I think you're correct that at some level the earring must contain a complete model of the wearer's brain (and potentially all brains). For the reasons above, I suspect most people wouldn't consider that less horrifying. Regardless of whether or not there's a copy of me in a system somewhere, atrophying the brain of current-me is still a bad thing. So the horror remains, even if there's an intact copy somewhere. (Worse, there's nothing in the story to suggest copies are retained, rather than scanned, used, and dumped.)

      Also I think your interpretation is incorrect within the facts presented by the story. It can't be doing a gradual upload, because it's advice is correct immediately. The story specifies that its first advice is on major life decisions, which requires are more complete model of a person than required to move particular muscles to move around. The gradual part of the process is training the wearer to accept the commands, the earring's understanding must be complete from the beginning.

      Intriguing perspective though, thank you.
      ```

      - u/FeepingCreature:
        ```
        Right, it's more an immediate upload with a gradual shutdown of vestigial algorithms. But that's equivalent to a gradual upload with regards to capacity to reverse it.
        ```

- u/None:
  ```
  [removed]
  ```

  - u/Reply_or_Not:
    ```
    >Wouldn’t a better decision be to keep it on person, and use it either in rare and important cases, or after first coming up with a list of possible solutions yourself for each given scenario, or both?

    I think the answer to the question depends on what you think about what it means to be yourself.  For example u/FeepingCreature makes the strong case that the earring is doing a  brain  upload, which has much less horrific  connotations than  those who are positing other negatives.
    ```

- u/Psortho:
  ```
  My pattern-loving brain really wanted the story to end with Kadmi-nomai saying "better for you if you locked it away."
  ```

- u/None:
  ```
  I think people are looking at the ring the wrong way.  If it always gives the right advice, instead of hiding the earring, a utilitarian would put the earring on.  Imagine a scientist unerringly guided towards correct conclusions.  It would be far better for society that they put the earring on.  The Nobel prizes they won would be earned by merit of their ultimate sacrifice for the good of others.
  ```

  - u/philip1201:
    ```
    You're assuming the ring doesn't cause (claimed) value drift as a side effect, but given the experimental data that seems unlikely. How many self-identified utilitarians would truly consider someone who retired to live a happy life with their SO a failure of a human being?

    So far, nobody with the earring has done something noteworthy, instead they all seem to have converged on a hedonic lifestyle, which is no doubt satisfying to whatever remains of their brain's value system.

    The brain is bad at math and will get far more satisfaction saving a bus full of orphans than donating a million dollars to an effective charity. As a utilitarian, you prefer the math, but you value the outcome of that math more than being able to do that math. As such the ring will give you better answers and your ability to do utilitarian considerations will atrophy. Once those considerations are sufficiently gone, what remains is "trust the earring to know what is right", which is then freed up to meet more deeply seated desires.
    ```

- u/eroticas:
  ```
  If the ear ring truly does what you want to the fullest degree (which it might not, hence the horror, but *if* it does), is (a version or copy of) you still alive inside the earring somehow?

  I wouldn't wear the earring because it may have alien goals ...but if it doesn't have alien goals then this seems like allowing a smarter copy of *me* to tell me what to do / wear my body
  ```

  - u/Charlie___:
    ```
    I can predict my dog quite well, and I'm sure if I applied myself I could choose actions that were basically always better for the dog than its own choices. I don't do this by running an upload of my dog in my brain, I do it by being a superintelligent alien artifact relative to my dog.
    ```

    - u/eroticas:
      ```
      Not alien, though, since you truly care about the dog and its true preferences (perhaps unlike the earring)

      I guess the earring is sort of like an overbearing guardian who knows what is best and isn't shy about telling you so but doesn't take into account that sometimes it's important to figure it out yourself
      ```

---

