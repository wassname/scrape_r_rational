## [D] Wednesday Worldbuilding Thread

### Post:

Welcome to the Wednesday thread for worldbuilding discussions!

/r/rational is focussed on rational and rationalist fiction, so we don't usually allow discussion of scenarios or worldbuilding unless there's finished chapters involved (see the sidebar).  It *is* pretty fun to cut loose with a likeminded community though, so this is our regular chance to:

* Plan out a new story
* Discuss how to escape a supervillian lair... or build a perfect prison
* Poke holes in a popular setting (without writing fanfic)
* Test your idea of how to rational-ify *Alice in Wonderland*

Or generally work through the problems of a fictional world.

^(Non-fiction should probably go in the Friday Off-topic thread, or Monday General Rationality)


### Comments:

- u/callmebrotherg:
  ```
  How would you go about designing a science fiction setting that wants to make at least some attempts in the direction of scientific plausibility? My problem here is that science has this awful tendency to keep advancing and spin off in weird directions so that any setting will probably look really weird after just five or ten years. 

  This isn't an issue for one-and-done stories, but if I like a setting enough to write it in then I'm probably going to like it enough to want to visit it on multiple occasions and I'd prefer to not create a setting that looks fine at first but ages badly and gradually transitions into soft science fiction over the course of its lifetime. 

  There are a few possibilities that occur to me: 

  * Get over it. This isn't as much of a problem as I think it is, and most readers don't actually care as much as I do. 
  * Set the story after some sort of technological collapse (either full or partial) and insert schizo tech on the justification that there are lots of neat gizmos that people know how to replicate (at least in some cases) but don't know to improve on. Probably easiest to do if, for example, there are machines that take care of production and nobody knows how the machines work. 
  * Use a science fiction setting with no humans at all, and therefore an entirely different history of scientific advancement. It's plausible that another civilization could miss some of the advances that we've made (and maybe make some that we've missed), or at least it's plausible enough to shut up the critic in my head. 
  * Since the setting will eventually become alternate history *anyway*, make it an alternate history from the beginning. In that case, I just have to figure out what the most interesting point of divergence would be. I'm tempted to go for something in the early 20th Century, but I'm not sure.
  ```

  - u/LiteralHeadCannon:
    ```
    I think that alternate history of technology is a sorely underdeveloped topic in the alternate history field.  For example, it occurred to me a few months ago that it would have been *completely plausible* for the telegram to be invented around ~1800, and this would have completely changed technological development.  In fact, someone actually *did* invent the telegram at that point, it was just extremely inefficient because he was trying to use a different wire for each symbol - the thing that was missing was Morse Code, the idea that you could encode all desired symbols as different patterns of signal on a single wire.
    ```

  - u/696e6372656469626c65:
    ```
    (partially joking, partially serious answer)

    *Go big or go home. Fourth option, divergence point: Big Bang.*

    The quantum fluctuation that led to the birth of our universe was subtly different, leading to laws of physics that are different from those of our home universe. These laws of physics are similar enough to our universe that it's still possible for life to evolve (and maybe even look similar to us), but higher-level technology functions based on completely different principles.

    * Advantages: You maintain the hard sci-fi atmosphere, but no one can accuse you of scientific implausibility, and you can pick the laws of physics to be whatever you find interesting and/or convenient.
    * Disadvantages: The amount of effort required for consistency and plausibility might make it more trouble than it's worth; it's also not going to suit those people who insist that their sci-fi stories be possible in *our* universe. (Then again, those people are pretty much doomed to be disappointed with any sort of science fiction, so...)
    ```

  - u/PeridexisErrant:
    ```
    Personally I find post-collapse and non-human-society stories tend to be... implausible, as there are too many arbitrary coincidences required to let readers identify with, well, anything in the story.

    Plan "just get over it" is inevitable, even if you go for alternative history.  "Alternative future" is a fun genre though - posit some technological-but-not-social divergence point within the last ~decade or so, and roll it forward to asteroid mining for 3D printers (if the hype was justified...).  The trick is to own it up front, and explicitly note that it's "the Future^tm as of YYYY, if X".
    ```

- u/LiteralHeadCannon:
  ```
  Let's look at a scenario that involves a sudden empowerment event that respects previously-artificial natural borders.

  Suddenly, superpowers are introduced to the world, and exactly one person in each country gets those superpowers - the leader of that country.  For example, in the United States, the superpowered individual would obviously be the President.  This introduces some major questions, obviously.  Here are some of the biggest that occur to me.

  * In some countries, there would be legitimate question as to who gets superpowers.  Does the President of Germany get powers, or the Chancellor?  My assumption is that the Prime Minister gets the UK's powers, but if the monarch got them instead, that fact alone would cause major changes to British politics because it would change how people mentally frame things.
  * What happens if the Thing-Granting-Powers' world map is distinct from the internationally-recognized one in some way?  (A bit of this is inevitable, since the internationally-recognized powers themselves don't even agree on the nation list.)  Say, it fucks absolutely everyone over by giving powers to Abu Bakr al-Baghdadi, recognizing ISIS as a sovereign nation.
  * Transfers of power.  Peaceful transfers of power are pretty clear - the old leader loses their power when they leave the office, and the new leader gains a new power when they're sworn in.  But what if there's an actual dispute, or if an elected leader launches a self-coup to stay in power, and tries to stonewall impeachment attempts?  What happens if we wind up with multiple people running around claiming to have the same position simultaneously?  Will the Thing-Granting-Powers settle the dispute itself by choosing one of them to give powers to?  At what point does it recognize one side of a civil war as having succeeded well enough to count as a distinct country with a distinct leader?  At what point does it recognize a country as having been conquered thoroughly enough to strip its leader of powers?
  * Of course, the actual strengths of the powers have a huge effect on the world.  The stronger the powers are in general, the more they'll really matter beyond giving supernatural legitimacy to nations.  If some world leaders can face down armies of normals, persuade anyone to do anything, or have some strong "Thinker" power, that's a game-changer.  The less of a correlation there is between the preexisting strength of a nation and the strength of that nation's leader-power, the more the empowerment will serve to equalize tiny, irrelevant countries with the old dominant nations.  Tuvalu has about 1 leader for every 10,000 people, while India only has 1 leader for every 1.3 billion people!  It would hurt nations more the larger they are, because they have more to defend and only one superpowered person to do it.  I'd say the most interesting thing for balance would be to have a correlation between preexisting nation strength and power strength, but not a 1:1 correlation - so sometimes the Thing-Granting-Powers will shake things up by giving a critical nation an unimpressive power, or an obscure nation a game-changing power.
  * The way some powers seem innately aligned with "good" or "evil", and how that effects their wielders' image, is a common theme in superhero stories.  In this specific scenario, how would that effect things?  How bad is it for a country if its leader happens to have a power that's fueled by death, or controls people, or is obviously useless outside of a fight to the death?  How good is it for a country if its leader happens to have a healing power, or one that can be used for humanitarian purposes, or one with a light motif?  How would powers be spun differently by their nations' supporters and enemies?
  * How does the "meta" for nations work going forward?  Secession attempts will be taken a lot more seriously, but some smarter nations may split up and decentralize, figuring that they'll be stronger as a group of small allies with a lot of powers than as one oversized nation with a single power.  The Thing-Granting-Powers may call bullshit on this, responding to overly close alliances formed in this way by only giving the dominant nation a power.  If the United States wanted fifty powers, for example, they'd need to actually balkanize; they couldn't just issue a statement saying "oh BTW the federal government is just an international body now (but still has all the same authority it did before)".  It *is* possible to munchkin this scenario, I think - just not by "fooling" the Thing-Granting-Powers.  You can't fool it.  It knows better.
  ```

  - u/callmebrotherg:
    ```
    > What happens if the Thing-Granting-Powers' world map is distinct from the internationally-recognized one in some way?

    Depending on how it views the matter of tribal sovereignty, aboriginal populations in the U.S. and possibly elsewhere might also get more influence by dint of their superpowered leaders. 

    Also, does a state's power stay the same from leader to leader, or can it change during the transfer of power? Personally, I like the idea that powers change but orbit the same "theme."
    ```

    - u/LiteralHeadCannon:
      ```
      I like that idea too; I was already firm that they change, and I hadn't come up with the idea that they orbit the same "theme" (kind of like powers running in families in Worm?) but I like it.  Probably gradually alters in nature over time - I'm already firm that they gradually alter in *strength* over time to correlate with national strength.
      ```

      - u/callmebrotherg:
        ```
        Regarding secession, since you like themes: 

        Any individual power can be thought of as having multiple components. For example, a power as simple as "shooting laser beams at people" can be divided into "lasers/light" and "energy beams." 

        If a country successfully secedes (whatever "successfully" means in this case) then the theme for that counter's leadership will develop from unused components in the power of the other country's leader. For example, if somebody seceded from the United States and the President at the time had "shoot laser beams" as a power, with "laser/light" being the running theme (and other Presidents having had powers like "bend light to become invisible"), then the leader of the seceding country would have powers that draw on the theme of "energy beams" (with "energy" interpreted very liberally, for the most interesting results).
        ```

        - u/LiteralHeadCannon:
          ```
          I was kind of thinking along these lines, yeah.  Similarly, if a country annexes another country, then the next time there's a leadership change, their power will have elements of the annexed country's theme.
          ```

  - u/ulyssessword:
    ```
    >In some countries, there would be legitimate question as to who gets superpowers...

    Now I want to read a crackfic where Queen Elizabeth II gets 16 superpowers (from the United Kingdom, Canada, Australia, New Zealand, Jamaica, Barbados, the Bahamas, Grenada, Papua New Guinea, Solomon Islands, Tuvalu, Saint Lucia, Saint Vincent and the Grenadines, Belize, Antigua and Barbuda, and Saint Kitts and Nevis).

    >...the new leader gains a new power when they're sworn in.

    Is this one power per person, one power per person/country pair, or one power per leadership term?  

    If it's one per person, then it would make sense to have as many people as possible cycle through being the leader of a country, in hopes on getting something useful (like precognition) instead of something useless (like flying).  Due to practical concerns, it would likely only be open to the leaders of the official opposition parties for a limited amount of time before the election (1 month each?), because they need real power to reveal their superpowers.  

    If the same power is maintained across country leaderships, this could even be done in other countries, similar to how some politicians start at the city level, move to the state level, then go federal.

    If it's per-term, then it might make sense to elect a *set* of people, with the one(s) with useless power(s) retiring in favor of their pre-elected successors.  This could be done as simply as swapping between a President and a Vice President until the current leader gets something good enough.

    >... tiny, irrelevant countries...

    The [Principality of Sealand](https://en.wikipedia.org/wiki/Principality_of_Sealand) would be about 4% superpowered people if it counted.  This would be a *huge* boost to the micronation movement, and perhaps enough to make a few hundred on its own.
    ```

  - u/Jakkubus:
    ```
    >Transfers of power. Peaceful transfers of power are pretty clear - the old leader loses their power when they leave the office, and the new leader gains a new power when they're sworn in. But what if there's an actual dispute, or if an elected leader launches a self-coup to stay in power, and tries to stonewall impeachment attempts? What happens if we wind up with multiple people running around claiming to have the same position simultaneously? Will the Thing-Granting-Powers settle the dispute itself by choosing one of them to give powers to? At what point does it recognize one side of a civil war as having succeeded well enough to count as a distinct country with a distinct leader? At what point does it recognize a country as having been conquered thoroughly enough to strip its leader of powers?

    What about the powers and their level being dependant on how much people in particular country actually considers empowered individual a leader?
    ```

- u/Noumero:
  ```
  Let's expand on what [this](https://www.reddit.com/r/rational/comments/5ldb9z/how_would_a_society_deal_with_an_easily_teachable/) idea leads to.

  Essentially, every human has a Death Note-lite. Any human can instantly kill any other human with a thought, if the former has enough information about the latter. ('Enough information' in information-theoretic sense, but for the sake of simplicity, let's assume that means a descriptor unique for that human: a name, a face, a username, etc; or their unique combination.)

  1. If every human was granted this ability in Stone Age, could a large society or a technological civilization arise, even in theory?

  2. In classical era?

  3. In modern era?

  4. You are tasked with designing the perfect world for these humans (population: ~1 billion). A genie would implement it. Restrictions: you can only use the already-developed technology; the ensuring civilization cannot consume more resources than *Kardashev I*; the population must be situated on Earth.

  5. **Edit:** Same as 4, except at any point in time, there's N 'normal' humans alive. Normal humans cannot kill with a thought and cannot *be* killed with a thought. If a normal human dies, a random deathnoter becomes normal, which both he/she and the other normal humans become magically aware of.

   What is the lowest N that would make a functional society possible?
  ```

  - u/696e6372656469626c65:
    ```
    1. No.
    2. No.
    3. No.
    4. Total anonymity is probably the only way to even *remotely* guarantee the possibility of safety, but that's really really hard to do (especially since you limited us to already-existing technology). And even if you *do* manage to implement this somehow, this makes a large-scale society nearly impossible to run, since you have no way of specifying who's who, making economic transactions impossible in principle. Truthfully, I doubt there's a feasible way to go about constructing a society with insta-kill powers unless the agents populating that society have a completely different psychology from humans. I realize this is an unsatisfying answer, but the fact is that the incentive structure present in the scenario you give makes things so unstable that I'm not sure an equilibrium state other than "everyone dies" even *exists*.

    TL;DR: Giving a single person Death Note-esque powers is bad. Giving *everyone* such powers is an extinction event.
    ```

    - u/KilotonDefenestrator:
      ```
      Well, let's turn it around. If there is zero privacy, you *could* kill anyone, but it would be known that you did it, and you'd probably die soon after. Do you hate someone so much that you'd be willing to die to kill them?

      Terrorists and the insane could do a lot of damage though... so the perfect society would have to produce few or none of those.
      ```

      - u/696e6372656469626c65:
        ```
        > but it would be known that you did it

        Um, how? Unless I missed something in /u/Noumero's post, there's no way to tell who killed someone using their power (or even that the person was killed via supernatural means at all). And even if there were some way to track who killed who, as you said yourself,

        > Terrorists and the insane could do a lot of damage though... so the perfect society would have to produce few or none of those.

        and there's no way to guarantee that *with our current technology levels*, which /u/Noumero limited us to.
        ```

        - u/Noumero:
          ```
          Yep, there's no way to tell who did it.

          >and there's no way to guarantee that *with our current technology levels*, which /u/Noumero limited us to.

          Because if I didn't do that you'd instantly solve everything by putting FAI in charge or something. I know how r/rational thinks.
          ```

          - u/KilotonDefenestrator:
            ```
            I see. 

            I was thinking not of supertech but just a society that is safe and stable, and all children were raised in loving families and mental illness detected early and treated before the powers set in (I assume infants being hungry or toddlers throwing a tantrum does not results in mass murder, but that the power sets in later in life).

            I do however have to concur with /u/696e6372656469626c65 and say that no, a society of death noters would cease to be a society fairly quickly if there were no accountability.
            ```

            - u/696e6372656469626c65:
              ```
              Let's try something else, then. Suppose that instead of being forced to populate your society with *humans*, you also get to design a new type of mind with which to fill that society.

              1. Easy mode: You get to design both the society itself and the type of mind that will populate it. Can you create a societal arrangement that is stable in the long term? (Again, with /u/Noumero's caveat that the technology level of the society in question cannot exceed our own.)
              2. Hard mode: You get to design the mind, but not the societal arrangement. The Death Noters start in the Stone Age with whatever psychology you specify. Can you specify a psychology such that a species of Death Noters with that psychology will eventually grow into a large-scale technological civilization?

              *****

              **EDIT:** I will also impose the additional restriction that whatever mind design you come up with must have comparable intelligence to humans. This is for the same reason as /u/Noumero's caveat: no FAI-style solutions.
              ```

              - u/Gurkenglas:
                ```
                Attempt at hard: Make a mind just like the human one, except that it doesn't want to use the death note.
                ```

              - u/Noumero:
                ```
                That seems way easier.

                A hivemind without sense of personal identity which considers other 'individuals' a part of itself, therefore incapable of using the power of killing at all.

                A more interesting one: humanlike minds with less intense emotions and fewer cognitive biases, designed to naturally develop enlightened self-interest and long-term thinking in early age; add sociopathy to the mix to make it more interesting. The ensuring society would be ridiculously cutthroat but I think functional.
                ```

                - u/696e6372656469626c65:
                  ```
                  > A hivemind without sense of personal identity which considers other 'individuals' a part of itself, therefore incapable of using the power of killing at all.

                  An actual hivemind is impossible given current technology levels, so I assume you're talking about a mind whose sense of empathy is so strong that it views other individuals as equivalent to itself despite not actually sharing their experiences and thoughts. How would such a species respond to scarcity? For example: suppose a food shortage occurs, and there's not enough food to ensure everyone lives. How would a hypothetical race of such people allocate their food supply? Distributing the food equally will simply cause everyone to die of malnutrition. (A sort of reverse [tragedy of the commons](https://en.wikipedia.org/wiki/Tragedy_of_the_commons), if you will.)

                  *****

                  **EDIT:** Never mind, randomization works (obviously). Don't know why I didn't think of it.
                  ```

              - u/vakusdrake:
                ```
                I mean this should be _really_ easy, just make people's minds such that everyone universally possesses certain qualities that you would like. For instance everybody invariably ending up with a moral system similar to your own, and nobody ever developing mental illnesses.          
                So nobody will ever want to use the death note except in scenarios you would consider acceptable, and nobody is ever deluded such that they believe it is acceptable to use the death note when it's not.             
                This is basically the equivalent of giving a GAI your ethical system instead of trying to place restrictions on its actions in hopes of preventing it from doing things you don't want.
                ```

  - u/Gurkenglas:
    ```
    I'm assuming that a message does not automatically identify its sender. If the set of all messages from some sender does not identify them, you can use that as a username.

    Attempt at 4: Let people form groups of up to about a hundred people each. Between groups, no identification-revealing contact is permitted. Within a group, there is zero privacy. If anyone dies by Death Note, everyone in their group is killed.

    You might relax the last bit by letting a judge review whether the killing was reasonable, such as a group killing a member that tried to take the group hostage. The judge would become able to kill them all, but he's already supposed to decide their fate.
    ```

    - u/Noumero:
      ```
      I'm pretty sure it ends with virtually every group dying within a few years.

      In general, humans are emotional, short-sighted and selfish. Sooner or later someone in any group will have a bad day, find someone to blame for this, and get the entire group killed; the larger the group, the sooner it happens. And making groups smaller would run into the same conspiracy problem as discussed above.
      ```

  - u/Noumero:
    ```
    New scenario: same as 4, except at any point in time, there's N 'normal' humans alive. Normal humans cannot kill with a thought and cannot *be* killed with a thought. If a normal human dies, a random deathnoter becomes normal, which both he/she and the other normal humans become magically aware of.

    What is the lowest N that would make a functional society possible?

    u/696e6372656469626c65, u/KilotonDefenestrator, u/Gurkenglas?
    ```

    - u/696e6372656469626c65:
      ```
      Okay, so this is where things start getting complicated. First of all, note that for higher values of N (say, exceeding 50% of the population), the Death Noters are likely to become normal themselves fairly quickly. So in order to not have our scenario quickly reduce to a society of normal people, we have to postulate a fairly low N.

      The question then becomes one of how Death Noters interact with "normals". A key thing to consider here is the fact that normals *cannot be killed via Death Note powers*. This means that a functional government is now possible, and is likely to be composed predominantly of normals. (The upper echelons will probably consists exclusively of normals, since having a Death Noter in a position of visible power is simply far too risky.) This then leads us to ask the obvious next question: *how large must a government (including enforcers) be in this situation to be effective?* An answer to this question will give us a minimum feasible value for N.

      I'll be needing to head out shortly, so I'll leave the analysis here for now. Anyone else is welcome to add to what I said.
      ```

---

