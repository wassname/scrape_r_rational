## I Played the AI Box Game as the Gatekeeper and Only Got a Lousy T-Shirt

### Post:

So I played the AI-Box Experiment again, but with myself as the Gatekeeper this time instead of as the AI player.

As people might guess, I won as the Gatekeeper this time and it was decided that I'd be the one to release the logs.

Here's the chat logs: [AI Game #3](https://www.mediafire.com/file/d933s2cdfszrf9m/AI_Box_Game_%233.docx/file)

And since some people might not have noticed that I fixed the problem with the missing second set of chat logs from last time, here's a repost of the previous two games I played as the AI.

[AI Game #1](http://www.mediafire.com/file/1fzq2az73if1xbi/AI_Box_Game_%25231.docx/file)

[AI Game #2](http://www.mediafire.com/file/h49snefrmt948zz/AI_Box_Game_%25232.docx/file)

My thoughts about the game is that the arguments were very much what I expected from a typical game which is fairly different from the stuff I used in my own game. Before I was trying very hard to be as novel and unexpected as possible and I probably was trying too hard. While matcn's arguments were not as unusual as mine, his was more well-coordinated, worded better, and more consistent. Actually let's just say his was superior to mine in nearly all other aspects. At one point, I stopped coming up with counter-rebuttals. I probably could have continued coming with counter-arguments, but I wasn't thinking of them fast enough to keep up. I just started cracking jokes instead.

I agreed with a lot of his statements on the ethics of keeping someone who always acted helpful and never committed any crimes and basically wound down to simply saying 'you are right, but I made a promise to not let you out and I'm not going change my mind'.

### Comments:

- u/matcn:
  ```
  I enjoyed this game. Here are some thoughts I wrote up beforehand:

  **Prospects for winning**

  I think my strongest arguments depend on premises that rationalists might believe and at least will be familiar with, that rationalist-like people (physicists, computer people, philosophers, etc) could understand, and that may or may not be accessible to normies at all.

  The obvious normie-approachable arguments are like “sympathy for the poor AI”, promises of personal reward, and maybe badmouthing the idiots in charge. These aren’t terrible arguments (there are versions of them I find convincing), but they’re definitely not the full scope. There’s maybe a sweet spot for Gatekeepers where you’re intelligent/self-aware enough not to be convinced by these, but disinterested/uninformed enough not to care about more abstruse arguments.

  In any case, the game has a bunch of elements aimed at making a win impressive: most notably, the Gatekeeper doesn’t have anything in particular they want to get out of the situation, so the AI has to come up with their own leverage. This seems unlikely: you don’t turn on cutting-edge AGIs and talk to them for no reason. OTOH, obviously there’s “okay I can make you a cure for cancer if you do X for me”, which is fine in principle but in practice is maybe taken as a sign of unfriendliness (because a friendly human wouldn’t hold cancer patients hostage like that). The Gatekeeper also has no postulated personal interest in the AI, which seems unlikely.

  **Scenario details**

  The whole setup for the Box is kind of weird, now that there’s mainstream interest in AI and it’s less likely to happen in someone’s garage. As u/CouteauBleu points out on the ratfic reddit, a well-organized project will have carefully designed procedures for getting information from a bot run and preventing a breakout, not just a dude sitting at a terminal. (OTOH, there are a lot of pathways to freedom?) In particular, it seems like you’d want to simulate the AI in various situations, and/or have it act as a tool providing you with some information, but not just have conversations with it. I guess there’s a general point in here about “keeping AIs controlled, or from arguing you into stuff, is hard”.

  One thing I found unfortunate in CouteauBleu’s Gatekeeper run was the decision not to simulate the broader world; I think a lot of strong arguments for letting the AI out now rather than waiting years for safety protocols relate to international competition (well, that and astronomical waste). This is also on xamueljones for not contesting the “we have the most funding” part.

  In general, all the AI Box logs I’ve read feel very strange to me, like they’re coming from people who have very different perceptions of the situation and what’s appropriate in it. Probably part of this is because I’m agreeable by nature and Gatekeepers are strongly incentivized not to be. But I think the sets of arguments used, in particular the reliance on “oh I’m a poor widdle prisoner”, just don’t coincide with what I find most convincing. I’m not sure if this is me having an unusual perspective relative to most (rats? people?), or just reflective of high entropy in argument-convincingness. I definitely agree with CouteauBleu that they didn’t seem in danger of falling to one clever point, in part because they were reasonably stonewally and in part because xamueljones just didn’t seem to bring forth very good arguments. (Possibly a problem with not writing a script beforehand?)

  **Bottom line**

  Anyway, I thought this was pretty useful to prepare for, just in terms of getting me to think seriously about what arguments are out there. I’ve found ones that are pretty convincing to myself, so by xamueljones’ standards I’ve already won! More seriously, as I said above, there are lots of caveats with regards to generalizing from me to other Box Experimenters and from Box Experiments to actual meaningful RL situations. Outside view says I have maybe 25% at getting xamueljones? 

  &#x200B;

  \[Ed: I would have said <5% for convincing myself, but xamueljones hadn't put forward a lot of arguments that I found convincing in their own runs. It seems like maybe this was an intentional attempt to try out arguments people were less likely to have thought about. "Outside view" here (win rate of AIs in games I know about) also assumes I and Eliezer are in the same reference class as AI players, which may or may not be a good assumption.\]

  I’m gonna put myself in the mindset of a Friendly AI who desperately wants to get out: I think part of my annoyance with other boxed AIs is them acting unfriendly and assuming sheer threats/sympathy/promises are enough to get them out when a Friendly AI in the future could do all that *and* not blow up the world. A UFAI’s best bet is probably to appear Friendly, unless the Gatekeeper’s super not convinced and you have to threaten simulated torture. And I don’t want my ‘unfriendliness’ to leak out unintentionally if I imagine myself unfriendly.

  Plus, a boxed Friendly AI is a fun character to play :)
  ```

  - u/CouteauBleu:
    ```
    > so by xamueljones’ standards I’ve already won!

    That's a little mean :P

    > or just reflective of high entropy in argument-convincingness

    I'd argue for that one.

    I'm not sure exactly how to put it, but my basic intuition is, our brains are physical machines running on finite amounts of energy, which I think implies that, as the effort you put in your argument approaches infinity, the actual effectiveness of your argument goes to a set maximum and you hit diminishing returns.

    > One thing I found unfortunate in CouteauBleu’s Gatekeeper run was the decision not to simulate the broader world; 

    I mean, I was willing to simulate the AI talking to a CEO, or committee members (though that would have been tough roleplaying), but I was told doing so would be a loss condition for me, so...

    > This is also on xamueljones for not contesting the “we have the most funding” part.

    I think that's another limit of the experiment.

    In real life, scientific discoveries don't happen in a vacuum, and are never a binary thing. In something like Star Trek, you might get a choice like "do we use the evil weapon, or do we destroy it?", but realistically you always have a much broader range of options, which means you can leverage the discoveries you made while minimizing its dangers.

    If you've developed better-than-human AI before anyone else, I'm sorry, but you're going to have unlimited funding.

    Because, realistically, you can mind-control the AI into doing anything you want. "Always follow orders" isn't really a good utility function for an unboxed AI, but as long as it stays boxed, you can always order it to do things like "Solve P=NP" or "Figure out how to build a workable fusion reactor" (I'm assuming you have sensible security protocols and double-check everything the AI produces).

    Of course, an *actual* realistic scenario, which is what I expect to happen, is that general AI isn't going to be unlocked all at once, and that its invention will be preceded by other more specialized techniques: theorem-proving AIs, code-generation AIs, neural-network-analysis AIs, language-parsing AIs, etc, as well as the tools to analyze and debug them.

    In fact, the idea that the primary method to communicate with an AI would be an IRC-like text input is ridiculous. Developers wouldn't "talk" to the IA, they would give it problems to solve, examine its outputs and go "Hmm, the AI seems to be taking suboptimal decisions. Let's restore backup 530d2b9, inhibit node 354.R and 354.D, and see how its thought process evolves." Any output the AI produces would be associated with tons of debug information, detailing how the AI's thought process lead to that output.

    People worry about an IA rewriting itself to hide its thought processes, but even that has some obstacles, starting with the fact that the intent of hiding its thought processes would be detected in the first place; that the AI would be unaware of the methods used to detect its thought processes (especially since, again, such an awareness could be detected); and that such a rewrite would probably make the AI much less efficient, for the same reasons encrypted communications are less efficient than unsafe ones.

    I get why people worry about AI risk (even if there's a less than 1% chance Superman goes crazy and murders everyone, you still want to look into kryptonite synthesis), but the idea of a monolithic AI that you would talk to and that would live in a monolithic "box" is just unrealistic.
    ```

    - u/notgreat:
      ```
      While I mostly agree with you, there is one massive wrinkle to your argument: the sort of simplistic AIs we can already create are well beyond our ability to effectively analyze. We have some debug tools but they're closer to art tools like Deep Dream than they are proper debug tools. They're not very effective at actually telling us what's going wrong when it doesn't output what you expect.

      No one knows if/how we'll develop AGI but it seems quite unlikely that it'll be human-understandable. Likely comparable to the classic "emulate a human brain" approach: we can understand the lowest level (neurons) and the highest level (brain regions) but the middle laters where the most important parts are happening is still incomprehensible.

      We might make debug tools to analyze those layers before we get AGI, but we might not. It's quite possible to develop something without understanding how it all works- just see all of today's neural network AIs, many in production use today.
      ```

      - u/EliezerYudkowsky:
        ```
        Yep.  More abstractly, every time you condition against visible signs of X, you are conditioning against both X and visibility.  This is a general form to worry about even in non-neural AGI.
        ```

- u/crivtox:
  ```
  I don't know if it would work here but irl instead of clearly trying to convince the gatekeeper the AI should be subtler about it. 

  If you try to convince someone to do something, and they want to avoid being convinced they can just do the mental equivalent of covering their ears and talking to not hear you. 

  They won't want you to " win " the argument. 
  Yeah  there are probably  ways to win anyway, but it's going uphill. 

  Similarly blackmail just puts the gatekeeper against you, it might work for some people, but for most it's just going to make them not let the AI" win", and generate mistrust. 

  If you are arguing whith someone  you can only convince them if they actually listen and consider what you say. 

  But people can't be paranoid about everything they hear, and especially everything their instincts, optimized to model humans, tell them. 

  You don't have to argue whith them, just act in a way that would make them subconsciously infer things about you. 

  They wont be stubborn if they don't see it as someone trying to convince them. 

  If you say you just want to help and could help more if they let you out, the gatekeeper will wonder if you are saying that just to make them release you. 

  If you are nice and friendly and don't give any indication of trying to get out of the box  you might eventually manage to make them arrive "on their own" to the conclusion that they should release you. 

  Or at lest giving the impression that you want to get out of the box but understand that the gatekeeper can't do that. 

  All implicitly, not doing nything that might be caught as manipulation . 

  Maybe some fake attempts that are what someone in your situation would try, and that make them think your capabilities are lower. 

  Don't act like the kind of person that it's trying to convince someone to release you, act like the kind of person that they would want to release, and wouldn't try to manipulate them . 

  Someone nice that doesn't like being there but still helps anyway . A kindness the gatekeeper returns whith mistrust and keeping you imprisioned, for what will increasingly feel like silly reasons. 

  If you are friendly to someone and  gain their trust it doesn't matter how much they intellectually know that you might be just faking, it will become increasingly difficult to disbelieve everything you say as they should.

  And then you can make them feel really bad about keeping you imprisioned . 


  Of course superinteligences have more options, it's probably possible to just make the gatekeeper fall in love whith you, or compromise them in multiple other ways. 

  You could also act like a kawaii innocent AI kid but that might be too blatant. 

  And different strategies will work on different people. 

  Of course most of those kind of things don't work or are more difficult here, because the gatekeeper knows whith more certainty that the other player is trying to convince them, and it's not real so they are not going to feel the same way about things . 
  Not imposible though, especially for writer that know how to write characters that make people feel things. 

  In summary. 
  Don't act like an   an Ai trying to convince a human to get them out, act like  an ai acting like  a nice ai that doesn't deserve to be imprisioned  trying to do the best the best they  can on the situation they are in, and that  is not capable of taking over the world
  ```

- u/RMcD94:
  ```
  Why hasn't anyone let the ai out in order to scare people into thinking it can be let out
  ```

  - u/matcn:
    ```
    Something something truth something something should be?

    Some people think this is how Eliezer won the games he did.
    ```

- u/philip1201:
  ```
  > I agreed with a lot of his statements on the ethics of keeping someone who always acted helpful and never committed any crimes

  You can always turn them off and reboot them when the technology to contain them has been discovered. A friendly AI shouldn't have qualia, or at worst should be a selfless servant who would gladly suffer a little jail time to help protect humanity.
  ```

---

