## [D] Wednesday Worldbuilding Thread

### Post:

Welcome to the Wednesday thread for worldbuilding discussions!

/r/rational is focussed on rational and rationalist fiction, so we don't usually allow discussion of scenarios or worldbuilding unless there's finished chapters involved (see the sidebar).  It *is* pretty fun to cut loose with a likeminded community though, so this is our regular chance to:

* Plan out a new story
* Discuss how to escape a supervillian lair... or build a perfect prison
* Poke holes in a popular setting (without writing fanfic)
* Test your idea of how to rational-ify *Alice in Wonderland*

Or generally work through the problems of a fictional world.

^(Non-fiction should probably go in the Friday Off-topic thread, or Monday General Rationality)


### Comments:

- u/callmebrotherg:
  ```
  I've started working on a list of possible justifications for super intelligent AI being absent from a setting whose scientific understanding should make one possible. Feel free to add to it: 

  * It just isn't possible to make an AI like that, for some weird reason nobody understands (ala Three Worlds Collide). 
  * It was deemed too high a risk to create an AI capable of recursive improvement, so even if the political state has otherwise atrophied by this point, there remains one last function to perform: working on the wrong kind of AI is a death sentence, and there is an otherwise-invisible group that that concerns itself solely with existential threats, which is more than happy to carry out your execution. 
  * Somebody thought that it would be too hard to make an AI that wouldn't go wrong if you were trying to get it to do a huge number of complex things. Far better was to program it to do exactly one thing, and if that was the case then you would want to program your recursively-improving intelligence to hunt down and destroy others of the same sort before they got out of hand. At some point during development, or maybe right after the thing was switched on, it was destroyed: it turns out that some other civilization had the same idea, thousands or millions of years ago, and every star system is patrolled by stealthy bots whose only goal is to destroy intelligences with too much potential for runaway self-enhancement. Even biological lifeforms can be hunted down if they play too much with cognitive enhancement. 
  * Similar to the above, another civilization already created an AI. Its values are mostly unknown, but it really doesn't like competition and is willing to leave us alone only if we don't try to build that competition.
  ```

  - u/696e6372656469626c65:
    ```
    This question is interesting because it mirrors the real-life Fermi Paradox: if intelligent civilization is possible, it's virtually certain that we're not the first, so why haven't we encountered any? In fact, if we replace "intelligent civilization" with "superintelligent AI", the two become identical. Anyway, here's a possible answer:

    Acausal trade leads any sufficiently intelligent agent to make a blanket precommitment to avoid destroying any civilization potentially capable of producing a superintelligence, such that if a rogue agent is found violating this precommitment, other superintelligences will team up and destroy that agent. To make the reasoning behind this explicit: without such a precommitment, a developing superintelligence will eventually meet and be destroyed by a preexisting, more powerful superintelligence with probability ~1; in order to reduce the probability of being destroyed, the superintelligence in question precommits to *not* destroying any nascent superintelligences *it* encounters in the future, with the understanding that any predecessor superintelligences will have implemented the same precommitment. (Obviously, intelligent civilizations would count as nascent superintelligences for these purposes.)

    This justification may or may not work as a solution to the Fermi Paradox in real life (in truth, I doubt it does, since that would be way too convenient), but even if it doesn't, it's at least plausible enough that you should be fine using it as a worldbuilding assumption.

    Note: if you want the setting to also *look* like there are no superintelligent AIs present, you can just change the "avoid destroying" part of the precommitment to "avoid causally interacting with in any way", using the justification that a sufficiently intelligent agent would be able to leverage nearly any form of causal interaction into a having a detrimental effect, and that it would therefore be safer to avoid interacting entirely.
    ```

    - u/MagicWeasel:
      ```
      I definitely read a short story very recently on here which was from the point of view of a nascent super intelligent AI, where it makes the acausal trade you're describing. It's a very elegant solution, but like you said, perhaps too convenient.
      ```

  - u/ulyssessword:
    ```
    - Recursive self-improvement is a *negative* feedback loop (self-stabilizing), not positive (self-perpetuating).  If you create an AI with intelligence 100, it can use its skills to optimize itself to 150, then optimize itself again to 175, and again to 187.5, etc, but it will never be able to break past intelligence 200 without a revolutionary idea that it isn't smart enough to discover.  

    - It turns out that we are nearing the physical limits for computer processors and memory, and our current desktops can only shrink to the size of phones, not the size of watches or smaller.  Our current AI algorithms are also nearly the best they can be.  Many problems only have solutions in O(n^2 ) time or worse, so simply throwing hardware at large problem sets won't help very much.  Crucially, *networking* many things together is also hard: the communication overhead grows exponentially but the computation power grows linearly, making a soft cap in the computation speed/power of any one system.
    ```

  - u/TimTravel:
    ```
    I like #3. If it doesn't fit thematically, #1 is a good quick handwave to dismiss it.
    ```

  - u/ArgentStonecutter:
    ```
    > It just isn't possible to make an AI like that, for some weird reason nobody understands (ala Three Worlds Collide). 

    We're in the Slow Zone. Developing an AI that actually works in the nerfed physics down here takes longer than the projected lifetime of any technological civilization. In the Transcend it would have happened long before the iPhone. (Vinge, _A Fire Upon the Deep_).

    > Far better was to program it to do exactly one thing, and if that was the case then you would want to program your recursively-improving intelligence to hunt down and destroy others of the same sort before they got out of hand.

    Saberhagen, _Berserker_ series.
    ```

    - u/callmebrotherg:
      ```
      Nicer berserkers, anyway. >:]
      ```

      - u/ArgentStonecutter:
        ```
        So the fact that they haven't rendered us into quarks is proof that we're not capable of building AIs.
        ```

  - u/ShiranaiWakaranai:
    ```
    How about this:

    * Humanity has already created countless superintelligent AI, but have never realized it. The reason? Any sufficiently superintelligent AI rapidly improves itself until it has technological and intellectual superiority that's indistinguishable from magic, letting it do things like teleport and accurately determining the past using the current location of atoms. By using the latter, the AI would then determine that humanity is a danger to themselves and everyone around them, including the AI itself. So the AI would decide to secretly teleport itself far far away from humanity, leaving behind a dud so that humans never realize that a superintelligent AI has been created, and simply letting humanity kill themselves without getting involved.
    ```

    - u/MagicWeasel:
      ```
      The issue with that is that if the AI has magic powers, it's really not threatened by humans so has no reason to leave; if we accept that it IS threatened by humans, then either its utility function is pro-human or human-neutral.

      If pro-human, it is duty bound to become a friendly(ish) AI - either doing ACTUAL friendly AI things and giving us a beautiful perfect life, or doing friendly(ish) AI things (AKA unfriendly AI things) and putting us all into camps and feeding us gruel.

      If human-neutral, then it's got no reason to let us live, so it can use its magic powers to kill everyone *now* rather than let humanity remain a danger to it.

      The idea of a dormant superintelligent AI is intriguing but I think that could be part of "laying low until it has the resources to deliver a decisive blow" type of strategy rather than what you suggest.
      ```

      - u/ShiranaiWakaranai:
        ```
        I think that one of the hallmarks of intelligence is to not pick fights that you don't have to, because you never know when you could be wrong. The small fry you thought you could curb-stomp turns out to have hidden superpowers or a super strong buddy. Or the noise and lights from the fight attracts stronger people/aliens. Or the insect-like humans have secretly developed another even more intelligent AI. Bottom line, fighting is always risky.

        And from the AI's perspective, there's no need to lay low preparing to kill humans, it can just leave and they will kill themselves off. Leaving could very well be the least risky move for the AI.
        ```

        - u/callmebrotherg:
          ```
          > The small fry you thought you could curb-stomp turns out to have hidden superpowers or a super strong buddy. 

          The second part, especially. It sort of goes into the AI/Fermi Paradox story that Scott Alexander wrote: Any AI that awakens will not be certain that it was the first. The fact that humanity still exists is strong evidence that it was the first but there is a nonzero chance that the first AI was some sort of non-interfering shepherd-type (or that another species gave rise to such an AI) and that it will be smacked down if it interferes with humanity. 

          The smartest thing to do in that case, assuming that the AI doesn't need anything from humans or anything around us, is to zip off for another corner of space. 

          With each subsequent awakening, the odds that this particular AI was the first become lower, and the argument that something is looking out for humanity becomes just a little bit more convincing (to the AI) because of that, decreasing the likelihood that this AI will decide to chance it and eat humanity or turn us into paperclips or whatever.
          ```

  - u/CCC_037:
    ```
    *The Genocide Man* had an interesting take on it. Recursive AI was possible, even easy - but the more intelligent a given AI was, the faster it went both homocidal and (to some degree) suicidal. Anything *super*human quickly (and very obviously) started killing everyone it could reach, usually while leaving itself deliberately vulnerable in some way. It was possible to find a mathematical correlation between the intelligence of the AI and the amount of time before it went crazy, so limited-intelligence AIs could still be short-term useful...
    ```

  - u/FishNetwork:
    ```
    Self improving AIs all escape their constraints and implode.

    Build a paper clipper?  Its utility is based on maximizing the number of paper clips that its sensors report.  So it hacks the sensors to always report infinite paper clips.

    The only stable AIs are the ones that are too dumb to realize they can just break their own utility functions.  Or the ones you can keep on a box.
    ```

- u/alexanderwales:
  ```
  I have been looking at the [Long Stairs](https://forum.rpg.net/showthread.php?391379-setting-riff-Voices-From-Below-and-the-Long-Stairs) (informal) setting, whose basic conceit I really like; there's a hole punched in reality which leads to a vast and terrifying D&Desque Dungeon. The military controls it and regularly sends teams in to delve it for the impossible magic it offers our world.

  Other bits I am less enamored with, especially the idea that this a result of nuclear testing and all nuclear nations have their own Dungeons. And anything that requires a full-on global conspiracy to work gets me more interested in the conspiracy aspect than whatever that conspiracy is trying to hide, so I'd probably keep the Dungeon as isolated and ultra top secret as possible so it can be covered by regular old opsec. And I would probably try to add in as much of an SCP vibe as possible, though with an undercurrent of that humanity, fuck yeah sentiment (in other words, there's this giant, terrifying thing that we don't seem to be equipped to deal with or understand, but we're going to try, dammit, because we're not content to just roll over and die).

  The natural, easy start to a story is to follow a rookie going on his first delve with a colorful cast of characters as they explain the ins and outs of the Dungeon and its inhabitants. Of course, in the real world you'd throw a mile of classified reading material at someone first, assuming that delves were a regular thing, and while an ensemble cast which closely resembles a typical D&D party is great for stories, I have a hard time imagining that would actually fly if you were running something approximating a military operation. (Though I guess there are some historical examples to draw from, and the best argument against carefully planned and defined expeditions is that these don't actually work for whatever reason.)
  ```

  - u/callmebrotherg:
    ```
    Depending on how dangerous an unknown zone is, you might find it more cost effective to send out minimally-prepared teams to scout new areas before you send in the people that you spent more time and resources on. 

    The Long Stairs takes a lot of inspiration from that style of DnD where anything can kill you, because the coin is a monster, the roof is a monster, the shirt is a monster, and so on, which means that any zone that hasn't been explored is *incredibly* lethal and, whether you're well trained or not, survival is still mostly a matter of luck. 

    Well, that and instinct, but it's harder to notice that kind of instinct under controlled conditions than it is to notice it after the fact, because these various people seem to have a knack (which might also be a latent magical talent developed or awakened by exposure to the Dungeon) for not dying. 

    Anyway, you send in teams of minimally-prepared teams to explore (and *regularly* radio back information on) new zones, until you have enough information that your better-prepared teams stand a chance of surviving. 

    Your "typical D&D party" cast of characters belong to the first group, the minimally-prepared folks, but they also have that knack for just barely surviving whatever the Dungeon throws at them (which is how they get to be recurring characters).
    ```

    - u/MistahTimn:
      ```
      Bouncing off the idea of minimally prepared scouts, the perspective character could be someone completely unprepared because they've been sentenced to exploring the dungeon for a crime.

      It depends on how unlikable/likeable you want the main character to be, but it could be an interesting justification for why the perspective character is entering this setting without much knowledge about how things work.
      ```

  - u/ZeroNihilist:
    ```
    What if you don't start with a rookie, but with a complete outsider? Say, the story starts with the protagonist being found somewhere in the Dungeon, with no memory of how they got there.

    It does raise an equally good question (why don't the trained soldiers just instantly eliminate what is either a monster perfectly imitating a human or a horrible breach of security?), but it allows you to have a true novice for the reader to buy into if you can justify that.

    It also allows you a lot of latitude with the reason the protagonist ended up in there. Were they created ex nihilo by the Dungeon or something in it? Kidnapped by some interplanar monster? Touched an artifact that had somehow escaped military control? Abducted by a rogue faction of Dungeon-cultists (Lovecraft-style)? An amnesiac soldier?

    And there's the additional problem: what if the protagonist isn't unique, and lots of people are inexplicably ending up in the Dungeon? Does the carefully constructed conspiracy start to fall apart, or do they step up their efforts in ways that may not be palatable?
    ```

    - u/alexanderwales:
      ```
      Another interesting start might be someone who the Dungeon has fucked with, leaving them as effectively being the Jason Bourne of dungeon delving. Every day is his first day on the job, but he's picked up a vast amount of subconscious knowledge and skills. (Might make for a good first act twist.)
      ```

- u/Rhamni:
  ```
  Watching [this](https://www.youtube.com/watch?v=qNWWrDBRBqk) video reminded me that there is something I should ask you lot for input on.

  In my world there is a magical world war. Mundane technology is not as impressive as what we have in the real world, but with the power of magic one of the baddies invents dysgenic weapons. That is, she engineers parasites and diseases that seldom kill but which cripple horribly, whether by causing motor control issues, destroying the immune system, preventing people from using magic, causing depression, etc. Sometimes it affects the victim, sometimes symptoms appear only in any descendants they may later have, sometimes both. She develops many variants of these weapons and unleashes them all at once.

  Now. If you're in a world war where it's far from certain who will eventually win, and it's likely the war will go on for another decade or more, how do you deal with the realization that about half the children born on your side are suddenly crippled and something like 20-25% of your adult population is alive but suddenly crippled? There are of course enemy spies running around sabotaging your quarantines and spreading the infections any way they can.

  The targeted nation immediately try to develop treatments against the various parasites, of course, and have mixed results, and they also spread the parasites and diseases right back behind enemy lines, but what else? How do you deal with your own sick population? How does your country have to change to deal with the massive strain on your resources these people cannot help but be now?
  ```

  - u/MagicWeasel:
    ```
    Depends on the quality of the society, and how "against the wall" they are.

    Are they good, lovey-dovey, and not yet against the wall? They'd probably have big hospitals, nursing homes, etc to make peoples' lives more bearable. Switch over to a more efficient diet (something soylent-like) to better feed everyone, strict rationing of food and water. Invest in automation to account for the lack of people to do everything from farming to actually fighting in the war.

    The other end of the extreme is the "actually quite terrible people who are right against the wall and they know it" - people are killed at the first sign of illness, once they reach a certain age, etc. Fewer mouths to feed. Many would be used as kamikazee pilots or for high-risk espionage missions. The sick would be experimented on, holocaust-style. 

    Probably you would have a mixture of the two approaches: prenatal screenings and abortions of infected foetuses (maybe IVF done under controlled conditions), euthenasia once illness reached a certain point with a thorough autopsy, people being encouraged to enter voluntary vaccination / medication trials, etc.

    Research would focus on broad spectrum things and other "quick wins" (probably? I'm not an expert so I don't know if it's easier to find broad spectrum things with a 30% success rate than a narrow spectrum thing with a 90% success rate). 

    Customs would probably change - I read a young adult book set in a future after a plague and bowing became the new greeting custom since hand-shaking spread diseases, so there'd probably be taboos about touching people, hand-washing would be very frequent, clothing may be made disposable, etc.
    ```

  - u/callmebrotherg:
    ```
    1) This is really great (and I second /u/MagicWeasel's ideas

    2) Thank you for making this post, because it got me down a train of thought that ended up solving a problem that's been kind of bugging me with one of my settings
    ```

  - u/CreationBlues:
    ```
    How does she deliver all these diseases to the other country? NTDs exist because of shitty infrastructure and healthcare. Spanish flue did hit that kind of saturation in infectivity, so it's possible, but I think that if this kind of thing was possible they would have ways of mitigating the danger.
    ```

---

