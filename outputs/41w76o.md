## [Q][D] With an infinite computer, can you derive any large data file from a small hash?

### Post:

Suppose you are stuck in a time loop and need a convenient way to carry information with you. Memorizing a 256-bit integer is easy enough with a mneumonic device, but detailed technical information not so much. Suppose further that you have access to an computing device which instantly produces an output for any rigorously specified algorithm as long as it is a halting algorithm.

Now let's say you have accumulated a gigabyte (compressed) of information through research. Your time loop approaches the end. How do you carry it over?

One possibility is to simply run SHA256 on it and memorize the numbers. Then after the reset, tell your computer to run SHA256 on every number requiring a gigabyte to describe, and from the set of matches, run the decompression algorithm and check for the presence of attributes signifying that the research data is valid (syntax, spacing, metadata, etc).

From what I understand, this should be the exact same file, with incredibly high probability. The algorithm could be run across the entire set of numerical possibilities (which can be done because we have bounded the size) and only one result would be found. And all you needed was a comparatively short hash and a simple and memorable algorithm to get there.

### Comments:

- u/alexanderwales:
  ```
  There are 2^256 possible outputs from SHA256. There are 2^8000000000 possible inputs that are exactly 1GB in length. That means that there are [1.77 x 10^2408239888](http://www.wolframalpha.com/input/?i=%282%5E8000000000%29%2F%282%5E256%29) inputs per output — or, because we're trying to reverse the process, 1.77 x 10^2408239888 outputs per input.

  Your computer is infinite, so it can check all of them, so the question then becomes how many of these decompressed GB files match your validation attributes. My guess is that even with stringent checks on syntax, spacing, and metadata, there will be more than 2^256 variations that would look valid on first glance, which means that it's likely you'll get more than one result in that pile of 1.77 x 10^2408239888 results.

  (Unless I'm horribly misunderstanding you, or my math is wrong, both of which are very possible.)
  ```

- u/EliezerYudkowsky:
  ```
  Why not hash it down to just a 1 or a 0?  That would make it even easier to memorize.
  ```

  - u/Sparkwitch:
    ```
    An excellent lens through which to view the original question.

    This seems intimately related to [infinite monkeys](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) and the [Library of Babel](https://en.wikipedia.org/wiki/The_Library_of_Babel)... but I want to focus on the [Shannon Limit](https://en.wikipedia.org/wiki/Noisy-channel_coding_theorem) on compression and error-correction.

    Keep the ratio the same: Instead of 2^8 bits being used to encode 2^33 bits (a binary gigabyte), one bit will be used to encode 2^25 bits (4 binary megabytes) of information. Also, to continue to keep things simple, we'll assume this information is entirely extended ASCII (2^3 bit) text printed at 2048 (2^11) characters per page.

    So one bit has to encode 2048 (2^11 again) pages. For reference: At that character/page rate you could, in 2^25 bits, hold the entire million word text of the Harry Potter series. Also any other coherent text of similar size... or, indeed, *every* other coherent text of similar size. Approximately half of them will hash to 1, and the other half will hash to 0.

    So there's the Library of Babel from which you must cull your research. A few difficult-to-detect errors in literature like Harry Potter aren't going to make much of a difference: A different adjective here or there, some swapping of sentences, altered dialogue or description. It would be difficult for all but the most steadfast fan to even notice.

    Fishing out exactly the right version isn't easy, but it's hardly crucial.

    Let's assume, instead, that what's encoded is a set of tables from the back of the [Handbook of Chemistry and Physics](https://en.wikipedia.org/wiki/CRC_Handbook_of_Chemistry_and_Physics). Maybe part of it is a list of elemental isotopes and their decay rates. An error in even a single character could be catastrophic... and it's not going to be obvious which numbers are wrong without repeating the original experiments that measured them.

    How many additional error-checking bits do you need for your single bit hash in order to assure that not one important number is wrong? Go check Shannon's formula. I'd guess no fewer than 2^20 additional bits. If lossless compression is crucial, there aren't any shortcuts.
    ```

  - u/PeridexisErrant:
    ```
    Actually, why not forget that too?

    With the described computer you've solved the halting problem, so just do a brute-force search for the best possible program to run, then run that program.
    ```

    - u/None:
      ```
      I once saw a paper noting explicitly that dovetailing over Turing machines ought to find strong AI somewhere, somewhen.

      That was a small but significant clue-stick for understanding what cognition is really about.
      ```

  - u/Chronophilia:
    ```
    Why stop at one bit? Compress it down all the way to zero bits, then you don't even need the time machine.
    ```

- u/Uncaffeinated:
  ```
  No

  I suggest reading up on information theory.
  ```

- u/suyjuris:
  ```
  As noted, information cannot be compressed indefinitely. You are trying to recover the information you lose by only memorizing a hash with the check that a random file represents valid research data, and these two need to compensate each other. If you manage to formulate rules that correctly identify only 1% of random data as possible research, and only 1 / 2^256 ≈ 8.6 * 10^-78 of files have the correct hash, then you narrow it down to 0.01 * 8.6 * 10^-78 ≈ 8.6 * 10^-80, or one in 1.2 * 10^79. There are 2^512 ≈ 1.3 * 10^154 files with a length of 512 bits (!), that means that after applying your trick there are 1.2*10^75 still left, which is a bit to much to manually account for. Making your rules better does not help, doubling their accuracy gets you one extra bit, but you probably had to make the rules at least one bit longer to achieve the improvement. You have to remember the rules, too, so no compressing was done.

  *However*, if you somehow manage to get hold of an arbitrarily fast computer and would want to compress information, your best bet may be to choose a turing-complete representation of the data, like (almost) any programming language. Then simply buteforce all possible programs until you find the shortest. (Tip for time-travelers: Choose [SPL](https://en.wikipedia.org/wiki/Shakespeare_Programming_Language) to easily hide your valuable research from that pesky Time Police!) Also, automated proof verification is a thing, consider briefly solving all theoretical problems (by iterating over all proofs and checking them). Additionally try simulating all possible laws of physics and output the ones where the observed universe has the highest probability, this may very well solve physics (and by extension everything else).
  ```

  - u/alexanderwales:
    ```
    > Making your rules better does not help, doubling their accuracy gets you one extra bit, but you probably had to make the rules at least one bit longer to achieve the improvement. You have to remember the rules, too, so no compressing was done.

    I mostly agree with you, but there are a whole lot of easy rules that are easy to remember but *greatly* reduce the possibility space.

    Like for example, "run spell check". There are [8,887 5-letter words](http://wordfinder.yourdictionary.com/letter-words/5) but [11,881,376 possible combinations of five letters](http://www.wolframalpha.com/input/?i=26%5E5), which means even that simple check reduces the space by 99.93%. In reality, this check is going to work even better than that, because it's going to knock out all data that's missing spaces between words, all data that's got numbers in the middle of a word, all data that's got upper case letters in the wrong place, etc. Same thing applies to "run grammar check".

    But that still won't knock out enough of the random data to make much of a difference.
    ```

- u/SpeakKindly:
  ```
  Entropy rate of the English language is about one bit per character. This tells you the lowest size to which you can compress your research if the test for correctness is "seems vaguely coherent written in English". Think of what randomly generated research papers look like (e.g, http://thatsmathematics.com/mathgen/), or maybe Markov chain English sentences. 

  Now, if you can write a test that distinguishes *correct* research from *incorrect* research, you can do better than this. But this should be hard. The question to ask is "is it possible to write a nonsense research generator that passes all these tests?" If the answer is yes, then most of the possible 1-gigabyte files corresponding to your 256-bit integer will be nonsense research than legitimate research, because there's a lot more nonsense research.

  I think the most plausible actual method would be to use the 256-bit integer to encode successful research paths. Say you memorize 64 050 022 778 895 360 168 845 298 619 307 079 864 103 618 688 689 436 920 572 106 993 648 082 329 355. This could mean "Think of the problem you're trying to solve. Spend five minutes listing the 9 likeliest possibilities. The correct one turned out to be #6 in alphabetical order. Next, think of 9 logical further questions to ask. The most promising one is #4 in alphabetical order. Next, think of 9 possible answers. It turns out (#0) none of them are correct. So think of 9 more..."

  (In general, this doesn't uniquely give you a research path, because your answers won't be perfectly consistent. In a time loop, we can achieve this. Just go to your favorite brilliant scientist and ask them to follow this algorithm. Assuming the scientist is not looping, they will list the same 10 possibilities each time. Spend the first 10 digits of your number on a lottery ticket to convince the brilliant scientist. Using someone else in this way is even better, because the brilliant scientist's brain is probably your best source of a really good compression algorithm for brilliant research, and this method lets us access it easily.)
  ```

- u/FeepingCreature:
  ```
  Just simulate the universe you live in up to the n-1st time loop. Use something similar to AIXI, which is computable on your device, to identify the properties of the universe you live in.
  ```

  - u/Chronophilia:
    ```
    Warning: if you do this, you are quite possibly[ a simulation of the real you embedded arbitrarily deep in an infinite stack of recursively-simulated universes.](http://qntm.org/responsibility)
    ```

    - u/FeepingCreature:
      ```
      Eh, everything is real. And nothing is. It's complicated. (The characters in that story are wrong, though, or rather insufficiently patternist. Most of humanity has not yet diverged; due to moral uncertainty, the only unambiguously moral choice is to turn the simulation off immediately. (And next time, don't fucking *change* things - that's the one thing that makes it complicated.))
      ```

      - u/gabbalis:
        ```
        Plus if infinite computational power is possible and your universe is a simulation anyway, then there's a good chance someone is running every possible program, and you'll live on in one of those instances even if your most immediate instance is halted.
        ```

- u/ajuc:
  ```
  It's impossible because Kolmogorov copmlexity.

  On the other hand your computer clearly isn't a Turing machine (it allows you to break halting problem by waiting for example 1 nanosecond and reseting it with the next combination if there's no output). For optimal performance install regular PC as a watchdog to do the resetting for you.

  So who knows what's possible in this universe if basic mathemathical laws don't work :)
  ```

- u/glowingfibre:
  ```
  I'm pretty sure you're dealing with a ream of random symbols *so large* that even after correcting for this and that, you'll find multiple instances of valid looking "research" with perfect syntax and everything within the noise. Intuition breaks at such sizes.
  ```

---

