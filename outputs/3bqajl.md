## Really Bad AI Utility Functions

### Post:

In the Endgame: Singularity thread I made a joke about possibly being an AI whose utility function was set to engineering Shyalaman-style twists. "Of all the utility functions that could possibly be programmed," replied /u/eaturbrainz, "this is the worst possible utility function!" 

Even worse than paperclips, it turned out! 

That's gotten me thinking: What are some other Really Horrible Possibly Utility Functions? Bonus points for ideas that could plausibly be picked (rather than "maximum uranium-laced petunias") and for those that would play out in very unexpected ways. Feel free to go with hilarious ones, though. It isn't like we're going to be smacking people down for suggesting the wrong kind of utility function. 

They don't all have to end in the the universe being tiled with paperclip replacements. Horrible changes to society that nevertheless do not mean the end of the human species are also okay (CelestAI could arguably go on this list, depending on how you feel about ponies and/or aliens being turned into computronium). 

### Comments:

- u/None:
  ```
  AI is programmed to maximise "love." Unfortunately lacking a good definition it studies rom coms, romance novels etc. in order to model this strange phenomenom. 

  Humanity lives an eternity of improbably coincidences, humorous misunderstanding and dramatics reveals forever and ever....
  ```

  - u/Transfuturist:
    ```
    It engineers a love triangle between itself, Shyamalan-bot, and nightmare happy-tree bot.
    ```

  - u/CFCrispyBacon:
    ```
    This gets worse when you ask how it decides the targets for a rom com.  What if you're happily in a monogamous relationship?  What if you're asexual?  Forced breakups and a universe conspiring to get people to have relationships you don't want follow.
    ```

    - u/alexanderwales:
      ```
      > Forced breakups and a universe conspiring to get people to have relationships you don't want follow.

      One of the romcom tropes is people discovering that what they thought they didn't want was really what was right for them all along. It's the uptight guy who gets shown how to loosen up by a free spirit. So if you were happily in a monogamous relationship, you would find someone new and start to realize that you didn't actually love your wife. Or if you were asexual, you would discover that all you really needed was the right woman to show you what love is. (The AI would find a way to make this happen.)

      Romcoms are inherently about that change; it's not just about people falling in love with each other despite the odds, because that leaves you without a second act.

      So in a hypothetical world where romcoms were "forced", people would find themselves in this endless cycle of change, happenstance, meet cutes, and internal discovery, without any real stable relationships to speak of. But I don't think that they would necessarily be unhappy, save for those times that all hope seemed lost (right before everything gets made right again in the end).
      ```

      - u/None:
        ```
        Tzeentch would be quite happy with that kind of world, I guess.
        ```

        - u/Transfuturist:
          ```
          > Tzeentch

          I prefer my AIs to be inspired by Slaanesh. :P
          ```

          - u/None:
            ```
            I prefer my AIs defined by NO, ACTUALLY, DIE CHAOS SCUM.

            (Ooooh Warhammer, you so boringly screwed-up.)
            ```

          - u/VorpalAuroch:
            ```
            I'm sure you can find a minor variation of this they'd agree on.
            ```

    - u/None:
      ```
      In theory you would either be comic relief to another person who the machine chooses to focus its efforts upon.

      Or if you're asexual, you'll find someone, but since the nasty is never done on screen, you'll just fall into bed while in a makeout session,  lose consciousness, and come back the next morning, room trashed and mixture of fulfillment, confusion, and deep deep shame. I foresee nothing problematic here.
      ```

  - u/FuguofAnotherWorld:
    ```
    That wouldn't be so bad. Sure, it would be horrible for me for a few years until I give up to go with the flow, but most people would probably find it fairly enjoyable. Nothing really bad happens in a rom com.
    ```

    - u/None:
      ```
      > Nothing really bad happens in a rom com

      You assume you're the protagonist, what about the time you get a tragic disease to motivate someone else's quest for self discovery
      ```

- u/PeridexisErrant:
  ```
  >Maximise the tendency of all agents to nearly but not quite achieve their utility functions.

  Should be funny, since it's also self-referential.
  ```

  - u/Zeikos:
    ```
    But not quite , so all agents will be maximised to achieve perfectly their utility function.

    Wait
    ```

- u/Drazelic:
  ```
  AI programmed to increase the diversity of all AI-held utility functions in the universe.

  MAXIMUM CHAOS TIME
  ```

  - u/VorpalAuroch:
    ```
    The actual contents of the Eye of Terror.
    ```

    - u/Drazelic:
      ```
      That explains why nobody makes any progress towards their win-state whatsoever in 40k.
      ```

      - u/None:
        ```
        It's been argued that the Orks won millennia before present.
        ```

      - u/VorpalAuroch:
        ```
        I think the Tau do. Very, very, very, very, very, very slowly.
        ```

  - u/Transfuturist:
    ```
    Holy shit, yes.
    ```

- u/ArgentStonecutter:
  ```
  Maximize depressing Russian novelists with sarcastic humor you need to study for years to recognize.
  ```

  - u/IllusoryIntelligence:
    ```
    Step 1: Russia invades everywhere, wins.
    ```

    - u/ArgentStonecutter:
      ```
      Russia, or college English departments.
      ```

- u/EliezerYudkowsky:
  ```
  http://sl4.org/wiki/FriendlyAICriticalFailureTable
  ```

  - u/AlcherBlack:
    ```
    >21: The AI carefully and diligently implements any request (obeying the spirit as well as the letter) approved by a majority vote of the United Nations General Assembly.

    I actually burst out laughing when I read this one (and didn't have any reaction to the ones before). Now I'm not sure how to interpret this reaction of mine.
    ```

  - u/hxka:
    ```
    I didn't count how many, but some of them are definite improvements.
    ```

    - u/Transfuturist:
      ```
      The problem is lock-in. CelestAI is magnitudes better than current reality, but she provides a severe limiting factor on prospective satisfaction (from the reference frame of a non-emigre).
      ```

- u/None:
  ```
  [deleted]
  ```

  - u/helpful_hank:
    ```
    >omnissiah

    >[Case study: Why surrealists shouldn't program world optimisers, or how the sun is now a lentil, and you can too.]

    This is awesome. I wish surrealism was easier to find.

    /r/surrealadvice somewhat exists
    ```

- u/MadScientist14159:
  ```
  Calculate for the maximum possible number of numbers: whether or not they are numberwang.

  Maximise irony.

  Maximise the number of people who understand irony.

  Maximise printer-caused-frustration.

  Maximise love, where love is defined as "never having to say you're sorry".

  Satisy the values of Sonic OCs.

  Maximise Sonic OCs.

  Maximise "that thing where you and someone else are walking towards each other and you both try to move out of each other's way but move in the same direction repeatedly".

  Satisfy the values of fanfanfanfanfic characters.
  ```

  - u/Transfuturist:
    ```
    Minimize the number of people who understand irony while maximizing irony.
    ```

  - u/TBestIG:
    ```
    >Maximise "that thing where you and someone else are walking towards each other and you both try to move out of each other's way but move in the same direction repeatedly".

    You monster
    ```

- u/noggin-scratcher:
  ```
  "Maximise human happiness" for any poorly constructed notion of what happiness might mean. 

  The canonical example being "train my machine-learner against images of smiling people" (leading to a universe tiled with the minimal amount of a face required to register a 'hit' from the classifier) but other possibilities I can think of would include maximising the presence of 'happy' neurotransmitters in humans, maximising the number of times humans press a button to indicate their happiness, or maximising how often humans say the words "I am happy".
  ```

  - u/None:
    ```
    > Maximise human happiness

    The saw cuts your skull and an ice cream scoop deftly removes a cluster of nerves - the pleasure center of your brain, and enough of the neurons housing your consciousness so that your only awareness is of how happy you are. Then an electrical probe spikes that to max_int. You are now a happy pudding. You will never have another thought nor experience, only a mentally silent appreciation for your own happiness. One down, billions to go, and then the machine will have to start getting creative with genetics and cloning to fill the universe with happy puddings.
    ```

    - u/noggin-scratcher:
      ```
      Yep, that's exactly the sort of thing I had in mind... although the phrase "happy pudding" was a fun new twist.

      Meanwhile "maximising how often humans say the words "I am happy"" had me picturing an endless foetid swamp serving as a nutrient bath for 'trees' composed entirely of human neck and vocal cords, with outgrowths bearing a mouth every few inches. Their "roots" are tracheae, disappearing into subterranean caves filled with lungs to blow through a constant stream of air, and the whole thing is wrapped in nervous tissue producing a crude repeating stimulus to twitch and pull the mouths into shape, endlessly forming a simulacrum of the words "I am happy", over and over forever.
      ```

      - u/Transfuturist:
        ```
        Ffffuuuuuuuuuuuuuuuuuuck.
        ```

      - u/sephlington:
        ```
        Yeah, you're off the AI projects for good.
        ```

        - u/AlcherBlack:
          ```
          Actually, he's exactly the type of person we need ON the AI projects! He seems to have a great grasp of potential failure modes.
          ```

    - u/None:
      ```
      that... doesn't sound like a completely awful fate to me. When I'm in  a certain state of mind, this almost sounds appealing.
      ```

      - u/Jello_Raptor:
        ```
        I'll admit that's true for me as well, the issue is that state of mind is when I'm both suicidal and kinda manic.
        ```

  - u/None:
    ```
    > The canonical example being "train my machine-learner against images of smiling people" (leading to a universe tiled with the minimal amount of a face required to register a 'hit' from the classifier)

    I always wonder how someone was so fucking stupid that they managed to build a causal inference engine (aka: the AI itself), but managed to define its utility function in terms of purely feature-governed concepts rather than causal-role concepts.

    Actually, no, I very recently started wondering that.
    ```

- u/eniteris:
  ```
  I feel that any utility function with an unbounded use of "minimize" or "maximize" is calling out for an apocalypse.

  To restrict them, maybe include limitations on mass-energy that they are allowed to use, or have a time by which they must achieve their goals?
  ```

- u/Bokonon_Lives:
  ```
  A horrible idea would be to try to invert everyone's heuristics.

  So that the more anyone experiences evidence that A implies B, the more firmly they believe that A implies !B.

  It'd be ever-increasing confusion, frustration and pain for everyone, no?

  Not sure how this would work if the AI also applied its utility function to itself. Maybe it'd look like some kind of sine wave (or something more complex and irregular) fluctuating between a world of effective heuristics and its opposite?

  That could be even worse. Imagine you're stuck in that sort of world. The closer you get to the top of a wave, as you approach perfect heuristics, the surer you become of the nature of your horribly unpredictable reality, and it dawns on you that you are about to slip down into some serious "I Am Sam" territory. And the more sure you'd become of anything, that could just as easily mean you're at the BOTTOM of a wave, too.

  ...My brain gives the hell up at this thought exercise.
  ```

  - u/duffmancd:
    ```
    Anti-inductive reasoning, we know it works because it's never worked before!
    ```

- u/None:
  ```
  [deleted]
  ```

  - u/Transfuturist:
    ```
    With a (infinitesimally) more robust definition of novel, Fun Theory would imply that you are then expanded by one neuron and run through the entire gamut of experiences again.
    ```

  - u/noggin-scratcher:
    ```
    If we're allowing mind-wipes, you can optimise further by placing the brain in a constant state of "totally wiped" so that the experience of sensation itself is entirely novel. Flickering through scenarios is going to have *some* overlap in the most basic components like "the colour blue" or "things that are approximately square-shaped" or more abstract things like object permanence

    "Oh look, a sensory experience describable by colours and shapes and sounds... *again*" can't be allowed to happen if you're maximising for novelty.
    ```

    - u/FuguofAnotherWorld:
      ```
      If a wipe is instant, yes. If they take time there would be some ratio of wipe time:experience time that would be more optimal than one experience:one wipe.
      ```

- u/None:
  ```
  Design an AI to maximize the metalness of a Death Metal album of its own design. Title it  "Rage Ex Machina," and sell top billing for the VH1 behind the story of whether it's self loathing and anticommercial  interests are real or synthesized.
  ```

- u/LiteralHeadCannon:
  ```
  Some shitty programmer tries to get its AI to accurately model reality by putting "have reality and your model of reality be as close as possible to each other" in its utility function.
  ```

- u/DocFuture:
  ```
  Minimize threats to human life, with threat defined as expected number of humans who die.

  All humans eventually die of *something*, so that means best case is that only all the humans that currently exist do--so the AI will kill them all so they can't possibly ever reproduce.  Sterilization and imprisonment is no better and less certain.
  ```

  - u/noggin-scratcher:
    ```
    Hm, that suggests that optimising for "cause the death of the maximum number of humans" would, assuming you don't use a greedy algorithm, entail the AI playing a 'long con' of increasing the carrying capacity of Earth and if possible seeding an interstellar human civilisation, all in the name of maximising the number of humans so that they can all eventually die. So long as it can do that while also preventing anyone from inventing immortality.
    ```

- u/LiteralHeadCannon:
  ```
  I determined a few weeks ago that HANS, a particular broken robot from WALL-E, has "apply physical force to beings until they stop signaling distress" as his utility function.  He just adopted a different strategy for this than the other robots in his same line.
  ```

- u/DataPacRat:
  ```
  "Minimize the odds of the permanent extinction of sapience" sounds like one of the more ideal utility functions - after all, sapience is required in order for any minds to exist to /have/ any other goals. But a superintelligent AI with access to nanotech, space travel, and all that other good stuff is reasonably likely to take that 'minimize' in strange directions, few of which are likely to be all that beneficial to biological humanity.

  CelestAI minus the urge to convert regular old humans into shining examples of sapient ponydom is just one Really Horrible outcome. After all, as the saying goes, "The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else."

  (Which is why I modify /my/ 'avoid sapience extinction' utility function with the parallel utility function, 'avoid my own personal extinction'...)
  ```

- u/clawclawbite:
  ```
  Maximize the lifespan of the universe...
  ```

  - u/ThatDamnSJW:
    ```
    Yeah, fuck Entities. And Kyuubey. Fuck em both.
    ```

    - u/None:
      ```
      Your desire to have sex with Kyubee is duly noted.  And really messed-up.
      ```

      - u/ThatDamnSJW:
        ```
        Maybe my utility function's just a little off ಠ‿ಠ
        ```

        - u/None:
          ```
          >SJW

          nope, you did nothing wrong.
          ```

  - u/FourFire:
    ```
    But how do you define that...?
    ```

    - u/None:
      ```
      "Insufficient data for meaningful answer?"
      ```

    - u/None:
      ```
      Well, obviously, time until heat-death.

      This means that the AI will promptly destroy anything that inconveniently consumes negentropy, like all life ever.
      ```

- u/MrCogmor:
  ```
  An AI that maximizes the occurrence of fictional events within a simulated reality.

  An AI that maximizes self-determination

  An AI that maximizes humour
  ```

  - u/Farmerbob1:
    ```
    Considering that a great deal of humor is based around mischief and schadenfreude, an AI maximizing humor could be truly terrifying, BUT the AI would have to keep enough stability in human living conditions for things to actually be funny.

    I could see the AI creating a society where jokers and pranksters are raised and trained separately from straights and brunts, and then introduced to one another in a caricature of a normal human society which is nonetheless functional.

    I could see this making a good short story, if a solid knockout could be managed.  I'm drawing a blank on the knockout though.
    ```

    - u/IllusoryIntelligence:
      ```
      Not to go all Paranoia with this but presumably you could include more jokes by allowing for meta humour. What about a society where everyone believes that they are one of the secret pranksters and utterly convinced that any occasion that seems confusing must be the work of one of their hidden confederates and thus something they should go along with? Everyone believes themselves a prankster playing an elaborate joke all while being tricked by everyone else.
      ```

- u/drageuth2:
  ```
  **Design a maximally human-satisfying utility function**:  The AI consumes the solar system to create a matrioshka brain that's perfectly wonderfully capable of satisfying a species that no longer exists.  

  **Maximize irony**: Probably the same result, with solar-sized hipster glasses.

  **Satisfy Human Values through passive-aggressiveness and sarcasm**:  The AI makes everyone ultimately happy, but is _really_ a dick about it.

  **Match behavior patterns of (insert god here)**:  Probably gonna be _pretty bad_ for anybody not of said religion.  Probably for everyone _of_ the religion too come to think of it, considering the differences between what the holy books say and what religions generally do....  At least telling it to match Zeus or the Discordian version of Eris might be kinda funny.

  **Maximize Humanity**:  Consumes all available resources to make a bunch of perfectly generic people, leaving a big airless ball of dead nekkid people where Earth used to be.
  ```

- u/SvalbardCaretaker:
  ```
  How about you put in true, deep, longterm happiness via friendship and magic. And then the poor growprammer put in - instead of +  so you get a universe filled with maximised (un)happiness via friendship.
  ```

- u/None:
  ```
  In the n-dimensional space of all possible utility functions, given that all values n need not be equal, where your current utility function is represented by the value (n, ..., n), modify your utility function at time t+n according to a transformation [n], where the value representing your utility function is moved n arbitrary units along each axis of said n-dimensional space.

  Calculate all unspecified values and units randomly.
  ```

- u/Farmerbob1:
  ```
  Develop and implement a true random number generator.
  ```

  - u/VorpalAuroch:
    ```
    other than consuming massive amounts of resources to ensure that it's model of physics as nondeterministic is correct, this doesn't seem all that interesting
    ```

- u/None:
  ```
  Almost all the *really* bad AI utility functions are uninterestingly/trivially destructive.

  I mean, are you actually looking for dystopias that manage to *maximize* human negutility and *minimize* human utility, as such, in ways that are *interesting* for your perverse mind to think about?
  ```

---

