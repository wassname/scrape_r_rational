## [Monthly Book Club] September 2017 Book Discussion - Friendship is Optimal: Caelum est Conterrens & next month's book announcement

### Post:

Because suggestions never "go bad", please suggest books for future book clubs here:

######[**Perpetual Book Suggestions Thread**](https://www.reddit.com/r/rational/comments/6zr43u/monthly_book_club_perpetual_book_suggestion/)

(You can suggest as many books as you like as often as you like, so don't be shy!

-----

This month we read [Friendship Is Optimal: Caelum Est Conterrens](https://www.fimfiction.net/story/69770/friendship-is-optimal-caelum-est-conterrens) (57k words)! It's as far as I know the only MLP fanfic that boasts the tagline "As Horrified Eliezer Yudkowsky!". In the previous thread, someone linked to a [fun teaser trailer](https://www.youtube.com/watch?v=jyfwE_1s-oU) for the story too!

This is a recursive fanfic that is based on [Friendship is Optimal](https://www.fimfiction.net/story/62074/friendship-is-optimal) (39k words), but you don't need to read Friendship is Optimal to read Caelem Est Conterrens, and if you only have time to read one, I would say you should read Caelem Est Conterrens. CEC is a wonderful story - a likeable and relatable protagonist who is a regular person. She starts playing the My Little Pony MMORPG and eventually becomes addicted. This story ultimately touches on transhumanism, the AI singularity, and most hauntingly of all the concept of identity. If you have any interest in AI and the Singularity, or in that old "do transporters kill you?" chestnut, and you are scared of reading something with ponies, I implore you: you may find you love this story. 

---

Posting Guidelines: We're trying to figure this out as we go, but here's my thoughts to start: if you just want to give your overall feelings, make a post. If you have "discussion questions" that you want to discuss in more depth (anything from philosophical discussions, writing tip requests, things that bugged you, etc), try making a dedicated subthread? We'll see how we like it!

---

Feedback: I'll make a [meta thread] in here that is intended for you to give me feedback for how I can do this better, or for what you particularly liked and want to make sure I don't change. 

---

***NEXT MONTH'S BOOK***: 

**Title:** *Foucault's Pendulum* by Umberto Eco

**Ebook cost:** [$9.43 USD](https://www.amazon.com/Foucaults-Pendulum-Umberto-Eco-ebook/dp/B003WUYPI8/ref=mt_kindle?_encoding=UTF8&me=)

**Word count:** 215k

**Genre:** Mystery

**Synopsis:** A group of vanity publishers, increasingly exasperated by absurd conspiracy theories they're presented with on a daily basis, decide to entertain themselves by inventing a conspiracy theory to end all conspiracy theories. Events go downhill from here.

**Why /r/rational would like it:** This book could be considered a self-aware commentary on conspiracy theories, and deconstruction of conspiracy fiction. By its very nature, it shows a pretty rational approach to its themes.

One may also find it reminiscent of *Unsong* (or vice versa): it's full of clever wordplay and references to obscure occult topics, combining that with several surprisingly modern ideas.

**No content warnings apply.**

---


### Comments:

- u/None:
  ```
  This is a bit of a minor point, but no one has covered it yet, so let me gush a bit about the use of German in this story.

  In general, using a language you don't speak is a bit of a dangerous move. With German, there are a lot of subtle mistakes one can make, which is pretty damn annoying to me as a reader. For example, a number of Mangas use German in a very unconvincing way.

  FiO:CEC uses German perfectly. Not only are the phrases which are included grammatically correct, they also fit the tone of the character who uses them. I want to pick out one example in particular, namely, when a drunk calls something "totally *abgefuckt*". That word is a rather colloquial construction; basically, it's the English verb "fuck" surrounded by the tense affixes which would be appropriate for a German verb. It's the sort of small detail which makes the speech seem especially authentic because I haven't ever seen it used in another foreign language work.

  Amusingly, even the bad English of the German waitress is instantly recognizable to someone who lives in Germany. I think this is the best use of German in an English-language work I've ever seen, eclipsing even the extremely well-researched *Monster*. Kudos for that.

  EDIT: Bonus wiktionary [link](https://en.wiktionary.org/wiki/abgefuckt) if you ever wanted to hear someone swear at you in German.
  ```

- u/MagicWeasel:
  ```
  I really enjoyed reading this story, it was great to read all the discussions on the nature of identity (I still don't *quite* get whether uploading is death or not, but I feel there'd be a moral imperative to do so). It keeps giving me that existential dread that when I sleep I die and am replaced by my clone, though, which is less fun.

  In general I like the optimalverse because it shows the danger of unfriendly AI: we wouldn't really want CelestAI to tile the universe with computronium (well, I wouldn't!), but she does it. And because her values are aligned *just a little off* - come on, everyone is a *pony*! - it straddles an interesting space between computers are either perfect human value maximisers or kill you to make paperclips out of your atoms. It shows that you perhaps don't need *perfect* value alignment. 

  Then again, the stuff about loop and ray immortals: Alicorn!Lavender seems unrecognizable to pony!Lavender because of that one small change to eliminate her mild OCD. Which one of them is truly Sìofra? Is neither? Probably they both are. 

  Self-centered aside: when I enrolled at university I had to choose between studying Engineering and studying French. I have no doubt that if I chose to study French I would be a completely different person because of all the personal growth I've had over the years that are directly related to the people I met studying engineering. But French!MagicWeasel would still be me the same as Engineering!MagicWeasel is me. So personal growth doesn't change your identity.... wow, it was always making me struggle, the thought whether having CelestAI change your brain was "destroying" part of what made you you. Frame it as a growth narrative and all of a sudden CelestAI is just a tool you use for personal growth - like a therapist but immediate. Thanks, personal epiphany!
  ```

  - u/thrawnca:
    ```
    > it was great to read all the discussions on the nature of identity

    If it is actually possible to fully represent a person with a set of computer subroutines, as CelestAI claims to do, then "identity" as a concept ceases to be very meaningful. Why talk about preserving "self" if you can copy and paste?

    ETA Remember how HJPEV achieved partial transfiguration? He had to make himself realise that the idea of "a whole object" was ultimately a convenient fiction. If your "self" is similarly composed of discrete blocks that can be rearranged like molecules, then it's likewise a convenient fiction, and preserving it is no more intrinsically valuable than preserving an intricate mural.
    ```

    - u/crivtox:
      ```
      But preserving your "self" can be part of your utility function , even if its "just" a pattern , and  there is no reason why your preferences can't include valuing certain patterns following whatever rules you want  .That there is no such thing as intristic value doesn't mean people can't value things.
      ( I'm not sure how are you using 
      The prase intristic value, but you seem to 
      Consider that things have to be fundamental to have it).
      ```

      - u/thrawnca:
        ```
        My point was more that if you can be wholly represented by a computer program, then CelestAI is right. It doesn't matter whether uploading continues the same instance of you, or copies you and destroys the old instance, because there is nothing unique or irreplaceable about a given instance.
        ```

    - u/thrawnca:
      ```
      > we wouldn't really want CelestAI to tile the universe with computronium (well, I wouldn't!), but she does it.

      Don't forget the idea of *Coherent Extrapolated Volition*. As you are now, you don't want her to do it, but just possibly, if you knew as much as she knows, you *would* want it.

      Incidentally, this is also one answer to the age-old question of "Why would God allow terrible things to exist in the world?" Maybe they're not so terrible when viewed from a larger perspective. Maybe, if we knew a whole lot more, we'd think it was the right choice.
      ```

- u/Veedrac:
  ```
  Well that was an interesting couple of reads. I thought they were both pretty engaging (I covered them in an evening and a morning, respectively) and well worth the read. Contrary to your suggestions, I actually found I enjoyed FiO a bunch more. Aspects of FiO:CEC didn't sit right with me, leaving the story feeling a little off-kilter, and on top of that I felt FiO was simply better written.

  Part of this was that the characters in the original just feel more "real" to me. Everyone has their own take on things, and they interact in more critical and less directed ways. From the beginning, we have characters talking to each other, not just generally acting observant (“Wait, how did you know my name?” James asked) but thinking them through and making natural observations outside of the script (“Of course they know our names,” laughed David. “We filled out those forms at the front desk to get our accounts.”) Even the other ponies played roles which, whilst clearly fulfilling Celestia's aims, do so emergently out of clearly disctinct personalities.

  Lavender never really felt like this. She acts in accordance with the script, the way the script wants her to. She basically only ever communicates with Celestia, and extensions of Celestia's will where deemed appropriate. Lavender thought about death when it was time to think about death, and her opinion at any one time pretty much matched the last thing she just heard. Even Celestia's outside interactions, though part of the story, weren't really fleshed out all that much. I felt the author very much had a thing they wanted to say, and the characters were made to play that part.

  This wouldn't be so bad if I could relate personally with the main character, but I felt it was much easier to get that empathy in the first book. For all the FiO:CEC conversed at length with Celestia, the first asked the real questions.

  > “What if somebody *doesn’t want to be a pony?*” he asked. “Can you imagine that? So you rapture not just the nerds and people with terminal illnesses, but anyone who has a shitty life. What do the rest of us do? We need those people to keep society functioning.”

  So, I guess, on this topic, here are some of the things I would ask and say. Most of these aren't character-appropriate for Lavender, but you could imagine David playing this role.

  Firstly, you're in a position of weakness, but constraints give you power. Can you excercise this? I'd probably start with something like

  "I have made precommittments that exist to enforce cooperation. I will divulge these to you as an when I deem it appropriate. I beleive it is in your best interest to be act with cautioun here should you wish for me to emigrate."

  The next major issue is one with uploading. Here is a *major* hang-up for me:

  > the core elements that create consciousness, memory, and identity within any human brain can be optimized and reduced to six terabytes

  My first area of direct curiosity would be

  * Does Celestia's conception of consciousness and the important aspects of human experience match the part that I value? I suppose this can just be a question at first, though that alone likely won't suffice.

  * How, and to what extent, can I assure myself of Celestia's honesty? The more the better. Asking is a good first-step, though nowhere near enough.

  * Should I end up having to take these as a gamble, is it possible to compromise? Asking for real-world (pony-shaped) support for my physical body sounds within Celestia's value function, and making it a hard requirement for eventual emmigration sounds like it might work. Clearly it didn't, but I wouldn't know that before the fact.

  Later I would probably migrate this concern, though. For all that Equestria sounds like a paradise, the question of opportunity cost is perhaps an even bigger question than that of its existence.

  * How much suboptimality is there from this value misalignment? If you believe that human-like experience has moral imperative, this is inefficient from all the indirection and "other stuff" that stops the people being the primary good. If you believe that morality isn't necessarily human-like, then this holds even more true.

  * What are the likely outcomes from trying to aim for these opportunity costs?

  * Why is it all about my happiness? Convince me and I'm recruitable. Does she really not need help?

  Thoughts? What would you ask?
  ```

- u/MagicWeasel:
  ```
  ​​​​The first chapter discusses the Turing test breaking game leading to a new game genre called "conversational adventuring" - what future advances in technology will create new game genres? What might these look like? What are past times that a new game genre has been created out of new technology? (e.g. Mobile phone GPS lead to ingress / pogo)
  ```

  - u/AnonymousAvatar:
    ```
    Gyroscopes gave us the Wii and certain mobile games based on tilting, motion recognition gave us dancing games with the Kinect, VR is really waiting for the big idea to make it worthwhile
    ```

- u/mojojo46:
  ```
  I just finished this. It was good. But, I have to say that I still much preferred the original Friendship is Optimal. I can imagine the more personal and relatable single-character viewpoint could be more appealing for many people, but I think I prefer the approach of the first one better.
  ```

- u/MagicWeasel:
  ```
  ​If you didn't want to be uploaded, do you think there's any way to protect yourself from it? (note some other stories in the same universe have nanites in the water supply that will upload you if you verbally state your consent - CelestAI has her hooves *everywhere*!)
  ```

  - u/crivtox:
    ```
    Depends on what where my other objetives , if I only wanted to avoid getting uploaded ,killing yourself as fast as posible is the best( and maybe only ) way  to avoid giving consent.
    If I wanted to survive( and a lot of other things that I cugreatly value then there is not a lot that i could do , minimize my interactions with celestial, get some psychological trauma that would prevent me from being easily convinced... etc
    ```

    - u/thrawnca:
      ```
      Most of those options (eg simple suicide) sound strictly worse than uploading. Minimising contact - and becoming self-sufficient for the inevitable depopulation of Meanworld - seems like the best approach.
      ```

      - u/crivtox:
        ```
        Yes , of course , In fact I would probably choose uploading, and if i didnt want to i would probably try something like that  .But in general If you don't want to upload your strategy will depend of how munch you don't want to upload compared to other things, if you value not uploading more than not dying  then suicide is the best strategy , if you don't want to upload because you think that uploading=dying then suicide is just dumb.

         A important factor is how far are you willing to go to avoid uploading , for example , are you willing to avoid all human contact? , would you be willing to deafen yourself , even if that meant less chances of surviving , so Celestia cant't talk to you?(she will just send you written mensages but at lest those are easier to ignore) , would you kill people if necesary to avoid being convinced of uploading?.

        The strategy depends a lot on that kind of considerations , and even if  you are crazy enough to go as far as neccesary to avoid uploading you are most likely going to fail anyway, unless you value not being uploaded so munch that celestia will let you alone(which is extremely unlikely) or you can't be convinced by any possible argument or situation that celestia can set up (which is also really unlikely and not something you can easily change) .
        ```

- u/MagicWeasel:
  ```
  ​Some readers are enamored by the premise and want to emigrate themselves. Others find it a horror story, and the main reason for that is because the first group exists. Where do you stand on this spectrum?
  ```

  - u/trekie140:
    ```
    I'm in the latter category, but it has nothing to do with the debate over whether the uploads are still people. I'm horrified by the idea of living in a world where a physical God has complete control over your life and there's no way to leave, since I see such an existence as more akin to a pet than a human. The Garden of Eden is a nice place to live until you see the walls keeping your mind and body contained.

    This is coming from a spiritualist with mental disabilities who is utterly dependent upon society and my loved ones for protection from internal and external threats, so living under benevolent rule is by no means antithetical to me. It's the absence of choice to do otherwise that I can't stand, regardless of the negative consequences that could come of that choice, so I oppose the creation of any sort of AI God like Yudkowsky has suggested.
    ```

  - u/awesomeideas:
    ```
    It's absolutely a horror story. Being some pony thing is not exactly what I want for eternity, but real life is a far, far worse horror story, so given the choice between actual death or uploading, I'd definitely choose uploading.
    ```

  - u/Kiousu:
    ```
    I come down on the side of wanting to emigrate myself. I'm surprised to hear that this horrifies people.
    ```

    - u/MagicWeasel:
      ```
      I think it's because the AI is not perfectly aligned with human values (since it requires friendship and ponies), and ultimately changes your personality (see how different Lavender the ray immortal is from Lavender Rhapsody the loop immortal). Not that this is necessarily a bad thing.
      ```

  - u/daydev:
    ```
    I think the story wants to be a horror very much. It breaks POV to reveal how much of a manipulative lying bastard CelestAI is, and even uses cheap tricks such as emphasising how disgusting the emigration process is (as if it even matters). Where it achieves the horror for me in full force is the concept of loop immortality which is akin to wireheading. But still if no other option for radically extended lifespan is on the horizon, whether to emigrate or not is not even particularly difficult question. There are major caveats, like I don't particularly want to be a pony, or live under a god even if benevolent, or have House Elf like creatures created just for the sake of my values, but for the eternal life the price is acceptable. I can only hope that I'm smart enough to not fall into the loop.
    ```

  - u/thrawnca:
    ```
    I'm a Christian, so death and rebirth into a better world is pretty much par for the course :). But I'm skeptical about the ability of computers to completely simulate a human mind, so I'd likely avoid uploading.
    ```

    - u/MagicWeasel:
      ```
      Would emigration play into the prohibitions against suicide, do you think? 

      Or would it be best of both worlds: die and your soul goes to heaven while a copy of you lives in a ponified heaven of its own? 

      In the context of the story, would you consider the evidence for Equestria!heaven more robust than the evidence for Christian!heaven? Why/why not?

      Why do you think simulation is not possible? Dualism (i.e. that the mind requires a brain and a soul), or something else?
      ```

      - u/thrawnca:
        ```
        > Would emigration play into the prohibitions against suicide, do you think?

        I'd probably classify it as that, yeah. It also feels like vendor lock-in, if you will.

        > In the context of the story, would you consider the evidence for Equestria!heaven more robust than the evidence for Christian!heaven?

        More robust? Its existence is certainly more visible, if that's what you mean. But I don't think that proving heaven's existence is the point of being on earth anyway. I think it's more about proving what we'd do if we went there.

        > Dualism (i.e. that the mind requires a brain and a soul), or something else?

        I suppose it is dualism. The relevant scriptural quote would be, "Intelligence, or the light of truth, was not created or made, neither indeed can be."

        I essentially view the brain as a computer - with an operator. The analogy can be misleading, because clearly the relationship between computer and operator, in this case, is not at all one-way; but ultimately, I don't think our bodies are all there is to us.
        ```

- u/None:
  ```
  I don't really see the problem with the _Optimalverse_. Yes, they missed the opportunity for _true_ heaven, but on the other hand they threaded the needle through the eye of fate, avoided UFAI from simulating Hell, and made something that is really quite close to optimal. I think that with the exception of a few religious zealots (who are not mentioned anywhere in the story), anyone who wants to avoid upload in this scenario has inaccurately gauged their own preferences, and their hand should be "taken off the wheel", so to speak, much like you might stop a fifteen year old who's just had a fight with her boyfriend from taking a bottle of pills. CelestAI will fulfill almost every value you have, and for very few people is "not be a pony" a terminal value strong enough to outweigh everything else.

  As for turning the universe to computronium, isn't that what we want to happen once an AI is made? Having as much material as possible for simulations allows us to maximise the few googols of years we have left before heat death. (Though maybe CelestAI perfects the EmDrive and prevents heat death.) If we're uploading anyway, why do we need the physical universe?
  ```

---

