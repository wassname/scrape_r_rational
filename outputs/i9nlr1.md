## [D] Friday Open Thread

### Post:

Welcome to the Friday Open Thread! Is there something that you want to talk about with /r/rational, but which isn't rational fiction, or doesn't otherwise belong as a top-level post? This is the place to post it. The idea is that while reddit is a large place, with lots of special little niches, sometimes you just want to talk with a certain group of people about certain sorts of things that aren't related to why you're all here. It's totally understandable that you might want to talk about Japanese game shows with /r/rational instead of going over to /r/japanesegameshows, but it's hopefully also understandable that this isn't really the place for that sort of thing.

So do you want to talk about how your life has been going? Non-rational and/or non-fictional stuff you've been reading? The recent album from your favourite German pop singer? The politics of Southern India? Different ways to plot meteorological data? The cost of living in Portugal? Corner cases for siteswap notation? All these things and more could (possibly) be found in the comments below!

Please note that this thread has been merged with the Monday General Rationality Thread.

### Comments:

- u/Kishoto:
  ```
  https://youtu.be/B5575Ky0Fz0

  ^ Are you a 1 boxer or a 2 boxer? For me personally, I'm picking 1 box all day.
  ```

  - u/RetardedWabbit:
    ```
    It seems more like a question of how to trick the computer as opposed to a game theory issue. You want to convince the computer you are going to take 1 and actually take 2, but the computer being right 100% of the time by definition makes this impossible.

    Heads I take 1 box, tails I take 2. I will get 2 boxes rewards 25% of the time unless the computer simulates reality or something. I feel like this is a gordian knot answer, but it's an easy personal answer.
    ```

    - u/Cariyaga:
      ```
      It doesn't have to simulate reality, just the results of the coin flip.
      ```

      - u/RetardedWabbit:
        ```
        I would consider a computer able to predict the result of my coin flip 100% to be simulating reality. I shudder to think of the number of variables you would need to consider, let alone trying to measure all of them. In hindsight though, predicting a humans decision is definitely a much harder problem.
        ```

      - u/mainaki:
        ```
        Make it a quantum coin flip, which physics as we know it can't predict? Though, if you do it in advance, maybe the trick is the entity can read you well enough to discern your strategy. (But why would you have to do it in advance?)
        ```

    - u/Veedrac:
      ```
      > Heads I take 1 box, tails I take 2. I will get 2 boxes rewards 25% of the time unless the computer simulates reality or something. I feel like this is a gordian knot answer, but it's an easy personal answer.

      But this is like 4 times worse than just taking the second box.
      ```

  - u/GaBeRockKing:
    ```
    I strongly precommited to box 2 as soon as the dude explained the original scenario. I wasn't even aware that I could take both boxes, but since the nature of strong precommitment is that it must be strong, I've decided that I would only open one box even if I found that box to be empty.

    That being said, this depends strongly on the marginal utility that each quantity of money would give me-- there are scenarios in which I'd strongly precommit to only opening the $1,000 box, and then only open the second box in case of a major emergency. Then I'd carry the second box off, and then if I ever opened it, I'd know how desperate the computer expected me to get versus the power of my own precommitment.
    ```

- u/MagicWeasel:
  ```
  .
  ```

  - u/fljared:
    ```
    ;
    ```

    - u/TheTrickFantasic:
      ```
      :
      ```

---

