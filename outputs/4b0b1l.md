## [D] How robust are human terminal goals on the ascent toward trans-humanity ?

### Post:

The title question is something I have wanted to explore for a while in fiction. [TAP] (http://www.infinityplus.co.uk/stories/tap.htm) by Greg Egan feels like an excellent reference point, though the title of the post only tangentially relates to the story.

disclaimer: I'm a huge fan of Egan and this post is as much a chance to link to his older work.

### Comments:

- u/SvalbardCaretaker:
  ```
  The prevailing fear is that they might not at all be robust. Value 
  drift seems to be a real danger. 

  Also the question of wether different parts of humanity actually do have convergent or divergent terminal goals is as of yet hotly debated. 

  Eliezer argues pro convergence here: http://lesswrong.com/lw/rl/the_psychological_unity_of_humankind/

  Some anthropologist on LW once strongly argued otherwise, and a few years ago Lukeprog said he wasnt sure, but leant towards divergence. 

  Other good starting points are, eg: 

  http://lesswrong.com/lw/y3/value_is_fragile/

  http://lesswrong.com/lw/y4/three_worlds_collide_08/ (Fiction)
  ```

  - u/VanPeer:
    ```
    Thanks for the response. I'll chew on it for a while.
    ```

- u/ArgentStonecutter:
  ```
  Since no human actually knows what their own goals really are, since they have no direct visibility into their own decision-making process, this is kind of unanswerable.

  Since you're a Greg Egan fan, might I direct you to _Mister Volition_?
  ```

- u/None:
  ```
  What's your actual question?  Humans don't *have* built-in terminal goals.
  ```

  - u/VanPeer:
    ```
    Sorry for being unclear. I was wondering how plausible a certain fictional scenario is: a male protagonist has what he considers his core values acquired through nature or nuture. To himself, his values feel right. Then, he gets augmented with the ability to scrutinize his feelings more deeply (read only access), and starts perceiving his "core" feelings as a composite of more basic drives. His original values no longer seem so... sublime. 
    Reasonable ? Laughable ? I'm not sure. For example, understanding the evolutionary roots of love doesn't make one stop loving his family. On the other hand, there may be no evolutionary defenses against trans-human self insight.
    ```

    - u/None:
      ```
      What's wrong with self-insight?
      ```

      - u/VanPeer:
        ```
        Nothing per se. I was just wondering if augmented self-insight might plausibly cause divergence from core values that exist before the augmentation. For example, if a male protagonist gets superhuman insight into his deeply cherished feelings of gallantry towards females, and then realizes his gallantry to be nothing more than sublimated physical attraction, might he not simply lose interest in being 'gallant' ?
        ```

        - u/None:
          ```
          That sounds like *exactly* the situation in which he *should* stop being gallant.
          ```

          - u/None:
            ```
            [deleted]
            ```

            - u/TK17Studios:
              ```
              Not sure if the use of the word "good" was irony, though the TM implies you're aware of it.

              This is the Dr. Manhattan problem, right?  The question becomes relevant if some humans are ascending and others aren't.  If the only individuals "left" are ones following this path, though, then it seems more severe than, but ultimately similar to, our species' moral development thus far.  We largely toned down our enslaving, raping, and murdering of one another; if we discover that other things we thought were true aren't, we'll change again/more.

              I would wager some terminal goals are more robust than others.  If you gave cavemen heroin, they'd probably go nuts for it and see nothing wrong with it; our distaste for wireheading comes with knowledge and perspective, and allows us to look at it and say, *kinda pathetic, huh?  Maybe we can get happiness that's more justified and legitimate and "real" some other way.*  Similarly, a lot of our current sources of happiness may become more transparently trivial or mechanistic, and lose some of their charm.  But I would expect values like curiosity to retain their allure, and values like kindness to shift in justification but remain relatively strong/popular (swapping good feels for decision theory, for example).
              ```

              - u/chaosmosis:
                ```
                I think the majority of our disgust for wireheading might be self-delusion and social fiction. I sort of think there is nothing but wireheading and nihilism, and different points between the two extremes.
                ```

                - u/TK17Studios:
                  ```
                  Maybe.  I respect wireheading as a force that could take decisive control away from "me" ... I don't kid myself that I could just *not* be addicted to heroin, for example.  But from *this* perspective, I have a what-seems-to-be principled stand against deriving satisfaction from the intrinsically meaningless.
                  ```

                  - u/chaosmosis:
                    ```
                    Do you also have a principled stance against eating chocolate? Chocolate seems rather intrinsically meaningless to me.
                    ```

                    - u/TK17Studios:
                      ```
                      I do have a gradient, which is perhaps unjustified, but I lean much further away from random hedonic pleasures (sex, alcohol, drugs, video games) than average, for basically this reason.
                      ```

                - u/lehyde:
                  ```
                  So why can't we desire a specific point between wireheading and nihilism? There is no reason our desires should coincide with one extreme, right? Maybe there is a sweet spot.
                  ```

                  - u/chaosmosis:
                    ```
                    I didn't mean the last part of what I said. I should have said only "there is just wireheading and nihilism". Any points that appear to exist between the two positions are due to human cognitive failings. The only reason we haven't all wireheaded ourselves already is technological limitations. Even with our current tech level, most people's free time is spent in escapism through fiction, drinking alcohol, or getting endorphin rushes from exercise, competition, and sex.

                    I even think the inevitability of wireheading might be a good explanation for Fermi's Paradox.

                    I would agree that it is desirable that we maintain our social fictions surrounding wireheading and nihilism, though. But that's why efforts to improve human rationality to transhuman extents make me so nervous. I think they will undermine a lot of the meaning in people's worldviews, including my own. At the same time, doing nothing and refusing to push our limits also seems intolerable to me. So it's a bit of an unwinnable situation.
                    ```

                - u/VanPeer:
                  ```
                  I suspect the same. Recently read an excellent piece on wire heading by Scott Alexander,  but it was never clear why exactly wire heading was bad as long as one doesn't have dependents who would be abandoned, and if one is fully aware of the consequence of pressing the button.
                  ```

              - u/VanPeer:
                ```
                Well said. Gives one more hope for the future.
                ```

              - u/None:
                ```
                >This is the Dr. Manhattan problem, right?

                Except that Dr. Manhattan was psychologically and scientifically unrealistic.  The differences between a live human being, a dead human being, and a merely brain-dead human being are *very* palpable and scientifically detectable.  There does not exist a level of Deep Truth at which these differences disappear, only levels of Deep Truth to which the merely ordinary, everyday truths are reducible.

                >  If you gave cavemen heroin, they'd probably go nuts for it and see nothing wrong with it

                Would they?  Even rats don't seem to actually prefer opiates or wireheading if they're offered a desirable alternative.
                ```

            - u/None:
              ```
              >It may be the case that with enough self-insight, you realize that much of what we call morality is merely an extension of the desire to conform.

              Yes, that *is* why people consider it "decent and moral" to wear pants in public.  It *is* a conformity thing.

              >For example, Emily's core values include the protection of conscious life, but upon augmented introspection, she realizes that a collection of atoms acting one way is not fundamentally different from or more meaningful than atoms acting in another way.

              That doesn't sound like augmented introspection.  That sounds like artificially-installed nihilism.

              >Is this plausible? Conceivable?

              Well, as implied above, I find it rather implausible.

              >Is self-insight always, necessarily, a good thingTM?

              Yes, more knowledge is always helpful for making decisions that are better in-tune with reality.
              ```

- u/Izeinwinter:
  ```
  Heck, it's entirely possible that terminal values are robust, but that enhancement would ruin your ability to be inconsistent, leading to hilarities like upgraded humans doing the math on suffering and promptly converting the entire biosphere into computronium as an animal welfare measure, because they loose the ability to be okay with feeding cows to dogs, mice to cats, and krill to whales.
  ```

---

