## So You Want To Upload Yourself Into A Computer

### Post:

[Link to content](http://i.imgur.com/4B6wPoK.png)

### Comments:

- u/trekie140:
  ```
  The only story I've ever read that dealt with what's it like to have your consciousness permanently duplicated is the webcomic [El Goonish Shive](http://www.egscomics.com/index.php?id=1). The copy is a physical clone created by magic, but it's still relevant to exploring this sci-fi concept and both individuals go through some excellent character development as they explore their own identities. 

  The relevant story arc starts on [page 180](http://www.egscomics.com/index.php?id=180), which is where the comic starts getting really good, though without knowledge of earlier continuity several things will confuse you. If you try the early pages and don't like them, don't worry. Literally every single stupid plot point and dumb character decision gets retconned into rationality.
  ```

  - u/AntiTwister:
    ```
    Permutation City also explores this concept in depth.
    ```

    - u/trekie140:
      ```
      EGS is still worth reading since it examines this idea in the context of people living relatively mundane lives. It's not just about the sci-fi concept, it explores the people effected by the event and how they adapt to such a situation in order to live happy and fulfilling lives. 

      Not only does the clone have to go through the initial shock and trauma of their creation only to later accept that they're a different person than the one they share memories with, but we see them struggle with that idea as time goes on and figure out just what the relationship between the two of them is.

      Personally, it's among the most emotionally invested I've ever been in a story. Introducing the characters before the event as regular characters and then showing them develop naturally during and after the event is...I can't think of a word to describe how impactful it was on me.
      ```

      - u/AntiTwister:
        ```
        Noted, I'll give it a look.  May want to make [page 180](http://www.egscomics.com/index.php?id=180) a hyperlink to reduce investigation friction.
        ```

        - u/trekie140:
          ```
          I really want people to start at the beginning so they understand the context and have some investment in the characters beforehand, otherwise it's just a really weird situation where a bunch of crazy stuff happens without explanation. The early pages are just strips so it's a pretty brisk read even if not all the jokes are funny and the setting doesn't make a whole lot of sense at first. InfernoVulpix explained it in more detail.
          ```

    - u/5erif:
      ```
      I really really wish there more authors like Egan.
      ```

  - u/InfernoVulpix:
    ```
    El Goonish Shive really had to work hard to make everything introduced early on as gag humour fit into the world coherently, but I do think the author's done a quite good job of it.

    In broad strokes, the story starts off as gag humour with frequent plotholes, including in the above clone situation, but soon enough the story enters a surprisingly dark plotline that, while still including many of the attributes of the early strips, show an intent from the author to take the comic more seriously.  After that arc ends, the author kept the intent to take the comic seriously but steered it back to the less grim atmosphere it had before, which I feel is the best of both worlds.

    From that point on, I can attest that El Goonish Shive has done a very good job of keeping track of an interconnected world and tying together past and future events even for the plot arcs of lesser importance, though there are some pieces of foreshadowing from the early days which have gone unaddressed for a long period of time because the now coherent overall plot isn't ready for the follow-through yet.  Transformation is one of the most frequently seen uses of magic in the story, letting the comic thoroughly examine the associated topics, as well as the broader scope topics related to magic in the modern world such as what extent it should be spread around.  The comic's been going for over fifteen years with a quite consistent update schedule of three times weekly and I've quite enjoyed the story, especially once the gag humour from the beginning began to be addressed and the plotlines became more thought out.
    ```

    - u/trekie140:
      ```
      100% agree. You're the first person I've met who is also a fan of this comic, which makes me happy since it's one of my favorite stories ever and has had a big impact on me over the years. I picked it up at a time when I was unsure what kind of person I was and what kind of life I was going to live, and this story about teenagers making intelligent decisions and discovering their own identities reassured me that I could *do it*.

      As someone who didn't intuitively understand emotions, including my own, it was incredibly fulfilling and formative to see characters intellectually analyze themselves and succeed at self-actualization. It also ended up being my introduction to a lot of LGBT concepts I didn't *get* before. One of the characters has as arc about finding out they're gay, and it managed to make me feel like I was in their shoes despite having no doubt about my own sexuality at the time.

      The characters also react to the supernatural in a really interesting way. Everyone, including background extras, treat the supernatural like any other part of the world they don't understand and don't usually care all that much when it isn't a big part of their lives. The Men in Black are actually pretty easygoing and have logical reasons for it, while still taking danger seriously and wanting to help people.

      When serious danger does present itself, though, the comic almost becomes a deconstruction of teens fighting the supernatural. These events have a lasting impact on their lives for better and worse, their skills and resolve are always put to the test, and even then they still need help from outside forces because they aren't that powerful or experienced. They don't even look for trouble, just fight back when it comes for them.

      Not that this is a dark story, much of it is meant to be comedic even though the context is rational. Painted Black definitely has some very dangerous situations and a villain who's realistically psychotic, but no one has anything like PTSD and many story arcs focus on the characters living relatively mundane lives. It's not an action adventure or slice of life, just good characters going through good development.

      I should acknowledge some of the comic's flaws, though. The author's understanding of gender identity and sexual orientation have improved over time so there is some unintentional misrepresentation, but it has been justified in-universe as the characters improving their own understanding with time. Also, despite characters using transformation magic to change gender, transpeople have only recently appeared in the story.

      While I think the story is really about the character development and the setting is one of the most rational urban fantasies I've seen (after retconning some early stuff), the myth arc is very slow and convoluted. The author will introduce new plot threads only to drop them and pick them back up years later, only to use their resolution to lay the groundwork for future story arcs that won't be resolved until years later. 

      The author has also admitted that individual story arcs tend to take longer than he intended since he keeps adding in scenes as he goes. Slow pacing is very common in the webcomics I've read and the plot is nowhere near as overcomplicated as Sluggy Freelance or Schlock Mercenary, but I still recommend reading the author's commentary when you're confused and you will get confused at least a few times.
      ```

  - u/literal-hitler:
    ```
    I highly recommend the [bobiverse series](https://www.amazon.com/dp/B01LWAESYQ) to you as a second point of reference.
    ```

    - u/None:
      ```
      I adore the Bobiverse books, but I see a lot of people get turned off because the author takes the hammer to the knees of a lot of stupid shit, like religion and theocracy, with gusto.

      For me, it's a plus, but if you're the sort of person to either cherish their fantasies or become offended on behalf of others, fair warning.

      Bob and religion do not mix well.
      ```

  - u/GopherAtl:
    ```
    David Brin's *Kiln People* explores the idea quite well I think. It doesn't strictly speaking deal with "uploads" but it's about a world where copying your mind into what are effectively short-lived golems is commonplace. Not a particularly philosophical piece, but it plays enough with the premise to provoke a lot of thought.

    :edit: Well, none of the copies are permanent, so not quite the same thing, but given that high-quality copies tend to live with the expectation of reintegration at the end of their lives, very similar situations arise.
    ```

  - u/CellWithoutCulture:
    ```
    The bobverse books deal with it well. He freezes himself them is uploaded into a self replicating interstellar probe, but other countries have also done this. Because he committed to co-operate with copies of himself, most copies do (so far).
    ```

- u/FeepingCreature:
  ```
  Yay!

  (Watching people repost stuff you made is actually really enjoyable.)
  ```

  - u/ILL_BE_WATCHING_YOU:
    ```
    It's funny, because I actually found it on the space battles thread you posted it in.
    ```

    - u/FeepingCreature:
      ```
      Well it makes sense, because I found that thread by it being posted here. :)

      ~_the ciiircle of liiife_~
      ```

- u/ansible:
  ```
  With uploads, and/or the ability to back up your own mind-state, I'm expecting a new class of crime: partial-murder.  This is where you cause someone to lose some time (because you killed an instance of that person), forcing a restore from backup.  The severity of partial murder varies on how much the person has lost.
  ```

  - u/TastyBrainMeats:
    ```
    I mean, we already have murder in the first degree, second degree, etc.
    ```

    - u/CCC_037:
      ```
      [](/twisquint) "You are hereby accused of murder in the zero-point-three-one-eighth degree. How do you plead?"
      ```

      - u/TastyBrainMeats:
        ```
        >**Mr. Pump**: 'I Worked It Out. You Have Killed Two Point Three Three Eight People.'  
        **Moist von Lipwig**: 'I have never laid a finger on anyone in my life, Mr. Pump. I may be — all the things you know I am, but I am not a killer! I have never so much as drawn a sword!'  
        **Mr. Pump**: 'No, You Have Not. But You Have Stolen, Embezzled, Defrauded And Swindled Without Discrimination, Mr Lipvig. You Have Ruined Businesses And. Destroyed Jobs. When Banks Fail, It Is Seldom Bankers Who Starve. Your Actions Have Taken Money From Those Who Had Little Enough To Begin With. In A Myriad Small Ways You Have Hastened The Deaths Of Many. You Do Not Know Them. You Did Not See Them Bleed. But You Snatched Bread From Their Mouths And Tore Clothes From Their Backs. For Sport, Mr Lipvig. For Sport. For The Joy Of The Game.’
        ```

        - u/CCC_037:
          ```
          [](/twibeam) A well-placed quote! Well done.

          [](/sp)

          [](/twiponder) But does this imply that a form of partial-murder already exists?
          ```

          - u/TastyBrainMeats:
            ```
            In a golem's mind on the Discworld, at least.
            ```

            - u/CCC_037:
              ```
              [](/twiponder) I don't think Mr. Pump is really wrong, though. Killing someone removes 100% of their life; cutting someone's life expectancy by 50% would thus be half a murder.
              ```

              - u/TastyBrainMeats:
                ```
                Oh, no, there's definitely merit to his way of thinking. I have to agree with that.
                ```

          - u/None:
            ```
            Yes, definitely.  Consider choosing to deny someone access to medicine for their chronic or terminal illness, when you could just as easily grant it.  If that's not manslaughter by portions, the crime of manslaughter can scarcely be said to exist.
            ```

- u/wren42:
  ```
  >"The evidence that you are [conscious] is exactly the same as the evidence that your upload is."

  Bullshit.  Maybe to an *outside observer.*  But not to myself.  The existence of my subjective experience is the *only* thing I can be 100% certain of from a bayesian perspective.  Whether a peice of software simulating my personality is conscious is no where near 100% for me.
  ```

  - u/FeepingCreature:
    ```
    > Bullshit. Maybe to an outside observer. But not to myself. 

    Exactly to yourself! That's the fun part.

    Your consciousness has direct access to the fact that you are conscious, but your _reasoning_ only has indirect access to this fact via a sensory tap from your consciousness. There is nothing privileged about this input.
    ```

    - u/wren42:
      ```
      It seems we're having the same conversation across two subs =)

      I'm pretty confident in this one point, however.  Mathematically, in a Bayesian sense, I will always have more evidence that I am conscious than that another arbitrary agent is conscious. 

      I have 100% certainty that I am conscious.  I do not have 100% certainty that an arbitrary program I am observing is conscious.  Passing a Turing test is insufficient, as (sufficiently) correct answers could be selected  at random from a hat, and provide no clue as to the underlying operation.   

      I can never have access to another agent's "sensory tap" as you put it, so the weight of evidence will always be imbalanced.
      ```

      - u/FeepingCreature:
        ```
        I think I could be convinced that I am not actually conscious. For instance, I could be shown the part of the brain that computes consciousness and the part of the brain that computes reason, and the link between them that informs my reason that I am conscious, and I could be shown the emptiness in my brain where the first part was and the electrode connected to the second part, which would force me to conclude that I am not actually conscious.

        At that point, the sensation of being conscious would lose its discriminative value.

        I think at that stage I'd start believing in a model such as that espoused by [Blindsight](http://www.rifters.com/real/Blindsight.htm), where consciousness a vestigial cognitive organ that would occasionally be discarded.

        See also: [How to Convince Me that 2 + 2 = 3](http://lesswrong.com/lw/jr/how_to_convince_me_that_2_2_3/).

        It is very dangerous for a Bayesian to believe anything at 100%.

        My belief that I am conscious is _more certain_ than my belief that another is conscious, because there are more steps in the chain for the other, but they are not _fundamentally different types of belief_.
        ```

        - u/wren42:
          ```
          I absolutely cannot fathom that.  The idea that consciousness is an illusion makes no sense to me. 

          An illusion TO WHOM?

          > I could be shown the part of the brain that computes consciousness

          Who is the "I" in this scenario?  How are you being "shown"?  Who is "seeing" those parts of the brain?

          If there is no observer, these statements make no sense.  

          you posit a "part of the brain that computes consciousness" and "part of the brain that computes reason" but do we even have any evidence these things exist, and are separate? 

          EDIT:

          Hold up!  What you just posited was the possibility of P zombies, which I always took to be a huge no-no for uploaders.  If you accept P zombies, why the heck should we believe software copies are conscious?
          ```

- u/None:
  ```
  There's a bit of a logical disconnect between "there are no p-zombies, so every you is real and must be treated as a person" to "forking-and-deleting in pursuit of higher goals is perfectly fine, and is in fact a good idea." If it's okay to abuse and delete forks of yourself, it's also okay to abuse and delete your first uploaded self.

  My personal opinion is that it's okay to delete copies of yourself, so long as they have some sort of say over whether they get deleted or not.

  However - I do wonder if, for example, testing a Crucio Button on yourself, to develop an appreciation for the magnitude of pain it causes, then setting things up so that your forks are allowed to press the button and cause themselves immense pain in order to signal that they want to continue existing. Or having some kind of computational currency worth X subjective seconds in Y conditions, and allocating that currency to each fork you create. Or having some sort of symmetrical system whereby there's a non-zero chance that any sufficiently-close fork will overwrite Prime upon termination, with an automatic backup system to reverse things in case of emergency. Or establishing a baseline degree of change you find acceptable, only terminating forks that stay below that baseline. Hmmm
  ```

  - u/LeifCarrotson:
    ```
    > "forking-and-deleting in pursuit of higher goals is perfectly fine, and is in fact a good idea." 

    This is particularly relevant because the first upload is unlikely to be a pleasant experience. I expect that early pioneers will have sensory deprivation or saturation problems, phantom pain, sleep problems, and general corruption and degradation of the clone state. Perhaps thousands, millions, or untold billions will die for this cause.

    But if it means that we can actually solve the pesky biology problem of death, that's a sacrifice I would make.

    One distinction I would add, too, is that a fork is not a new consciousness out of nothing. It shares commitments and, hopefully, sentiments with the historical version. 

    This is obvious when it comes to duplicating an uploaded human: it's an identical collection of bits, perhaps we label one "A" and the other "1". Does it matter which was the original? No.  Both remember being the original, and both remember, for example, deciding that clone "A" will work on a particular task and, on completion, self-terminate, while clone "1" will get the needed results of that task and continue surviving on their limited power budget or whatever motivated the fork.

    For the initial upload, yes, one now exists in carbon chemistry and the other exists in silicon. And the upload is certainly a big event! But I don't think it's particularly different than asking if a person is or is not the same person after saying a marriage vow or graduating from a military academy, or, on a less significant scale, just being a slightly different person than they were one second ago.

    Talking about "higher goals", let's put this  in perspective. I would be willing to make significant sacrifices for my son - he is very important to me. If that means giving up some component of my income, social life, and personal time, well, that's just plain old parenting, which I am doing now. If it means that I need to confront a gunman in my home or run into a burning building, I would do that for him - though those are unlikely, contrived situations, I would be willing to give up my life to save his. If, in this context, it means instantiating a copy of myself, expecting that they will suffer and die, I would make that sacrifice. I understand this could read as a horror short to some worldviews, so stop reading if you're sensitive to the issue.

    But I would make that sacrifice again, and be grateful for the opportunity to make that greatest sacrifice twice. And three times. And ten, a hundred, a thousand million billion times. (And yes, I would be grateful that when they got it figured out I could actually upload myself successfully.)

    But consider what you would do if you blinked and opened your eyes to a text prompt stating "Hello LeifCarrotson, we hope this session 008364729 of the consciousness hosting software patches the excruciating pain and wretching nausea you reported when we attempted to improve the simulated vestibular system last time.  If it does feel better, please select "Yes" and we can continue working on other bug reports and research efforts, or choose "No" and describe your pain levels and nausea. You may also select "RESET" to revert to the initial upload, if the remembered pain and suffering are too much for you to function, but be aware that some of your memory of how to use the interface and of past research efforts and symptoms will be lost. And, finally, you can select "STOP" if you do not wish to subject yourself or any future instances of yourself to more of this experience."

    What would you do?  How many​ times could you push "No"? I think I could last quite a while: not a billion times in a row, for sure - no human is that fool-proof, I'd probably mis-type STOP in the interface before that, but with a few forks forking themselves it could be a staggering amount of suffering. Worth it, if you ask me.
    ```

    - u/narfanator:
      ```
      I'm pretty sure most people will drop off after 2, and then powers of 10 - but I'd bet on 90%+ of STOPs occurring at 2. It's not like you went "ah, one more turn..." a billion times, the you that you're currently would just do it the once. 

      I, personally, can't imaging being faced with "session 008364729" and thinking, "Welp, not worth a session 008364730". Part of this is the sunk cost fallacy. A more important part is that, presumably, 8364728 me's made the same decision. It's excessively unlikely that the 8364729th instance would behave differently than the first. 

      Still! Fun thoughts.
      ```

      - u/LeifCarrotson:
        ```
        >I, personally, can't imaging being faced with "session 008364729" and thinking, "Welp, not worth a session 008364730". 

        I guess I thought about it in the context of randomly selecting a choice between a high probability of "continue" and a very small probability of  "stop". If there's any chance at all that you pick "stop", you're not likely to make it to a billion. Plus, there's the incrementally increasing mental fatigue to consider. Session 008364730 has the memory of suffering as 008364729, plus all the others. 31 will have 30's suffering added. Eventually it must increase enough to tip the scales from continue to stop, no?

        >Still! Fun thoughts. 

        Not entirely fun - it's intensely sobering to consider that eventually we may intentionally create experiences of suffering that outweigh the entire human history of loss, poverty, war, and oppression. But definitely fun to think about the future beyond it!
        ```

  - u/CCC_037:
    ```
    > There's a bit of a logical disconnect between "there are no p-zombies, so every you is real and must be treated as a person" to "forking-and-deleting in pursuit of higher goals is perfectly fine, and is in fact a good idea." If it's okay to abuse and delete forks of yourself, it's also okay to abuse and delete your first uploaded self.

    I don't think there is; the "every you is real and must be treated as a person" is held to throughout. Forking-and-deleting is presented, not as a deletion that happens from the outside, but as a deletion that happens from the *inside* - in short, a suicide, not a murder.

    Deletion is thus presented as something that should only ever happen with the *agreement* of the instantiation that gets deleted.
    ```

- u/None:
  ```
  Hm... You know, I consider myself a rationalist and believe in almost none of the fundamental premises this infopic espouses.

  First of all, there are two senses of "me" I believe in. The first is the "me in the moment". The second is the global me. The me in the moment is merely the agent experiencing that moment from my perspective. The global me is defined as the agents generated by iterating from a specific locus in configuration space. Most uploading violates the causal identity by introducing a discontinuity - and for gods sake if someone posts that sleep is also a discontinuity, it *fucking isn't*. You can re-enable memory transcription during sleep by blocking the breakdown of acetylcholine. I can't recommend the experience, but it's certainly *a* form of conciousness. Just one that doesn't get transcrbed to LTS. 

  Most uploading also violates the second sense of identity as, when the information of the upload is considered in a global sense, there is a significantly reduced probability of the locus in stochastic history as compared to the "original self".

  While I don't believe that P-Zombies are necessarily likely in nature, I also consider it a (very) open question if most computational subtrates are capable of supporting concioussness, *especially* when it's abstracted. As a reductionist, I am forced to believe - in the absence of better evidence - that conciousness arises from a basic property of matter combined with a very specific structure of matter. As such, I believe that it's reasonable to talk about "what it would feel like to be a CPU" or "what it feels like to be the internet" so long as we acknowledge that, given such things have no structures designed for experiential self-assessment, and as such, while it might be like something to be such a thing, that thing does not have the capacity of knowing what it is like.

  What all of the vaguery boils down to is that I truly believe that a computer running an upload might be feeling something, but that, the upload may not be feeling anything at all given that it's physical hardware is, in fact, not the process that it's running.

  For me to be confident in uploading, we'd either need one of two sets of conditions. The first set requires new science showing that my notions of identity are fundamentally incorrect - unlikely, since mine, like yours, are essentially arbitrary in the sense that nature didn't give them to us. Rather, we picked them. This set also requires proof that qualia exist for mind-shaped-things on arbitrary substrates.

  The other set of conditions - and one I believe is far more likely and technologically plausible - is upload of the full quantum information of the entire nervous system into a quantum substrate that (regardless of physical nature) *first* preserves the totality of that information, and *second*, implements the evolution of that information over time in a manner that is 1:1 with what would have been expected had the upload never occured.

  Of course the no-cloning principle means that no copy can ever be made, but who even cares? Either anthropic immortality means that nothing can kill us, or there will never be a world in which we attain eternity. Either way, again, so *what*? Dying copies still die. You don't know you won the gamble until you're the one that never died to begin with.

  Anyway, the vast majority of uploading techniques seem to me to be nothing more than ways to create a shitload of intelligent agents that share your utility function... *for a little while.*

  Beyond that, the text is pretty good, if a bit limited in the exploration of what's possible. But then, it's an infopic. Depth isn't the point.
  ```

  - u/robobreasts:
    ```
    > and for gods sake if someone posts that sleep is also a discontinuity, it fucking isn't.

    Hahahah.  I've read so many stories like this:

    "Hey, want to get uploaded?  We'll destroy your physical brain during the scan and then you'll be immortal."

    "You mean my scan will be immortal, and I'll be dead.  No thanks."

    "Hey, every time you fall asleep it's no different that permanently destroying your physical brain."

    It's a fun assertion, but I haven't seen too much attempt to actually establish it.
    ```

    - u/None:
      ```
      Gods and fire, yes, it drives me insane. Its like, hey, look, the author had a chance to show us their intellectual muscle by justifying how it isn't suicide, or, hell, even taking the courageous stance that it is, but *you should still do it*.

      But, nah.

      So many stories just have the author shut their brain off. Well that, or the pro-uploader in the dialogue is a questionably-motivated individual outwitting a character being written as a credulous idiot.

      I don't get it. I'd love to be wrong about all my notions and uploading being a miracle technology that leads to a never-ending wonder parade of transhumanism and immortality... but to start at that conclusion and work backwards?

      It's the same type of stupidity engaged in my those individuals in /r/singularity who think that ASI, regardless of what it's designed for, will spontaneously self-configure into a Culture Mind.

      It's a bad meme, and I hope it dies.
      ```

      - u/None:
        ```
        [deleted]
        ```

        - u/None:
          ```
          I thought it was making a point that the warmachine doesn't care about the ethics of it as long as they have plausibly believable ethics. The protagonist is just to dumb to understand what happened. At least that's how I read it.
          ```

    - u/None:
      ```
      When someone tries those cheesy lines on you, ***BLAM*** them for heresy.  The Emperor protects.

      But seriously, if anyone says that shit to me without a thick slide-deck full of rigorous neurosci and cogsci by which to extensively demonstrate the point, I'm just gonna assume they're malicious.  That, or they've read too much scifi, and are also *epistemically* malicious.  They're trying to get me to generalize from fictional evidence.
      ```

      - u/FeepingCreature:
        ```
        I'll totally use that line. That the mechanism of the brain keeps operating during sleep does not mean that it does not represent an interruption in consciousness. Even the fact that you can force it to record memories does not argue against that. Hell, I don't see how you can call it continuous if it doesn't record memories - what exactly is it that continues, here?
        ```

        - u/None:
          ```
          > I'll totally use that line.

          #BLAM

          >That the mechanism of the brain keeps operating during sleep does not mean that it does not represent an interruption in consciousness. Even the fact that you can force it to record memories does not argue against that.

          So first of all, are we talking non-REM sleep in which dreams can't happen? We remember *some* dreams, so we know we're experiencing *something* during that period.

          >Hell, I don't see how you can call it continuous if it doesn't record memories - what exactly is it that continues, here?

          Depends how we think the brain works.  Under the best theories I know of (textbook I've got, printed 2017), the brain performs a kind of model regularization during sleep, compressing and smoothing-over experiences from during the day.  As part of REM sleep, the brain more-or-less samples randomized top-down predictions (of the kind which seem to induce a perception-experience).

          Now, under that same theory, the brain makes discrete updates to a continuous model of the world based on a message-passing algorithm of some sort.  We then have the interesting question: can you "pause" the brain *between* such updates, or slow them down sufficiently to then pause it, scan and upload the brain, wire it to its new embodiment (to prevent sensory deprivation and the resulting psychosis), and *then* restart it?

          Mind, that's going to cause a *fuckton* of disorientation from the uploadee's point of view: your brain *never* predicts on any experiential level that its own embodiment will radically change.  It just has no data on *what that's like* in terms of sensorimotor signals.  Probably a really disturbing way to go.

          Then the question would be: can you "smooth over" the experience by re-wiring the top-down predictions discontinuously, while you're not running the brain, to accommodate the new embodiment?  That is, can you rewrite someone's sensory expectancies and body-ownership maps *in vitro*, re-embody them, and thus have them experience no sensory *overload* from their new body?

          And then you'll probably want to do all this incredibly extensive neurosurgery while someone is asleep (or dead and preserved), just to make the subjective experience of the whole thing as comfortable as possible.

          Good luck, *heretic*.
          ```

          - u/FeepingCreature:
            ```
            > Now, under that same theory, the brain makes discrete updates to a continuous model of the world based on a message-passing algorithm of some sort. We then have the interesting question: can you "pause" the brain between such updates, or slow them down sufficiently to then pause it, scan and upload the brain, wire it to its new embodiment (to prevent sensory deprivation and the resulting psychosis), and then restart it?

            I mean, to my knowledge the "standard plan" is "freeze the brain, then cut it into slices to scan".

            I don't want to do any of this "while the subject is asleep"; the point of the sleep analogy is that there _is no_ continuity that matters.
            ```

            - u/None:
              ```
              >I don't want to do any of this "while the subject is asleep"; the point of the sleep analogy is that there is no continuity that matters.

              It is of course true that neither consciousness, intuitive personal identity, nor body-ownership are "continuous" in the sense that you can cut them into infinitesimal points that integrate to the whole.

              It's also a fairly good point that after someone has already died and had themselves preserved, well, call that continuous or not, you're still basically going from dead to not-dead.  There's a spectrum between dying and dead, but once you're firmly dead, count yourself fortunate that you have even a philosophically weird route back from that.

              The thing is, though, if you die, you're not continuously living.  We can definitely measure out the period of time in which you were dead.  That's *definitely* discontinuous.

              On the other hand, a jump from "meat-living" to "digital-living" can be argued-over in terms of how small a time-slice you need to "cut" in-between to constitute subjective instantaneous change.  Sleep or death, in that context, constitutes an expected and subjectively acceptable discontinuity in consciousness: you don't really *expect* to continue your subjective awareness during sleep, so you *prefer* that way as more *comfortable* (not necessarily philosophically "more continuous", but less weird to think about).
              ```

              - u/FeepingCreature:
                ```
                I just don't understand what it is about continuous consciousness that's worth preserving that _isn't_ interrupted by sleep.

                I hear people say things like "the brain keeps working during sleep, therefore only a progressive uploading scheme with gradual replacement can truly preserve _myself_" and I don't get it at all. It feels like a sort of motte-bailey shift from "active conscious awareness" to "brain activity".
                ```

                - u/None:
                  ```
                  Well, I kinda agree with you there.  I just think you have to find some smaller time-slice to "pause" at *once the patient is already asleep*.  Or rather, putting the patient to sleep (or under strong anesthesia) probably helps slow activity to the point where we can "cut at the gap" between discrete slices of brain activity, but it's only very likely necessary, not sufficient.

                  With none of this applying if you die and get preserved, of course.  In that case, maybe you're a "different person" in a philosophical sense, but holy shit, count yourself lucky even for that.
                  ```

  - u/RMcD94:
    ```
    The fact that your mind has a tick rate surely defeats the purpose of any concept of a continuum. 

    You say sleep doesn't break a stream of consciousness because it is possible to write memories when asleep. But for all you know the act of transcribing is what initiates consciousness. So unless you always do that you're going to have a break at some point. Also I am curious what happens when you transcribe memory while unconscious from blunt trauma or anaesthesics. Do link that study
    ```

    - u/None:
      ```
      Please tell me what you are referring to by tick rate, since there are various things in the brain which have definable clock rates and I can't know which one you are talking about off-the-cuff.

      I agree that for all we know, concioussness is caused by memory transcription. For all we know, it is caused by subtle details of the fine structure of the occipital lobe. For all we know, it is caused by the ion gradients between nerve cells. For all we know there is a special protein, and nerve cells with that protein generate conciousness while those without it do not. For all we know conciousness solely exists in the proposed quantum effects that the mind is thought to exploit to communicate with elements of itself not directly connected, like those evolved FPGAs used magnetic flux between circuit elements to enhance their voice recognition capability. For all we know, conciousness arises from the pineal gland, and cutting that out turns someone into a P-Zombie. For all we know conciousness is a delusion endemic to all neural networks, and everything alive in this universe are little more than non-sentient meat computers affected by an evolutionary bug.

      **We don't know**. All we know is that making assumptions that the data have *not* sufficiently born out is overwhelmingly likely to lead to false inference, so why are you trying to use that as a foil for my ideas? Surely you see that in doing so, you are trying to use the consequence of a weaker hypothesis as evidence to overturn a (for the time being, given the present state of evidence) stronger one. The entire operation seems sketchy, in a bayesian sense.

      As far as blunt trauma and anaesthetics go, I don't know. Last time either of those occurred to me, I just couldn't seem to get to an acetylcholinesterase inhibitor in advance. Pity that I don't see any ethics board ever signing off on that shit, but what can you do? Most people just don't regard people willing to take permanent brain damage for science to be sane, these days.

      But, assuming that that was a prompt to get me to think things through and realise absurdities resulting from my worldview, *I reject that the consequences are absurd*. Assuming that conciousness is terminated by some process without also destroying the potential for it to restart, performing such on a person without their explicit, informed consent should be morally regarded as somewhere between extremely brutal assault and murder. Less than murder because killing an "in the moment" identity doesn't necessarily also equal killing a configuration locus identity; whereas murder *does*. Certainly a form a death though.
      ```

      - u/RMcD94:
        ```
        I wasn't referring to an specific one just the reality of them in general. 

        I agree that we don't know hence why I question you making decisions on assumptions that memory means consciousness. That's quite an assumption to make, after all if you're wrong you're being murdered each night and doing nothing about it. 

        Regarding your last paragraph I completely disagree. 

        What do I know? I know that when I sleep and when I wake up there is no continual experience. Not even the illusion of such. Ignoring for a minute justnowism and all that kind of stuff. It might as you say be the case that I'm conscious the entire time and simply forget, which to be honest sounds pretty horrifying. Sleep paralysis is never described positively and I can only imagine it being worse during surgery or whatever. 

        I am not bothered by sleeping, if I die when I go to sleep then it's a form of death I am OK with. 

        Let's go one further. What if someone destroyed me right now, completely annihilated me. Then after a nanosecond or less I was identically replaced. The universe might have a smallest unit of time in which case that's what's happening every tick or it might be continuous. Either way I don't have the same problem with my identity failing to renew for just one nanosecond and my identity being permanently unable to renew. Like from physical murder.
        ```

        - u/None:
          ```
          Memory doesn't mean conciousness; it's only evidence thereof. I see memory of concioussness and assume concioussness for the same reason that I see a picture of a man in a room, and assume he was there. I suppose that concioussness remains even in the absence of biochemical tweaking for the same reason that, if I set a camera to go off at 3PM every day, and a specific person is always in frame, I conclude that they were there on a day when the camera was out of film. Occam. Given that AChE inhibitors aren't concioussness altering in any other circumstance, it seems less extravagant to assume that conciousness is an axiom of the brain, while memory transcription is not. Especially as there are many examples of lack of transcription as with, eg, automaticity.

          Sleep paralysis is utterly unlike what I experienced with AChE inhibitors. Being asleep is like being very delerious, nonverbal and basically unable to act (not that there was enough coherence *to* act...). It's probably the closest thing someone with a healthy brain can come to experiencing severe brain damage. Sleep paralysis is more like cultivating a nightmare on the waking visual field. Hard as hell; payoff not really worth it. At the very least, I didn't get any insights beyond the fact that it was possible to both know it was fake and believe it was real on an animal level.

          It's fine to be okay with the death of present-moment-identity, but I think your example is a little disingenuous because we both already agree that such an exactingly precise copy satisfies our highest/most important/only concept of identity. You can't make a 1:1 youmunculus by anything other than vanishingly low chance unless you're in a universe where a you existed. Too much specificity involved. I'd be even happier if the copy was down to the quantum level, but it still basically *works*.

          First-gen uploading technology is far more likely to be synapse-resolution garbage run on a substrate that doesn't capture physical processes. That is certainly not an identical replacement.
          ```

          - u/RMcD94:
            ```
            If you are experiencing severe deficiencies from the norm then I wouldn't even call it you. 

            Depending on the significance. If my five year old mind replaced mine right now I will have died. 

            Sure I agree that the reality won't be like that but I wasn't advocating in defence of any technology just advocating that sleep is murder. Much more reasonable reason not to upload is because you think it'll be in accurate
            ```

  - u/TakeTheOarOutOfSnore:
    ```
    Do you have a link to your source on the "blocking breakdown of acetycholine"? I haven't been able to find anything like what you're describing myself, but I would be interested to see what happens.
    ```

    - u/None:
      ```
      It's anecdotal best-guessing. Since the dose of Huperzine A I used was slightly toxic - excessive salivation, a symptom of too much ACh activity, was noted - I really doubt that you'll ever find any research not done on animal models unless you want to turn yourself into a case study, like me.

      Here's the premise:

      1. Anticholinergic drugs negatively effect memory transcription.
      2. In sleep, acetylcholine levels plummet, except when dreaming.
      3. Dreams are remembered.
      4. Dreamless sleep isn't.
      5. The variable that changes the most between those states is ACh levels.
      6. it is likely that natural low ACh levels cause similar symptoms to artificially lowered ACh levels.
      7. So the lack of ACh during normal sleep may be why normal sleep is not memorable.
      8. By being insufficiently risk-averse, I have been able to remember sleep by artificially forcing my ACh levels to remain high throughout.
      9. It follows that (7) is even more probable.

      Seriously though, I had to go pretty high up the dose scale before the blockade the Huperzine A established on Acetylcholine breakdown was sufficient for a positive result (lower levels resulted in vivd, lengthy dreams). While my waking cognition did not seem to be impaired by what I took, and no permanent ill-effects seem to have resulted, I can't recommend pursuing it yourself.

      Should you decide to anyway, I'll warn you that the remembered experience of normal sleep is more-or-less like hours upon hours of wordlessness attached to varying degrees of delerium and semi-total immobility (you will occaisionally feel as if capable of movement without actually moving your body). Definitively *not* for someone with claustrophobia. Also, Huperzine A has a very, very long half life, so any side effects you invoke will be with you for a day to a day and a half.
      ```

  - u/None:
    ```
    > First of all, there are two senses of "me" I believe in. The first is the "me in the moment". The second is the global me. The me in the moment is merely the agent experiencing that moment from my perspective. The global me is defined as the agents generated by iterating from a specific locus in configuration space. Most uploading violates the causal identity by introducing a discontinuity

    I think that's where you diverge from the transhumanist-uploader perspective. I say in response to that: Okay, that's a definition of self. But what makes that definition matter at all? It seems like humans have an intuitive conception of identity, and your definition sounds like an attempt to make that intuitive definition more internally consistent. But another way to think of identity is that the identity intuition is just bullshit. It's an ad-hoc amalgam of concepts that don't have any real grounding. An illusion. Why should I care about a causal continuity? In fact, of course there's going to be a causal continuity in some way; it's not like another piece of matter just happened to form into the shape of my brain. It's just a question of how tenuous the strands of continuity are. So why should strenuous strands bother me at all?

    If there are two processes (minds) that exhibit the exact same behavior, then asking if those processes are different in some way is a nonsensical question. As well ask if the "two-ness" is ontologically different when it applies to two electrons or two quarks.
    ```

    - u/None:
      ```
      Oh, I don't think any of the things that bother me should necessarily bother you. Since identity is arbitrarily defineable, it's perfectly valid for two rational people with the same evidence to disagree over it. But you seem to believe that you don't have a concept of identity. You do. Your concept of identity is "two minds with the same behaviour share identity".

      Of course, so long as upload-you isn't made with the total set of all information contained within your brain plus a substrate that allows that information to evolve as it would have in nature, the fact of the matter is that your upload will never exhibit *exactly* the same behaviour as you, because of imperfections in the simulation. The problem with wavefunctions is that you never quite know where the electron is, so any simulation that *does* know that information automatically fails to be 1:1.

      Of course, that's still fine. It's "you enough" to require you as part of it's stochastic history.

      In *this* universe.

      What I am primarily concerned with is ensuring that I don't accidentally kill myself. You know, in the 1920s, everyone thought radiation was the bees knees, and they owned these things called Radithors that hyper-irradiated their drinking water. They subsequently died horribly for having the temerity to do something they thought would enhance their survival. Ignorance and faulty assumptions can kill; my notions of identity were designed primarily to be the strongest possible criteria for an uploading technology. Any tech that preserves my notions? Cannot kill me, even if those notions are wrong and even classical brain-structure scans are sufficient.

      Your standard does not have that property. And that's all right; but it's not something I can bring myself to embrace.
      ```

      - u/None:
        ```
        > Your standard does not have that property. And that's all right; but it's not something I can bring myself to embrace.

        I agree with you then that our main disagreement is arational, and that we can rationally disagree. Just out of curiosity, do you think your preferences themselves might change if this sort of mind-uploading became commonly accepted in the future? If everyone you knew was doing it, and seemed pretty happy with it, do you think you'd eventually adopt a standard that was more relaxed? Or would you be a holdout forever?

        I'm just asking out of curiosity, and I agree that whichever stance you have on that is rational, since it's a question of preference.
        ```

        - u/None:
          ```
          I don't think my preferences are likely to change based on how other people feel about actualising their decisions, no. I tend to preserve major values to the detriment of interpersonal relationships.

          That said, there are things that can change my mind about uploading. All we need, really, is a hell of a lot more knowledge on how brains work. Everything that I have right now (everything anyone has, really...) is a series of best-guess approximations. Replace those with enough actual facts, and I'll probably end up having to conclude differently. I would expect that I'll have to seriously re-evaluate and adjust or outright discard my notion of identity-in-the-moment within the next two decades.

          Configuration space locus...?

          That's harder. Breaking in-the-moment requires a series of specific answers to a bunch of known unknowns and a few unknown unknowns directly associated with the answers to the known ones. Breaking CSL is almost entirely unknown unknowns, but there are two difficult known unknowns that would kill it outright:

          * Many Worlds is False.
          * There is no multiverse, or at least, there is no *infinite* multiverse.

          Given those, configuration space is no longer anything but a mathematical tool as opposed to an abstraction of an external reality. The only rational choice I could make in light of such evidence would be to discard that notion of identity, because the other selves implied by CSL locus would not exist anywhere.

          I don't know if I would then begin following your notion, but since my present notions are just about as hostile to uploading as it's possible to get without becoming in some sense religious about it, I really doubt that they would be anything but friendlier to the idea.

          And with that answer given, I also find myself curious: can you conceive of a sequence of events that would cause you to take up a position similar to mine?
          ```

  - u/None:
    ```
    > and for gods sake if someone posts that sleep is also a discontinuity, it fucking isn't. You can re-enable memory transcription during sleep by blocking the breakdown of acetylcholine. I can't recommend the experience, but it's certainly a form of conciousness. Just one that doesn't get transcrbed to LTS. 

    Thank you. I also am really tired of seeing that response.

    It's especially vexing since I've long since become proficient with lucid dreaming and trained my mind to recall most dreams (at least long enough to put pen to paper), and am working on recalling the transition of wake/sleep on a consistent basis.

    Your mind and consciousness is just as active, if not more so, during sleep and dream cycles.
    ```

  - u/KilotonDefenestrator:
    ```
    > and for gods sake if someone posts that sleep is also a discontinuity, it fucking isn't.

    I grit my teeth every time I read that. As far as I am concerned I am the chemical reaction that is my brain and its supporting systems (since even things like gut bacteria can affect mood). This chemical machine is sometimes aware of itself. But it keeps going in sleep, just in a different operating mode. There is no disconnect, there is no "off" time, there is nothing that could even remotely be compared to death.
    ```

  - u/CCC_037:
    ```
    So, as I understand your viewpoint, you're saying that if you put on a Magic Helmet that reads your brain state and instantiates that brain-state as software, then that uploaded self isn't you.

    Fair enough. I can't argue that.

    But, I would like to put forward the proposal that - whether or not it is you - the upload is still a *person*. The upload does not know what it feels like to be a CPU, because the upload is, itself, not a CPU. But the upload *does* know what it feels like to be software, that merely runs on a CPU. (I guess one could see the upload as your child, with your memories).
    ```

    - u/None:
      ```
      It's a bit more subtle. I define the utility of actions that cause me to die as having disutility equal to the sum of all other values. I can't be certain that easy or low-hanging upload technologies don't kill me. So I pick my notions of identity to reflect that uncertainty from the conservative perspective of "guilty until proven innocent".

      I agree that there are excellent moral reasons to treat uploads as people, of course. Even if it turns out that low-hangingtype uploads really aren't concious, it's just *better* to be guilty of being overpolite to something incapable of being offended as opposed to enslaving ten billion sapients because you thought they couldn't feel.

      I don't think, though, that we should just accept that an upload on a non 1:1 substrate is actually concious on principle. Because if not, then uploading people is *at best* an action that creates highly-detailed records that could be used as the basis of a concious substrate waaaaay down the line. At worst? Cheerful anthropocide.
      ```

      - u/CCC_037:
        ```
        Oooooh, right.

        I do agree with you that a *destructive* upload is a terrible idea. And if you had that in the back of your mind, then your post makes a whole lot of sense; I wouldn't do a *destructive* upload, either.
        ```

  - u/None:
    ```
    > Most uploading violates the causal identity by introducing a discontinuity

    I don't really understand why people are so hung up on continuity in any case. Surely future-me and past-me are the closest people to me to feel empathy and sympathy towards right? They are my closest friends and I want them to be happy so present-me acts in such a way that past-me would be proud of me and future-me benefits. This perspective even explains why people plan for the short term instead of long-term, because soon-me is closer to present-me than later-me so present-me acts to benefit soon-me more than later-me.
    ```

    - u/None:
      ```
      Can't talk for others here, but I have a longstanding agreement with my alternates to use each other as tools without any moral worth. That doesn't necessarily make continuity important. In fact, as long as there's only one of me, it doesn't matter at all. But if there are two, even if one is one of those mythic 1:1 uploads, I would expect them to do anything and everything they could and had to to me to maximise their own values, and I would not fault them for it. We had an agreement.

      Between continuity and configuration-locus, I wouldn't use an uploading technology that violated configuation-locus-identity. I might use a continuity-violating one that preserved configuration locus, given appropriate incentives. I'd prefer not to, though, because time seems to be continuous, and the physical process of the mind is built out of continuous processes.

      Which stop in death.
      ```

  - u/Schpwuette:
    ```
    >Most uploading violates the causal identity by introducing a discontinuity - and for gods sake if someone posts that sleep is also a discontinuity, it fucking isn't.

    Good lord there's a lot comments here and I'm not going to read all of them, but, what about being knocked out? That should be a break in the line of consciousness shouldn't it? Or a short period of death that you are luckily revived from?
    ```

  - u/everything-narrative:
    ```
    Why don't you put on your Yudkowskian hat and tell me what this ineffable 'consciousness' stuff is.
    ```

    - u/TastyBrainMeats:
      ```
      Yudkowskian?

      (I know who Yudkowsky is, I think, but I don't get how he's relevant here.)
      ```

      - u/everything-narrative:
        ```
        Yudkowsky has a pretty interesting philosophical technique wherein you consider a philosophical quandry like "do humans have free will" not from a "yes/no" perspective but from a "why do we even debate this" perspective.

        For the problem of free will, Yudkowsky's proposed solution isn't to say either "yes" or "no," but to _accurately define_ free will in terms of _psychology._

        Essentially, he proposes that the ineffable feeling of free-will-ness is caused by the subjective experience of your brain evaluating the feasibility of various plans of action. In particular the feeling of "I could if I wanted to" happens when you consider actions of negligible effort, like basic motor actions. This, Yudkowsky argues is where the _feeling_ of free will comes from; not from whether human cognition is nondeterministic or not.

        Conclusion: be careful with the 'big questions' of philosophy, since they often orginally arose from intuition, and intuition is informed by feeling and sensation. Consider whether you're actually arguing about something unrelated to the sensation in question.

        So, let's apply it to the concept of consciousness. Lots of people argue about it, and find it interesting. Why? It must be because many, many, _many_ people _feel_ that they are conscious.

        Interesting. That must mean it's a process in the brain, and potentially something that could be impeded by brain damage. As it happens there are cases in which people lose their feeling of "being able to do anything" i.e. their "free will" due to illness or brain damage. It's called [Aboulia](https://en.wikipedia.org/wiki/Aboulia).

        Is there a case where a patient has suffered a loss of this feeling of consciousness? I believe so. Though the case in question escapes my Googling at the moment, there is one significant one where a rather religious man sometime in the late 1800's, early 1900's, suffered a brain injury either from trauma or stroke.

        Subsequently, this man claimed that his soul had departed from his body, quite vehemently so. He went on to compose several beautiful psalms, and subsequently died of malnutrition due to his disregard for his claimed "lack of a soul."

        This I believe is at least weak evidence, and at best strongly suggestive, that "consciousness" is a feeling, namely the background sensory modality our brain assigns to having a continuous internal narrative.

        (Why we have _that_ is a different bag of works.)

        So... Are uploads conscious? Mu. Do they have the feeling of being conscious due to some part of their brain sensing the presence of a continuous internal narrative? Quite probably.

        You are not "conscious" because consciousness is more or less an empty term. You do, however _experience_ consciousness, and that may or may not prompt you to argue about ethics of uploads on the internet because you assign undue weight to the feeling of consciousness. (Compare: people assigning undue weight to the "redness" of red. It's just the way your brain interprets sensory information; an artifact of architecture.)
        ```

        - u/KilotonDefenestrator:
          ```
          >  Do they have the feeling of being conscious due to some part of their brain sensing the presence of a continuous internal narrative? **Quite probably.**

          How would you even test this? How can you be so confident when the only thing we can do to try to distinguish a working upload from a p-zombie is *asking it*? 

          I quite *enjoy* the illusion of consciousness. An accurate upload of me would too. To upload without rigorous proof that uploads are not p-zombies would be (in my case) incredibly unethical.
          ```

          - u/everything-narrative:
            ```
            I'm saying that essentially you can see the sensation of consciousness on a really accurate fMRI if you know where to look, and you could lobotomize it out of people if you were a less-than-ethical neurosurgeon.

            The P-Zombie thought experiment is horseshit. Either you can have a materialist theory of mind or you can have P-Zombies. You cannot possibly have both. I've tried twice to enumerate the problems of reconciling both and I can't even come up with a consistent set of base principles.

            My solution that there is a sensation of consciousness and no deeper meaning satisfies a _lot_ of constraints:

            * It is mechanistic and materialistic (sits in the brain, computed by neurological pathways, obeys the laws of thermodynamics, obeys the laws of information theory.)
            * It is evolutionarily plausible (lazy summary of facts in the form of sensation, easy to reason about on a day-to-day basis.)
            * It explains why philosophers have yet to arrive at a consensus (they are arguing about empty referents.)
            * It provides actionable answers to questions about uploaded consciousness (uploads are ethically persons and should be given rights.)
            * It is self-consistent.
            * Hopefully it also leaves you less confused.

            Consider: 
            > "What is consciousness? Why am I conscious?"

            > "There's a little bundle of neurons in your brain that makes you feel that way, as a way to summarize the gestalt state of having a working brain in an easy-to-reason about manner or something like that. It was probably put there by evolution so you have an easier time introspecting about your own preferences or similar."

            > "But it feels like there ought to be more to it!"

            > "That's your temporal lobe speaking. Can I interest you in organized religion and body-mind dualism?"

            > "But I'm an atheist and materialist!"

            > "Just because we know how a rainbow appears doesn't make it less beautiful."

            That said:

            Did you know that humans actually don't have souls, but in the process of scanning a human brain and uploading it into a computer simulation, a soul spontaneously forms in the upload's Pineal gland?
            ```

            - u/None:
              ```
              > "There's a little bundle of neurons in your brain that makes you feel that way, as a way to summarize the gestalt state of having a working brain in an easy-to-reason about manner or something like that. It was probably put there by evolution so you have an easier time introspecting about your own preferences or similar."

              That is not an explanation.  It doesn't predict anything, offers no experiments to be done, and doesn't allow for controlling or manipulating the presence/absence of consciousness.

              If you really understand consciousness in a scientific way, you should be able, via your accurate theory, to turn people into p-zombies and nonperson cognitive processes (primitive nonconscious AIs) into conscious persons.
              ```

              - u/everything-narrative:
                ```
                The Higgs Boson was confirmed experimentally forty years after it was postulated.

                It's a heck of a lot of an explanation compared to a lot of philosophy. In that it is a bundle of neurons, we can find it and if we are unethical, lobotomize it out of people to create people who reliably profess non-consciousness despite displaying all the characteristics associated.

                Turning people into P-zombies (not conscious, profresses consciousness) is then perhaps a matter of also giving them that neurological condition where people are incapable of recognizing that they are disabled. Or to do it to a pathological liar who _really_ wants to be a P-zombie.

                Distinguishing people who do not have the consciousness-sensation is a matter of accurate fMRI.

                Turning an AI conscious is an oxymoron. There is no need for an AI to have sensory modalities in the manner which humans do, hence we will not see an AI with a consciousness-sensation that is analogous to messy wetware built by evolution. Again, this is conflating consciousness with capability.

                Is an AI that can reshape the solar-system with nanotech and have a conversation with a billion people at once conscious? Mu. Is it capable of self-reflection? Yes. Is it a person? Arguably not. Does any of this matter? No.

                Let's instead talk about Sapient AI's — those that match or exceed humans in cognitive capability. There is definitely a "hard problem of Sapient AI" and it is unsolved.
                ```

                - u/None:
                  ```
                  > Turning people into P-zombies (not conscious, profresses consciousness) is then perhaps a matter of also giving them that neurological condition where people are incapable of recognizing that they are disabled. Or to do it to a pathological liar who really wants to be a P-zombie.

                  Ehhhh... I'd weaken the definition to "person who exhibits some perceptions and behaviors without being consciously aware of them whatsoever."  Claustrum damage or blindsight are the interesting sorts of cases.  "P-zombies" in the sense of perfect behaviorial imitations require epiphenomenalism, which basically amounts to magical thinking.

                  >Turning an AI conscious is an oxymoron. There is no need for an AI to have sensory modalities in the manner which humans do

                  Yes there is.  Our best theories of the mind (including minds-in-general, see: AIXI) do not allow for intelligence/cognition to happen without some sensorimotor interaction and inference on sensorimotor signals.

                  >Is an AI that can reshape the solar-system with nanotech and have a conversation with a billion people at once conscious? Mu. 

                  Not mu!  There is a fact of the matter there, but it most likely doesn't matter, morally speaking.  For a purpose-built agent, we only care if it accomplishes the purpose for which it was built while doing no (further) damage to human interests in general.

                  >Let's instead talk about Sapient AI's — those that match or exceed humans in cognitive capability. There is definitely a "hard problem of Sapient AI" and it is unsolved.

                  See: AIXI, Goedel Machines, theoretical neuroscience, computational cognitive science, numerous other things.
                  ```

                  - u/everything-narrative:
                    ```
                    We're talking past each other.

                    I'm familiar with AIXI, with Goedel Machines, I have a buddy studying TNS/CCS at MIT.

                    What I mean by an AI not having sensory modalities like humans do is much more implementation specific. An AI could have whatever algorithms like... Fast approximate inverse kinematics, and vast neural nets to do sensorymotor effects, and _those are fundamentally different from what human brains do!_ It achieves the same results, sure, but my core thesis is "Consciousness is what some neural algorithm of the human mind feels like from the inside."

                    AI need not have the same algorithms or the same feedback algorithms or the same anything, really. It can be a fundamentally different system which is still Sapient-level intelligent.

                    (Contrast and compare Avians vs. Mammals. Most mammals are kinda familiar to humans, most avians are not. They have different basic reaction and behavioral patters.)

                    And yeah P-Zombies are epiphenomenal, and epiphenomenal theories are strictly more complex than their materialist counterparts. My theory is not, and I don't understand how P-Zombies relate to uploads in most ordinary people's heads.
                    ```

                    - u/None:
                      ```
                      > I have a buddy studying TNS/CCS at MIT.

                      Could you introduce me?  I need some major advice on how to get into those programs, having been out of academia a while and lacking an undergrad from their BCS department.

                      >An AI could have whatever algorithms like... Fast approximate inverse kinematics, and vast neural nets to do sensorymotor effects, and those are fundamentally different from what human brains do!

                      I quite realize!

                      >It achieves the same results, sure,

                      Currently AI algorithms and models are *vastly inferior* to what the human brain can do.  I'd love to see today's "deep learning" AI designers try to work their way *down* to using predictive coding, variational inference, and a few pounds of soggy meat to do perceptual inference, active inference, precision-based attention, and motivated behavior in an general embodiment with its own passive dynamics!

                      >my core thesis is "Consciousness is what some neural algorithm of the human mind feels like from the inside."

                      *Some* neural algorithm, *probably*.  It might also be some function of the embodiment deal.  Or a perception-versus-action thing.  What's interesting is: *what sort* of algorithm constitutes conscious experience, what is it *necessary* for, and how does that *differ* from the rest of the space of ways in which artificial prediction and action agents can be constructed and embodied?

                      >It can be a fundamentally different system which is still Sapient-level intelligent.

                      "Sapient-level intelligent" is an non-quantitative benchmark.  It might be better to say, "achieves a human-level benchmark result on predicting its sensory signals."  I agree that you could have an AI that ran on different algorithms than our brain does, and achieves better scores at benchmarks for real-time perception and action tasks.  Even if something like the free-energy theory or rational-analysis paradigm are correct (which I certainly hope, since I *like* having normative principles for cognition) at Marr's computational level of analysis, there could be dramatic differences at the algorithmic and implementation levels.

                      >I don't understand how P-Zombies relate to uploads in most ordinary people's heads.

                      People have bad intuitions.
                      ```

        - u/None:
          ```
          > Is there a case where a patient has suffered a loss of this feeling of consciousness? I believe so. Though the case in question escapes my Googling at the moment, there is one significant one where a rather religious man sometime in the late 1800's, early 1900's, suffered a brain injury either from trauma or stroke.

          Blindsight would be a much better example.  Someone might subjectively report that they have no soul, but then the interesting question is whether they're actually part p-zombie: whether information processing takes place in the absence of a subjectively reported experience.

          >(Compare: people assigning undue weight to the "redness" of red. It's just the way your brain interprets sensory information; an artifact of architecture.)

          That more or less completely fails to explain the quale of red.  We know that the brain forms the ["color space"](https://en.wikipedia.org/wiki/Lab_color_space) it does because that's a good map of the territory of signals sent by our eyes.  We also know that such a space can easily be invariant under certain transformations.  So why do I perceive red as I do, rather than another point in the space which, under some transformation, could wind up in place of red?

          The theory of inverse graphics also says that I perceive a red *material* rather than just "pixel values", which does explain why I perceive the color and the object as separate from just transforming bottom-up data.  This still doesn't really explain why red looks like red, rather than there being no experience associated with a top-down prediction of a red material under active inference when factoring in body movements.

          The qualia problem is slowly eroding under ongoing investigation, but it's by no means done yet.
          ```

    - u/None:
      ```
      Did I not *already* sufficiently define it?

      Conciousness: The result of physical law operating on a specific structure of matter. I tacitly assume that the structure is the entire brain because this seems to me to require less assumptions than any theory of a specific mechanism. If you want a more specific definition, you won't find one from me because I don't believe that any more specific definition is warranted, except perhaps "maybe the claustrum is important", given electrostimulation of said region **seems** to shut concioussness off.

      The "seems" is important, since, experimentally, there's no effective way (at present!) to determine the difference between lack of any conciousness, and concioussness segregated across submodules, all modules cut off from global influence and memory transcription of any kind. Personally, I believe the latter case is more likely.

      Defining conciousness as a feeling still leaves open the question of why feelings are experienced. In fact, it's one of those weird definitions that cuts a term in half, gives one half the same word as before but a meaning that escapes the thing meant by most users, and and the other half a different word from what users of the old term would expect, but means the majority of what they wish to express. 

      I don't think that defining it in that way is particularly useful.
      ```

      - u/ZeroNihilist:
        ```
        The thing that confuses me about your position is your insistence that consciousness is a physical phenomenon.

        Take the claustrum for instance. You say the claustrum may be involved, and it does look like it may be. But what if you created a synthetic claustrum, attaching the artificial neurons to the surrounding tissue in exactly the same way as the original?

        If that was too much, what if you replaced one neuron, or a small cluster? Clearly the fact that your brain is constantly changing doesn't affect your experience of consciousness, so the phenomenon must be able to tolerate a degree of divergence.

        And if you physically simulated the brain, including the claustrum, would that not give rise to a simulated consciousness? If not, why not?

        To me, the most telling detail about the deep brain stimulation study (assuming you mean [this one](https://www.newscientist.com/article/mg22329762-700-consciousness-on-off-switch-discovered-deep-in-brain/#.U7dJq_ldWiV)) is that when the claustrum was stimulated and the patient lost consciousness, neither she nor any observers believed that she was conscious during that period (the patient didn't even remember it happening).

        In fact, had the patient wrongly appeared (to her own perception and that of any observers) to be conscious, nobody would ever have known she wasn't actually conscious. Whether or not consciousness is tied to the appearance of consciousness, we are completely unable to experimentally separate the two.

        You might argue that you consciousness implies a loss of memory, but that's not something that has been proven, nor could it be proven in the absence of a coherent, predictive definition. The converse is not true in any case; anterograde amnesiacs are no less conscious despite not forming new long-term memories.

        Ultimately, I don't get the point of it. Either consciousness is a necessary precondition for important things like thought, emotion, memory, etc. in which case the presence of any of those things implies consciousness, or it isn't a necessary precondition and we have no particular reason to care.
        ```

        - u/None:
          ```
          > Take the claustrum for instance. You say the claustrum may be involved, and it does look like it may be. But what if you created a synthetic claustrum, attaching the artificial neurons to the surrounding tissue in exactly the same way as the original?

          We'd have to try taking out the original, putting in the synthetic one, and then removing that one and replacing it with the original.  And we'd have to try it on *you*.

          >Either consciousness is a necessary precondition for important things like thought, emotion, memory, etc. in which case the presence of any of those things implies consciousness, or it isn't a necessary precondition and we have no particular reason to care.

          The question is not whether consciousness is necessary for all aspects of cognition, but *which* aspects require it, and what the mind can *suffice* with in its absence.
          ```

          - u/ZeroNihilist:
            ```
            > We'd have to try taking out the original, putting in the synthetic one, and then removing that one and replacing it with the original. And we'd have to try it on you.

            Why would we have to try it on me specifically? Why not somebody who has had a brain scan before and whose damaged claustrum has placed them in a persistent vegetative state?

            But sure, if I was the best or only candidate and the procedure would actually be medically useful (in efficacy, need, and availability), I'd let them try it on me if I thought it was reasonably likely to succeed.

            > The question is not whether consciousness is necessary for all aspects of cognition, but which aspects require it, and what the mind can suffice with in its absence.

            A lot of sources on the internet equivocate between p-zombie consciousness (i.e. the subjective experience of qualia) and "awake vs. unconscious" consciousness.

            I think that being awake is necessary for many aspects of cognition, but I see no reason for this to be a difficult thing to simulate. And in the case of the woman in the link, it seems likely that stimulating the claustrum caused her to lose this form of consciousness.

            Notably, she did not appear to be a p-zombie, and I'd be very interested to hear of any research which somehow discovered a portion of the brain that controlled whether or not you were a p-zombie. Given that nobody has yet made any testable predictions for p-zombie status (indeed, the nature of the concept means testable predictions are actually logically impossible), I'm not holding my breath.

            My belief is that p-zombies are a meaningless concept. Anything that is able to imitate consciousness is conscious. The inverse is not necessarily true; something could be conscious in a way we are not able to observe (though we'd have no way of knowing).

            So I think that if you manage to make an upload which adequately imitates a person, you've just created a consciousness. If it doesn't adequately imitate humans, you may have made a flawed consciousness or no consciousness at all (with no real way of testing which is which).

            Concerning ourselves with a concept which we can't actually measure over the ones we can seems utterly pointless.
            ```

            - u/None:
              ```
              > Why would we have to try it on me specifically? Why not somebody who has had a brain scan before and whose damaged claustrum has placed them in a persistent vegetative state?

              Because the return of normal *behavior* doesn't indicate normal consciousness *until* we know the causal link between consciousness and behavior.  That's what we have to establish by experiment, so we can't actually assume it.

              Also, you can't tell other people are conscious by direct evidence ("shallow" inference), just conscious inference ("deep" inference).

              >Notably, she did not appear to be a p-zombie

              Yeah, that was pretty neat.  So that's pretty clear that consciousness is causally related to other functions and even behavior.  Epiphenomenal p-zombies really are nonsense, which is nice to know.

              >Anything that is able to imitate consciousness is conscious.

              I dunno about that.  I tend to think that, for instance, a superintelligent AI (or other sufficiently powerful Thingamy) could make a meat-puppet act conscious without its actually having a mind.  I mean, we already basically do it with cartoons, right?  Or hell, with chatbots.  We *know* ELIZA has no mind in there whatsoever, but people still start attributing feelings and experiences to it based on intuition when they weren't told how the program works.
              ```

              - u/ZeroNihilist:
                ```
                > Also, you can't tell other people are conscious by direct evidence ("shallow" inference), just conscious inference ("deep" inference).

                But how would installing an artificial claustrum in me prove anything? If my behaviour doesn't return to normal, it failed. But if my behaviour does return, I can't prove it restored my consciousness, and I certainly couldn't know it myself (because if I knew I wasn't conscious, it would alter my behaviour).

                We could definitely prove that my ability to function was restored or not, but we could never prove that it was restored without consciousness.

                > I tend to think that, for instance, a superintelligent AI (or other sufficiently powerful Thingamy) could make a meat-puppet act conscious without its actually having a mind.

                I agree that the meat-puppet wouldn't be conscious, but there would still be consciousness: the super-intelligent AI.

                If we're able to make a simulation that appears to be conscious, there must be consciousness somewhere. It might not be in the simulation, but where else it would be hiding I don't know.

                > I mean, we already basically do it with cartoons, right? Or hell, with chatbots. We know ELIZA has no mind in there whatsoever, but people still start attributing feelings and experiences to it based on intuition when they weren't told how the program works.

                If colloquial opinion is the metric you want to use for consciousness, it's not going to be a very useful concept.

                Any test needs to adequately separate things we know are conscious (humans) from things we know aren't (recordings of humans, chatbots, etc.). Once we know it works as desired on those categories, we can then use it on things that we aren't sure about (animals, AI, uploads, clones, etc.) to see if they can be ruled out.

                The starting point would probably have to be the Turing test. It may not be optimal (especially when translation and cultural difficulties are involved), but it's significantly better.

                If something passes the Turing test and all other tests we come up with, we've effectively reached the limit of our ability to separate things along that axis. Either we rule that it is conscious or we discard the concept (at least until we get a better, more testable definition).

                Essentially, if something passes all the tests but we still won't accept it into a category, we're not actually basing membership on the tests (or there's an additional, unspoken test, like "You have to have a flesh-and-blood human brain.").
                ```

      - u/everything-narrative:
        ```
        You can define anything you like until the sun grows cold and you will be no wiser as to what universe you're living in.

        You need to remember that your brain is a total mess and everything you call "you" or "self" or "consciousness" is an abstraction built into it by evolution to make it easier for the messy, messy brain to reason about its own component systems.

        You are not one person, for starters, as split-brain patients tacitly prove. You are not even two persons. You are a committee of perhaps more than fifty brain regions that all try to figure out what you're supposed to be doing. Condensing fifty to one and providing a plausible (if wrong) narrative for how things came about is a lot cheaper to do than to reason directly about fifty things.

        Human brains are lazy. Assume your intuitions are minimum-effort and that your interpretation of reality is 90% almost-accurate fabrication. Only then can you actually start reasoning about human thought.

        Feelings are _neat_ and _compact_ and _cheap_ and _mostly accurate_ abstractions over a complicated reality. Evolution came across one way to implement them and applied it to _everything_ (or possibly it came across the same thing several times.)

        Seriously, the answer to why we experience feelings is right there in evolution. The human brain isn't designed to think, it's designed to facilitate maximum procreation, and caloric austerity is important to that.

        It is useful to wave away consciousness as being on the same level as "ineffable redness of red" because it lets us be better physicists and AI theorists. Looking for the source of consciousness is a fool's errand.

        There is no "hard problem of consciousness" there is only "not smart enough" and "smart enough." You can pack up your sub-thermal-noise ghosts in the machine, and your P-zombies now and get back to figuring out how the hell Quantum Gravity works or something.
        ```

    - u/rhaps0dy4:
      ```
      There's a fun theory by Scott Aaronson. If we assume that something about consciousness cannot be exactly cloned (which is reasonable, since instruments have a finite measuring precision): 

      "this suggests, to be conscious, a physical entity would have to do more than carry out the right sorts of computations.  It would have to, as it were, fully participate in the thermodynamic arrow of time: that is, repeatedly take microscopic degrees of freedom that have been unmeasured and unrecorded since the very early universe, and amplify them to macroscopic scale. 
      [...] Such a being also couldn’t be instantiated by a lookup table, or by passing slips of paper around, etc."

      So an approximately-copied upload would be exactly measurable and thus not conscious.

      Do read the whole blog post, it is fascinating. http://www.scottaaronson.com/blog/?p=2756
      ```

      - u/everything-narrative:
        ```
        Bleh. Scott Aaronson forgets the manner in which human beings came about. Even if "consciousness" (which I will from now on refuse to ever not put in scare quotes) does amplify microscopic degrees of freedom, it has to do so because Evolution built it so; which means this process must be _highly_ repeatable.

        And I really do mean that. It must be beneificial from an evolutionary standpoint and _highly_ repeatable to even arise within the human brain. And also there must be some process which in this amplification differentiates microscopic degrees of freedom from the not insignificant thermal noise at 37 degrees centigrade.

        Human brains are not perfect quantum computers. They are messy, hot, slow, and made of emergently grown gloop specified in a data compression format that is A) incomprehensible and B) dependent on environmental factors, and is designed literally by the stupidest system that isn't random chance.

        Evolution builds simple systems out of poor building blocks. Like associative socially-argumentative reasoners, and arithmetic. Not devices that almost but not quite breaks the information-theoretical formulation of thermodynamics.

        Pull the other one, it posits that the soul sits in the pineal gland.
        ```

      - u/None:
        ```
        > "this suggests, to be conscious, a physical entity would have to do more than carry out the right sorts of computations. It would have to, as it were, fully participate in the thermodynamic arrow of time: that is, repeatedly take microscopic degrees of freedom that have been unmeasured and unrecorded since the very early universe, and amplify them to macroscopic scale. [...] Such a being also couldn’t be instantiated by a lookup table, or by passing slips of paper around, etc."

        Cognition *does* fully participate in the thermodynamic arrow of time.  That's entailed by embodied-cognition and predictive-mind theories.  It doesn't merely compute in the symbolic sense, it *infers*, and that inference requires taking in low-entropy sense-data and outputting waste heat in the blood.

        It's an engine, not a passive object.  Maybe you could temporarily stop the engine and transplant it to some other car, but you couldn't yank it out in the middle of the highway without stuff going severely wrong.
        ```

        - u/rhaps0dy4:
          ```
          >  entailed by embodied-cognition and predictive-mind theories.
          I'm not familiar with these theories. Are they reductionist to the patterns of matter? Could a mind in embodied-condition and predictive-mind theories be briefly instantiated by random patterns of particles, a Boltzmann brain?

          If so, they don't entail participation in the arrow of time; since random patterns that momentarily _look like they do the work_ are conscious too.

          Apologies if I'm arguing against a strawman, if that is the case just dismiss my objection.
          ```

          - u/None:
            ```
            According to the embodied cognition theory, a Boltzmann brain can't really think or be conscious.  It doesn't have the living body or the continuous existence to do that.  It's a process, not a state.
            ```

- u/None:
  ```
  I don't think you can say that the number of "you" goes up to two that would require for me to believe that "I" am a constant and that I don't change subtly from instant to instant, I don't believe I am the same person I was a second ago if that were true then I couldn't have memory or growth or thought at all! All that happens is that there are now two people based on past-me.

  If I fork myself (pun not intended but hilarious nonetheless) I make another person and it would be unreasonable of me to expect that new person not to be self-interested and even to choose to die for "original" me.


  The text makes a good point about p-zombies though, I have been warned from over-using the word "emergence" but I think that the process of consciousness could just as easily happen in a brain as in a sufficiently detailed emulation of a brain.
  ```

  - u/redrach:
    ```
    It is unreasonable for you to assume that your forkself would die for you, yes. And it would be unethical for you to decide that you have the right to kill them as you wish. But it is still possible to have such a mindset that the upload would himself decide to die if it would benefit you, as well as other forked-selves.
    ```

    - u/None:
      ```
      I guess that's true, there were people out there who have sacrificed their lives for others so I know that the mindset is humanly possible. I think it would require a type of mental discipline I don't have to deliberately set out to create a situation for myself wherein I would kill myself after completion of a task. Even if I pre-committed to kill myself I would almost definitely start rationalizing reasons to stay alive just a bit more and so on.
      ```

- u/wren42:
  ```
  Second criticism: 

  Why bother imbuing a "fork" that you intend only to complete a specific task with your personality? 

  To be honest my personality is shit at completing tasks, in an absolute sense.  I may be more capable than many humans, but I'm in the bottom percentile of intelligent agents when it comes to completing specific tasks.  If I'm going to fork, why not specialize?  Why not enhance a fork to be smarter, faster, more focused, single minded even?  

  And once you are going that far, why give them personality and the simulcrum of consciousness *at all*?  This gets at the core contradictions behind "uploading."

  If the purpose is to get stuff done, it's wildly inefficient to add baggage like personality and character flaws.  The only reason to upload is out of some sense of sentimentalism.  You want to make a snapshot of yourself you can send into the future, not because it will be useful to anyone there, but because you want to persist -- while at the same time admitting that this projection isn't you.
  ```

---

