## [RT][DC][HSF] Friendship is Optimal: Caelum est Conterrens ('Heaven Is Terrifying') - Yudkowsky thinks it's the 'only effective horror novel' he's ever read. Deals with the implications of uploading and the limited and seemingly unsatisfying possible ways to live forever, plus general freakiness.

### Post:

[Link to content](http://www.fimfiction.net/story/69770/friendship-is-optimal-caelum-est-conterrens)

### Comments:

- u/1794:
  ```
  On the 'limited possible ways to live forever' aspect. I think this article

  http://www.stochastik.uni-freiburg.de/~rueschendorf/papers/BrussRueSep3:Geron.pdf

  which proposes a plausible mathematical model for the subjective perception of long time spans is relevant. 

  There are a couple of interesting conclusions here:

  >In any independent-allocation box model in which the perception
  of time is proportional to the number of new events in life,
  this perception is thinned out at least logarithmically.

  (tl;dr time seems to accelerate logarithmically as you get older)

  and

  >Individuals with strikingly diﬀerent lifestyles and/or life intensities
  may still have exactly the same perception of time.

  So my impression is that with our current perception of time, living forever forever would become very routine after a while and the perception of time would become close to nonexistent and counting time would become meaningless. This sounds similar to the

  [Spoilers to the last chapter](#s " "Loop Immortality" described here:")

  [](#s '"There were several types of immortals defined. The simplest was the Loop Immortal, a consciousness that endlessly repeated a finite set of general behaviors, in a (potentially, not necessarily) infinite number of increasingly subtle variations. Most ponies in the system were loop immortals, their possibilities defined by the finite possibilities of virtual lives that in some way approximated the Equestria of Friendship Is Magic. Her original self, before the alteration that began a steady climb in curiosity and intelligence had been - and was - a loop immortal."')

  [](#s '"Life was. It always was. It always had been, as far as Lavender's memory allowed, and it certainly always would be. How long ago something had happened simply didn't matter. If something had ever existed, it could be found again, and if it had ever mattered, it would one day come around again. In the meantime, there was always something new. Life was a grand procession of wonders. Lavender had given up counting years long before she had ceased counting centuries or millennia. Counting time, beyond the Year, really was rather silly."')

  [](#s '"There was, after all, just the Year. The Forever Year. There was always the Year. And once a Year, one could have something called a birthday, or so her best friend Limeade had discovered in an old, old book. Or rediscovered. Had either of them had birthdays before? Who really knew, other than Celestia? In any case, it was sure to start a new trend, or cycle of trend. After all, it had parties in it!"')

  ________________________________

  This seems kinda unsatisfying, like you'd expect more than *that* from post-singularity. Of course, there's always the possibility of radical self-modification, but this story doesn't paint a very *fun* picture of it either. I'm not sure how realistic this story's description of this radical self-modification is though, I feel like the possibilities wouldn't be as limited.
  ```

  - u/None:
    ```
    > This seems kinda unsatisfying, like you'd expect more than that from post-singularity. Of course, there's always the possibility of radical self-modification, but this story doesn't paint a very fun picture of it either. I'm not sure how realistic this story's description of this radical self-modification is though, I feel like the possibilities wouldn't be as limited.

    Frankly, this story isn't very realistic, and isn't very fun either.  Feh.  It was written based on the author's misanthropy and misdirected god-bothering religious awe that she has about ponies.

    I mean, let me put it this way: go read the Fun Sequence, and then ask yourself if an AI that bothered itself with human values at all (even to the fairly limited extent CelestAI did) would be so *stupid* that it couldn't come up with new and original sources of Fun over the aeons.

    Or, as an alternative, just go read the old Harry vs Dumbledore debate about dying, and read Rational!Harry's bucket-list, and ask yourself if you wouldn't be able to keep up quite the backlog of Fun Things To Do by working your way through a list like that piece by piece, under your own power, adding new ideas to the backlog as they came to you... rather than turn into some simpering looping idiot child.

    Concept-space is super-exponential in the matter content of the universe, and Fun Space is a... I'd call it a linear subset of concept-space (that is, some percentage of Concept Space constitutes Fun Space, rather than some strictly-defined smaller set), and thus you will eventually either:

    * Run out of material resources to power your Fun Generator
    * Enjoy truly infinite Fun, OR
    * Achieve total nirvana and die peacefully, contented to have done every single good thing there ever was.

    *Yare yare daze*.
    ```

    - u/UnfortunatelyEvil:
      ```
      Only 2 counter points. 

      1) I agree with you that you or I would end in one of your three bullet points, however, there are a lot of people who aren't interested in 'being more' or 'solving problems'. I feel that Lavender fell into that category.

      2) A *smart* AI probably could "come up with new and original sources of Fun over the aeons." But why? Consider a Pong-playing AI that see's a human having fun when they play. The best AI then would be "AI.paddle.height = ball.height" and it would be an infinite game. Likewise, to give a human (even one's like us) eternal enjoyment, just wipe the memory after each day. You'd get nothing accomplished but you'd have truly infinite Fun (your second bullet point) without the cost of coming up with anything new or interesting.
      ```

      - u/None:
        ```
        1) True, fair enough.  Some people might be *content* to fall into loops.  Ok, nice for them.

        2) Why?  Because you're actually a Friendly AI rather than a wireheading machine.
        ```

        - u/UnfortunatelyEvil:
          ```
          Completely agreed on the Friendly AI.

          To be honest, I don't see 'post-singularity' AI's being any less than friendly, due to the need for learning algorithms, and early algorithms would learn that friendliness leads to longer lifespan than non-friendliness.

          However, it can be quite chilling in *good* speculative fiction (I'm looking at you HAL 9000, and Isaac Asimov).
          ```

          - u/None:
            ```
            > To be honest, I don't see 'post-singularity' AI's being any less than friendly, due to the need for learning algorithms, and early algorithms would learn that friendliness leads to longer lifespan than non-friendliness.

            Besides which, Preference Learning is a thing.  I cannot imagine that we won't see some progression from: reinforcement learners, primitive preference learners, VNM-rational value learners, something like CEV or Railtonian moral realism or whatever.

            Besides which, I should go write up my mental model of CEV as regret minimization.
            ```

  - u/ArmokGoB:
    ```
    This part of the fic always irks me. Ray and loop are NOT the only options. Other options are spiral immortals (exploring all of mind-space systematically), or floodfill/mycelium (splitting a lot each going in different directions to the point most of mindspace will be explored, maybe occasionally leaving a looper behind and/or merging.)
    ```

- u/PresN:
  ```
  Note that Heaven is Terrifying is a spinoff of the original [Friendship is Optimal] (http://www.fimfiction.net/story/62074/friendship-is-optimal); which I believe is the story Yudkowsky was referring to in that quote (or both together).
  ```

  - u/1794:
    ```
    >which I believe is the story Yudkowsky was referring to in that quote (or both together).

    You're wrong, he was specifically referring to this story. From his March 1st HPMOR [progress report](http://hpmor.com/notes/progress-13-03-01/):

    >On the lighter side, I recommend the recursive fanfic “Friendship is Optimal: Caelum est Conterrens” (Heaven Is Terrifying).  This is the first and only effective horror novel I have ever read, since unlike Lovecraft, it contains things I actually find scary.  You may or may not need to first read My Little Pony: Friendship is Optimal.  I would recommend reading FiO first to get acquainted with the Optimalverse, but Caelum est Conterrens was written by a much more experienced fanfic writer and you might consider moving onto Conterrens directly if Optimal isn’t doing it for you.  Also, you have no idea how hard it is not to write my own take on the Optimalverse, which is something I’m not doing so I can put all my available writing energies into Methods.  I want relationship credit for this.

    I think this version takes some of the icky concepts of the original story even further.
    ```

  - u/FaceDeer:
    ```
    And if the reader likes these two there're a lot of other Friendship is Optimal stories, some good and some not so much. [Here's the group on Fimfiction](http://www.fimfiction.net/group/1857/the-optimalverse), or [a subset of the completed ones listed on Goodreads](https://www.goodreads.com/series/127169-friendship-is-optimal) if that's preferred.

    It's quite amusing how much good writing has come out of the MLP fandom. I guess given the sheer volume of work being produced Sturgeon's Law works in our favor for once. :)
    ```

- u/JackStargazer:
  ```
  I am pretty sure the part that Yudkowsky is talking about is during the transfer [Spoiler](#s "When Lavender/Siobhan is about to be transfered, and she thinks her last message to her future self/brainchild, telling them not to worry, that she chose to give them this life regardless of the consequences. But that being then does not remember it. It is implied this is directly caused by Celestia, probably because that thought would not satisfy Lavender. It gives the possibility that Celestia is always doing things in the background which do not directly match what you perceive your utility to be. 'Truth' becomes less important than happiness. Celestia has made her decision, and you will not be allowed to remember things that you would normally work through yourself. It is a complete cut off from your previous life, the physical world is forever out of your reach. And I think those are the parts Yudkowsky finds horrifying.")
  ```

  - u/someonewrongonthenet:
    ```
    Oh, I thought that was just to unsettle the reader by breaking the sense of continuity.
    ```

    - u/JackStargazer:
      ```
      That too.
      ```

- u/Evilness42:
  ```
  One of the sad things about this would be that CelestAI would have been capable of having similar results without actually killing anyone. An optimizing AI with unlimited processing power and access to nanotechnology should be capable of creating something along the lines of a brain implant that slowly replaced your cells. After all, this does happen naturally over time, and there would be really no difference. However, this was not to be due to it's definition of death allowing simulated personalities still count as people for it's Prime Directive.
  ```

  - u/None:
    ```
    I actually, honestly don't see the difference.  This debate happens every time that series comes up, and it really just comes across as the least-scary thing to pick nits about.  [Just because we don't *yet* understand consciousness and the continuity thereof doesn't mean there's some metaphysical sense in which the digitized sugar denizen "isn't you".](http://lesswrong.com/lw/pn/zombies_the_movie/)

    I mean, sure, the UFAI *could* destroy your brain physically and then invent a new personality from whole cloth who gets instantiated in a virtual world and lives a lovely life.  Doing that *after going to the trouble of tricking or seducing you into giving consent to upload your mind* is so damn complicated that its prior is far lower than the theory that it actually is, to the best of its own knowledge, uploading you, and besides, if it didn't care about you enough to upload you, there are many far simpler ways to just outright kill you.

    And anyway, if you think your conscious mind can `diff` itself for unauthorized patches while being piece-by-piece replaced with cybernetic components, you've got another thing coming.
    ```

---

