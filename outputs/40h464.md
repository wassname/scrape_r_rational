## [D] Monday General Rationality Thread

### Post:

Welcome to the Monday thread on general rationality topics!  Do you really want to talk about something non-fictional, related to the real world?  Have you:

* Seen something interesting on /r/science?
* Found a new way to get your shit even-more together?
* Figured out how to become immortal?
* Constructed artificial general intelligence?
* Read a neat nonfiction book?
* Munchkined your way into total control of your D&D campaign?


### Comments:

- u/trifith:
  ```
  So for the last few years I've been tweaking my personal finance and budgeting system. And now it turns out I've basically re-invented double book accounting. 

  Obviously, I didn't do as good a job as the accountants who've been using and refining the method for the last thousand plus years. 

  Now I'm looking for some good software to transition all my records over to. Playing with GnuCash, but I'd ideally like something with better Android support.
  ```

- u/Atilme:
  ```
  I realized that I have no good reason to be upset because of other's words, and have mostly stopped being such. How I got from point A to point B is largely a mystery, and probably isn't replicable.
  ```

- u/blazinghand:
  ```
  Something I sent to a friend:

  EV rising above break-even on powerball doesn't mean you should buy powerball tickets (and note, EV STILL isn't above break-even; even at 900 million. Why?

  Well, odds of winning are one in 292 million.

  A 1/292 million chance of winning 900 million dollars well, then our EV for a ticket of 900 / 292 = about 3 dollars. Given that a ticket costs about 2 bucks, this seems like it might be a good deal, right? But wait! If you take the payout now in lump sum rather than over time, it's only 500 million! so your EV (500 / 292) = about $1.72. Not too great for a $2 ticket.

  It gets worse, though! don't forget Uncle Sam! He'll come and take about 40%, leaving you with an EV of about $1.02 for your $2 ticket. 

  That being said, EV isn't a great way to approach this. 

  Why? Well, let's say the jackpot was even HUGER. Let's say the jackpot was so huge that it was actually an EV of $3 for a $2 ticket. Should you get one?

  No.

  In general, any powerball jackpot is the same size. They're all the size of "If carefully managed, you never have to work again and can live in comfort for the rest of your life. Your children won't have to work, either." Like, even the "smaller" powerball jackpots are a hundred million, easily. Even taking it as a lump sum and Uncle Sam taking a fat wet bite out, you're left with 20+ million dollars. Carefully shepherded to, 20 million dollars can easily put out several hundred thousand dollars a year without eating into principal and while fighting inflation. 

  So given that even a small jackpot would have you and all your descendants living an upper class lifestyle forever, you really shouldn't care that the jackpot is larger now. If you weren't buying tickets before, the fact that the jackpot will end up paying out more shouldn't change anything for you. Either way, it was going to change your life in the same way.
  ```

  - u/IomKg:
    ```
    Unless you are organizing a fund with a bunch of people and pooling your money so you will simply get a proportional outcome to the money invested.
    ```

- u/scooterboo2:
  ```
  What are humanity's long-term goals? What do you think is important for humanity to achieve in the next 20, 100, 1000, 10000 years?
  ```

  - u/None:
    ```
    Now planning by backwards chaining...

    FUN!

    What would be the most FUN?

    Why aren't we having FUN yet?  Why's everyone so damn miserable much of the time?  Enumerate reasons, line them up by feasibility of elimination, and solve them.

    Top reasons we're not having FUN:

    * Bad belief systems that teach us not to have FUN, or in fact to treat our own lives and sentiments as worthless from the get-go.  These systems are often disguised under words like "normativity", "rationality", "freedom", "security", "God", and "identity".
    * Artificial scarcity
    * Artificial oppression, often related to above malignant belief-systems
    * Natural decline of human condition with age and entropy.
    ```

    - u/Transfuturist:
      ```
      Top reasons we're not having FUN:

      * Moloch
      * Too busy lifting Moloch to Heaven
      ```

    - u/Transfuturist:
      ```
      How does LW rationality teach people that their lives and sentiments are worthless?
      ```

      - u/Roxolan:
        ```
        I don't think /u/eaturbrainz is talking about LW rationality (which, yes, explicitly says that feelings and fun are ok). 

        Outside LW, "rationality" is often portrayed as the opposite of emotions, which are bad (or, if the author likes emotions, [rationality is bad](http://tvtropes.org/pmwiki/pmwiki.php/Main/StrawVulcan) instead).
        ```

        - u/Transfuturist:
          ```
          He's talking about 'bad belief systems,' not the straw Vulcan.
          ```

      - u/None:
        ```
        Well that opened a major can of worms.

        Most usages of the word "rationality" don't refer to LW.  In this case, it partially does, and partially doesn't.  This isn't trolling with my Tzeentch hat on, this is just being weirded out by certain things.

        [Some of them](http://nostalgebraist.tumblr.com/post/83006103140/what-is-bayesianism-we-i-just-dont-know) [*are* LW-associated.](http://bactra.org/weblog/569.html)  [Others are less associated, at a remove.](http://delong.typepad.com/sdj/2011/08/economic-downturns-the-social-darwinist-waltz-and-the-navigation-of-the-starship-asgard.html)

        The result is just that I've become extremely suspicious of the tendency to apply "rational" or "rationality" to mean, "Use algorithm X" or "Solve well-specified problem Y", with a vast body of assumptions just *lurking behind things* about why I should use algorithm X, or about *whether* well-specified problem Y even *can* be solved tractably, and how desirable it is to solve problem Y using algorithm/technique X instead of solving a similar problem, call it Z', which takes actual explicit account of the flaws in the preconceptions about Y and thus can be solved with a much more tractable, robust algorithm W, which the Xians will promptly yell at you for using because it isn't X and doesn't solve problem Y all that well.

        Actually, the links about statistics are way obscure.  If you really want to get what I mean, just look at the economics example, and then think of all the other times fucking economists have basically said, "Homo Economicus does X, actual human beings do different-thing Y, and we can therefore conclude that human beings are *irrational*, not because Y has *no reasons behind it*, no cognitive processes that could make sense or optimize some goal, but instead because Homo Economicus is the *normative* theory of a *rational* agent."  (See: Robin Hanson, Tyler Cowen, Bryan Caplan, and in fact much of the rest of economics.)

        Where this becomes problematic for things like the "rationality community" is that the entire edifice of the dual-process, heuristics-and-biases, and evolved-modularity approach to cognition is the work in behavioral *economics* by Tversky and Kahneman, which founds itself on... yep, taking Homo Economicus (eg: the expected-VNM-utility maximizer with Bayesian updating of unlimited numerical accuracy and no causal reasoning) as the *normative model* of a *rational agent*.

        I mean, honestly, what the hell is the point of calling stereotypes of corporations the "normative theory" of how *human beings* should act?  Even the corporations themselves only act that way because someone *told them* the damned theory was "normative".

        In which case, sure, all the normal things like base-rate neglect seem like Bad Ideas *to me*, but what algorithm is generating them?  Are they really *worse* than *completely ignoring causal structure* because you think a good predictive distribution is *all that matters*?

        In summary, you'd think that the definition of concepts like "ought" would be an obscure matter for overly metaphysical philosophers, but actually, confusion over what "should" (ahaha) count as normativity seems to play a role in most *willfully held* delusions, as people start asserting that by gosh, *it's a normative theory*, and that means it doesn't have to correlate with *anything* else or match up to *anything* or bear any resemblance to, for instance, the thing you would choose in its place given full information and full cognitive accuracy.
        ```

        - u/Transfuturist:
          ```
          >[nostalgebraist]

          "we live in a universe in which theory X holds" does not strike me as meaningfully different from "theory X holds", and can be enumerated in pretty much the same way with a prior over the mutually exclusive theories to which theory X belongs.

          Not that I have ever actually done or used such an enumeration. I'm not quite sure if we should be using systems where theories/hypotheses are the unit of currency, or systems where evidences/data are. I'm not sure if that distinction means anything. But I still think diachronous Bayes is correct.

          I have never seen people pull arbitrary small probabilities out of their ass in the manner described to get '0.01'. As SSC puts it, even statistics with guessed numbers is better than guessed results, because the results can surprise. Additionally, this is why a log-odds formulation of probability is recommended, because it puts the probabilities in less alien terms. I've never actually seen a Bayesian, though.

          > The result is just that I've become extremely suspicious of the tendency to apply "rational" or "rationality" to mean, "Use algorithm X" or "Solve well-specified problem Y", with a vast body of assumptions just lurking behind things about why I should use algorithm X, or about whether well-specified problem Y even can be solved tractably, and how desirable it is to solve problem Y using algorithm/technique X instead of solving a similar problem, call it Z', which takes actual explicit account of the flaws in the preconceptions about Y and thus can be solved with a much more tractable, robust algorithm W, which the Xians will promptly yell at you for using because it isn't X and doesn't solve problem Y all that well.

          Is this paragraph motivated by AIXI-worship vs. bounded intelligence?

          >all the other times fucking economists have basically said

          Well, I mean, those economists are wrong. *We know* they're wrong. The last section covered in my microeconomics class was all about how Homo economicus differs from humans. It was not presented as a "normative theory" at all, and I've never seen Homo economicus be presented as the way humans "should" be, save perhaps some very deluded ancaps.

          >Are they really worse than completely ignoring causal structure because you think a good predictive distribution is all that matters?

          Yeah, I'm guessing AIXI.
          ```

          - u/None:
            ```
            > I have never seen people pull arbitrary small probabilities out of their ass in the manner described to get '0.01'. 

            Funny thing: I've been *asked* for such a thing.  "What's the subjective probability you'll come work here after you graduate?" was the question.

            And of course I didn't give an answer, because even back then I knew my brain didn't have a neat mechanism built-in for giving a probability mass to some arbitrary question like that.
            ```

    - u/BadGoyWithAGun:
      ```
      If the majority of humanity's institutions and belief systems do not feature or are opposed to the idea of maximising fun, doesn't it therefore stand to reason that FUN is not humanity's long term goal?
      ```

      - u/None:
        ```
        I think there's a big problem with claiming there's actually a unified goal-seeking entity called "humanity", period, and then on top of that, that actually-existing institutions and belief systems have anything to do with "humanity's long-term goals" rather than to do with the material and educational conditions of the people who create and maintain them.
        ```

  - u/Chronophilia:
    ```
    20: Eradicate polio and take steps to cure other preventable diseases (malaria, HIV/AIDS, TB, etc.). Lift a billion or two people above the poverty line.

    100: Fix a limit on the number of living humans that the Earth can support long-term. (Might be anywhere from 5 billion to 500 billion, depending on how tech advances in that time.) Make plans under the assumption that the population and economy will stay fixed in the long-term instead of steadily growing. Solve income inequality and get everyone living to first-world standards.

    1000: Make AIs, contact aliens, solve ageing and enough diseases that we qualify as biologically immortal.

    10000: I don't know, but I'm sure we'll discover plenty of new and exciting disasters to avert in the intervening time. Either that or our species will go extinct.
    ```

  - u/Gaboncio:
    ```
    Getting a handle on global climate change is definitely the first thing to aim for in the next 20 and 100 years. Once there are concerted efforts in that direction, we need to have some big breakthroughs in computing power in the next 10-20 years to get past the Power Wall in Moore's law. After that, goals become less concrete and I haven't given them much thought.
    ```

  - u/Predictablicious:
    ```
    20y fix diabetes, get a better grip on cancer and degenerative diseases (e.g. Alzheimer).

    100y off world colonies, solve aging.

    1000y solve friendly AGI.

    10000y solve entropy.
    ```

    - u/Jace_MacLeod:
      ```
      10,000 years to beat the second law of thermodynamics might be a *wee bit* optimistic.
      ```

      - u/Predictablicious:
        ```
        If we have friendly AGI (the previous goal) 9k years is an absurd amount of time.
        ```

  - u/UltraRedSpectrum:
    ```
    Industrial automation is definitely priority #1. We like to emphasize FAI, but we *can* get to post-scarcity without it, and from them on we're on easy mode. With an arbitrary budget, we can approach aging, cancer, and disease from a much better position.

    Social problems are somewhere at the bottom of the list, around "dryer lint" and "protecting the sanctity of <blank>". As always, the little things will remain unsolvable until we acquire sufficient wealth, at which point they'll solve themselves.
    ```

    - u/Transfuturist:
      ```
      Industrial automation without socially unhooking capitalism and other economic problems is likely to result in bad things. Particularly with uneven development and regulation around the globe.

      Additionally, automation itself will develop unevenly, and will not result in an 'arbitrary budget', nor the sort of attention to futurist problems that you might assume.
      ```

    - u/BoilingLeadBath:
      ```
      A naive definition of "Post scarcity" is that the amount of work man wants to do produces enough stuff that nobody who wants it can't have it. I suspect that pursuing this with "automated factories" is going to work about as well in the short term future as it has in the mid-term past. (Where's the 15 hour work week Keynes forcast back in 1930?) (Barring an AI foom or something) Instead, I expect the post-scarcity scene to be a gradually growing opt-in philosophical movement.

      At least, the following can be said:

      1) I suspect that there's enough people out there who view wealth as a relative-social-status thing (or at least are sufficiently ignorant of hedonic adaptation) that you would simply run out of matter in the universe before we got the last 20% of them happy. 

      1.2) I would suggest that this demand curve is very steep in the first world. I mean, how many more people retire early now, compared to in 1950, when we made much less? Almost zero, either way?

      2) The "Financially Independent, Retired Early" people, despite society being basically pitted against them, are able bootstrap themselves (and their progeny) into a "post scarcity" situation, in the present day, with about 15 years of work. (This is, perhaps, not sustainable - but that's not my point.)

      3) The difference between the FIRE people and most of society is mostly philosophical, rather than technological. (nevermind that philosophy is a sort of tech...)

      (Edit for formatting only)
      ```

      - u/Transfuturist:
        ```
        > Where's the 15 hour work week Keynes forcast back in 1930?

        There are alternate explanations for this failure. I'm not researched enough on the topic to elaborate, but I don't accept your use of it here. Although that might be the very point you're getting at.

        [Factories are not even the tip of the iceberg.](http://www.thisismoney.co.uk/money/news/article-2642880/Table-700-jobs-reveals-professions-likely-replaced-robots.html)
        ```

      - u/UltraRedSpectrum:
        ```
        Any society in which production is decoupled from labour is, for all intents and purposes, post-scarcity. Because consumers and producers are separate, we can ramp up the ratio as high as we want. Ten factories per human being? A hundred? A thousand? Why not? It's not like we're running out of space in the solar system, here.

        For all the fear-mongering about 1% of the population owning the robots and everyone else starving in the streets, it seems somewhat more likely that, with some effort, we'll be able to solve the *mind-bendingly difficult task* of having enough of everything for everyone.
        ```

        - u/BoilingLeadBath:
          ```
          Not to nitpick, but wouldn't a society in which production is decoupled from labor only be post-scarcity if the rate of increase of the rate of production exceeds the rate of increase of demand. (ie, if p' = (1-a)p and d' = bd then ap > bd)

          For a (pretty bad) historical example: slavery-based societies were not post-scarcity, even though the consumers were not the producers.

          In any case, barring some REALLY good AI, I expect that automation will simply increase the effectiveness of what human workers do. (Thus the "Short term future" disclaimer) In this version of events, the case where production is truly decoupled doesn't actually happen.
          ```

---

