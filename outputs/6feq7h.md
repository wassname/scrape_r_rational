## [D] Monday General Rationality Thread

### Post:

Welcome to the Monday thread on general rationality topics!  Do you really want to talk about something non-fictional, related to the real world?  Have you:

* Seen something interesting on /r/science?
* Found a new way to get your shit even-more together?
* Figured out how to become immortal?
* Constructed artificial general intelligence?
* Read a neat nonfiction book?
* Munchkined your way into total control of your D&D campaign?


### Comments:

- u/waylandertheslayer:
  ```
  Lately I've noticed that when I'm drunk I tend to be *more* responsible than when sober (including one memorable occasion when I went clubbing, got drunk and immediately got a cab home to work some more on an important assignment). After some more analysis, I think that when I'm intoxicated I'm more likely to do things I know rationally are good decisions but don't really want to.

  This extends to other things, like being more sociable/romantically adventurous/open to new ideas that I don't like/etc. I *think*, based on a combination of how I feel when tipsy and what sorts of good decisions I tend to make, that I'm generally too risk-averse (or perhaps not able to correctly envision unlikely but very bad events) when sober, but knowing that hasn't helped me much. The other possibility I considered was that it ties to how much shame I feel around others, and when I'm more uninhibited I'm less concerned with status issues around updating based on new evidence (on a subconscious level), since I don't feel as challenged when people disagree in aggressive ways.

  It's not such a vast gap that I think regularly drinking would help me, (especially after factoring in health costs/risk of addiction), but I was wondering if anyone else has experienced something similar and, if so, what other techniques they found helpful in bringing about a similar state?
  ```

  - u/Frommerman:
    ```
    When I'm drunk I become even more humanist than usual. My parents have noticed because my drunktexts are some of the nicest things I say to them.
    ```

  - u/None:
    ```
    While I don't drink often I have noticed that I do make some of my best decisions under the influence of mild sleep deprivation. I feel like shit for a while but I'm much less likely to procrastinate decisions and I'm much more honest with myself.
    ```

- u/Sagebrysh:
  ```
  So yesterday we started working on what we're calling [The Origin Sequence](https://hivewired.wordpress.com/origin/), on our blog. The Origin Sequence is the draft one blueprint to building the rationality community into a powerful, stable, multigenerational force for goodness and truth in the world. Also if any of you haven't seen our blog, it might interest you.
  ```

  - u/electrace:
    ```
    Just read "Until we Build dath ilan" and in an effort to help you out, I'll be blunt. It reads like hero worship. Reminds me of a conservative blog talking about Reagan. Some people don't like Eliezer (he doesn't bother me, but not everyone agrees). And the continual name-drops and references don't really add to the point of the post, in my opinion. The many references to people in the in-group comes off as bubble-like. The sequences are verbose, and not every rationalist reads/retains them.

    So basically my advice, (and feel free to ignore it) is:

    1) Be as concise as possible. My number one complaint about the sequences is that they are too got'dang long for the points they make. Your audience is smart, make your point and move on. Extended discussion can happen in the comment section.

    2) Good references have one of the following two properties:  
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; a) Is subtle, like "We call those who follow the project virtue of Goodness a singer." (if you didn't have the explainer above, this would be a nice subtle reference).    
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b) Contains necessary info that can not be easily avoided by making the same point in a different way. For example, it's fine to reference [Dunbar's Number](https://en.wikipedia.org/wiki/Dunbar%27s_number) when talking about max group sizes because it would be more difficult to explain from first principles than it is to just skim the wikipedia page as a refresher. On the other hand, it's more difficult to explain all the background to dath ilan than it is to say "A process that continually brings us closer to utopia through rational optimization." 

    Reference are shortcuts for people who immediately recognize them, but detours for people who don't. Too many references can easily overload your audience and cause them to give up.

    3) Think hard about who you want your audience to be. 

    "We take a recursive loop through the meta level" seems like its for a much different crowd than the next paragraph that rhetorically asks "What  is rationality good for anyways". The intersection of people who understand that first bit, and who need you to answer the second bit is approximately zero. The standard advice would be aim for the lowest common denominator of people *that would be interested in your blog.*  Here, that would probably be closer to "EA people," but not necessarily "LW people."

    ___________________________________________________________

    And let's finish off with a couple nitpicks.

    It was pointed out by someone here that "Aspiring rationalist" would inevitably become "aspie rationalist." If you aren't familiar, an "aspie" refers to someone with Aspergers.

    Wordpress comments are awful for reasons that you probably already know. Linking to a reddit post is better. Maybe try to revive r/RationalistDiaspora, or even make your own sub, or something? 

    Even if Eliezer doesn't capitalize "dath ilan" it should be capitalized in the title, cuz it's a title.

    Writing is hard and criticizing is easy, so feel free to ignore me if you feel confident. You probably know what you want to do better than me.
    ```

    - u/WikiTextBot:
      ```
      ##Dunbar's number
      Dunbar's number is a suggested cognitive limit to the number of people with whom one can maintain stable social relationshipsâ€”relationships in which an individual knows who each person is and how each person relates to every other person. This number was first proposed in the 1990s by British anthropologist Robin Dunbar, who found a correlation between primate brain size and average social group size. By using the average human brain size and extrapolating from the results of primates, he proposed that humans can comfortably maintain only 150 stable relationships. Dunbar explained it informally as "the number of people you would not feel embarrassed about joining uninvited for a drink if you happened to bump into them in a bar".

      ***

      ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://www.reddit.com/message/compose?to=WikiTextBot&message=Excludeme&subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://www.reddit.com/r/rational/about/banned) ^| [^Information](https://reddit.com/r/WikiTextBot)   ^]
      ```

  - u/rineSample:
    ```
    You had a chance to call it The Foundation and you didn't? For shame!
    ```

    - u/Teal_Thanatos:
      ```
      I think the failure of that comes down to irrationality
      ```

  - u/DamenDome:
    ```
    I think your "about me" page on the blog could be presented better, if you want these posts to be taken by the rational community at large. Would perhaps be better to learn more about the authors and the information there about the authors isn't particulary rational (INTJ scores and "dislikes: Republicans" are a red flag to me). Intended to be helpful -- I had never been to your blog and this was my first impression.
    ```

- u/None:
  ```
  [removed]
  ```

  - u/throwaway47351:
    ```
    It's definitely appropriate to talk about this here, and a basic set of your views would be helpful to any other potential pmers. It's hard to debate views when one side doesn't give specifics. Here's a few of mine:

    Simply put, artificial intelligence isn't how we're going to preserve life. Something like CRISPR is more likely to get us to that stage, where we can cure telomere degradation, stop cancer so that the lack of telomere degredation doesn't kill us, and cure all the other billion things that contribute to aging. The ides of mind uploading is stupid on the face of it, as the uploaded mind wouldn't be you in the way that counts. If there can be two of you, then at least one isn't you in the sense that you are yourself.

    Second, you seem to have that common belief that any ethical frame that we imprint on a super-intelligent AI will either be insufficient, have unfortunate and unseen consequences or loopholes, or will be disregarded by the AI itself. I will not claim that we as a species are morally advanced enough to create anything resembling an airtight set of morals, but I will claim that this problem simply will not matter. The types of AI we can create in the next 20 years or so will all be specialized enough that, even if they gained a form of intelligence, they will not be able to commit any large evils even if they tried. The real problem with this is a generalized AI that can solve problems in unexpected ways, and that's far enough in the future that there is the possibility of us developing a better moral framework before that happens. You seem to know this, but you don't seem to even consider that as a species, we can make ethical progress. I'd prefer to wait on that possibility, rather than make any action that was depending on us not developing better morals.

    Honestly though, I'd really like it if you could explain some of your fears on this subject.
    ```

---

