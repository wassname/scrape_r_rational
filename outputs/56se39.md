## [D] Monday General Rationality Thread

### Post:

Welcome to the Monday thread on general rationality topics!  Do you really want to talk about something non-fictional, related to the real world?  Have you:

* Seen something interesting on /r/science?
* Found a new way to get your shit even-more together?
* Figured out how to become immortal?
* Constructed artificial general intelligence?
* Read a neat nonfiction book?
* Munchkined your way into total control of your D&D campaign?


### Comments:

- u/CarVac:
  ```
  In a discussion with a relative who's a practitioner of alternative medicine, I came to an interesting conclusion. I was trying to get him to explain the mechanism by which he claimed his variety of alternative medicine (Reiki) works, and he said that sometimes it's important not to understand, that it gets you more.

  But I then realized that for me, knowledge and understanding are how I connect with the world. Instead of simply trying to be at peace with traffic jams, I understand how they form from waves, and I can actively counteract that, and now I actually enjoy getting stuck in traffic.

  Viewing a person's mind as a *natural* neural network explains a lot of why people behave the way they do, and it really makes the idea of a soul completely unnecessary in my mind. When faced with an unfamiliar situation, people and artificial neural networks alike behave unpredictably. Emotions are like different nodes of a layer deep inside of the neural network that makes each of us who are are. A person's personality is firmly rooted in physical brain structures.

  Other people may have it easier finding meaning in the world through spirituality, but for me, a deep enough understanding of the physical mechanisms of the world gives me all the meaning I need.

  And it is through understanding the world that I can effect changes upon myself and my surroundings, whether that be fixing something, writing a program, or learning how to control my emotions better.
  ```

  - u/trekie140:
    ```
    Good on you for achieving that level of self-actualization, but I think it is important to understand that not everyone's mind is like yours. A while back I had [a long discussion here about how I couldn't stop following my spiritual beliefs despite how irrational they were.](https://www.reddit.com/r/rational/comments/4mda32/d_friday_offtopic_thread/d3umkmc) In the end, I remained a spiritualist because fighting against myself was psychologically unhealthy, and instead worked around my belief system to reduce my irrationality like falling for pseudoscience.
    ```

    - u/CarVac:
      ```
      In your case it sounds like you just have less-connected, isolated parts of your neural network that you can access...

      I wouldn't call them spirits, but there's no reason why I wouldn't believe that there might be entities you and you alone communicate with, separate from your usual self. Regardless of the fact that they're not available when you don't believe they're spirits, they are probably not spirits.

      Nothing unscientific about it to me. However, if your access to them *requires* your belief that they are spirits, I don't particularly mind. Especially if you benefit from them being around. Just like how I don't mind that my cousin's husband believes that his feelings reconfigure the water inside his clients' bodies to effect healing.
      ```

    - u/None:
      ```
      I've done a lot of work using the Internal Family Systems therapy model, which involves personifying subagents of myself, and I can see pretty easily how letting them exist and be personified can lead to them apparently leading their own independent existences. I also did a bit of dabbling into tulpas back in the day, and have some friends with Dissociative Identity Disorder, and have at least one friend who is completely convinced that they can hear the voice of Freya (and is otherwise quite sane). So I have a fully natural understanding of how things like this can happen.

      Which is to say: Have you read the four posts from meltingasphalt that explain Jaynes' bicameral mind theory? [Here's the fourth one](http://www.meltingasphalt.com/hallucinated-gods/); it links to the other three within the first paragraph, and you should probably read them in order.

      Which is to say: There is a meaningful sense in which these 'spirits' are indeed different entities from you. They're still all patterns in parts of your head, it's just that if they have their own independently-derived sense of personhood they can produce the kind of experiences that you are having.
      ```

      - u/trekie140:
        ```
        I understand that it's entirely possible that they're all in my head, but I'm serious when I say I can't talk to them if I think they're in my head. I didn't adopt spiritualism because I could talk to them, I spoke to them because I am a spiritualist. To conclude that they aren't spirits and never have been would risk invalidating my belief system, which is something I find abjectly horrifying.

        If you're a person who can live without a religion, that's great and I understand why you think it's good for other people to live without it, but I am a person who needs to believe in it or I will fall into existential depression. It's happened multiple times before and it was always one of the most miserable times of my life, so I've decided to accept the fact that I have faith even if it isn't rational.
        ```

        - u/CarVac:
          ```
          How can anyone say that making a conscious cost-benefit decision to believe in spirits is not rational?

          Seems perfectly rational to me.

          For most people, certainly there's much less of an excuse, but in your case it's perfectly understandable.
          ```

        - u/TennisMaster2:
          ```
          > ... I've decided to accept the fact that I have faith even if it isn't [epistemically] rational.

          However, it's perfectly instrumentally rational, since emotional and mental well-being are two of your goals.
          ```

  - u/None:
    ```
    It now bugs me that you and I have structurally different theories of mind but can't cash out the difference in empirical predictions.
    ```

  - u/Polycephal_Lee:
    ```
    I too do not find any reason to hypothesize a "soul". But I find that many in the rational community focus on the "physical" materialism and discount spirituality, and that's a little too narrow for me too.

    In my view, there is only one type of substance, and that is matter that feels. It's a sort of [neutral monism](http://isites.harvard.edu/fs/docs/icb.topic1298298.files/SPHI_Main13121213110.pdf). This view dissolves the hard problem of consciousness by noticing that there is no definable separation between Mind and Body to begin with. And after I convinced myself of neutral monism, that led directly to a choice between solipsism and [panpsychism](http://consc.net/papers/panpsychism.pdf). 

    I guess what I'm saying is that a deep understanding of the physical *is* spirituality to me. You don't need souls or magic, you just need a recognition that this giant computational universe feels. And our responsibility as powerful agents is to shape the universe so that the future feels even better.
    ```

    - u/CarVac:
      ```
      My argument really is not that the universe doesn't feel or that souls definitely don't exist. Rather, it's that I, personally, can come to an understanding without needing to invoke anything supernatural.

      My relative was wondering how I could find meaning and connect with the world by dissecting everything and trying to understand them, whereas it's precisely in doing that that I can achieve meaningful connection with the world.
      ```

- u/DaystarEld:
  ```
  As [previously mentioned](https://www.reddit.com/r/rational/comments/55o2ah/d_monday_general_rationality_thread/d8cethq), I'm designing an AGI risk board game, and will continue to document my progress here.

  1) Definitely going for the competitive format. The current plan is that each player will choose or be randomly distributed what kind of research team they are. Each will have different benefits and win conditions: For example, the Military researchers will start with much more funding, but its end game will only result in either Everyone Loses or You Win.  This acts as a disincentive for people to team up with them, opposed to the Humanist researchers, whose end game results can be either Everyone Loses or Everyone Wins.

  2) Players are going to have a set amount of actions represented by tokens available to them each turn, which they can divide up among Funding, Research and Development. To get more Action tokens, they would hire new scientists and researchers through a bidding system. Cards representing new staff will appear at the beginning of every round, and each player will have to bid on trying to secure the ones they want. Each researcher will have special abilities and benefits and synergies.

  3) The Risk of testing or activating your AGI won't be a dice roll anymore, and instead will be something akin to Blackjack, where you use the cards for the machine you've developed, which will have a % of Risk reduction associated with them, to try and lower the Risk to 0. I'm not quite sure yet how to best structure this part to have there be 3 outcomes: Success, Failure, and Partial Success, which grants you some benefits but doesn't win you the game.  My current idea is that overshooting the mark is Failure, and stopping early is Partial Success, whereas hitting the mark exactly is Success, but I have to do some playtesting to figure out exactly how it would work.

  I'm not quite sure how complex I want the game to be yet, in terms of additional activities like seeking research grants and sabotaging one-another's research. Going to try and nail down the core aspects of the gameplay before I start working in extra features like that.

  [Next post](https://www.reddit.com/r/rational/comments/5bmlsq/d_monday_general_rationality_thread/d9pxhja/)
  ```

  - u/LiteralHeadCannon:
    ```
    I'm assuming that with "everyone loses" and "everyone wins", you get some number of points for winning (and maybe some lower number of points for not losing) and the game would be played over many rounds?
    ```

    - u/DaystarEld:
      ```
      I'm not currently thinking that it would be played over multiple rounds, since the game so far wouldn't be particularly quick, and the end-game situation is someone kickstarting the singularity (or killing everyone, or becoming hegemon).
      ```

- u/trekie140:
  ```
  This weekend, I removed Cracked.com from my bookmarks. That may not sound like a big deal, but it's because I finally realized that the hilarious jokes and insightful commentary that I grew up on from that website are not coming back. I have kept following Cracked for months now without bothering to read a single article and have enjoyed fewer and fewer over the past few years. The only thing I'm still following is their podcast, and even then I still skip episodes.

  I know a lot of people like to hate on Cracked for changing from humorous to serious, [and some of it is justified](https://www.reddit.com/r/KotakuInAction/comments/2ufws5/what_i_saw_on_the_crackedcom_writer_boards_during/), but I really liked a lot of the serious articles. Sure, not all of them were good, but the ones that were still taught me things I wouldn't have learned anywhere else and gave me a different perspective on events while still managing to make me laugh.

  Reading that website brought as much joy to my life as Jon Stewart did, and some of those articles changed the way I see the world. It wasn't just a comedy site for me, it was a source of existential hope in myself and the world. David Wong, John Cheese, Robert Evans, and more comedians on the Internet changed my life just by being a part of it at a time when I needed them and now all that is over.

  Instead of whining about how things end, though, I want to find something new. If I can't find what I'm looking for with Cracked, I'll get it somewhere else. Where can I find someplace that doesn't just entertain me, but encourage me to be a better person? Where is someplace else where I can see an intelligent commentary that convinces me to have faith in myself and humanity when I'm tempted to be cynical? Is there still such a place?
  ```

  - u/rineSample:
    ```
    What Cracked used to excel at is introducing high level commentary and concepts to the masses in an approachable, extremely funny, down-to-earth style. XKCD and WaitButWhy may not be as funny, but they still sort of do this, and mostly avoid the culture war to boot. 

    This is almost definitely the most circlejerky thing I've said on reddit, but I think that the leaders of the grey tribe LW diaspora- where you are now- is what you're looking for, especially given people like Scott Alexander and Gwern.
    ```

    - u/trekie140:
      ```
      I follow XKCD and have read all of it, but a lot of it goes over my head and even when it doesn't it can still be difficult to relate to. It's still great, I just don't find it very "down to Earth". I go to it for weird intellectualism that pulls me out of my headspace, not for commentary on people and the world. LessWrong is in the same boat, though I visit it far less because, as an outsider, I find it less palatable. The closest thing I've found to what I'm looking for so far is vlogbrothers, which I've been a casual fan of for years.
      ```

      - u/rineSample:
        ```
        My bad, bro. Do you use [explainxckd](http://explainxkcd.com)?
        ```

        - u/trekie140:
          ```
          Don't feel bad, it's still a good recommendation. XKCD is just a different part of my life than Cracked was, and I still learned a lot from LessWrong that had a big impact on me. I do use that site for explanations, but that doesn't always made the the comic more relatable, just comprehensible.
          ```

  - u/xamueljones:
    ```
    How about [Wait But Why](http://waitbutwhy.com/)?
    ```

- u/LieGroupE8:
  ```
  A lot of people here seem to believe that total immortality (at least until the heat death of the universe) is obviously a moral good, all other things being equal. Well...

  [Puts on Devil's Advocate hat]

  Here is a counterargument that I haven't seen discussed before.

  **A moral argument against immortality**

  1. There are a limited number of available resources in the universe, and hence using resources to sustain one particular person prevents other potential persons from existing.

  2. At some point in any person's life, more good would be brought into the universe by creating a completely new person than the evil (if it's an evil at all at that point) of the original person ceasing to exist.

  3. Therefore, every person has a moral obligation to die at some point in the future, freeing up resources to make new people.

  Premise 1 should be uncontroversial - even if the universe is infinite, the amount of matter and free energy we could ever hope to encounter is bounded and finite due to the expansion of the universe and the lightspeed limit.

  Premise 2 will be the most controversial, I think, and I will discuss it more below.

  The inference from 1 & 2 to conclusion 3 could also be attacked, as it presupposes some sort of utilitarianism for weighing the net good of actions without reference to means. But I suspect that similar inferences could be formulated in terms more acceptable to deontologists or virtue ethicists. In my discussion I will mostly assume that the inference from 1 & 2 to 3 is defensible.

  **Answering objections to Premise 2**

  One could simply assert that premise 2 is false, on the grounds that there is no difference in the amount of good between one unit of person-time (call it 1 prtm for short) for a long existing person and for a new person. But it seems plausible to me that goodness is path-dependent, so that the utility of 1 prtm depends on the totality of a person's prior experiences and memories. People are finite, so their memories are finite, and at some point they will not be able to form new memories without replacing old ones. This could create a point of diminishing returns on new experiences, especially if memory erasure counts as a negative utility. There would also be diminishing returns if mere novelty has any weight at all in our utility function - over time people will have fewer and fewer completely novel experiences (to them).

  It could be objected that memories do not need to be erased: a person's memory capacity could be expanded over time so that forgetting is unnecessary. But this objection fails, because a larger memory uses more resources, so the opportunity cost of not creating new people grows right along with the expanded memory and cancels out the positive effects.

  It could be objected that a utility function should have no dependence on prior memories. Then you would have to accept that a person with extremely limited memory formation ability, such as someone with anterograde amnesia, has no difference in quality of life than a person who can form memories normally.

  You could object that memory erasure is not bad or that novelty should not be a factor in the utility function. Both of these objections are implausible. If the erasure of all memories is like death, which is assumed to be bad, then it seems reasonable to consider the erasure of one memory as a partial death which is just a little bit bad. And novelty, of course, is the spice of life.

  **Is mere discontinuity really all that bad?**

  Assuming that there is no aging, so that full quality of life is present right up until the end, death becomes a mere discontinuity in experience, like going under anesthesia and waking up as a completely different person.

  We must also consider that the badness of a death depends not only on the badness of a particular person's discontinuation, but on the effects of this on other people. But in the same vein as before, it could be argued that at some point it is more good to find new friends than to eternally interact with the same people over and over (hell is other people!). Furthermore, strong contrast of emotions could be necessary for overall well-being, and leaving an old tired friend for new ones would certainly create such a contrast.

  **Intuition pumps**

  Pump #1: The above problem is highly related to the problem of how many people should ever exist. Supposing the universe has the resources to support 10^100 prtm through the entire future, there is the question of whether we should divide this into 10^98 different people with 10^2 prtm each, or 10^50 people with 10^50 prtm each, or 10^20 people with 10^80 prtm each, etc. It is not clear that the bias toward a much higher per-person power is morally optimal.

  Pump #2: As entropy increases, the same amount of matter will be able to sustain fewer and fewer people. Thus, some people will inevitably have to die so that others can continue existing.

  Pump #3: Suppose that there is strong disutility to discontinuities, so that there should be no death as normally conceived. Instead, to create new people, existing people enter an accelerated program of mental change, so that over a period of time they rapidly become a fundamentally different person, without loss of the continuity of consciousness. Does this make the above arguments more acceptable?
  ```

  - u/AugSphere:
    ```
    > At some point in any person's life, more good would be brought into the universe by creating a completely new person than the evil (if it's an evil at all at that point) of the original person ceasing to exist.

    Does the whole argument hinge on assigning moral value to non-existent agents? I prefer to think of creating new agents only in terms of the impact on already existing ones, and incentivising agents to suicide so that someone else may "get their turn" seems pretty evil to me.
    ```

  - u/DaystarEld:
    ```
    Isn't this argument only applicable to a universe in which we don't inhabit yet, though?  I appreciate it as an argument against a population growth path that reaches toward infinity on a finite resource environment, but since we're so far from that situation, I don't think it really applies to people wanting everyone to live forever today.

    If we ever get to the point where it's a serious problem that has to be addressed, there are a number of things just off the top of my head that might help solve this problem. Like why not just cycle people through longer and longer hibernation to allow new births to occur without straining resources?

    +1 for the devil's advocacy though, it's definitely a point worth addressing.
    ```

  - u/suyjuris:
    ```
    You cannot argue using negative utility of partial memory erasure if your proposed alternative is death, which (as you state yourself) is equivalent to total memory erasure. Example: 

    * Alternative 1: Person A lives for 2 prtm without memory erasure (me), then lives for 1 prtm with 1 prtm of me, then lives for 1 prtm with 1 prtm of me.
    * Alternative 2: Person A lives for 2 prtm without me, then dies (2 prtm of me). It is replaced by person B, living 2 prtm without me. 

    In both cases 2 prtm of me have happened.

    I would even argue that the selective memory erasure to accommodate new experiences has significantly higher utility than the total memory erasure on death (with both being, of course, negative). Would rather lose the memories of the first half of your life, or have a 50% chance of dying on the spot? For me, at least, this is not a difficult decision.
    ```

    - u/LieGroupE8:
      ```
      First of all, total prtm is not the same as utility. Utility is a function over sequences of prtm. If it is perfectly linear, then there is no difference between slow memory erasure vs immediate total replacement with a new person - in which case the argument about novelty breaks the tie in favor of creating a new person. If it is convex, then the original argument succeeds, and it is better to create a new person. If, however, the utility function is concave, then your argument works and slow memory erasure is preferable.

      But there is an even deeper philosophical issue here - over a long period of time, isn't selective memory erasure equivalent to slow death? After all, if a person is a bundle of memories and thoughts, then drift over long periods of time means that eventually you will become an entirely new person - see intuition pump 3. A possible corollary is that immortality as commonly desired is impossible - you either stagnate or become someone else, inevitably. In this sense, mere continuation of physical life is a separate, easier problem than that of "true immortality."
      ```

  - u/None:
    ```
    My objection to Premise 2 is that goodness without an agent is undefined.  I also don't see how you solve the "ocean warming itself around a candleflame" problem of trying to balance the goods of uncountably many counterfactual people and finitely many actual people in whom you create and whom you destroy.
    ```

  - u/artifex0:
    ```
    I think there may be a confusion in that argument between the value of potential ends and the value of potential means to ends.

    If something is a means to an end, it makes sense to promote it even if it doesn't yet exist- it has value even when it's just a possibility. I don't think the same can be said of things that are valued as ends unto themselves- like people.

    Compare the morality of killing a child with that of convincing a couple not to have a child.  Although the end result of both is the non-existence of a child, only one of the two is inherently wrong, since it would make no sense to promote the interests of a potential child for that child's own sake.

    Of course, that's complicated by the fact that human life isn't only an end unto itself, but also a means for promoting other ends.  For example, if everyone in the world decided not to have children, that would be a problem, since we value humanity as a unit, and individual humans are, in addition to being ends unto themselves, also necessary means for the existence of that unit.

    Still, exchanging an existent person for a potential person is never a morally justifiable trade- regardless of whether that new person will live a better life.  Although that new person will have just as much right to live as the previous person, when it comes to ends as opposed to means to ends, I don't think "will have value" is ever a rational reason to act.

    Otherwise, you could reduce the immorality of murder by having a child, and any time you prevented a person from being born, you'd be culpable for the mass murder of all of their potential descendants.
    ```

  - u/thecommexokid:
    ```
    +1. No thoughts, still ruminating. But this is the strongest argument against universal immortality I have heard to date.
    ```

  - u/Escapement:
    ```
    This seems to be highly related to philosophy problems like [The Repugnant Conclusion](http://plato.stanford.edu/entries/repugnant-conclusion/). One of the primary philosophical challenges to utilitarianism and it's variants is the lack of a sensible way to aggregate and compare utility of different potential populations. I don't pretend have a wholly satisfactory solution to this sort of question.
    ```

  - u/vakusdrake:
    ```
    It seems like followed to its logical conclusion this would be the exact sort of thing that would quickly wipe out humanity if put as a GAI's utility function. 

    This ought to incentivize the creation of a singleton wiping out humanity, _after all people are made of resources that could be used to simulate lots of perfectly happy ems_, simulated people can live their lives _absurdly fast_, so these kinds of problems aren't just going to come up after massive amounts of time either.

    Of course since your model places some small penalty on death, by far the better solution would be to just create a singleton that can just simulate all that pleasure for itself and doesn't have any diminishing returns; if you still penalize memory erasable enough for that to come up then the AI will just replace itself with a predecessor every so often. So obviously the much simpler solution is just effectively a paperclipper. Of course if you place utility on distinctively human forms of pleasure the AI might make itself a bit more human, so maybe it'll even feel bad, which i'm _sure_ will console you.

    Of course I think the big reason to reject you premises as **AugSphere** pointed out, is that this type of valuing _potential_ people is extremely fishy. It also requires you _place value on people based on their happiness_, that's kind of unavoidable.
    ```

  - u/CCC_037:
    ```
    Your argument, in short, is that memory erasure is bad and should therefore be avoided; and that it makes sense to allow one person to die so that another might step in in his place.

    But death erases (or at least makes inaccessible to the living) *all* memories. If memory erasure is bad, then death is surely worse? And one should not accept a greater evil in the place of a lesser.

    On these grounds, I believe your point (2) fails.
    ```

  - u/TennisMaster2:
    ```
    > Assuming that there is no aging, so that full quality of life is present right up until the end, death becomes a mere discontinuity in experience, like going under anesthesia and waking up as a completely different person.

    How can death be a discontinuity in experience if it's the end of experience?  This point presupposes reincarnation, in my understanding, as it's the only case where one may wake up after death.
    ```

- u/eniteris:
  ```
  Should the placebo effect have been kept secret so that it could be used more efficaciously among the general population?

  I mean, it'd be much more difficult to keep secret than the atomic bomb, but imagine the use we could get out of it.

  (Currently the best argument I've heard against prescribing placebos is that it lowers the trust of doctors)
  ```

  - u/Anderkent:
    ```
    Placebo works even if you know it's placebo though. Why keep it secret?
    ```

    - u/CarVac:
      ```
      Yup, when I have a headache I'll chug a big glass of water, and think to myself "thank goodness for the placebo effect" when enjoying the resulting relief, free of painkillers.
      ```

      - u/electrace:
        ```
        Dehydration can cause headaches....
        ```

    - u/eniteris:
      ```
      Placebo works even better when you don't know it's a placebo.

      Also, by keeping the placebo effect a secret, you could give doctors free reign to prescribe placebos for treatments.

      The placebo effect is crazy, man.
      ```

  - u/ulyssessword:
    ```
    > (Currently the best argument I've heard against prescribing placebos is that it lowers the trust of doctors)

    [This article](http://www.dcscience.net/2015/12/11/placebo-effects-are-weak-regression-to-the-mean-is-the-main-reason-ineffective-treatments-appear-to-work/) ([Found via SSC](http://slatestarcodex.com/2015/12/18/links-1215-winter-wond-url-and/)), brings up another reason.  

    It attributes much of the purported power of placebos to regression to the mean, as opposed to any effect that the placebos actually have on the person.  Including the effect of regression to the mean in "placebo effects" is fine if you're comparing them to a drug (which has the same placebo effects and the same regression to the mean), but is bad when comparing placebos to simply waiting.
    ```

---

